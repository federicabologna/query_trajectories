{"clarifying_information": [{"clarifying_question1": "Are you most interested in the technical implementation details of how 'dark knowledge' is transferred from teacher to student in knowledge distillation (such as temperature scaling, soft label generation, and loss function design)?", "clarifying_answer1": {"clarifying_answer": "Yes, I am most interested in the technical implementation details of how 'dark knowledge' is transferred from teacher to student in knowledge distillation, including temperature scaling, soft label generation, and loss function design."}}, {"clarifying_question2": "Do you want an in-depth explanation of what constitutes 'dark knowledge' itself\u2014specifically, the types of information encoded in the teacher's probability distributions beyond one-hot labels?", "clarifying_answer2": {"clarifying_answer": "Yes, I would like an in-depth explanation of what constitutes 'dark knowledge' itself, specifically the types of information encoded in the teacher's probability distributions beyond one-hot labels."}}, {"clarifying_question3": "Are you seeking examples or a review of the various practical applications and adaptations of dark knowledge transfer beyond standard image classification\u2014such as in object detection, medical imaging, or use with missing modalities?", "clarifying_answer3": {"clarifying_answer": "Yes, I am interested in examples or a review of how dark knowledge transfer has been adapted and applied in diverse practical scenarios beyond standard image classification, such as in object detection, medical imaging, handling missing modalities, or other specialized domains."}}], "better_query": "What are the technical implementation details of transferring 'dark knowledge' in knowledge distillation from teacher to student networks, including temperature scaling, soft label generation, and specific loss function formulations?", "better_answer": {"sections": [{"title": "Introduction/Background to Knowledge Distillation and Dark Knowledge", "tldr": "Knowledge distillation is a technique that transfers knowledge from a complex teacher model to a simpler student model by using the teacher's output probability distributions. The key insight is the transfer of \"dark knowledge\" - the information embedded in the teacher's confidence scores for incorrect classes - which helps the student model achieve better generalization. (9 sources)", "text": "\nKnowledge distillation (KD) is a technique in deep learning where a smaller, more efficient neural network (the \"student\") is trained to mimic the behavior of a larger, pre-trained model (the \"teacher\") <Paper corpusId=\"265502078\" paperTitle=\"(Witt et al., 2024)\" isShortName></Paper>. Rather than directly transferring model parameters, KD aligns the output distributions of both models, allowing the student to inherit the teacher's generalization capabilities <Paper corpusId=\"269921267\" paperTitle=\"(Mei et al., 2024)\" isShortName></Paper>. This approach was first introduced by Hinton et al., who demonstrated that ensembles of models could be compressed into a single, more deployable model while maintaining performance <Paper corpusId=\"227013462\" paperTitle=\"(Nayak et al., 2020)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nThe core concept in knowledge distillation is the transfer of \"dark knowledge\" - the latent information hidden in the confidences assigned by the teacher to incorrect categories <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper>. Unlike traditional training that uses one-hot encoded labels, KD leverages the teacher's soft probability distributions across all classes <Paper corpusId=\"211096976\" paperTitle=\"(Mobahi et al., 2020)\" isShortName></Paper>. These soft labels capture nuances in decision boundaries and provide richer guidance to the student model than hard ground-truth labels alone <Paper corpusId=\"265502078\" paperTitle=\"(Witt et al., 2024)\" isShortName></Paper>.\n\nThe typical implementation of knowledge distillation follows a teacher-student framework with distinct phases <Paper corpusId=\"269921267\" paperTitle=\"(Mei et al., 2024)\" isShortName></Paper>:\n\n1. Pre-train a complex teacher model that performs and generalizes well on the target dataset\n2. Generate predictions (soft targets) from the teacher model on available data\n3. Train the smaller student model using both the original labels and the teacher's predictions\n\nThe student model benefits from this additional \"dark knowledge\" and often performs better than if it was trained solely on the actual labels <Paper corpusId=\"211096976\" paperTitle=\"(Mobahi et al., 2020)\" isShortName></Paper>. This is typically achieved by minimizing a loss function that incorporates both the traditional cross-entropy loss with ground truth labels and a distillation loss that measures the difference between the student's and teacher's predicted probability distributions <Paper corpusId=\"234336288\" paperTitle=\"(Jaiswal et al., 2021)\" isShortName></Paper>. Many implementations use the Kullback-Leibler (KL) divergence to regularize the student model's output probabilities, aligning them with those of the teacher model <Paper corpusId=\"271244914\" paperTitle=\"(Giakoumoglou et al., 2024)\" isShortName></Paper>.\n\nThis approach has proven particularly effective for enhancing the performance of lightweight student networks by leveraging the comprehensive knowledge embedded in larger teacher networks <Paper corpusId=\"267657497\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>, making it a valuable technique for deploying efficient models in resource-constrained environments.", "citations": [{"id": "(Witt et al., 2024)", "paper": {"corpus_id": 265502078, "title": "Decentralized and Incentivized Federated Learning: A Blockchain-Enabled Framework Utilising Compressed Soft-Labels and Peer Consistency", "year": 2024, "venue": "IEEE Transactions on Services Computing", "authors": [{"name": "Leon Witt", "authorId": "2115471566"}, {"name": "Usama Zafar", "authorId": "2067050909"}, {"name": "KuoYeh Shen", "authorId": "2115288561"}, {"name": "Felix Sattler", "authorId": "46223357"}, {"name": "Dan Li", "authorId": "2150385385"}, {"name": "Songtao Wang", "authorId": "2268713838"}, {"name": "Wojciech Samek", "authorId": "2242938198"}], "n_citations": 4}, "snippets": ["Knowledge Distillation (KD), depicted in Fig. 1, is a technique in deep learning where a smaller NN model (often called the \"student\") is trained to mimic the behavior of a larger, pre-trained model (referred to as the \"teacher\") [27]. This is accomplished not by transferring the model parameters directly, but rather by aligning the output distributions of both models. Traditional training methods involve training a model directly on ground-truth labels, using a cross-entropy loss that measures the discrepancy between the model's predictions and these true labels. In contrast, KD employs a divergence-based loss, such as the Kullback-Leibler (KL) divergence, to measure the difference between the student's predicted probabilities and those of the teacher model. This divergence provides insights into how closely the student is able to mimic the behavior of its teacher. A distinct feature of KD is the use of \"softened\" labels. In traditional classification tasks, hard labels are used, which unequivocally classify a data point into one category. However, the teacher model in KD provides \"soft\" labels in the form of probabilities, indicating the confidence levels across various categories. These probabilities can be further softened using a temperature parameter T to yield a smoother distribution, capturing the nuances of decision boundaries and offering richer guidance to the student model. This process allows the student to inherit not just the overt knowledge from the ground-truth labels but also the implicit, or \"dark\", knowledge embedded in the teacher model's predictions."], "score": 0.7255859375}, {"id": "(Mei et al., 2024)", "paper": {"corpus_id": 269921267, "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks", "year": 2024, "venue": "2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)", "authors": [{"name": "Taiyuan Mei", "authorId": "2298916636"}, {"name": "Yun Zi", "authorId": "2298918720"}, {"name": "X. Cheng", "authorId": "2222987403"}, {"name": "Zijun Gao", "authorId": "2297725659"}, {"name": "Qi Wang", "authorId": "2297735971"}, {"name": "Haowei Yang", "authorId": "2302372513"}], "n_citations": 20}, "snippets": ["The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution."], "score": 0.8330078125}, {"id": "(Nayak et al., 2020)", "paper": {"corpus_id": 227013462, "title": "Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation", "year": 2020, "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "authors": [{"name": "Gaurav Kumar Nayak", "authorId": "143747407"}, {"name": "Konda Reddy Mopuri", "authorId": "2217000"}, {"name": "Anirban Chakraborty", "authorId": "1429640900"}], "n_citations": 19}, "snippets": ["The core idea of Knowledge Distillation, as discussed in the seminal paper by Hinton et al. [7], is to transfer the (input to output) learned mapping function from Teacher to Student via sharing the \"dark knowledge\" extracted by the Teacher on the training images. This typically is achieved via matching the soft targets (or soft labels, i.e., output of softmax layer) predicted by the Student to that of the Teacher for the same inputs. This is the distillation mechanism that enables transfer of the better generalization capability (i.e., the \"knowledge\") of the Teacher to the Student."], "score": 0.73193359375}, {"id": "(Hinton et al., 2015)", "paper": {"corpus_id": 7200347, "title": "Distilling the Knowledge in a Neural Network", "year": 2015, "venue": "arXiv.org", "authors": [{"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "J. Dean", "authorId": "49959210"}], "n_citations": 19742}, "snippets": ["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."], "score": 0.0}, {"id": "(Nayak et al., 2019)", "paper": {"corpus_id": 159041346, "title": "Zero-Shot Knowledge Distillation in Deep Networks", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Gaurav Kumar Nayak", "authorId": "143747407"}, {"name": "Konda Reddy Mopuri", "authorId": "2217000"}, {"name": "Vaisakh Shaj", "authorId": "7155274"}, {"name": "R. Venkatesh Babu", "authorId": "144682140"}, {"name": "Anirban Chakraborty", "authorId": "1429640900"}], "n_citations": 245}, "snippets": ["The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence."], "score": 0.82470703125}, {"id": "(Mobahi et al., 2020)", "paper": {"corpus_id": 211096976, "title": "Self-Distillation Amplifies Regularization in Hilbert Space", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "H. Mobahi", "authorId": "3232655"}, {"name": "Mehrdad Farajtabar", "authorId": "1682124"}, {"name": "P. Bartlett", "authorId": "1745169"}], "n_citations": 235}, "snippets": ["The original formulation achieves this by training the student model using the output probability distribution of the teacher model in addition to original labels. The student model benefits from this \"dark knowledge\" (extra information in soft predictions) and often performs better than if it was trained on the actual labels."], "score": 0.72802734375}, {"id": "(Jaiswal et al., 2021)", "paper": {"corpus_id": 234336288, "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification", "year": 2021, "venue": "International Journal of Advanced Computer Science and Applications", "authors": [{"name": "Bhavesh Jaiswal", "authorId": "2139643077"}, {"name": "Nagendra Gajjar", "authorId": "32136431"}], "n_citations": 0}, "snippets": ["Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model", ".In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process."], "score": 0.7685546875}, {"id": "(Giakoumoglou et al., 2024)", "paper": {"corpus_id": 271244914, "title": "Relational Representation Distillation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Nikolaos Giakoumoglou", "authorId": "2196360101"}, {"name": "Tania Stathaki", "authorId": "2292259667"}], "n_citations": 0}, "snippets": ["The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs."], "score": 0.82177734375}, {"id": "(Kim et al., 2023)", "paper": {"corpus_id": 267657497, "title": "Maximizing discrimination capability of knowledge distillation with energy function", "year": 2023, "venue": "Knowledge-Based Systems", "authors": [{"name": "Seonghak Kim", "authorId": "2268350036"}, {"name": "Gyeongdo Ham", "authorId": "2156910329"}, {"name": "Suin Lee", "authorId": "2268370058"}, {"name": "Donggon Jang", "authorId": "2268310103"}, {"name": "Daeshik Kim", "authorId": "2145154407"}], "n_citations": 4}, "snippets": ["Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks."], "score": 0.77783203125}], "table": null}, {"title": "Temperature Scaling Implementation", "tldr": "Temperature scaling is a core technique in knowledge distillation that controls the \"softness\" of probability distributions by dividing logits by a temperature parameter T before applying softmax. Higher temperature values smooth out the probability distributions, revealing more of the dark knowledge in teacher outputs and helping student networks learn subtle class relationships. (13 sources)", "text": "\nKnowledge distillation relies on temperature scaling to extract the rich \"dark knowledge\" contained in a teacher model's output distributions. The implementation begins with applying a modified softmax function to the logits (pre-softmax activations) of both teacher and student networks <Paper corpusId=\"198179767\" paperTitle=\"(Zhao et al., 2019)\" isShortName></Paper> <Paper corpusId=\"219687782\" paperTitle=\"(Trofimov et al., 2020)\" isShortName></Paper>:\n\n```\np_i(T) = exp(z_i/T) / \u03a3_j exp(z_j/T)\n```\n\nwhere z_i represents the logit for class i, and T is the temperature parameter <Paper corpusId=\"265444951\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269317596\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\nThe temperature parameter T is crucial for controlling the \"softness\" of the probability distribution. When T=1, the equation reduces to the standard softmax function, producing sharper distributions with high confidence for the predicted class. When T>1, which is typical in knowledge distillation, the distribution becomes smoother, emphasizing the relative differences between class probabilities <Paper corpusId=\"214623014\" paperTitle=\"(Algan et al., 2020)\" isShortName></Paper> <Paper corpusId=\"252596141\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\nThis smoothing effect is essential because it reveals the \"dark knowledge\" - the information about how the teacher model relates different classes to each other. For instance, if an image contains features that slightly resemble multiple classes, these subtle similarities become more apparent in the softened distribution <Paper corpusId=\"234336288\" paperTitle=\"(Jaiswal et al., 2021)\" isShortName></Paper> <Paper corpusId=\"236984375\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>.\n\nThe implementation requires careful consideration of the temperature value. If T is too small, the distribution remains peaky and similar to one-hot labels, losing the rich class similarity information. If T is too large, the distribution becomes too uniform, potentially over-emphasizing irrelevant classes <Paper corpusId=\"235421655\" paperTitle=\"(Lee et al., 2021)\" isShortName></Paper> <Paper corpusId=\"260447668\" paperTitle=\"(Cui et al., 2022)\" isShortName></Paper>.\n\nRecent research has explored adaptive temperature approaches that assign different temperatures to different samples based on their characteristics. For example, Li et al. proposed an entropy-based method that learns node-specific temperatures by considering the teacher's confidence for each node <Paper corpusId=\"274436184\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nDuring implementation, both teacher and student networks must use the same temperature value when generating soft targets and predictions, respectively. This ensures the student learns to mimic the teacher's softened output distribution accurately <Paper corpusId=\"253655030\" paperTitle=\"(Borza et al., 2022)\" isShortName></Paper> <Paper corpusId=\"265384964\" paperTitle=\"(Xie et al., 2023)\" isShortName></Paper>.\n\nThe temperature parameter also affects gradient scaling. To compensate for the smaller gradients produced by higher temperatures, a factor of T\u00b2 is often incorporated into the distillation loss term <Paper corpusId=\"219687782\" paperTitle=\"(Trofimov et al., 2020)\" isShortName></Paper>. This scaling ensures that the gradients from both the soft targets and hard labels are of similar magnitude, facilitating effective optimization.", "citations": [{"id": "(Zhao et al., 2019)", "paper": {"corpus_id": 198179767, "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching", "year": 2019, "venue": "IEEE Transactions on Cybernetics", "authors": [{"name": "Haoran Zhao", "authorId": "50981688"}, {"name": "Xin Sun", "authorId": "144326521"}, {"name": "Junyu Dong", "authorId": "1964397"}, {"name": "Changrui Chen", "authorId": "10944885"}, {"name": "Zihe Dong", "authorId": "2087106420"}], "n_citations": 59}, "snippets": ["Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process.\n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function:"], "score": 0.70361328125}, {"id": "(Trofimov et al., 2020)", "paper": {"corpus_id": 219687782, "title": "Multi-Fidelity Neural Architecture Search With Knowledge Distillation", "year": 2020, "venue": "IEEE Access", "authors": [{"name": "I. Trofimov", "authorId": "144035647"}, {"name": "Nikita Klyuchnikov", "authorId": "143888131"}, {"name": "Mikhail Salnikov", "authorId": "49614468"}, {"name": "Alexander N. Filippov", "authorId": "8792436"}, {"name": "Evgeny Burnaev", "authorId": "51139941"}], "n_citations": 15}, "snippets": ["The knowledge distillation (KD) assumes two models: a teacher and a student. The teacher is typically a large and accurate network or an ensemble. The student is trained to fit the softmax outputs of the teacher together with ground truth labels. The idea is that outputs of the teacher capture not only the information provided by ground truth labels but also the probabilities of other classes -\"dark knowledge\". The knowledge distillation can be summarized as follows. \n\nLet z i be logits (pre-softmax activations) and q i -probabilities of classes as predicted by a neural network. Knowledge distillation smooths z i with the temperature \u03c4 \n\nNeural networks often do very confident predictions (close to 0 or 1) and smoothing helps to provide for student more information during training [11]. The KD loss is a linear combination of the logistic loss and cross-entropy between predictions of the teacher and the student \n\nwhere z T i , z S i are logits of the teacher and the student, H(p, q) = \u2212p log(q) is the cross-entropy function. The factor \u03c4 2 is used for scaling gradients of both parts of the loss function to be the same order."], "score": 0.7265625}, {"id": "(Su et al., 2023)", "paper": {"corpus_id": 265444951, "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation", "year": 2023, "venue": "", "authors": [{"name": "Chi-Ping Su", "authorId": "2268314644"}, {"name": "Ching-Hsun Tseng", "authorId": "1569686364"}, {"name": "Bin Pu", "authorId": "2338265886"}, {"name": "Lei Zhao", "authorId": "2338506240"}, {"name": "Zhuangzhuang Chen", "authorId": "2328588941"}, {"name": "Shin-Jye Lee", "authorId": "2116351339"}], "n_citations": 2}, "snippets": ["The goal of KD [8] is to transfer the dark knowledge encapsulated in the soft probability output of the teacher model to the student model. In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by \n\nwhere p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher. The core idea of KD lies in minimizing the KL divergence loss function to align the soft logits of the teacher and student. The loss of KD L KD is defined as \n\nwhere p T and p S are the output logits of the teacher T and the student S, respectively."], "score": 0.8193359375}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 269317596, "title": "Shared Knowledge Distillation Network for Object Detection", "year": 2024, "venue": "Electronics", "authors": [{"name": "Zhen Guo", "authorId": "2158003499"}, {"name": "Pengzhou Zhang", "authorId": "2297246012"}, {"name": "Peng Liang", "authorId": "2297096848"}], "n_citations": 3}, "snippets": ["Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2). \n\nwhere T represents temperature, which can adjust the softening of the output probability. z i z j are the logits input for softmax, p i represents the output probability of each category in the teacher network, and q i represents the output probability of each category in the student network. Cross-entropy between the distilled teacher and student models calculates the soft loss. The hard loss is calculated based on the predicted values of the student model and the true values. \n\nThe ground truth label (also known as the hard target label) for the i-th sample is represented by y i . The teacher and student models' predicted probability are represented by p i and q i , respectively. The total loss function is calculated as below. \n\nThe loss function used in Knowledge Distillation involves two types of losses: soft target loss L soft and hard target loss L hard . The former guides the student to replicate the teacher's probability distribution, whereas the latter reflects the guidance from the actual ground truth labels. The parameter \u03b1 balances the effect of these two losses. During the Knowledge Distillation process, the student receives both the challenging and soft target knowledge. The loss function can be written as follows: \n\nwhere the loss of the cross-entropy is represented by L CE . The softmax function is represented by \u03c3. y represents the ground truth label. The output logits of the student and teacher networks are denoted by z S and z T . The balancing hyperparameter is represented by \u03b1."], "score": 0.75048828125}, {"id": "(Algan et al., 2020)", "paper": {"corpus_id": 214623014, "title": "Label Noise Types and Their Effects on Deep Learning", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "G. Algan", "authorId": "1454230053"}, {"name": "I. Ulusoy", "authorId": "152986771"}], "n_citations": 53}, "snippets": ["In the original work, the authors used distillation to transfer knowledge from the big teacher network to a much smaller student network without decreasing the performance. The idea is mainly motivated by learning from soft labels where the similarity of each instance to each class is emphasized by temperature hyperparameter.\n\nClass probabilities on softmax output, beyond the true class probability, are usually very low. But, compared with each other, some classes may have a much higher probability than others, and this carries important information about that data instance, which is also called as dark knowledge. By making probability distribution smoother, this relation is emphasized, as shown in Equation 2.\n\nInstead of being trained on hard labels, the student network is trained on the weighted sum of hard labels and soft labels produced by the teacher network. So, the loss function is defined as follows,\n\nwhere q i represents the soft labels produced by the teacher network using temperature T and y i represents the given label."], "score": 0.73095703125}, {"id": "(Nguyen et al., 2022)", "paper": {"corpus_id": 252596141, "title": "Label driven Knowledge Distillation for Federated Learning with non-IID Data", "year": 2022, "venue": "", "authors": [{"name": "Minh-Duong Nguyen", "authorId": "2162621330"}, {"name": "Viet Quoc Pham", "authorId": "145436642"}, {"name": "D. Hoang", "authorId": "2233724"}, {"name": "Long Tran-Thanh", "authorId": "1389575160"}, {"name": "Diep N. Nguyen", "authorId": "30479205"}, {"name": "W. Hwang", "authorId": "2054135428"}], "n_citations": 2}, "snippets": ["Knowledge Distillation (KD) Hinton et al. (2015) employs a technique to transfer the learned knowledge from a pre-trained teacher model to another model with less or similar complexity (student model). The model transfer process is implemented in two steps. In the first step, a surrogate output probability function, namely a temperature-softmax function, is utilized. By adding a temperature scaling variable T, the conventional softmax function then becomes: \n\nwhere zj is the output set corresponding to class j of the given DNN with batch of input data X. \n\nThe subscript l denotes the index of softmax output which corresponds to the prediction on class l of the DNN. The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T, the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input zj is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective. \n\nIn the second step, to help implement the transfer of knowledge from teacher to student, the authors in Hinton et al. (2015) presented a new loss function, called distillation loss function (also known as soft-loss function). This loss function comprises two terms: the intrinsic entropy function of the teacher and the cross-entropy function between the teacher and the student's outputs, which can be expressed as follows: \n\nThe purpose of this function is to compare the output distribution between teacher p(X) and student model q(X). In terms of information theory, this measurement shows the under-performance of the distribution set created by student, when the output distribution set of the teacher is taken as the sample distribution set, with precision of 100%. By minimizing this function, we reduce the functional difference between the two deep networks. As a result, the student model tends to make it's output distribution become more identical to the teacher model's behavior."], "score": 0.791015625}, {"id": "(Jaiswal et al., 2021)", "paper": {"corpus_id": 234336288, "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification", "year": 2021, "venue": "International Journal of Advanced Computer Science and Applications", "authors": [{"name": "Bhavesh Jaiswal", "authorId": "2139643077"}, {"name": "Nagendra Gajjar", "authorId": "32136431"}], "n_citations": 0}, "snippets": ["Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model", ".In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process."], "score": 0.7685546875}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 236984375, "title": "Combine-Net: An Improved Filter Pruning Algorithm", "year": 2021, "venue": "Inf.", "authors": [{"name": "Jinghan Wang", "authorId": "2109643902"}, {"name": "Guangyue Li", "authorId": "2151302904"}, {"name": "Wenzhao Zhang", "authorId": "2107940856"}], "n_citations": 3}, "snippets": ["Knowledge distillation (Figure 3) is put forward by Hinton et al. (Hinton et al., 2015). It is a widely used knowledge transfer technology in the deep learning field. First, a well-trained, robust, high-precision teacher network is needed. Its output is softened with temperature T to provide more information entropy, which extracts hidden knowledge behind its output layer. Then, a relatively small student network is trained to imitate the teacher network's probability output distribution, obtaining a better output result. The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics", "To improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become", "where Q T s , Q T t are the softmax probability distribution of the student network and teacher network after softening according to temperature T."], "score": 0.765625}, {"id": "(Lee et al., 2021)", "paper": {"corpus_id": 235421655, "title": "Energy-efficient Knowledge Distillation for Spiking Neural Networks", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Dongjin Lee", "authorId": "2109519891"}, {"name": "Seongsik Park", "authorId": "120425550"}, {"name": "Jongwan Kim", "authorId": "2157129258"}, {"name": "Wuhyeong Doh", "authorId": "2111845679"}, {"name": "Sungroh Yoon", "authorId": "2999019"}], "n_citations": 12}, "snippets": ["[11] proposed the concept of soft label which is distilled from the teacher model to the student model. Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized."], "score": 0.76025390625}, {"id": "(Cui et al., 2022)", "paper": {"corpus_id": 260447668, "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback", "year": 2022, "venue": "IEEE Transactions on Communications", "authors": [{"name": "Yiming Cui", "authorId": "2172485573"}, {"name": "Jiajia Guo", "authorId": "47093519"}, {"name": "Zheng Cao", "authorId": "2113999930"}, {"name": "Huaze Tang", "authorId": "120710335"}, {"name": "Chao-Kai Wen", "authorId": "2257212132"}, {"name": "Shi Jin", "authorId": "2227268421"}, {"name": "Xin Wang", "authorId": "2288090155"}, {"name": "Xiaolin Hou", "authorId": "2240356164"}], "n_citations": 3}, "snippets": ["Compared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss."], "score": 0.83837890625}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 274436184, "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chengyu Li", "authorId": "2327960212"}, {"name": "Debo Cheng", "authorId": "2320341"}, {"name": "Guixian Zhang", "authorId": "2152233401"}, {"name": "Yi Li", "authorId": "2316406036"}, {"name": "Shichao Zhang", "authorId": "2186753918"}], "n_citations": 0}, "snippets": ["Learning node-specific temperatures addresses the limitations inherent in the temperature mechanism used in traditional distillation methods. In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge. However, using a fixed temperature treats the softening of all teacher logits equally, ignoring variations in node representations. This uniform approach fails to provide precise guidance for individual nodes, which can hinder knowledge transfer. Moreover, existing research has shown that temperature scaling can improve model performance (Z. (Zhang et al., 2020). \n\nTo confront these issues, we propose an entropy-based approach to learn node-specific temperatures. This approach assigns a unique temperature \u03c4i to each node, controlling the degree of softening individually. The softmax function with node-specific temperatures is defined as: \n\nwhere the temperature parameter \u03c4i is the specific temperature of the i-th node. Obviously, \u03c4i determines the softening direction of each node. The temperature of the node is related to the teacher's confidence for each respective node (Zhichun Guo et al., 2023). Therefore, we can judge the teacher's confidence for each node through the entropy of the teacher's logits. The lower the entropy value, the higher the teacher's confidence for this node. Specifically, we calculate the node-specific temperature through the probability distribution of the teacher and the confidence of the teacher, defined as follows:"], "score": 0.80810546875}, {"id": "(Borza et al., 2022)", "paper": {"corpus_id": 253655030, "title": "Effective Online Knowledge Distillation via Attention-Based Model Ensembling", "year": 2022, "venue": "Mathematics", "authors": [{"name": "D. Borza", "authorId": "144583200"}, {"name": "A. Darabant", "authorId": "1821352"}, {"name": "Tudor Alexandru Ileni", "authorId": "74810447"}, {"name": "Alexandru-Ion Marinescu", "authorId": "150020994"}], "n_citations": 2}, "snippets": ["Knowledge distillation (Bucila et al., 2006) was proposed as an effective and elegant compression technique to derive a lighter and faster network (student) from a more complex one (teacher), by penalizing the difference between their logits. Later, this mechanism was formalized by (Hinton et al., 2015) to distill the \"dark-knowledge\" from the teacher to the student. The authors noticed that a powerful and confident teacher does not bring more knowledge than ground truth data, as its prediction tends to be a narrow probability distribution with a single peak for the ground truth class. To alleviate this issue, the teacher's logits are \"softened\" by a temperature scaling factor of the softmax activation. In such a manner, the lightweight network can infer what other classes were found similar by the teacher network. More formally, this can be expressed as \n\nwhere \u03c4 is the temperature of the softmax function (equation from (Hinton et al., 2015)). When \u03c4 is greater than 1, the small probabilities of the softmax function are increased and the output is \"softened\". \n\nIn the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions. Other methods proposed using the root-mean-square error loss [9] or distilling from hardlabels (Touvron et al., 2020)."], "score": 0.78076171875}, {"id": "(Xie et al., 2023)", "paper": {"corpus_id": 265384964, "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation", "year": 2023, "venue": "Fire", "authors": [{"name": "Jinzhou Xie", "authorId": "2268047558"}, {"name": "Hongmin Zhao", "authorId": "2268031861"}], "n_citations": 6}, "snippets": ["Knowledge distillation (Bucila et al., 2006) involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\""], "score": 0.88232421875}], "table": null}, {"title": "Soft Label Generation Techniques", "tldr": "Soft label generation is central to knowledge distillation, where instead of using one-hot encoded labels, the teacher's softened probability distributions across all classes are used to train the student model. These soft labels contain valuable \"dark knowledge\" about inter-class similarities that helps student models generalize better. (13 sources)", "text": "\nKnowledge distillation's effectiveness relies heavily on the generation of soft labels that capture the rich \"dark knowledge\" embedded in a teacher model's outputs. Soft labels are created from the teacher's softmax outputs, providing more nuanced supervision than one-hot vectors by including relative probabilities across all classes, including incorrect ones <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper>. This information is particularly valuable because it encodes the similarity relationships between different classes that the teacher model has learned during its training <Paper corpusId=\"270077283\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nThe typical implementation of soft label generation involves processing the output logits (pre-softmax activations) of the teacher model through a softmax function with an elevated temperature parameter <Paper corpusId=\"235489777\" paperTitle=\"(Zhao et al., 2021)\" isShortName></Paper>. The resulting probability distribution becomes the target for the student model to mimic during training. This process can be formalized as:\n\n```\nP_T = softmax(a_T/T)\n```\n\nwhere a_T represents the teacher's logits and T is the temperature parameter <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nSeveral approaches exist for incorporating soft labels into student training. The most common method is to combine a standard cross-entropy loss (with hard labels) and a Kullback-Leibler divergence loss that aligns the student's softened outputs with the teacher's soft labels <Paper corpusId=\"263789814\" paperTitle=\"(Ding et al., 2020)\" isShortName></Paper>. This dual-objective training can be expressed as:\n\n```\nL = \u03bb\u2081L_CE(y, P_S) + \u03bb\u2082L_KD(P_T, P_S)\n```\n\nwhere \u03bb\u2081 and \u03bb\u2082 are weighting factors for balancing the two loss components <Paper corpusId=\"237453494\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\nRecent research has explored alternative formulations for soft label generation. For instance, Zhao et al. proposed decoupling the KL divergence into target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD) components, finding that the non-target class information is particularly important for effective knowledge transfer <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247476179\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper>.\n\nBeyond logit-based approaches, some methods transfer knowledge using intermediate feature representations. Zhao et al. note that features from different layers of deep neural networks can also serve as valuable dark knowledge for guiding student training <Paper corpusId=\"198179767\" paperTitle=\"(Zhao et al., 2019)\" isShortName></Paper>. For example, Zhang et al. employed a hint learning loss that encourages similarity between corresponding branches of teacher and student networks <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nAnother technique to enhance the effectiveness of soft labels is the top score difference (TSD) approach, which creates softer distributions by penalizing the teacher for having too much confidence in its primary prediction. This helps the teacher distribute confidence to secondary classes, resulting in more informative soft targets for the student <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"54986302\" paperTitle=\"(Yang et al., 2018)\" isShortName></Paper>.\n\nThe data used for generating soft labels, known as the \"transfer set,\" can be either the original training data or arbitrary data. To maximize information transfer per sample, it's beneficial to generate soft labels with high entropy (non-peaky distributions), typically achieved through higher temperature values <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper>.\n\nInterestingly, research has shown that even relatively weak teacher models can provide valuable soft labels, suggesting that the regularization effect of soft targets may be as important as the specific class similarity information they contain <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"219962714\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Nayak et al., 2019)", "paper": {"corpus_id": 159041346, "title": "Zero-Shot Knowledge Distillation in Deep Networks", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Gaurav Kumar Nayak", "authorId": "143747407"}, {"name": "Konda Reddy Mopuri", "authorId": "2217000"}, {"name": "Vaisakh Shaj", "authorId": "7155274"}, {"name": "R. Venkatesh Babu", "authorId": "144682140"}, {"name": "Anirban Chakraborty", "authorId": "1429640900"}], "n_citations": 245}, "snippets": ["The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence."], "score": 0.82470703125}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 270077283, "title": "Self-Knowledge Distillation via Progressive Associative Learning", "year": 2024, "venue": "Electronics", "authors": [{"name": "Haoran Zhao", "authorId": "2292247795"}, {"name": "Yanxian Bi", "authorId": "2297863565"}, {"name": "Shuwen Tian", "authorId": "2304028661"}, {"name": "Jian Wang", "authorId": "2237771070"}, {"name": "Peiying Zhang", "authorId": "40075749"}, {"name": "Zhaopeng Deng", "authorId": "2292213958"}, {"name": "Kai Liu", "authorId": "2301512529"}], "n_citations": 3}, "snippets": ["Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network. They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\". The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs. In the process of knowledge transfer, soft targets often contain richer information than one-hot targets."], "score": 0.71142578125}, {"id": "(Hinton et al., 2015)", "paper": {"corpus_id": 7200347, "title": "Distilling the Knowledge in a Neural Network", "year": 2015, "venue": "arXiv.org", "authors": [{"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "J. Dean", "authorId": "49959210"}], "n_citations": 19742}, "snippets": ["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."], "score": 0.0}, {"id": "(Zhao et al., 2021)", "paper": {"corpus_id": 235489777, "title": "Knowledge Distillation via Instance-level Sequence Learning", "year": 2021, "venue": "Knowledge-Based Systems", "authors": [{"name": "Haoran Zhao", "authorId": "50981688"}, {"name": "Xin Sun", "authorId": "144326521"}, {"name": "Junyu Dong", "authorId": "1964397"}, {"name": "Zihe Dong", "authorId": "2087106420"}, {"name": "Qiong Li", "authorId": "2108144626"}], "n_citations": 24}, "snippets": ["The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels.\n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ).\n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss:\n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4, the student network will be trained to optimize the following loss function:\n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses."], "score": 0.77490234375}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 256900863, "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xiongtao Zhang", "authorId": "2135847074"}, {"name": "Zezong Yin", "authorId": "2206403890"}, {"name": "Yunliang Jiang", "authorId": "3247526"}, {"name": "Yizhang Jiang", "authorId": "1390650781"}, {"name": "Da-Song Sun", "authorId": "2340356"}, {"name": "Yong Liu", "authorId": "2189281"}], "n_citations": 1}, "snippets": ["Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence (Hinton et al., 2015), the mean squared error (Fang et al., 2021) and the Jensen-Shannon divergence (Yin et al., 2019), etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. (Zhao et al., 2022) pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence."], "score": 0.88525390625}, {"id": "(Ding et al., 2020)", "paper": {"corpus_id": 263789814, "title": "Multi-level Knowledge Distillation via Knowledge Alignment and Correlation", "year": 2020, "venue": "", "authors": [{"name": "Fei Ding", "authorId": "2064424445"}, {"name": "Yin Yang", "authorId": "2257087597"}, {"name": "Hongxin Hu", "authorId": "2256778126"}, {"name": "V. Krovi", "authorId": "2095713717"}, {"name": "Feng Luo", "authorId": "2140495064"}], "n_citations": 4}, "snippets": ["Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals."], "score": 0.763671875}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 237453494, "title": "Self Supervision to Distillation for Long-Tailed Visual Recognition", "year": 2021, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Tianhao Li", "authorId": "1652032884"}, {"name": "Limin Wang", "authorId": "2109120086"}, {"name": "Gangshan Wu", "authorId": "39914710"}], "n_citations": 103}, "snippets": ["Knowledge distillation (Hinton et al., 2015) is first introduced for transferring knowledge from high-capability networks (teacher models) to small networks (student models) via soft labels", "the dark knowledge in soft labels can be helpful by transferring knowledge from head classes to tail classes", "We denote x a training image with its hard label y and soft label y", "The feature vector f will be sent to two linear classifiers G hard and G sof t to get output logits z hard = G hard (f ) and z sof t = G sof t (f ). Let z denote the output logits of teacher model, then the soft label is given by:\n\nwhere i is the category index and T is the temperature which is set to 2 by default. Then, the knowledge distillation loss is written as:\n\n(4) For hard label supervision, we utilize the standard cross entropy loss L ce . Thus, the final loss is the combination of these two losses:\n\nwhere both of \u03bb 1 and \u03bb 2 are the weight of each loss and set to 1 in our experiments."], "score": 0.82470703125}, {"id": "(Zhao et al., 2022)", "paper": {"corpus_id": 247476179, "title": "Decoupled Knowledge Distillation", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Borui Zhao", "authorId": "2112526021"}, {"name": "Quan Cui", "authorId": "144802953"}, {"name": "Renjie Song", "authorId": "2067622132"}, {"name": "Yiyu Qiu", "authorId": "2153278419"}, {"name": "Jiajun Liang", "authorId": "1387852255"}], "n_citations": 549}, "snippets": ["State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we re-formulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the \"difficulty\" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megviiresearch/mdistiller."], "score": 0.0}, {"id": "(Zhao et al., 2019)", "paper": {"corpus_id": 198179767, "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching", "year": 2019, "venue": "IEEE Transactions on Cybernetics", "authors": [{"name": "Haoran Zhao", "authorId": "50981688"}, {"name": "Xin Sun", "authorId": "144326521"}, {"name": "Junyu Dong", "authorId": "1964397"}, {"name": "Changrui Chen", "authorId": "10944885"}, {"name": "Zihe Dong", "authorId": "2087106420"}], "n_citations": 59}, "snippets": ["Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process.\n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function:"], "score": 0.70361328125}, {"id": "(Zhang et al., 2020)", "paper": {"corpus_id": 212855595, "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Zhenyu Zhang", "authorId": "47295143"}, {"name": "Xiaobo Shu", "authorId": "2269366"}, {"name": "Yu Bowen", "authorId": "48613402"}, {"name": "Tingwen Liu", "authorId": "2079682"}, {"name": "Jiapeng Zhao", "authorId": "48019474"}, {"name": "Quangang Li", "authorId": "2108645146"}, {"name": "Li Guo", "authorId": "48358041"}], "n_citations": 35}, "snippets": ["This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by (Yang et al., 2018), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes", "Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations", "The Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. (Chen et al., 2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss", "Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:"], "score": 0.869140625}, {"id": "(Yang et al., 2018)", "paper": {"corpus_id": 54986302, "title": "Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students", "year": 2018, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Chenglin Yang", "authorId": "2154170445"}, {"name": "Lingxi Xie", "authorId": "3041937"}, {"name": "Siyuan Qiao", "authorId": "2383133"}, {"name": "A. Yuille", "authorId": "145081362"}], "n_citations": 136}, "snippets": ["We focus on the problem of training a deep neural network in generations. The flowchart is that, in order to optimize the target network (student), another network (teacher) with the same architecture is first trained, and used to provide part of supervision signals in the next stage. While this strategy leads to a higher accuracy, many aspects (e.g., why teacher-student optimization helps) still need further explorations.This paper studies this problem from a perspective of controlling the strictness in training the teacher network. Existing approaches mostly used a hard distribution (e.g., one-hot vectors) in training, leading to a strict teacher which itself has a high accuracy, but we argue that the teacher needs to be more tolerant, although this often implies a lower accuracy. The implementation is very easy, with merely an extra loss term added to the teacher network, facilitating a few secondary classes to emerge and complement to the primary class. Consequently, the teacher provides a milder supervision signal (a less peaked distribution), and makes it possible for the student to learn from inter-class similarity and potentially lower the risk of over-fitting. Experiments are performed on standard image classification tasks (CIFAR100 and ILSVRC2012). Although the teacher network behaves less powerful, the students show a persistent ability growth and eventually achieve higher classification accuracies than other competitors. Model ensemble and transfer feature extraction also verify the effectiveness of our approach."], "score": 0.0}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 264590688, "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jiatai Wang", "authorId": "2186275686"}, {"name": "Zhiwei Xu", "authorId": "2244022079"}, {"name": "Xin Wang", "authorId": "2262805047"}], "n_citations": 1}, "snippets": ["KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation", "Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes (Yuan et al., 2020). With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge (Yuan et al., 2020)."], "score": 0.76220703125}, {"id": "(Yuan et al., 2020)", "paper": {"corpus_id": 219962714, "title": "Revisiting Knowledge Distillation via Label Smoothing Regularization", "year": 2020, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Li Yuan", "authorId": "2087091296"}, {"name": "Francis E. H. Tay", "authorId": "40983412"}, {"name": "Guilin Li", "authorId": "2108696460"}, {"name": "Tao Wang", "authorId": "2155456820"}, {"name": "Jiashi Feng", "authorId": "33221685"}], "n_citations": 527}, "snippets": ["Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. In this work, we challenge this common belief by following experimental observations: 1) beyond the acknowledgment that the teacher can improve the student, the student can also enhance the teacher significantly by reversing the KD procedure; 2) a poorly-trained teacher with much lower accuracy than the student can still improve the latter significantly. To explain these observations, we provide a theoretical analysis of the relationships between KD and label smoothing regularization. We prove that 1) KD is a type of learned label smoothing regularization and 2) label smoothing regularization provides a virtual teacher model for KD. From these results, we argue that the success of KD is not fully due to the similarity information between categories from teachers, but also to the regularization of soft targets, which is equally or even more important. Based on these analyses, we further propose a novel Teacher-free Knowledge Distillation (Tf-KD) framework, where a student model learns from itself or manually-designed regularization distribution. The Tf-KD achieves comparable performance with normal KD from a superior teacher, which is well applied when a stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly deployed for training deep neural networks. Without any extra computation cost, Tf-KD achieves up to 0.65\\% improvement on ImageNet over well-established baseline models, which is superior to label smoothing regularization."], "score": 0.0}], "table": null}, {"title": "Loss Function Formulations", "tldr": "Knowledge distillation loss functions typically combine cross-entropy loss on hard labels with KL divergence between teacher and student soft outputs. Various formulations exist, including temperature-scaled softmax, weighting mechanisms, and specialized losses that separately handle target and non-target class knowledge. (13 sources)", "text": "\nThe loss function is a crucial component in knowledge distillation as it determines how effectively dark knowledge is transferred from teacher to student networks. The standard formulation, introduced by Hinton et al., combines two objectives: (1) the traditional cross-entropy loss between student predictions and ground truth labels, and (2) a distillation loss that aligns the student's probability distribution with the teacher's softened distribution <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nThis combined loss function can be formally expressed as:\n\n```\nL = \u03bb\u2081L_CE(y, P_S) + \u03bb\u2082L_KD(P_T, P_S)\n```\n\nwhere L_CE is the cross-entropy loss with hard labels, L_KD is the Kullback-Leibler (KL) divergence between teacher and student probability distributions, and \u03bb\u2081 and \u03bb\u2082 are hyperparameters that balance these two components <Paper corpusId=\"235489777\" paperTitle=\"(Zhao et al., 2021)\" isShortName></Paper> <Paper corpusId=\"237453494\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\nThe KL divergence loss term specifically targets the transfer of dark knowledge by measuring the difference between the teacher's and student's softened probability distributions:\n\n```\nL_KD = T\u00b2 * KL(softmax(z_T/T), softmax(z_S/T))\n```\n\nwhere z_T and z_S are the logits from teacher and student networks, respectively, and T is the temperature parameter <Paper corpusId=\"219687782\" paperTitle=\"(Trofimov et al., 2020)\" isShortName></Paper>. The T\u00b2 factor is crucial for scaling gradients appropriately, ensuring that gradients from both hard and soft targets are comparable in magnitude <Paper corpusId=\"219687782\" paperTitle=\"(Trofimov et al., 2020)\" isShortName></Paper>.\n\nMany implementations use \u03b1 and \u03b2 as weighting coefficients instead of \u03bb\u2081 and \u03bb\u2082, with the loss function expressed as:\n\n```\nL = \u03b1L_soft + \u03b2L_hard\n```\n\nwhere L_soft is the KL divergence loss and L_hard is the standard cross-entropy loss <Paper corpusId=\"266555759\" paperTitle=\"(Gupta et al., 2023)\" isShortName></Paper> <Paper corpusId=\"3759373\" paperTitle=\"(Hashemi et al., 2018)\" isShortName></Paper>.\n\nRecent research has proposed more sophisticated loss formulations. Zhao et al. introduced a decoupled approach that separates the KL divergence into target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD) components <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247476179\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper>. This decoupling allows more flexible and efficient transfer of dark knowledge by addressing the limitations of the traditional coupled formulation, which can suppress the effectiveness of non-target class knowledge distillation <Paper corpusId=\"247476179\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper>.\n\nBeyond logit-based approaches, some loss functions incorporate feature-based knowledge transfer. Zhang et al. proposed a hint learning loss that encourages similarity between corresponding intermediate representations of teacher and student networks <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. Their formulation combines this hint learning loss with the traditional KL divergence:\n\n```\nL = L_GT + \u03bb_ht(L_HT + L_KD)\n```\n\nwhere L_GT is the ground truth loss, L_HT is the hint learning loss, and L_KD is the KL divergence loss <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nAnother innovative approach is the top score difference (TSD) loss, which creates softer distributions by penalizing the teacher for having excessive confidence in its primary prediction <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"54986302\" paperTitle=\"(Yang et al., 2018)\" isShortName></Paper>. This encourages the teacher to distribute confidence among secondary classes, resulting in more informative soft targets <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nFor multi-teacher distillation scenarios, Chen et al. proposed ensemble-based approaches where outputs from multiple teachers are combined to provide stronger guidance for the student model <Paper corpusId=\"232147231\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>. In these setups, the loss function typically includes terms for each teacher-student pair.\n\nSome researchers have also explored alternative divergence measures beyond KL divergence, such as mean squared error <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233033712\" paperTitle=\"(Fang et al., 2021)\" isShortName></Paper> and Jensen-Shannon divergence <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"209405263\" paperTitle=\"(Yin et al., 2019)\" isShortName></Paper>, finding that these can sometimes provide better performance for specific applications.", "citations": [{"id": "(Hinton et al., 2015)", "paper": {"corpus_id": 7200347, "title": "Distilling the Knowledge in a Neural Network", "year": 2015, "venue": "arXiv.org", "authors": [{"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "J. Dean", "authorId": "49959210"}], "n_citations": 19742}, "snippets": ["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."], "score": 0.0}, {"id": "(Zhao et al., 2021)", "paper": {"corpus_id": 235489777, "title": "Knowledge Distillation via Instance-level Sequence Learning", "year": 2021, "venue": "Knowledge-Based Systems", "authors": [{"name": "Haoran Zhao", "authorId": "50981688"}, {"name": "Xin Sun", "authorId": "144326521"}, {"name": "Junyu Dong", "authorId": "1964397"}, {"name": "Zihe Dong", "authorId": "2087106420"}, {"name": "Qiong Li", "authorId": "2108144626"}], "n_citations": 24}, "snippets": ["The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels.\n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ).\n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss:\n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4, the student network will be trained to optimize the following loss function:\n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses."], "score": 0.77490234375}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 237453494, "title": "Self Supervision to Distillation for Long-Tailed Visual Recognition", "year": 2021, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Tianhao Li", "authorId": "1652032884"}, {"name": "Limin Wang", "authorId": "2109120086"}, {"name": "Gangshan Wu", "authorId": "39914710"}], "n_citations": 103}, "snippets": ["Knowledge distillation (Hinton et al., 2015) is first introduced for transferring knowledge from high-capability networks (teacher models) to small networks (student models) via soft labels", "the dark knowledge in soft labels can be helpful by transferring knowledge from head classes to tail classes", "We denote x a training image with its hard label y and soft label y", "The feature vector f will be sent to two linear classifiers G hard and G sof t to get output logits z hard = G hard (f ) and z sof t = G sof t (f ). Let z denote the output logits of teacher model, then the soft label is given by:\n\nwhere i is the category index and T is the temperature which is set to 2 by default. Then, the knowledge distillation loss is written as:\n\n(4) For hard label supervision, we utilize the standard cross entropy loss L ce . Thus, the final loss is the combination of these two losses:\n\nwhere both of \u03bb 1 and \u03bb 2 are the weight of each loss and set to 1 in our experiments."], "score": 0.82470703125}, {"id": "(Trofimov et al., 2020)", "paper": {"corpus_id": 219687782, "title": "Multi-Fidelity Neural Architecture Search With Knowledge Distillation", "year": 2020, "venue": "IEEE Access", "authors": [{"name": "I. Trofimov", "authorId": "144035647"}, {"name": "Nikita Klyuchnikov", "authorId": "143888131"}, {"name": "Mikhail Salnikov", "authorId": "49614468"}, {"name": "Alexander N. Filippov", "authorId": "8792436"}, {"name": "Evgeny Burnaev", "authorId": "51139941"}], "n_citations": 15}, "snippets": ["The knowledge distillation (KD) assumes two models: a teacher and a student. The teacher is typically a large and accurate network or an ensemble. The student is trained to fit the softmax outputs of the teacher together with ground truth labels. The idea is that outputs of the teacher capture not only the information provided by ground truth labels but also the probabilities of other classes -\"dark knowledge\". The knowledge distillation can be summarized as follows. \n\nLet z i be logits (pre-softmax activations) and q i -probabilities of classes as predicted by a neural network. Knowledge distillation smooths z i with the temperature \u03c4 \n\nNeural networks often do very confident predictions (close to 0 or 1) and smoothing helps to provide for student more information during training [11]. The KD loss is a linear combination of the logistic loss and cross-entropy between predictions of the teacher and the student \n\nwhere z T i , z S i are logits of the teacher and the student, H(p, q) = \u2212p log(q) is the cross-entropy function. The factor \u03c4 2 is used for scaling gradients of both parts of the loss function to be the same order."], "score": 0.7265625}, {"id": "(Gupta et al., 2023)", "paper": {"corpus_id": 266555759, "title": "PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models", "year": 2023, "venue": "IEEE Conference on High Performance Extreme Computing", "authors": [{"name": "Neelesh Gupta", "authorId": "2276489527"}, {"name": "Pengmiao Zhang", "authorId": "2110123989"}, {"name": "Rajgopal Kannan", "authorId": "2243055156"}, {"name": "Viktor K. Prasanna", "authorId": "2202866066"}], "n_citations": 4}, "snippets": ["The original knowledge distillation work primarily focuses on single-label classification. In this context, a custom loss function L is designed that considers both the hard labels from ground truth and soft labels from a teacher model. Importantly, the soft labels are obtained using a T-temperature softmax activation function. \n\nThe softmax activation function returns probabilities P (z i , T ) is defined as: \n\nTherefore, the complete loss L can be represented as: \n\nL = \u03b1L soft + \u03b2L hard (Hashemi et al., 2018) where \u03b1 and \u03b2 are hyperparameters, t i are the logits from the teacher model, s i are the logits from the student model, c i is the ground truth for the i-th instance, N is the total number of instances, and T is the temperature parameter for softmax."], "score": 0.79345703125}, {"id": "(Hashemi et al., 2018)", "paper": {"corpus_id": 3759373, "title": "Learning Memory Access Patterns", "year": 2018, "venue": "International Conference on Machine Learning", "authors": [{"name": "Milad Hashemi", "authorId": "33798741"}, {"name": "Kevin Swersky", "authorId": "2329092656"}, {"name": "Jamie A. Smith", "authorId": "2119124568"}, {"name": "Grant Ayers", "authorId": "46369381"}, {"name": "Heiner Litz", "authorId": "2655459"}, {"name": "Jichuan Chang", "authorId": "1698747"}, {"name": "Christos Kozyrakis", "authorId": "117272782"}, {"name": "Parthasarathy Ranganathan", "authorId": "1770926"}], "n_citations": 205}, "snippets": ["The explosion in workload complexity and the recent slow-down in Moore's law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research."], "score": 0.0}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 256900863, "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xiongtao Zhang", "authorId": "2135847074"}, {"name": "Zezong Yin", "authorId": "2206403890"}, {"name": "Yunliang Jiang", "authorId": "3247526"}, {"name": "Yizhang Jiang", "authorId": "1390650781"}, {"name": "Da-Song Sun", "authorId": "2340356"}, {"name": "Yong Liu", "authorId": "2189281"}], "n_citations": 1}, "snippets": ["Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence (Hinton et al., 2015), the mean squared error (Fang et al., 2021) and the Jensen-Shannon divergence (Yin et al., 2019), etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. (Zhao et al., 2022) pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence."], "score": 0.88525390625}, {"id": "(Zhao et al., 2022)", "paper": {"corpus_id": 247476179, "title": "Decoupled Knowledge Distillation", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Borui Zhao", "authorId": "2112526021"}, {"name": "Quan Cui", "authorId": "144802953"}, {"name": "Renjie Song", "authorId": "2067622132"}, {"name": "Yiyu Qiu", "authorId": "2153278419"}, {"name": "Jiajun Liang", "authorId": "1387852255"}], "n_citations": 549}, "snippets": ["State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we re-formulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the \"difficulty\" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megviiresearch/mdistiller."], "score": 0.0}, {"id": "(Zhang et al., 2020)", "paper": {"corpus_id": 212855595, "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Zhenyu Zhang", "authorId": "47295143"}, {"name": "Xiaobo Shu", "authorId": "2269366"}, {"name": "Yu Bowen", "authorId": "48613402"}, {"name": "Tingwen Liu", "authorId": "2079682"}, {"name": "Jiapeng Zhao", "authorId": "48019474"}, {"name": "Quangang Li", "authorId": "2108645146"}, {"name": "Li Guo", "authorId": "48358041"}], "n_citations": 35}, "snippets": ["This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by (Yang et al., 2018), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes", "Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations", "The Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. (Chen et al., 2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss", "Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:"], "score": 0.869140625}, {"id": "(Yang et al., 2018)", "paper": {"corpus_id": 54986302, "title": "Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students", "year": 2018, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Chenglin Yang", "authorId": "2154170445"}, {"name": "Lingxi Xie", "authorId": "3041937"}, {"name": "Siyuan Qiao", "authorId": "2383133"}, {"name": "A. Yuille", "authorId": "145081362"}], "n_citations": 136}, "snippets": ["We focus on the problem of training a deep neural network in generations. The flowchart is that, in order to optimize the target network (student), another network (teacher) with the same architecture is first trained, and used to provide part of supervision signals in the next stage. While this strategy leads to a higher accuracy, many aspects (e.g., why teacher-student optimization helps) still need further explorations.This paper studies this problem from a perspective of controlling the strictness in training the teacher network. Existing approaches mostly used a hard distribution (e.g., one-hot vectors) in training, leading to a strict teacher which itself has a high accuracy, but we argue that the teacher needs to be more tolerant, although this often implies a lower accuracy. The implementation is very easy, with merely an extra loss term added to the teacher network, facilitating a few secondary classes to emerge and complement to the primary class. Consequently, the teacher provides a milder supervision signal (a less peaked distribution), and makes it possible for the student to learn from inter-class similarity and potentially lower the risk of over-fitting. Experiments are performed on standard image classification tasks (CIFAR100 and ILSVRC2012). Although the teacher network behaves less powerful, the students show a persistent ability growth and eventually achieve higher classification accuracies than other competitors. Model ensemble and transfer feature extraction also verify the effectiveness of our approach."], "score": 0.0}, {"id": "(Chen et al., 2021)", "paper": {"corpus_id": 232147231, "title": "Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation", "year": 2021, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Shuaijun Chen", "authorId": "3461777"}, {"name": "Xu Jia", "authorId": "1641711590"}, {"name": "Jianzhong He", "authorId": "12874945"}, {"name": "Yongjie Shi", "authorId": "6658208"}, {"name": "Jianzhuang Liu", "authorId": "2144167531"}], "n_citations": 64}, "snippets": ["After obtaining two pre-trained domain-mixed teachers, we employ knowledge distillation (KD), a technique to distilling knowledge by minimizing the KL-divergence between outputs of these two models. Here we adapt it to extract \"dark knowledge\" from these two complementary teachers. The pipeline of multi-teacher KD is shown in Fig. 1 (b), including two pre-trained domain-mixed teachers and one student with the same network architecture as teacher. The outputs of two teachers are ensembled as a stronger guidance to supervise the training of the student model on unlabeled target data. Besides, the student model is also supervised by the labels on the small amount of labeled target data."], "score": 0.78564453125}, {"id": "(Fang et al., 2021)", "paper": {"corpus_id": 233033712, "title": "Compressing Visual-linguistic Model via Knowledge Distillation", "year": 2021, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Zhiyuan Fang", "authorId": "2113601486"}, {"name": "Jianfeng Wang", "authorId": "2110257737"}, {"name": "Xiaowei Hu", "authorId": "2148941781"}, {"name": "Lijuan Wang", "authorId": "29957038"}, {"name": "Yezhou Yang", "authorId": "1784500"}, {"name": "Zicheng Liu", "authorId": "2145253136"}], "n_citations": 99}, "snippets": ["Despite exciting progress in pre-training for visual-linguistic (VL) representations, very few aspire to a small VL model. In this paper, we study knowledge distillation (KD) to effectively compress a transformer based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention distributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Student\u2019s detector while the features are from Teacher\u2019s own object detector. With aligned network inputs, the adapted Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block, and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly improves the performance of small VL models on image captioning and visual question answering tasks. It reaches 120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its non-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effectiveness of VL distillation in both pre-training and fine-tuning stages."], "score": 0.0}, {"id": "(Yin et al., 2019)", "paper": {"corpus_id": 209405263, "title": "Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion", "year": 2019, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Hongxu Yin", "authorId": "1989015"}, {"name": "Pavlo Molchanov", "authorId": "2824500"}, {"name": "Zhizhong Li", "authorId": "49969902"}, {"name": "J. \u00c1lvarez", "authorId": "2974008"}, {"name": "Arun Mallya", "authorId": "36508529"}, {"name": "Derek Hoiem", "authorId": "2433269"}, {"name": "N. Jha", "authorId": "144874163"}, {"name": "J. Kautz", "authorId": "1690538"}], "n_citations": 569}, "snippets": ["We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We ``invert'' a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance - (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning."], "score": 0.0}], "table": null}, {"title": "Additional Technical Implementation Details", "tldr": "Knowledge distillation implementation requires careful consideration of various technical details beyond the core components, including transfer set selection, learning rate adjustments, and specialized techniques that enhance the transfer of dark knowledge. (18 sources)", "text": "\nHere are key technical implementation details that practitioners should consider when implementing knowledge distillation:\n\n1. **Transfer Set Selection**: The data used for distillation (the \"transfer set\") can be either the original training data or arbitrary unlabeled data. To maximize information transfer per sample, it's beneficial to generate soft labels with high entropy (non-peaky distributions) <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper>.\n\n2. **Learning Rate Considerations**: Due to the softer, less peaky nature of teacher labels, the training gradients typically have lower variance, enabling the use of higher learning rates which can lead to faster convergence <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper>.\n\n3. **Top Score Difference (TSD)**: This technique creates softer distributions by penalizing the teacher for having excessive confidence in its primary prediction, encouraging it to distribute confidence among secondary classes and producing more informative soft targets <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"54986302\" paperTitle=\"(Yang et al., 2018)\" isShortName></Paper>.\n\n4. **Alternative Divergence Measures**: While KL divergence is the standard approach, some implementations use mean squared error <Paper corpusId=\"233033712\" paperTitle=\"(Fang et al., 2021)\" isShortName></Paper> or Jensen-Shannon divergence <Paper corpusId=\"209405263\" paperTitle=\"(Yin et al., 2019)\" isShortName></Paper> as alternative measures for aligning teacher and student distributions <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\n5. **Decoupled Knowledge Distillation**: This approach separates the KL divergence into target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD) components for more flexible knowledge transfer <Paper corpusId=\"247476179\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\n6. **Multi-Teacher Distillation**: This technique combines outputs from multiple teacher models to provide stronger guidance for the student model, particularly effective when different teachers have complementary knowledge <Paper corpusId=\"232147231\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>.\n\n7. **Intermediate Feature Distillation**: Beyond logits, knowledge can be transferred through intermediate feature representations from different layers of the teacher network <Paper corpusId=\"198179767\" paperTitle=\"(Zhao et al., 2019)\" isShortName></Paper> <Paper corpusId=\"272969060\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.\n\n8. **Hint Learning**: This approach encourages similarity between corresponding branches of teacher and student networks, stabilizing the training process and improving final performance <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\n9. **Adaptive Temperature Approaches**: Instead of a fixed temperature, some methods use adaptive approaches that assign different temperatures to samples based on their characteristics <Paper corpusId=\"225040647\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>.\n\n10. **Temperature Selection Considerations**: Choosing an appropriate temperature value is critical - too small and the distribution remains peaky (like one-hot labels), too large and the distribution becomes too uniform, resulting in information loss <Paper corpusId=\"260447668\" paperTitle=\"(Cui et al., 2022)\" isShortName></Paper>.\n\n11. **Self-Distillation**: Some approaches distill knowledge from a model to itself across different distorted versions of the same data, without requiring separate teacher and student models <Paper corpusId=\"251066725\" paperTitle=\"(Liang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>.\n\n12. **Student-Aware Teacher Training**: Recent approaches focus on training teachers that are specifically designed to be \"student-friendly,\" considering the student's capacity during the teacher training process <Paper corpusId=\"231925118\" paperTitle=\"(Park et al., 2021)\" isShortName></Paper>.\n\n13. **Feature Alignment**: When teacher and student architectures differ significantly, techniques for aligning feature dimensions between networks are necessary for effective knowledge transfer <Paper corpusId=\"272969060\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.\n\n14. **Combining Multiple Distillation Losses**: Many implementations combine different types of distillation losses (e.g., attention-based, hidden representation-based, and dark knowledge-based) to capture different aspects of the teacher's knowledge <Paper corpusId=\"231648215\" paperTitle=\"(Feng et al., 2021)\" isShortName></Paper>.\n\n15. **Progressive Distillation**: For cases with a large gap between teacher and student capacities, a sequence of intermediate-sized \"teacher assistants\" can bridge the gap and facilitate more effective knowledge transfer <Paper corpusId=\"203642130\" paperTitle=\"(Cho et al., 2019)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>.", "citations": [{"id": "(Nayak et al., 2019)", "paper": {"corpus_id": 159041346, "title": "Zero-Shot Knowledge Distillation in Deep Networks", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Gaurav Kumar Nayak", "authorId": "143747407"}, {"name": "Konda Reddy Mopuri", "authorId": "2217000"}, {"name": "Vaisakh Shaj", "authorId": "7155274"}, {"name": "R. Venkatesh Babu", "authorId": "144682140"}, {"name": "Anirban Chakraborty", "authorId": "1429640900"}], "n_citations": 245}, "snippets": ["The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence."], "score": 0.82470703125}, {"id": "(Zhang et al., 2020)", "paper": {"corpus_id": 212855595, "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Zhenyu Zhang", "authorId": "47295143"}, {"name": "Xiaobo Shu", "authorId": "2269366"}, {"name": "Yu Bowen", "authorId": "48613402"}, {"name": "Tingwen Liu", "authorId": "2079682"}, {"name": "Jiapeng Zhao", "authorId": "48019474"}, {"name": "Quangang Li", "authorId": "2108645146"}, {"name": "Li Guo", "authorId": "48358041"}], "n_citations": 35}, "snippets": ["This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by (Yang et al., 2018), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes", "Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations", "The Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. (Chen et al., 2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss", "Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:"], "score": 0.869140625}, {"id": "(Yang et al., 2018)", "paper": {"corpus_id": 54986302, "title": "Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students", "year": 2018, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Chenglin Yang", "authorId": "2154170445"}, {"name": "Lingxi Xie", "authorId": "3041937"}, {"name": "Siyuan Qiao", "authorId": "2383133"}, {"name": "A. Yuille", "authorId": "145081362"}], "n_citations": 136}, "snippets": ["We focus on the problem of training a deep neural network in generations. The flowchart is that, in order to optimize the target network (student), another network (teacher) with the same architecture is first trained, and used to provide part of supervision signals in the next stage. While this strategy leads to a higher accuracy, many aspects (e.g., why teacher-student optimization helps) still need further explorations.This paper studies this problem from a perspective of controlling the strictness in training the teacher network. Existing approaches mostly used a hard distribution (e.g., one-hot vectors) in training, leading to a strict teacher which itself has a high accuracy, but we argue that the teacher needs to be more tolerant, although this often implies a lower accuracy. The implementation is very easy, with merely an extra loss term added to the teacher network, facilitating a few secondary classes to emerge and complement to the primary class. Consequently, the teacher provides a milder supervision signal (a less peaked distribution), and makes it possible for the student to learn from inter-class similarity and potentially lower the risk of over-fitting. Experiments are performed on standard image classification tasks (CIFAR100 and ILSVRC2012). Although the teacher network behaves less powerful, the students show a persistent ability growth and eventually achieve higher classification accuracies than other competitors. Model ensemble and transfer feature extraction also verify the effectiveness of our approach."], "score": 0.0}, {"id": "(Fang et al., 2021)", "paper": {"corpus_id": 233033712, "title": "Compressing Visual-linguistic Model via Knowledge Distillation", "year": 2021, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Zhiyuan Fang", "authorId": "2113601486"}, {"name": "Jianfeng Wang", "authorId": "2110257737"}, {"name": "Xiaowei Hu", "authorId": "2148941781"}, {"name": "Lijuan Wang", "authorId": "29957038"}, {"name": "Yezhou Yang", "authorId": "1784500"}, {"name": "Zicheng Liu", "authorId": "2145253136"}], "n_citations": 99}, "snippets": ["Despite exciting progress in pre-training for visual-linguistic (VL) representations, very few aspire to a small VL model. In this paper, we study knowledge distillation (KD) to effectively compress a transformer based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention distributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Student\u2019s detector while the features are from Teacher\u2019s own object detector. With aligned network inputs, the adapted Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block, and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly improves the performance of small VL models on image captioning and visual question answering tasks. It reaches 120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its non-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effectiveness of VL distillation in both pre-training and fine-tuning stages."], "score": 0.0}, {"id": "(Yin et al., 2019)", "paper": {"corpus_id": 209405263, "title": "Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion", "year": 2019, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Hongxu Yin", "authorId": "1989015"}, {"name": "Pavlo Molchanov", "authorId": "2824500"}, {"name": "Zhizhong Li", "authorId": "49969902"}, {"name": "J. \u00c1lvarez", "authorId": "2974008"}, {"name": "Arun Mallya", "authorId": "36508529"}, {"name": "Derek Hoiem", "authorId": "2433269"}, {"name": "N. Jha", "authorId": "144874163"}, {"name": "J. Kautz", "authorId": "1690538"}], "n_citations": 569}, "snippets": ["We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We ``invert'' a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance - (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning."], "score": 0.0}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 256900863, "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xiongtao Zhang", "authorId": "2135847074"}, {"name": "Zezong Yin", "authorId": "2206403890"}, {"name": "Yunliang Jiang", "authorId": "3247526"}, {"name": "Yizhang Jiang", "authorId": "1390650781"}, {"name": "Da-Song Sun", "authorId": "2340356"}, {"name": "Yong Liu", "authorId": "2189281"}], "n_citations": 1}, "snippets": ["Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence (Hinton et al., 2015), the mean squared error (Fang et al., 2021) and the Jensen-Shannon divergence (Yin et al., 2019), etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. (Zhao et al., 2022) pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence."], "score": 0.88525390625}, {"id": "(Zhao et al., 2022)", "paper": {"corpus_id": 247476179, "title": "Decoupled Knowledge Distillation", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Borui Zhao", "authorId": "2112526021"}, {"name": "Quan Cui", "authorId": "144802953"}, {"name": "Renjie Song", "authorId": "2067622132"}, {"name": "Yiyu Qiu", "authorId": "2153278419"}, {"name": "Jiajun Liang", "authorId": "1387852255"}], "n_citations": 549}, "snippets": ["State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we re-formulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the \"difficulty\" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megviiresearch/mdistiller."], "score": 0.0}, {"id": "(Chen et al., 2021)", "paper": {"corpus_id": 232147231, "title": "Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation", "year": 2021, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Shuaijun Chen", "authorId": "3461777"}, {"name": "Xu Jia", "authorId": "1641711590"}, {"name": "Jianzhong He", "authorId": "12874945"}, {"name": "Yongjie Shi", "authorId": "6658208"}, {"name": "Jianzhuang Liu", "authorId": "2144167531"}], "n_citations": 64}, "snippets": ["After obtaining two pre-trained domain-mixed teachers, we employ knowledge distillation (KD), a technique to distilling knowledge by minimizing the KL-divergence between outputs of these two models. Here we adapt it to extract \"dark knowledge\" from these two complementary teachers. The pipeline of multi-teacher KD is shown in Fig. 1 (b), including two pre-trained domain-mixed teachers and one student with the same network architecture as teacher. The outputs of two teachers are ensembled as a stronger guidance to supervise the training of the student model on unlabeled target data. Besides, the student model is also supervised by the labels on the small amount of labeled target data."], "score": 0.78564453125}, {"id": "(Zhao et al., 2019)", "paper": {"corpus_id": 198179767, "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching", "year": 2019, "venue": "IEEE Transactions on Cybernetics", "authors": [{"name": "Haoran Zhao", "authorId": "50981688"}, {"name": "Xin Sun", "authorId": "144326521"}, {"name": "Junyu Dong", "authorId": "1964397"}, {"name": "Changrui Chen", "authorId": "10944885"}, {"name": "Zihe Dong", "authorId": "2087106420"}], "n_citations": 59}, "snippets": ["Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process.\n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function:"], "score": 0.70361328125}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 272969060, "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation", "year": 2024, "venue": "European Conference on Computer Vision", "authors": [{"name": "Yaomin Huang", "authorId": "2180087917"}, {"name": "Zaoming Yan", "authorId": "2257133494"}, {"name": "Chaomin Shen", "authorId": "2242115637"}, {"name": "Faming Fang", "authorId": "152786529"}, {"name": "Guixu Zhang", "authorId": "2323414943"}], "n_citations": 0}, "snippets": ["Logits-based KD was first proposed in [9], where the student learns the logits knowledge from the teacher through KL divergence. In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions: \n\nFeature-based KD transfers the knowledge of intermediate layers features from the teacher to the student. Generally, we can formulate such distillation methods as: \n\nwhere F t i and F s i are the corresponding features of the teacher and student at i-th layer. w(\u2022) is a mapping function that aligns the dimensions of features from the student to the teacher."], "score": 0.8115234375}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 225040647, "title": "Robustness-aware 2-bit quantization with real-time performance for neural network", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Xiaobin Li", "authorId": "2109348932"}, {"name": "Hongxu Jiang", "authorId": "2292508"}, {"name": "Shuangxi Huang", "authorId": "2118018442"}, {"name": "Fangzheng Tian", "authorId": "152978298"}], "n_citations": 1}, "snippets": ["Wu et al. train the student network by taking the output of the teacher network as the soft target, and achieve the knowledge transfer by replacing the L2 loss with cross entropy loss (Wu et al., 2015). Adriana Romero et al. fit the complexity of the teacher network by inputting more no-tag data into the student network. Junho Yim et al. optimized the knowledge transfer by refining knowledge distillation to the layer (Yim et al., 2017)."], "score": 0.7265625}, {"id": "(Cui et al., 2022)", "paper": {"corpus_id": 260447668, "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback", "year": 2022, "venue": "IEEE Transactions on Communications", "authors": [{"name": "Yiming Cui", "authorId": "2172485573"}, {"name": "Jiajia Guo", "authorId": "47093519"}, {"name": "Zheng Cao", "authorId": "2113999930"}, {"name": "Huaze Tang", "authorId": "120710335"}, {"name": "Chao-Kai Wen", "authorId": "2257212132"}, {"name": "Shi Jin", "authorId": "2227268421"}, {"name": "Xin Wang", "authorId": "2288090155"}, {"name": "Xiaolin Hou", "authorId": "2240356164"}], "n_citations": 3}, "snippets": ["Compared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss."], "score": 0.83837890625}, {"id": "(Liang et al., 2022)", "paper": {"corpus_id": 251066725, "title": "Efficient One Pass Self-distillation with Zipf's Label Smoothing", "year": 2022, "venue": "European Conference on Computer Vision", "authors": [{"name": "Jiajun Liang", "authorId": "1387852255"}, {"name": "Linze Li", "authorId": "2304362074"}, {"name": "Z. Bing", "authorId": "2056410266"}, {"name": "Borui Zhao", "authorId": "2112526021"}, {"name": "Yao Tang", "authorId": "2179286812"}, {"name": "Bo Lin", "authorId": "2179192518"}, {"name": "Haoqiang Fan", "authorId": "1934546"}], "n_citations": 19}, "snippets": ["Instead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,(Cho et al., 2019)(Mirzadeh et al., 2019)(Zhu et al., 2021), designing student-friendly architecture [16,(Park et al., 2021), improving the distillation efficiency (Furlanello et al., 2018)(Kim et al., 2020)(Xu et al., 2019)(Yun et al., 2020) and explaining the distillation's working mechanism [1](Stanton et al., 2021)."], "score": 0.869140625}, {"id": "(Furlanello et al., 2018)", "paper": {"corpus_id": 4110009, "title": "Born Again Neural Networks", "year": 2018, "venue": "International Conference on Machine Learning", "authors": [{"name": "Tommaso Furlanello", "authorId": "2067208583"}, {"name": "Zachary Chase Lipton", "authorId": "32219137"}, {"name": "Michael Tschannen", "authorId": "143902495"}, {"name": "L. Itti", "authorId": "7326223"}, {"name": "Anima Anandkumar", "authorId": "2047844"}], "n_citations": 1034}, "snippets": ["Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."], "score": 0.0}, {"id": "(Park et al., 2021)", "paper": {"corpus_id": 231925118, "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "D. Park", "authorId": "2122903402"}, {"name": "Moonsu Cha", "authorId": "9959922"}, {"name": "C. Jeong", "authorId": "48366572"}, {"name": "Daesin Kim", "authorId": "122204255"}, {"name": "Bohyung Han", "authorId": "40030651"}], "n_citations": 101}, "snippets": ["We propose a novel knowledge distillation approach to facilitate the transfer of dark knowledge from a teacher to a student. Contrary to most of the existing methods that rely on effective training of student models given pretrained teachers, we aim to learn the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. In other words, at the time of optimizing a teacher model, the proposed algorithm learns the student branches jointly to obtain student-friendly representations. Since the main goal of our approach lies in training teacher models and the subsequent knowledge distillation procedure is straightforward, most of the existing knowledge distillation methods can adopt this technique to improve the performance of diverse student models in terms of accuracy and convergence speed. The proposed algorithm demonstrates outstanding accuracy in several well-known knowledge distillation techniques with various combinations of teacher and student models even in the case that their architectures are heterogeneous and there is no prior knowledge about student models at the time of training teacher networks."], "score": 0.0}, {"id": "(Feng et al., 2021)", "paper": {"corpus_id": 231648215, "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation", "year": 2021, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Lingyun Feng", "authorId": "2152751276"}, {"name": "Minghui Qiu", "authorId": "2642333"}, {"name": "Yaliang Li", "authorId": "2110479359"}, {"name": "Haitao Zheng", "authorId": "16215052"}, {"name": "Ying Shen", "authorId": "2115382645"}], "n_citations": 10}, "snippets": ["For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits.\n\nWe combined the above three types of loss as our final KD loss, namely: L KD = L att + L hidden + L dark ."], "score": 0.73583984375}, {"id": "(Cho et al., 2019)", "paper": {"corpus_id": 203642130, "title": "On the Efficacy of Knowledge Distillation", "year": 2019, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Jang Hyun Cho", "authorId": "39878282"}, {"name": "B. Hariharan", "authorId": "73710317"}], "n_citations": 619}, "snippets": ["In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don\u2019t make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher\u2019s training early. Our results generalize across datasets and models."], "score": 0.0}, {"id": "(Mirzadeh et al., 2019)", "paper": {"corpus_id": 212908749, "title": "Improved Knowledge Distillation via Teacher Assistant", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Seyed Iman Mirzadeh", "authorId": "145156788"}, {"name": "Mehrdad Farajtabar", "authorId": "1682124"}, {"name": "Ang Li", "authorId": "2112839418"}, {"name": "Nir Levine", "authorId": "153898744"}, {"name": "Akihiro Matsukawa", "authorId": "2063980545"}, {"name": "H. Ghasemzadeh", "authorId": "144600887"}], "n_citations": 1081}, "snippets": ["Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."], "score": 0.0}], "table": null}], "cost": 0.6061529999999999}}
