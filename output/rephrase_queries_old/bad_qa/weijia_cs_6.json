{"original_query": "What are the common model architectures for retrieval-augmented language models?\n", "input": "are the common model retrieval language models \n", "output": {"sections": [{"title": "Introduction to Retrieval Language Models", "tldr": "Retrieval Language Models (RLMs) enhance traditional language models by incorporating external knowledge through retrieval mechanisms. They combine parametric neural networks with non-parametric retrieval components to access relevant information during inference, addressing limitations of purely parameter-based models. (12 sources)", "text": "\nRetrieval Language Models (RLMs) represent a significant advancement in natural language processing by augmenting traditional language models with external knowledge retrieval capabilities. These models have demonstrated substantial improvements across various tasks including language modeling, machine translation, question answering, and code generation <Paper corpusId=\"246431219\" paperTitle=\"(Alon et al., 2022)\" isShortName></Paper>. The fundamental innovation of RLMs lies in their ability to leverage training examples at test time, rather than relying solely on information encoded in the model's parameters <Paper corpusId=\"246431219\" paperTitle=\"(Alon et al., 2022)\" isShortName></Paper>.\n\nThe architecture of RLMs typically consists of two main components: a retriever model that searches for relevant information from an external corpus, and a reader or generator model that incorporates this retrieved information to produce outputs <Paper corpusId=\"272330251\" paperTitle=\"(Monath et al., 2024)\" isShortName></Paper> <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper>. This dual-component design allows RLMs to dynamically access and utilize contextually relevant information during inference <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>.\n\nOne prominent example of a retrieval-based model is kNN-LM, which extends pre-trained neural language models by linearly interpolating their output with a k-nearest neighbors distribution <Paper corpusId=\"246431219\" paperTitle=\"(Alon et al., 2022)\" isShortName></Paper> <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>. This approach has achieved state-of-the-art results in language modeling tasks, with particular effectiveness in predicting rare patterns and factual knowledge <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>.\n\nRetrieval-augmented language models can be implemented using both encoder-decoder architectures, as seen in models like RAG <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>, and decoder-only architectures, as demonstrated by kNN-LM and RETRO <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper> <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>. These models follow different paradigms for incorporating external knowledge, with some using a \"retrieve-and-read\" approach where retrieved documents are provided as additional context <Paper corpusId=\"263605962\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\nThe retrieval mechanisms in these models can be categorized as either dense or sparse <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>. Sparse retrievers utilize bag-of-words representations to find documents with high term overlap with the query, while dense retrievers employ neural network embeddings to capture semantic similarities <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>.\n\nRecent advances have also introduced more flexible approaches such as \"In-Context RALM,\" which simply prepends retrieved documents to the input without modifying the language model architecture <Paper corpusId=\"272310589\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256459451\" paperTitle=\"(Ram et al., 2023)\" isShortName></Paper>. This method has shown surprising effectiveness while simplifying deployment <Paper corpusId=\"256459451\" paperTitle=\"(Ram et al., 2023)\" isShortName></Paper>.\n\nOverall, retrieval-augmented language models have demonstrated impressive performance across diverse NLP tasks <Paper corpusId=\"263866951\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper>, particularly excelling in knowledge-intensive applications where access to external information is crucial for generating accurate and contextually appropriate responses.", "citations": [{"id": "(Alon et al., 2022)", "paper": {"corpus_id": 246431219, "title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Uri Alon", "authorId": "47051926"}, {"name": "Frank F. Xu", "authorId": "40027632"}, {"name": "Junxian He", "authorId": "6215698"}, {"name": "Sudipta Sengupta", "authorId": "2072419570"}, {"name": "D. Roth", "authorId": "144590225"}, {"name": "Graham Neubig", "authorId": "1700325"}], "n_citations": 63}, "snippets": ["Retrieval-based language models (R-LMs) have recently been shown to improve over standard neural models in a variety of tasks such as unconditional language modeling (Guu et al., 2018;He et al., 2020), machine translation (Zhang et al., 2018;Gu et al., 2018;Khandelwal et al., 2021), question answering (Karpukhin et al., 2020;Ram et al., 2021), and code generation (Hayati et al., 2018;Hashimoto et al., 2018). The key ingredient of R-LMs is their ability to utilize training examples at test time without having to rely on the information encoded in the model's weights only", "One prominent example of such a retrieval-based model is kNN-LM (Grave et al., 2017;(Khandelwal et al., 2019), which predicts a token by linearly interpolating the base LM's output with a nonparametric nearest neighbor distribution."], "score": 0.97412109375}, {"id": "(Monath et al., 2024)", "paper": {"corpus_id": 272330251, "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Nicholas Monath", "authorId": "2121348263"}, {"name": "Will Sussman Grathwohl", "authorId": "2319130233"}, {"name": "Michael Boratko", "authorId": "51020741"}, {"name": "Rob Fergus", "authorId": "2300098510"}, {"name": "Andrew McCallum", "authorId": "2286335051"}, {"name": "M. Zaheer", "authorId": "1771307"}], "n_citations": 0}, "snippets": ["Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020)Izacard & Grave, 2021;Izacard et al., 2022). The input to a retrieval augmented language model is a natural language text sequence, x. This input text will be encoded using a dual-encoder retrieval model, f (x). Retrieval will be performed over a corpus of targets, Y, returning k targets relevant to x, denoted S x (Y). The reader model takes as input the retrieved targets, S x (Y), and the text x, and generates text."], "score": 0.98388671875}, {"id": "(Guu et al., 2020)", "paper": {"corpus_id": 211204736, "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Kelvin Guu", "authorId": "2091768"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Zora Tung", "authorId": "9941702"}, {"name": "Panupong Pasupat", "authorId": "2616463"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}], "n_citations": 2119}, "snippets": ["Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."], "score": 0.0}, {"id": "(Zayyad et al., 2024)", "paper": {"corpus_id": 274982275, "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Majd Zayyad", "authorId": "2336913948"}, {"name": "Yossi Adi", "authorId": "2727584"}], "n_citations": 0}, "snippets": ["RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023]", "RAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts."], "score": 0.9755859375}, {"id": "(Khandelwal et al., 2019)", "paper": {"corpus_id": 207870430, "title": "Generalization through Memorization: Nearest Neighbor Language Models", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "Dan Jurafsky", "authorId": "1746807"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "M. Lewis", "authorId": "35084211"}], "n_citations": 842}, "snippets": ["We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."], "score": 0.0}, {"id": "(Huang et al., 2023)", "paper": {"corpus_id": 260900354, "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jie Huang", "authorId": "1490651934"}, {"name": "Wei Ping", "authorId": "2056440915"}, {"name": "Peng Xu", "authorId": "145011005"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "K. Chang", "authorId": "143922493"}, {"name": "Bryan Catanzaro", "authorId": "2301680"}], "n_citations": 35}, "snippets": ["Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures."], "score": 0.95361328125}, {"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."], "score": 0.0}, {"id": "(Borgeaud et al., 2021)", "paper": {"corpus_id": 244954723, "title": "Improving language models by retrieving from trillions of tokens", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "A. Mensch", "authorId": "1697879"}, {"name": "Jordan Hoffmann", "authorId": "46616544"}, {"name": "Trevor Cai", "authorId": "2072572294"}, {"name": "Eliza Rutherford", "authorId": "2143538252"}, {"name": "Katie Millican", "authorId": "2143434227"}, {"name": "George van den Driessche", "authorId": "47568983"}, {"name": "Jean-Baptiste Lespiau", "authorId": "143783339"}, {"name": "Bogdan Damoc", "authorId": "2143374656"}, {"name": "Aidan Clark", "authorId": "31993415"}, {"name": "Diego de Las Casas", "authorId": "40550616"}, {"name": "Aurelia Guy", "authorId": "40895205"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "Roman Ring", "authorId": "81387328"}, {"name": "T. Hennigan", "authorId": "4629007"}, {"name": "Saffron Huang", "authorId": "2148653469"}, {"name": "Lorenzo Maggiore", "authorId": "108173905"}, {"name": "Chris Jones", "authorId": "2115601070"}, {"name": "Albin Cassirer", "authorId": "51042571"}, {"name": "Andy Brock", "authorId": "2065040422"}, {"name": "Michela Paganini", "authorId": "35550664"}, {"name": "G. Irving", "authorId": "2060655766"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "Simon Osindero", "authorId": "2217144"}, {"name": "K. Simonyan", "authorId": "34838386"}, {"name": "Jack W. Rae", "authorId": "34269227"}, {"name": "Erich Elsen", "authorId": "152585800"}, {"name": "L. Sifre", "authorId": "2175946"}], "n_citations": 1100}, "snippets": ["We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."], "score": 0.0}, {"id": "(Lin et al., 2023)", "paper": {"corpus_id": 263605962, "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xi Victoria Lin", "authorId": "2255374957"}, {"name": "Xilun Chen", "authorId": "1769736"}, {"name": "Mingda Chen", "authorId": "46221498"}, {"name": "Weijia Shi", "authorId": "2254168373"}, {"name": "Maria Lomeli", "authorId": "2253400960"}, {"name": "Rich James", "authorId": "2191899140"}, {"name": "Pedro Rodriguez", "authorId": "2253404757"}, {"name": "Jacob Kahn", "authorId": "2253401183"}, {"name": "Gergely Szilvasy", "authorId": "2253402270"}, {"name": "Mike Lewis", "authorId": "2253417398"}, {"name": "Luke S. Zettlemoyer", "authorId": "2137813791"}, {"name": "Scott Yih", "authorId": "2253400757"}], "n_citations": 153}, "snippets": ["Retrieval-Augmented Language Models RALMs fuse language models (LMs) with a retrieval module that explicitly augments the LM with external knowledge stores (Guu et al., 2020;Lewis et al., 2020). One mainstream type of RALM follows the \"retrieveand-read\" paradigm, where the retrieval module supplies external knowledge as additional context which the LM (reader) leverages to produce the final output (Izacard et al., 2022b;Borgeaud et al., 2022;Shi et al., 2023b;Ram et al., 2023)."], "score": 0.97998046875}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 272310589, "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Weijie Liu", "authorId": "2319081035"}, {"name": "Zecheng Tang", "authorId": "1576234850"}, {"name": "Juntao Li", "authorId": "2257093356"}, {"name": "Kehai Chen", "authorId": "2266796043"}, {"name": "Min Zhang", "authorId": "2258690229"}], "n_citations": 3}, "snippets": ["Much effort has been made to enhance Retrieval-Augmented Language Modeling (Lewis et al., 2020)Izacard and Grave, 2020;(Ram et al., 2023)Yu et al., 2022;Asai et al., 2023). While some approaches use external retrievers, non-parametric information fusion often falls short compared to parametric methods within the model. We concentrate on integrating retrieval concepts directly into the model. REALM (Guu et al., 2020) suggests that relying solely on internal model knowledge is inefficient and advocates for the model to learn to retrieve and comprehend. kNN-LM (Khandelwal et al., 2019) enhances language modeling by blending the LLM's next-word predictions with those from a retrieval-based mechanism. MemTrm (Wu et al., 2022) introduces a memory bank but risks shifting memory distributions due to parameter adjustments. LongMEM (Wang et al., 2023) mitigates this by training a sub-network, though this adds significant overhead."], "score": 0.96826171875}, {"id": "(Ram et al., 2023)", "paper": {"corpus_id": 256459451, "title": "In-Context Retrieval-Augmented Language Models", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Ori Ram", "authorId": "73775461"}, {"name": "Yoav Levine", "authorId": "152754428"}, {"name": "Itay Dalmedigos", "authorId": "1491822146"}, {"name": "Dor Muhlgay", "authorId": "51918041"}, {"name": "A. Shashua", "authorId": "3140335"}, {"name": "Kevin Leyton-Brown", "authorId": "2066411743"}, {"name": "Y. Shoham", "authorId": "1701353"}], "n_citations": 605}, "snippets": ["Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1"], "score": 0.0}, {"id": "(Asai et al., 2023)", "paper": {"corpus_id": 263866951, "title": "Retrieval-based Language Models and Applications", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Akari Asai", "authorId": "2290402940"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Zexuan Zhong", "authorId": "49164966"}, {"name": "Danqi Chen", "authorId": "2286629648"}], "n_citations": 88}, "snippets": ["Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks."], "score": 0.990234375}], "table": null}, {"title": "Types of Retrieval Models", "tldr": "Retrieval models can be categorized into sparse, dense, and hybrid approaches. Sparse models rely on term-matching and inverted indices, while dense models use neural embeddings to capture semantic relationships, with specific architectures including bi-encoders, cross-encoders, and late interaction models. (17 sources)", "text": "\nRetrieval models have evolved significantly over time, transitioning from traditional lexical-matching approaches to neural-based methods. These models can be broadly categorized into several types:\n\n## Sparse Retrieval Models\n* **Traditional Sparse Models**: Classic approaches like TF-IDF and BM25 utilize bag-of-words representations and rely on exact lexical matching between query and document terms <Paper corpusId=\"276421942\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"269302879\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"207178704\" paperTitle=\"(Robertson et al., 2009)\" isShortName></Paper>.\n* **Neural Sparse Models**: More recent sparse retrieval models like SPLADE enhance traditional approaches by applying neural techniques while maintaining the efficiency of inverted indexes <Paper corpusId=\"258676146\" paperTitle=\"(Weller et al., 2023)\" isShortName></Paper> <Paper corpusId=\"235792467\" paperTitle=\"(Formal et al., 2021)\" isShortName></Paper> <Paper corpusId=\"252212320\" paperTitle=\"(Choi et al., 2022)\" isShortName></Paper>.\n\n## Dense Retrieval Models\nDense retrievers represent documents and queries as continuous vector representations, capturing semantic relationships beyond exact matching. These models can be further divided into several architectures:\n\n* **Bi-Encoders (Dual Encoders)**: These models independently encode queries and documents into single vector representations, comparing them using simple similarity metrics like dot product or cosine similarity <Paper corpusId=\"267377589\" paperTitle=\"(Liu et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"258676146\" paperTitle=\"(Weller et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268856885\" paperTitle=\"(Gomez et al., 2024)\" isShortName></Paper>. Examples include DPR <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>, SentenceTransformer <Paper corpusId=\"201646309\" paperTitle=\"(Reimers et al., 2019)\" isShortName></Paper>, and general-purpose embeddings like E5 and Contriever <Paper corpusId=\"263835099\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\n* **Cross-Encoders**: These models process the query and document together through a single encoder, enabling more complex interaction patterns but at higher computational cost <Paper corpusId=\"269302879\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275337144\" paperTitle=\"(Ros et al., 2025)\" isShortName></Paper> <Paper corpusId=\"252993059\" paperTitle=\"(Zhuang et al., 2022)\" isShortName></Paper>.\n\n* **Late Interaction Models**: Models like ColBERT represent a middle ground, encoding queries and documents into multiple vectors (one per token) and computing fine-grained interactions between them at query time <Paper corpusId=\"258676146\" paperTitle=\"(Weller et al., 2023)\" isShortName></Paper> <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>.\n\n## Other Retrieval Model Types\n* **Matching-focused Models**: Models like MatchPLM, MatchPyramid, and DRMM focus specifically on capturing matching patterns between queries and documents <Paper corpusId=\"252781830\" paperTitle=\"(Wang, 2022)\" isShortName></Paper>. For example, DRMM builds word-level similarity matrices and uses histogram mapping functions <Paper corpusId=\"5688521\" paperTitle=\"(Guo et al., 2016)\" isShortName></Paper>, while MatchPyramid views the matching matrix as an image and applies convolutional neural networks to identify matching patterns <Paper corpusId=\"3993933\" paperTitle=\"(Pang et al., 2016)\" isShortName></Paper>.\n\n* **Task-Specific Retrievers**: Some retrievers are designed for specific tasks, such as knowledge enhancement (AAR) or in-context learning (LLM-R), optimizing for particular use cases rather than general-purpose retrieval <Paper corpusId=\"263835099\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\nEach model type offers different trade-offs between effectiveness, efficiency, and computational requirements. While dense retrievers have shown superior performance in capturing semantic relationships, sparse models maintain advantages in efficiency and interpretability, leading to ongoing research in both areas and hybrid approaches.", "citations": [{"id": "(Liu et al., 2025)", "paper": {"corpus_id": 276421942, "title": "HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hao Liu", "authorId": "2345972557"}, {"name": "Zhengren Wang", "authorId": "2288675277"}, {"name": "Xi Chen", "authorId": "2346461266"}, {"name": "Zhiyu Li", "authorId": "2268429641"}, {"name": "Feiyu Xiong", "authorId": "2268399953"}, {"name": "Qinhan Yu", "authorId": "2289597580"}, {"name": "Wentao Zhang", "authorId": "2344098350"}], "n_citations": 4}, "snippets": ["Retrieval models have evolved from early sparse retrievers, such as TF-IDF (Jones, 1973) and BM25 (Robertson and Zaragoza, 2009b), which rely on word statistics and inverted indices, to dense retrievers (Lewis et al., 2020b) that utilize neural representations for semantic matching."], "score": 0.92578125}, {"id": "(Zeng et al., 2024)", "paper": {"corpus_id": 269302879, "title": "Planning Ahead in Generative Retrieval: Guiding Autoregressive Generation through Simultaneous Decoding", "year": 2024, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Hansi Zeng", "authorId": "2029235362"}, {"name": "Chen Luo", "authorId": "2294680636"}, {"name": "Hamed Zamani", "authorId": "2257027392"}], "n_citations": 16}, "snippets": ["With the emergence of large language models (LLMs) [14,(Devlin et al., 2019)39,43,(Raffel et al., 2019) and large-scale information retrieval datasets [4,(Kwiatkowski et al., 2019), neural-based IR models have demonstrated superior results over the traditional lexical-matching models, such as BM25 (Robertson et al., 2009).In general, these IR models can fall into three categories: (1) cross-encoder models [42,48,(Zhuang et al., 2022), (2) dense retrieval models [22,(Hofst\u00e4tter et al., 2021)(Karpukhin et al., 2020)(Khattab et al., 2020)38,(Zeng et al., 2022), and (3) sparse retrieval models [12](Choi et al., 2022)18,(Formal et al., 2021)."], "score": 0.9921875}, {"id": "(Robertson et al., 2009)", "paper": {"corpus_id": 207178704, "title": "The Probabilistic Relevance Framework: BM25 and Beyond", "year": 2009, "venue": "Foundations and Trends in Information Retrieval", "authors": [{"name": "S. Robertson", "authorId": "144430625"}, {"name": "H. Zaragoza", "authorId": "2833561"}], "n_citations": 3760}, "snippets": ["The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters."], "score": 0.0}, {"id": "(Weller et al., 2023)", "paper": {"corpus_id": 258676146, "title": "NevIR: Negation in Neural Information Retrieval", "year": 2023, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Orion Weller", "authorId": "47433471"}, {"name": "Dawn J Lawrie", "authorId": "2539674"}, {"name": "Benjamin Van Durme", "authorId": "7536576"}], "n_citations": 20}, "snippets": ["We evaluate a wide variety of models in order to show a comprehensive evaluation across common neural IR model types. We note that although there are other models we do not use (as well as many different strategies for model training), all the major types of retrieval models are accounted for here. We evaluate on the following IR model categories: \n\nSparse We evaluate sparse IR models that use the bag-of-words representation during retrieval. This includes TF-IDF (the only non-neural IR method, here as a baseline), and two variants of SPLADE v2++ (Formal et al., 2022(Formal et al., , 2021;;Lassance and Clinchant, 2022), the ensemble distillation and selfdistillation methods.\n\nLate Interaction Late interaction models like ColBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022b) embed documents and queries into one vector for each sub-word token. At inference time, these models need to compute a MaxSim operation between query vectors and document vectors to determine similarity. We use both ColBERT v1 and v2 in our experiments.\n\ni-Encoders Another common category of IR models are bi-encoders, which embed both documents and queries into a single vector representation. At inference time the similarity is computed via a simple dot product or cosine similarity. Due to the popularity of this category, we include a broad spectrum: models from Sen-tenceTransformer (Reimers and Gurevych, 2019) trained on MSMarco and/or Natural Questions, DPR (Karpukhin et al., 2020), CoCondenser (Gao and Callan, 2022), and RocketQA (Qu et al., 2021;Ren et al., 2021)."], "score": 0.92138671875}, {"id": "(Formal et al., 2021)", "paper": {"corpus_id": 235792467, "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "year": 2021, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Thibault Formal", "authorId": "1630412772"}, {"name": "Benjamin Piwowarski", "authorId": "1703777"}, {"name": "S. Clinchant", "authorId": "2207074"}], "n_citations": 325}, "snippets": ["In neural Information Retrieval, ongoing research is directed towards improving the first retriever in ranking pipelines. Learning dense embeddings to conduct retrieval using efficient approximate nearest neighbors methods has proven to work well. Meanwhile, there has been a growing interest in learning sparse representations for documents and queries, that could inherit from the desirable properties of bag-of-words models such as the exact matching of terms and the efficiency of inverted indexes. In this work, we present a new first-stage ranker based on explicit sparsity regularization and a log-saturation effect on term weights, leading to highly sparse representations and competitive results with respect to state-of-the-art dense and sparse methods. Our approach is simple, trained end-to-end in a single stage. We also explore the trade-off between effectiveness and efficiency, by controlling the contribution of the sparsity regularization."], "score": 0.0}, {"id": "(Choi et al., 2022)", "paper": {"corpus_id": 252212320, "title": "SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval", "year": 2022, "venue": "International Conference on Information and Knowledge Management", "authors": [{"name": "Eunseong Choi", "authorId": "115153451"}, {"name": "Sunkyung Lee", "authorId": "2154280101"}, {"name": "Minjin Choi", "authorId": "2111881386"}, {"name": "Hyeseon Ko", "authorId": "2184725617"}, {"name": "Young-In Song", "authorId": "1693485"}, {"name": "Jongwuk Lee", "authorId": "1865093"}], "n_citations": 17}, "snippets": ["Sparse document representations have been widely used to retrieve relevant documents via exact lexical matching. Owing to the pre-computed inverted index, it supports fast ad-hoc search but incurs the vocabulary mismatch problem. Although recent neural ranking models using pre-trained language models can address this problem, they usually require expensive query inference costs, implying the trade-off between effectiveness and efficiency. Tackling the trade-off, we propose a novel uni-encoder ranking model, Sparse retriever using a Dual document Encoder (SpaDE), learning document representation via the dual encoder. Each encoder plays a central role in (i) adjusting the importance of terms to improve lexical matching and (ii) expanding additional terms to support semantic matching. Furthermore, our co-training strategy trains the dual encoder effectively and avoids unnecessary intervention in training each other. Experimental results on several benchmarks show that SpaDE outperforms existing uni-encoder ranking models."], "score": 0.0}, {"id": "(Liu et al._1, 2024)", "paper": {"corpus_id": 267377589, "title": "An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models", "year": 2024, "venue": "ACM Trans. Inf. Syst.", "authors": [{"name": "Qi Liu", "authorId": "2260822142"}, {"name": "Gang Guo", "authorId": "2163425408"}, {"name": "Jiaxin Mao", "authorId": "2265811336"}, {"name": "Zhicheng Dou", "authorId": "2273086037"}, {"name": "Ji-Rong Wen", "authorId": "2260701602"}, {"name": "Hao Jiang", "authorId": "2279223883"}, {"name": "Xinyu Zhang", "authorId": "2282242196"}, {"name": "Zhao Cao", "authorId": "2282520819"}], "n_citations": 7}, "snippets": ["Neural Dense Retrieval Models use dense vectors to represent queries and documents. Two widely adopted architectures of dense retrieval models are cross-encoders and bi-encoders."], "score": 0.97900390625}, {"id": "(Gomez et al., 2024)", "paper": {"corpus_id": 268856885, "title": "Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems", "year": 2024, "venue": "International Workshop on Spoken Language Translation", "authors": [{"name": "Frank Palma Gomez", "authorId": "2294562390"}, {"name": "Ramon Sanabria", "authorId": "2294572013"}, {"name": "Yun-hsuan Sung", "authorId": "2294570198"}, {"name": "Daniel Cer", "authorId": "2266238595"}, {"name": "Siddharth Dalmia", "authorId": "35186886"}, {"name": "Gustavo Hern\u00e1ndez Abrego", "authorId": "2124016663"}], "n_citations": 4}, "snippets": ["Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining."], "score": 0.9208984375}, {"id": "(Karpukhin et al., 2020)", "paper": {"corpus_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ledell Yu Wu", "authorId": "51183248"}, {"name": "Sergey Edunov", "authorId": "2068070"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 3794}, "snippets": ["Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."], "score": 0.0}, {"id": "(Reimers et al., 2019)", "paper": {"corpus_id": 201646309, "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "year": 2019, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Nils Reimers", "authorId": "2959414"}, {"name": "Iryna Gurevych", "authorId": "1730400"}], "n_citations": 12316}, "snippets": ["BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."], "score": 0.0}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 263835099, "title": "Retrieve Anything To Augment Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Peitian Zhang", "authorId": "2153419738"}, {"name": "Shitao Xiao", "authorId": "2051175765"}, {"name": "Zheng Liu", "authorId": "2240687341"}, {"name": "Zhicheng Dou", "authorId": "2257039188"}, {"name": "Jian-Yun Nie", "authorId": "2261086903"}], "n_citations": 63}, "snippets": ["In practice, there are two common types of retrievers. One is to leverage the general purpose retrievers, such as sparse models like BM25 [69], and dense models, like DPR [37], contriever [30], E5 [81], BGE [89], OpenAI text embedding [56]. The other option is develop task-specific retriever, e.g., AAR for knowledge enhancement [96], LLM-R [85] for in-context learning. The general purpose methods are praised for their generality and simplicity for usage, but may suffer from an inferior retrieval quality."], "score": 0.94189453125}, {"id": "(Ros et al., 2025)", "paper": {"corpus_id": 275337144, "title": "Interactive Information Need Prediction with Intent and Context", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Kevin Ros", "authorId": "79769007"}, {"name": "Dhyey Pandya", "authorId": "2338832917"}, {"name": "ChengXiang Zhai", "authorId": "2253607011"}], "n_citations": 0}, "snippets": ["We choose these methods because they are commonly used in the retrieval literature, with bi-encoders often used in the initial retrieval stage [25], and cross-encoders often used in the reranking stage (Reimers et al., 2019)."], "score": 0.97509765625}, {"id": "(Zhuang et al., 2022)", "paper": {"corpus_id": 252993059, "title": "RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Honglei Zhuang", "authorId": "39371343"}, {"name": "Zhen Qin", "authorId": "2099586642"}, {"name": "R. Jagerman", "authorId": "1886219"}, {"name": "Kai Hui", "authorId": "47214884"}, {"name": "Ji Ma", "authorId": "2109919783"}, {"name": "Jing Lu", "authorId": "2115404379"}, {"name": "Jianmo Ni", "authorId": "2148023"}, {"name": "Xuanhui Wang", "authorId": "1526973500"}, {"name": "Michael Bendersky", "authorId": "1815447"}], "n_citations": 140}, "snippets": ["Pretrained language models such as BERT have been shown to be exceptionally effective for text ranking. However, there are limited studies on how to leverage more powerful sequence-to-sequence models such as T5. Existing attempts usually formulate text ranking as a classification problem and rely on postprocessing to obtain a ranked list. In this paper, we propose RankT5 and study two T5-based ranking model structures, an encoder-decoder and an encoder-only one, so that they not only can directly output ranking scores for each query-document pair, but also can be fine-tuned with pairwise or listwise ranking losses to optimize ranking performance. Our experiments show that the proposed models with ranking losses can achieve substantial ranking performance gains on different public text ranking data sets. Moreover, ranking models fine-tuned with listwise ranking losses have better zero-shot ranking performance on out-of-domain data than models fine-tuned with classification losses."], "score": 0.0}, {"id": "(Khattab et al., 2020)", "paper": {"corpus_id": 216553223, "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "year": 2020, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "O. Khattab", "authorId": "144112155"}, {"name": "M. Zaharia", "authorId": "143834867"}], "n_citations": 1377}, "snippets": ["Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."], "score": 0.0}, {"id": "(Wang, 2022)", "paper": {"corpus_id": 252781830, "title": "Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval", "year": 2022, "venue": "IEEE International Joint Conference on Neural Network", "authors": [{"name": "Zhaowei Wang", "authorId": "2144715839"}], "n_citations": 12}, "snippets": ["Common deep retrieval models include: \n\n\u2022 MatchPLM: MatchPLM (Yang et al., 2020) uses a PLM to encode both queries and candidates. A dense layer is used to compute similarities between representations of queries and candidates. \u2022 MatchPyramid: MatchPyramid (Pang et al., 2016) is an interactionfocused model, which utilizes a CNN layer to capture matching patterns on a word-level similarity matrix. \u2022 DRMM: DRMM (Guo et al., 2016) also builds a word-level similarity matrix and uses a histogram mapping function. All histograms are transformed by a feed forward network and selected by a gating network to produce the final representation."], "score": 0.94677734375}, {"id": "(Guo et al., 2016)", "paper": {"corpus_id": 5688521, "title": "A Deep Relevance Matching Model for Ad-hoc Retrieval", "year": 2016, "venue": "International Conference on Information and Knowledge Management", "authors": [{"name": "J. Guo", "authorId": "1777025"}, {"name": "Yixing Fan", "authorId": "7888704"}, {"name": "Qingyao Ai", "authorId": "144922928"}, {"name": "W. Bruce Croft", "authorId": "144456145"}], "n_citations": 880}, "snippets": ["In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models."], "score": 0.0}, {"id": "(Pang et al., 2016)", "paper": {"corpus_id": 3993933, "title": "Text Matching as Image Recognition", "year": 2016, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Liang Pang", "authorId": "48537499"}, {"name": "Yanyan Lan", "authorId": "37510256"}, {"name": "J. Guo", "authorId": "1777025"}, {"name": "Jun Xu", "authorId": "39474114"}, {"name": "Shengxian Wan", "authorId": "2019211"}, {"name": "Xueqi Cheng", "authorId": "1717004"}], "n_citations": 558}, "snippets": ["Matching two texts is a fundamental problem in many natural language processing tasks. An effective way is to extract meaningful matching patterns from words, phrases, and sentences to produce the matching score. Inspired by the success of convolutional neural network in image recognition, where neurons can capture many complicated patterns based on the extracted elementary visual patterns such as oriented edges and corners, we propose to model text matching as the problem of image recognition. Firstly, a matching matrix whose entries represent the similarities between words is constructed and viewed as an image. Then a convolutional neural network is utilized to capture rich matching patterns in a layer-by-layer way. We show that by resembling the compositional hierarchies of patterns in image recognition, our model can successfully identify salient signals such as n-gram and n-term matchings. Experimental results demonstrate its superiority against the baselines."], "score": 0.0}], "table": null}, {"title": "Architectural Components of Retrieval Language Models", "tldr": "Retrieval Language Models comprise essential components including language modeling frameworks, retrieval mechanisms, and neural architectures that work together to enable effective information retrieval. These components have evolved from traditional probabilistic approaches to transformer-based architectures that support various query-document matching paradigms. (16 sources)", "text": "\nRetrieval Language Models (RLMs) integrate several key architectural components that enable effective information retrieval across various applications. At their foundation, many retrieval models utilize language modeling frameworks where documents are represented as probabilistic language models, and relevance is determined by the likelihood that a document would generate the terms of a query <Paper corpusId=\"13892149\" paperTitle=\"(Borg, 2016)\" isShortName></Paper>.\n\n## Language Modeling Components\n\nTraditional language modeling approaches for retrieval often start with maximum likelihood estimation of term probabilities, which are then smoothed to account for unseen terms and adjust for common terms in the corpus <Paper corpusId=\"18819434\" paperTitle=\"(Erkan, 2006)\" isShortName></Paper>. Smoothing techniques, such as Bayesian smoothing with Dirichlet prior, play a crucial role in preventing zero probabilities for unseen terms and providing TF-IDF-like effects <Paper corpusId=\"18819434\" paperTitle=\"(Erkan, 2006)\" isShortName></Paper>.\n\nMore advanced statistical generative models have been developed to better capture linguistic phenomena. For example, the multivariate P\u00f3lya urn model (SPUD) improves upon traditional multinomial language models by accounting for term dependency and word burstiness\u2014the tendency of terms to repeat within documents <Paper corpusId=\"21012910\" paperTitle=\"(Cummins, 2017)\" isShortName></Paper> <Paper corpusId=\"9519540\" paperTitle=\"(Cummins et al., 2015)\" isShortName></Paper>.\n\n## Neural Architectures for Retrieval\n\nModern retrieval models have increasingly incorporated neural architectures, particularly transformer-based pre-trained language models, to better capture query-document relevance <Paper corpusId=\"256389465\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>. These architectures can be categorized into several types based on how they process queries and documents:\n\n1. **Cross-Encoders**: These models process query and document pairs together with full cross-attention between all tokens, providing high accuracy but at a computational cost that makes indexing challenging <Paper corpusId=\"257232958\" paperTitle=\"(Scao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\n2. **Dual/Bi-Encoders**: This architecture independently encodes queries and documents into vector representations, enabling efficient retrieval through vector similarity computations like inner products <Paper corpusId=\"256389465\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>. These models form the foundation for dense retrieval systems that allow pre-computation and indexing of document representations.\n\n3. **Hybrid Approaches**: Some systems combine cross-encoders and bi-encoders to achieve a balance between accuracy and efficiency <Paper corpusId=\"257232958\" paperTitle=\"(Scao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"222177208\" paperTitle=\"(Chen et al., 2020)\" isShortName></Paper>.\n\n## Training and Optimization Components\n\nRetrieval models are typically trained using contrastive learning techniques, where the objective is to minimize the distance between query and relevant document representations while maximizing the distance to irrelevant documents <Paper corpusId=\"267412330\" paperTitle=\"(Kang et al., 2024)\" isShortName></Paper>. The quality of a retrieval model can be measured by the variance of this contrastive loss, with lower variance indicating better retrieval performance <Paper corpusId=\"267412330\" paperTitle=\"(Kang et al., 2024)\" isShortName></Paper>.\n\nRecent innovations include the Differentiable Search Index (DSI) paradigm, which encodes corpus information directly within the parameters of a transformer model <Paper corpusId=\"269921622\" paperTitle=\"(He et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246863488\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>. This approach uses \"Learn to Index\" and \"Learn to Retrieve\" training tasks, potentially simplifying the retrieval process by eliminating the need for separate indexing structures <Paper corpusId=\"269921622\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>.\n\n## Model Types and Attention Mechanisms\n\nWhile traditional retrieval models often utilize bidirectional attention mechanisms like those in BERT, newer approaches are exploring decoder-style large language models (LLMs) with unidirectional attention for retrieval tasks <Paper corpusId=\"277621440\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"263908865\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>. These LLM-based retrievers have shown strong performance and generalizability across various retrieval tasks <Paper corpusId=\"273654180\" paperTitle=\"(Tejaswi et al., 2024)\" isShortName></Paper>.\n\nA significant architectural consideration is the trade-off between expensive full-interaction ranking models and more efficient dual encoder approaches <Paper corpusId=\"259203489\" paperTitle=\"(Jong et al., 2023)\" isShortName></Paper>. Full-interaction models provide more comprehensive query-document matching but at higher computational costs, while dual encoders offer efficiency through pre-computed representations but may sacrifice some relevance precision <Paper corpusId=\"259203489\" paperTitle=\"(Jong et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Borg, 2016)", "paper": {"corpus_id": 13892149, "title": "Advancing Trace Recovery Evaluation - Applied Information Retrieval in a Software Engineering Context", "year": 2016, "venue": "arXiv.org", "authors": [{"name": "Markus Borg", "authorId": "145654045"}], "n_citations": 3}, "snippets": ["A subset of probabilistic retrieval estimate Language Models (LM) for each document. Documents are then ranked based on the probability that a document would generate the terms of a query (Ponte et al., 1998)."], "score": 0.994140625}, {"id": "(Erkan, 2006)", "paper": {"corpus_id": 18819434, "title": "Language Model-Based Document Clustering Using Random Walks", "year": 2006, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}], "n_citations": 44}, "snippets": ["In the language modeling framework, each document in the database defines a language model. The relevance of a document to a given query is ranked according to the generation probability of the query based on the underlying language model of the document. To induce a (unigram) language model from a document, we start with the maximum likelihood (ML) estimation of the term probabilities", "This estimation is often smoothed based on the following general formula", "where ! \" # 7 89 @ A is the ML estimation of over an entire corpus which usually is a member of. \n\n3 is the general smoothing parameter that takes different forms in various smoothing methods. Smoothing has two important roles (Zhai et al., 2004). First, it accounts for terms unseen in the document preventing zero probabilities. This is similar to the smoothing effect in NLP problems such as parsing. Second, smoothing has an \u00a4\u00a5\u00a2like effect that accounts for the generation probabilities of the common terms in the corpus. A common smoothing technique is to use Bayesian smoothing with the Dirichlet prior (Zhai et al., 2004)(Liu et al., 2004)"], "score": 0.97265625}, {"id": "(Cummins, 2017)", "paper": {"corpus_id": 21012910, "title": "Modelling Word Burstiness in Natural Language: A Generalised Polya Process for Document Language Models in Information Retrieval", "year": 2017, "venue": "arXiv.org", "authors": [{"name": "Ronan Cummins", "authorId": "3161572"}], "n_citations": 3}, "snippets": ["The general model outlined here (Eq. 1) is an intuitive statistical generative model of documents. The vector u i can be seen as storing the state of the model at a particular time i. Both the multinomial and multivariate P\u00f3lya urn (SPUD (Cummins et al., 2015)) language model are specific instances of this model and are instantiated by different settings of M. Given that the SPUD language model significantly improves upon the multinomial model in information retrieval, the further extensions hold the promise of improved performance and of greater theoretical understanding."], "score": 0.9755859375}, {"id": "(Cummins et al., 2015)", "paper": {"corpus_id": 9519540, "title": "A P\u00f3lya Urn Document Language Model for Improved Information Retrieval", "year": 2015, "venue": "ACM Trans. Inf. Syst.", "authors": [{"name": "Ronan Cummins", "authorId": "3161572"}, {"name": "Jiaul H. Paik", "authorId": "2071527"}, {"name": "Yuanhua Lv", "authorId": "40282698"}], "n_citations": 35}, "snippets": ["The multinomial language model has been one of the most effective models of retrieval for more than a decade. However, the multinomial distribution does not model one important linguistic phenomenon relating to term dependency\u2014that is, the tendency of a term to repeat itself within a document (i.e., word burstiness)."], "score": 0.96630859375}, {"id": "(Kim et al., 2023)", "paper": {"corpus_id": 256389465, "title": "EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Seungyeon Kim", "authorId": "2109548913"}, {"name": "A. Rawat", "authorId": "2241094"}, {"name": "M. Zaheer", "authorId": "1771307"}, {"name": "Sadeep Jayasumana", "authorId": "3078751"}, {"name": "Veeranjaneyulu Sadhanala", "authorId": "3148908"}, {"name": "Wittawat Jitkrittum", "authorId": "2141405"}, {"name": "A. Menon", "authorId": "2844480"}, {"name": "R. Fergus", "authorId": "2276554"}, {"name": "Surinder Kumar", "authorId": "49596260"}], "n_citations": 7}, "snippets": ["Neural models for information retrieval (IR) are increasingly used to model the true ranking function in various applications, including web search [Mitra and Craswell, 2018], recommendation [Zhang et al., 2019], and question-answering (QA) (Chen et al., 2017). Notably, the recent success of Transformers (Vaswani et al., 2017)-based pre-trained language models (Devlin et al., 2019), Liu et al., 2019(Raffel et al., 2019) on a wide range of natural language understanding tasks has also prompted their utilization in IR to capture query-document relevance [see, e.g., Dai and Callan, 2019b(MacAvaney et al., 2019), Nogueira and Cho, 2019, Lee et al., 2019(Karpukhin et al., 2020). A typical IR system comprises two stages: (1) A retriever first selects a small subset of potentially relevant candidate documents (out of a large collection) for a given query; and (2) A re-ranker then identifies a precise ranking among the candidates provided by the retriever. Dual-encoder (DE) models are the de-facto architecture for retrievers [Lee et al., 2019(Karpukhin et al., 2020). Such models independently embed queries and documents into a common space, and capture their relevance by simple operations on these embeddings such as the inner product."], "score": 0.9638671875}, {"id": "(Scao et al., 2023)", "paper": {"corpus_id": 257232958, "title": "Joint Representations of Text and Knowledge Graphs for Retrieval and Evaluation", "year": 2023, "venue": "International Joint Conference on Natural Language Processing", "authors": [{"name": "Teven Le Scao", "authorId": "1379806208"}, {"name": "Claire Gardent", "authorId": "2065132524"}], "n_citations": 2}, "snippets": ["Natural Language Retrieval Models. For natural language, a first class of retrieval models focuses on retrieving sentences that are similar to some input sentence. BERT (Devlin et al., 2019) has been used as a cross-encoder. Two sentences are given with a separator token, cross-attention applies to all input tokens and the resulting representation is fed into a linear layer to score the match. However, this is computationally inefficient as it is not possible to pre-compute and index such representations. A pre-computable model was proposed by (Reimers et al., 2019) who used twin encoders pre-trained on Natural Language Inference data (Bowman et al., 2015) to set new state-of-the-art performance on a large set of sentence scoring tasks. Further work (Chen et al., 2020)Humeau et al., 2019) combined cross-and bi-encoders to reach a tradeoff between accuracy and efficiency."], "score": 0.9619140625}, {"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(Karpukhin et al., 2020)", "paper": {"corpus_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ledell Yu Wu", "authorId": "51183248"}, {"name": "Sergey Edunov", "authorId": "2068070"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 3794}, "snippets": ["Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."], "score": 0.0}, {"id": "(Chen et al., 2020)", "paper": {"corpus_id": 222177208, "title": "DiPair: Fast and Accurate Distillation for Trillion-ScaleText Matching and Pair Modeling", "year": 2020, "venue": "Findings", "authors": [{"name": "Jiecao Chen", "authorId": "2809410"}, {"name": "Liu Yang", "authorId": "1988888746"}, {"name": "K. Raman", "authorId": "2062947723"}, {"name": "Michael Bendersky", "authorId": "1815447"}, {"name": "Jung-Jung Yeh", "authorId": "34727699"}, {"name": "Yun Zhou", "authorId": "2118116642"}, {"name": "Marc Najork", "authorId": "1763978"}, {"name": "Danyang Cai", "authorId": "1833259140"}, {"name": "Ehsan Emadzadeh", "authorId": "2465392"}], "n_citations": 29}, "snippets": ["Pre-trained models like BERT ((Devlin et al., 2018) have dominated NLP / IR applications such as single sentence classification, text pair classification, and question answering. However, deploying these models in real systems is highly non-trivial due to their exorbitant computational costs. A common remedy to this is knowledge distillation (Hinton et al., 2015), leading to faster inference. However \u2013 as we show here \u2013 existing works are not optimized for dealing with pairs (or tuples) of texts. Consequently, they are either not scalable or demonstrate subpar performance. In this work, we propose DiPair \u2014 a novel framework for distilling fast and accurate models on text pair tasks. Coupled with an end-to-end training strategy, DiPair is both highly scalable and offers improved quality-speed tradeoffs. Empirical studies conducted on both academic and real-world e-commerce benchmarks demonstrate the efficacy of the proposed approach with speedups of over 350x and minimal quality drop relative to the cross-attention teacher BERT model."], "score": 0.0}, {"id": "(Kang et al., 2024)", "paper": {"corpus_id": 267412330, "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Mintong Kang", "authorId": "2153110066"}, {"name": "Nezihe Merve Gurel", "authorId": "51274710"}, {"name": "Ning Yu", "authorId": "2282538184"}, {"name": "D. Song", "authorId": "2242706269"}, {"name": "Bo Li", "authorId": "2267398406"}], "n_citations": 22}, "snippets": ["To quantify the quality of retrieval models, we introduce the concept of V rag -retrieval model, where V rag measures the variance of the contrastive loss of the retrieval model. A small V rag implies a well-trained low-variance retrieval model and can be theoretically linked to the retrieval quality, which is measured by the number of retrieved positive examples with respect to the query text. \n\nDefinition 1 (V rag -retrieval model). Consider a retrieval model with similarity measurement s \u03b8r (\u2022, \u2022) parameterized with \u03b8 r and trained with contrastive loss L cont . Let x + , x \u2212 be positive and negative samples to sample x. Consider common contrastive loss L cont = \u2212 log (\u03c3 sig (exp{s \u03b8 (x, x \u2212 ) \u2212 exp{s \u03b8 (x, x + ))), where \u03c3 sig (\u2022) is the sigmoid function."], "score": 0.9248046875}, {"id": "(He et al., 2024)", "paper": {"corpus_id": 269921622, "title": "Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation", "year": 2024, "venue": "Web Search and Data Mining", "authors": [{"name": "Zhankui He", "authorId": "51002202"}, {"name": "Zhouhang Xie", "authorId": "2153722757"}, {"name": "Harald Steck", "authorId": "2290487114"}, {"name": "Dawen Liang", "authorId": "2261449339"}, {"name": "Rahul Jha", "authorId": "2302321905"}, {"name": "Nathan Kallus", "authorId": "3174388"}, {"name": "Julian McAuley", "authorId": "2258552056"}], "n_citations": 7}, "snippets": ["The transformer model has shown proficiency in retrieval tasks, encoding item information within its parameters, which is a method termed Differentiable Search Index (DSI) (Tay et al., 2022).DSI involves two key training tasks for pre-trained language models: Learn to Index (L2I) and Learn to Retrieve (L2R), which can be used to train a model jointly or in a sequential order", "Compared to common two-tower models, DSI models require only a single model for item recommendations, by indexing item information into its parameters (Tay et al., 2022)."], "score": 0.92626953125}, {"id": "(Tay et al., 2022)", "paper": {"corpus_id": 246863488, "title": "Transformer Memory as a Differentiable Search Index", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yi Tay", "authorId": "144447820"}, {"name": "Vinh Q. Tran", "authorId": "2057663102"}, {"name": "Mostafa Dehghani", "authorId": "3226635"}, {"name": "Jianmo Ni", "authorId": "2148023"}, {"name": "Dara Bahri", "authorId": "2119725651"}, {"name": "Harsh Mehta", "authorId": "18138802"}, {"name": "Zhen Qin", "authorId": "145144957"}, {"name": "Kai Hui", "authorId": "47214884"}, {"name": "Zhe Zhao", "authorId": "48634137"}, {"name": "Jai Gupta", "authorId": "143702064"}, {"name": "Tal Schuster", "authorId": "32303439"}, {"name": "William W. Cohen", "authorId": "50056360"}, {"name": "Donald Metzler", "authorId": "1680617"}], "n_citations": 286}, "snippets": ["In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup."], "score": 0.0}, {"id": "(Zhang et al., 2025)", "paper": {"corpus_id": 277621440, "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hengran Zhang", "authorId": "2260353683"}, {"name": "Keping Bi", "authorId": "2249759496"}, {"name": "Jiafeng Guo", "authorId": "2316783018"}, {"name": "Xiaojie Sun", "authorId": "2269761948"}, {"name": "Shihao Liu", "authorId": "2310512864"}, {"name": "Daiting Shi", "authorId": "2104450297"}, {"name": "Dawei Yin", "authorId": "2310342915"}, {"name": "Xueqi Cheng", "authorId": "2244825947"}], "n_citations": 0}, "snippets": ["Large language models (LLMs) are being widely applied across various fields [27,54,61], and garnering increasing attention for their application in retrieval tasks [25,(Ma et al., 2023)[51]. Unlike bidirectional attention mechanisms in encoder-style pre-trained language models (PLMs) such as BERT, LLMs are typically decoder-style models that employ unidirectional attention."], "score": 0.98095703125}, {"id": "(Ma et al., 2023)", "paper": {"corpus_id": 263908865, "title": "Fine-Tuning LLaMA for Multi-Stage Text Retrieval", "year": 2023, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Xueguang Ma", "authorId": "2461713"}, {"name": "Liang Wang", "authorId": "145769448"}, {"name": "Nan Yang", "authorId": "2242947624"}, {"name": "Furu Wei", "authorId": "2257346447"}, {"name": "Jimmy Lin", "authorId": "2257085301"}], "n_citations": 223}, "snippets": ["While large language models (LLMs) have shown impressive NLP capabilities, existing IR applications mainly focus on prompting LLMs to generate query expansions or generating permutations for listwise reranking. In this study, we leverage LLMs directly to serve as components in the widely used multi-stage text ranking pipeline. Specifically, we fine-tune the open-source LLaMA-2 model as a dense retriever (repLLaMA) and a pointwise reranker (rankLLaMA). This is performed for both passage and document retrieval tasks using the MS MARCO training data. Our study shows that finetuned LLM retrieval models outperform smaller models. They are more effective and exhibit greater generalizability, requiring only a straightforward training strategy. Moreover, our pipeline allows for the fine-tuning of LLMs at each stage of a multi-stage retrieval pipeline. This demonstrates the strong potential for optimizing LLMs to enhance a variety of retrieval tasks. Furthermore, as LLMs are naturally pre-trained with longer contexts, they can directly represent longer documents. This eliminates the need for heuristic segmenting and pooling strategies to rank long documents. On the MS MARCO and BEIR datasets, our repLLaMA-rankLLaMA pipeline demonstrates a high level of effectiveness."], "score": 0.0}, {"id": "(Tejaswi et al., 2024)", "paper": {"corpus_id": 273654180, "title": "RARe: Retrieval Augmented Retrieval with In-Context Examples", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Atula Tejaswi", "authorId": "2303846034"}, {"name": "Yoonsang Lee", "authorId": "2269690359"}, {"name": "Sujay Sanghavi", "authorId": "2303847572"}, {"name": "Eunsol Choi", "authorId": "2304136439"}], "n_citations": 1}, "snippets": ["Retrieval Large language models pre-trained with autoregressive setups (Jiang et al., 2023;Dubey et al., 2024) have shown remarkable performance when adapted to retrieval tasks (Wang et al., 2024b;BehnamGhader et al., 2024), outperforming encoder-style retrievers (Izacard et al., 2021)Wang et al., 2024a)."], "score": 0.9208984375}, {"id": "(Jong et al., 2023)", "paper": {"corpus_id": 259203489, "title": "GLIMMER: generalized late-interaction memory reranker", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Michiel de Jong", "authorId": "21379393"}, {"name": "Yury Zemlyanskiy", "authorId": "2220287479"}, {"name": "Nicholas FitzGerald", "authorId": "143883142"}, {"name": "Sumit K. Sanghai", "authorId": "144074891"}, {"name": "William W. Cohen", "authorId": "50056360"}, {"name": "J. Ainslie", "authorId": "1643737606"}], "n_citations": 5}, "snippets": ["retrieval procedures face a trade-off between expensive ranking with full interaction (Chen et al., 2020) and the more common dual encoder approaches such as DPR (Karpukhin et al., 2020) and GTR (Ni et al., 2021) that scores based on inner product similarity with a corpus of pre-computed passage representations."], "score": 0.93701171875}], "table": null}, {"title": "Applications and Performance", "tldr": "Retrieval Language Models have demonstrated strong performance across various NLP applications including question answering, language generation, semantic textual similarity, and information retrieval. Their effectiveness stems from combining transformer-based architectures with retrieval mechanisms that enhance factual accuracy and domain adaptation capabilities. (16 sources)", "text": "\nRetrieval Language Models (RLMs) have been successfully applied to a wide range of natural language processing tasks, showcasing their versatility and effectiveness in different application domains.\n\n## Question Answering and Knowledge-Intensive Tasks\n\nRetrieval-augmented approaches have achieved particularly impressive results on open-domain question answering tasks. Models like REALM have demonstrated how unsupervised pre-training of knowledge retrievers, using masked language modeling as the learning signal, can significantly improve performance on challenging question answering benchmarks <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper>. Similarly, Retrieval-Augmented Generation (RAG) models have set state-of-the-art results on multiple open-domain QA tasks by combining pre-trained parametric and non-parametric memory, outperforming both traditional parametric sequence-to-sequence models and task-specific retrieve-and-extract architectures <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>.\n\n## Language Modeling and Text Generation\n\nIn language modeling, kNN-LM has established new state-of-the-art performance by extending pre-trained neural language models with k-nearest neighbors interpolation <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>. This approach has been particularly effective for predicting rare patterns and factual knowledge, suggesting that similarity-based retrieval provides significant advantages for language modeling in the long tail of data distributions.\n\nTransformer-based architectures form the foundation of most modern RLMs, with different variants optimized for specific tasks. BERT-based masked language modeling exploits bidirectional information for effective sequence encoding, while causal language models like GPT-2 are unidirectional and better suited for text generation <Paper corpusId=\"253384615\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper> <Paper corpusId=\"198953378\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>. Some innovative approaches attempt to combine the strengths of both paradigms through techniques such as augmenting suffix embeddings during sequence generation <Paper corpusId=\"253384615\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\n## Semantic Understanding and Similarity Tasks\n\nFor semantic understanding tasks, retrieval models frequently utilize specialized pre-trained encoders. SimCSE has emerged as an effective model for producing semantically meaningful sentence embeddings through contrastive learning approaches <Paper corpusId=\"233296292\" paperTitle=\"(Gao et al., 2021)\" isShortName></Paper> <Paper corpusId=\"211096730\" paperTitle=\"(Chen et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"207930212\" paperTitle=\"(He et al., 2019)\" isShortName></Paper>. These embeddings enable high-quality retrieval based on semantic similarity rather than just lexical matching, which is crucial for tasks like semantic textual similarity and information retrieval.\n\n## Multimodal Applications\n\nRetrieval models have also been extended to multimodal domains, particularly for tasks involving images and text. Several approaches associate visual and language modalities by embedding their representations into a common space, enabling bidirectional retrieval between images and sentences <Paper corpusId=\"8486003\" paperTitle=\"(Tan et al., 2016)\" isShortName></Paper>. These methods typically use CNNs to extract image features and language models to represent sentences, embedding both into a shared multimodal space and optimizing the model with ranking objectives <Paper corpusId=\"2315434\" paperTitle=\"(Karpathy et al., 2014)\" isShortName></Paper> <Paper corpusId=\"2317858\" paperTitle=\"(Socher et al., 2014)\" isShortName></Paper> <Paper corpusId=\"928608\" paperTitle=\"(Hodosh et al., 2013)\" isShortName></Paper>.\n\n## Domain Adaptation and Specialization\n\nOne of the key advantages of retrieval-augmented models is their ability to adapt to specific domains without extensive retraining. By simply varying the nearest neighbor datastore, kNN-LM has demonstrated effective domain adaptation capabilities without requiring further training <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>. This flexibility makes RLMs particularly valuable for specialized applications where domain-specific knowledge is crucial.\n\nFor specialized domains like recommendation systems, dedicated pre-trained language models such as BLaIR have been developed <Paper corpusId=\"271891894\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>, while general-purpose language models like BERT, RoBERTa, and BART continue to serve as strong foundations for retrieval tasks across domains.\n\n## Document Retrieval and Factual Answers\n\nDocument retrieval approaches have shown that large language models can provide more accurate factual answers when augmented with retrieved content. These systems retrieve relevant documents based on semantic relevance between user queries and documents, then concatenate these documents to the prompt input, enabling LLMs to better answer factual questions <Paper corpusId=\"269605025\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This approach has become increasingly important as language models scale up, with larger models demonstrating emergent abilities that cannot be predicted by extrapolating from smaller models <Paper corpusId=\"249674500\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Guu et al., 2020)", "paper": {"corpus_id": 211204736, "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Kelvin Guu", "authorId": "2091768"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Zora Tung", "authorId": "9941702"}, {"name": "Panupong Pasupat", "authorId": "2616463"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}], "n_citations": 2119}, "snippets": ["Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."], "score": 0.0}, {"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."], "score": 0.0}, {"id": "(Khandelwal et al., 2019)", "paper": {"corpus_id": 207870430, "title": "Generalization through Memorization: Nearest Neighbor Language Models", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "Dan Jurafsky", "authorId": "1746807"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "M. Lewis", "authorId": "35084211"}], "n_citations": 842}, "snippets": ["We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."], "score": 0.0}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 253384615, "title": "Suffix Retrieval-Augmented Language Modeling", "year": 2022, "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "authors": [{"name": "Zecheng Wang", "authorId": "70452651"}, {"name": "Yik-Cheung Tam", "authorId": "1789138"}], "n_citations": 1}, "snippets": ["Most recent development in language modeling is based on transformers (Vaswani et al., 2017). BERT-based Masked language modeling (Devlin et al., 2019)(Liu et al., 2019) exploits bi-directional information of a sentence to predict the word identity of the masked tokens. While BERT is effective in encoding sequences, it is not suitable for sequence generation due to its non-causal nature. Causal language modeling such as GPT2 [11] is uni-directional. Our proposed model attempts to retain the best of the two worlds as autoregressive and simulated bi-directional via augmentation of suffix embeddings during sequence generation."], "score": 0.95947265625}, {"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(Liu et al., 2019)", "paper": {"corpus_id": 198953378, "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019, "venue": "arXiv.org", "authors": [{"name": "Yinhan Liu", "authorId": "11323179"}, {"name": "Myle Ott", "authorId": "40511414"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Jingfei Du", "authorId": "3048577"}, {"name": "Mandar Joshi", "authorId": "144863691"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Veselin Stoyanov", "authorId": "1759422"}], "n_citations": 24556}, "snippets": ["Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."], "score": 0.0}, {"id": "(Gao et al., 2021)", "paper": {"corpus_id": 233296292, "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Tianyu Gao", "authorId": "4800645"}, {"name": "Xingcheng Yao", "authorId": "2087141625"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 3413}, "snippets": ["This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman\u2019s correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show\u2014both theoretically and empirically\u2014that contrastive learning objective regularizes pre-trained embeddings\u2019 anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available."], "score": 0.0}, {"id": "(Chen et al._1, 2020)", "paper": {"corpus_id": 211096730, "title": "A Simple Framework for Contrastive Learning of Visual Representations", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Ting Chen", "authorId": "145358498"}, {"name": "Simon Kornblith", "authorId": "40464924"}, {"name": "Mohammad Norouzi", "authorId": "144739074"}, {"name": "Geoffrey E. Hinton", "authorId": "1695689"}], "n_citations": 18878}, "snippets": ["This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels."], "score": 0.0}, {"id": "(He et al., 2019)", "paper": {"corpus_id": 207930212, "title": "Momentum Contrast for Unsupervised Visual Representation Learning", "year": 2019, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Kaiming He", "authorId": "39353098"}, {"name": "Haoqi Fan", "authorId": "146884473"}, {"name": "Yuxin Wu", "authorId": "98264506"}, {"name": "Saining Xie", "authorId": "1817030"}, {"name": "Ross B. Girshick", "authorId": "2983898"}], "n_citations": 12131}, "snippets": ["We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks."], "score": 0.0}, {"id": "(Tan et al., 2016)", "paper": {"corpus_id": 8486003, "title": "phi-LSTM: A Phrase-Based Hierarchical LSTM Model for Image Captioning", "year": 2016, "venue": "Asian Conference on Computer Vision", "authors": [{"name": "Y. Tan", "authorId": "143692915"}, {"name": "Chee Seng Chan", "authorId": "2863960"}], "n_citations": 29}, "snippets": ["To model the relationship between image and language, some works associate both modalities by embedding their representations into a common space (Hodosh et al., 2013)(Frome et al., 2013)(Socher et al., 2014)(Karpathy et al., 2014). First, they obtain the image features using a visual model like CNN (Frome et al., 2013)(Socher et al., 2014), as well as the representation of sentence with a language model such as recursive neural network (Socher et al., 2014). Then, both of them are embedded into a common multimodal space and the whole model is learned with ranking objective for image and sentence retrieval task. This framework was also tested at object level by Karpathy et al. (Karpathy et al., 2014) and proved to yield better results for the image and sentence bi-directional retrieval task."], "score": 0.93896484375}, {"id": "(Karpathy et al., 2014)", "paper": {"corpus_id": 2315434, "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "year": 2014, "venue": "Neural Information Processing Systems", "authors": [{"name": "A. Karpathy", "authorId": "2354728"}, {"name": "Armand Joulin", "authorId": "2319608"}, {"name": "Li Fei-Fei", "authorId": "48004138"}], "n_citations": 937}, "snippets": ["We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit."], "score": 0.0}, {"id": "(Socher et al., 2014)", "paper": {"corpus_id": 2317858, "title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "year": 2014, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "R. Socher", "authorId": "2166511"}, {"name": "A. Karpathy", "authorId": "2354728"}, {"name": "Quoc V. Le", "authorId": "2827616"}, {"name": "Christopher D. Manning", "authorId": "144783904"}, {"name": "A. Ng", "authorId": "34699434"}], "n_citations": 897}, "snippets": ["Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image."], "score": 0.0}, {"id": "(Hodosh et al., 2013)", "paper": {"corpus_id": 928608, "title": "Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics", "year": 2013, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "Micah Hodosh", "authorId": "2170746"}, {"name": "Peter Young", "authorId": "2052690705"}, {"name": "J. Hockenmaier", "authorId": "3118681"}], "n_citations": 1325}, "snippets": ["The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated."], "score": 0.0}, {"id": "(Ren et al., 2024)", "paper": {"corpus_id": 271891894, "title": "EasyRec: Simple yet Effective Language Models for Recommendation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xubin Ren", "authorId": "2163180478"}, {"name": "Chao Huang", "authorId": "2305137970"}], "n_citations": 5}, "snippets": ["General Language Models: BERT [4], RoBERTa [21], and BART [14]; (ii) Language Models for Dense Retrieval: SimCSE [7], GTR [23], and BGE [39]; (iii) Pre-trained Language Models for Recommendation: BLaIR [12]."], "score": 0.94580078125}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 269605025, "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models", "year": 2024, "venue": "European Conference on Artificial Intelligence", "authors": [{"name": "Taolin Zhang", "authorId": "2146342371"}, {"name": "Dongyang Li", "authorId": "2257089368"}, {"name": "Qizhou Chen", "authorId": "2300139675"}, {"name": "Chengyu Wang", "authorId": "50097294"}, {"name": "Longtao Huang", "authorId": "2292090586"}, {"name": "Hui Xue", "authorId": "2292128230"}, {"name": "Xiaofeng He", "authorId": "2257159827"}, {"name": "Junyuan Huang", "authorId": "2272790856"}], "n_citations": 0}, "snippets": ["In the literature, there are two major research aspects in this field: \n\n(1) Datastore Indexing (Khandelwal et al., 2019)10,(Wei et al., 2022)[48] and (2) Document Retrieval [35,27]. For Datastore Indexing, these approaches utilize pre-trained models to generate static embeddings for documents, which are viewed as mounted external memory, and they leverage various semantic similarities to enhance indexing. For Document Retrieval, the system initially retrieves a collection of relevant documents based on the semantic relevance between the user query and the documents. Then, the LLMs concatenate these highly related documents in an unordered manner to the prompt input [4], which makes LLMs better at answering factual questions."], "score": 0.94482421875}, {"id": "(Wei et al., 2022)", "paper": {"corpus_id": 249674500, "title": "Emergent Abilities of Large Language Models", "year": 2022, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Jason Wei", "authorId": "119640649"}, {"name": "Yi Tay", "authorId": "144447820"}, {"name": "Rishi Bommasani", "authorId": "150272855"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "Dani Yogatama", "authorId": "1755465"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "Donald Metzler", "authorId": "1680617"}, {"name": "Ed H. Chi", "authorId": "2226805"}, {"name": "Tatsunori Hashimoto", "authorId": "2117567142"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "P. Liang", "authorId": "2075292388"}, {"name": "J. Dean", "authorId": "48448318"}, {"name": "W. Fedus", "authorId": "26958176"}], "n_citations": 2516}, "snippets": ["Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models."], "score": 0.0}], "table": null}, {"title": "Evolution of Retrieval Models", "tldr": "Retrieval models have evolved from statistical language modeling approaches to sophisticated neural architectures. This progression includes advancements from basic probabilistic document-query generation models to cache-based techniques accounting for word burstiness, and translation-based methods addressing lexical gaps between queries and documents. (7 sources)", "text": "\nThe evolution of retrieval models has its roots in statistical language modeling (SLM), which has been applied to information retrieval for decades. In early approaches, documents were ranked according to their probability of generating a given query, establishing a fundamental probabilistic framework for retrieval systems <Paper corpusId=\"2862121\" paperTitle=\"(Chiu et al., 2009)\" isShortName></Paper>. These basic language models assigned non-zero probabilities to terms in a language, with a document's relevance estimated as the probability that a query was generated from that document <Paper corpusId=\"90238494\" paperTitle=\"(Petersen, 2019)\" isShortName></Paper>.\n\nAs retrieval models developed, researchers identified limitations in the basic multinomial language model, particularly its inability to account for term dependency phenomena such as word burstiness\u2014the tendency of terms to repeat within documents <Paper corpusId=\"9519540\" paperTitle=\"(Cummins et al., 2015)\" isShortName></Paper>. This led to the development of cache-based models that recognized how word probability in a document is influenced not only by global frequency and N-gram context but also by local document frequencies. These models leveraged the observation that while most words are rare at the corpus level, they tend to occur in bursts when they do appear, making local estimates from a cache potentially more reliable than global estimates <Paper corpusId=\"10844118\" paperTitle=\"(Wintrode, 2015)\" isShortName></Paper>.\n\nTo improve retrieval capabilities, researchers also explored structured query representations through region expressions. These approaches used logical operators to combine probability assessments, such as producing documents containing specific terms and ranking them according to term occurrence probabilities <Paper corpusId=\"644854\" paperTitle=\"(Hiemstra et al., 2005)\" isShortName></Paper>. This introduced greater flexibility in query formulation and document matching.\n\nAnother significant advancement came through translation-based retrieval models that addressed lexical gaps between queries and documents. By viewing information retrieval as statistical document-query translation, these models introduced conditional probability distributions to map query words to document words <Paper corpusId=\"7742063\" paperTitle=\"(Lee et al., 2008)\" isShortName></Paper>. This approach effectively produced an implicit query expansion effect, allowing query words not present in a document to be mapped to related words in the document, thereby improving retrieval performance when reliable translation probability distributions were available.\n\nCross-language information retrieval further extended these concepts by decomposing the retrieval problem into components that were easier to estimate. Rather than translating a query before estimating a query model, some approaches directly estimated the query model in the target language by combining translation models with familiar probability distributions <Paper corpusId=\"5764728\" paperTitle=\"(Kraaij et al., 2003)\" isShortName></Paper>. This method effectively mapped probability distribution functions from the source language event space onto the target language vocabulary.\n\nThis evolutionary trajectory from basic statistical language models to sophisticated translation-based approaches established the foundation for the modern neural retrieval architectures discussed in previous sections, demonstrating how innovations in handling term dependencies, lexical gaps, and cross-language mapping have contributed to the current state of retrieval language models.", "citations": [{"id": "(Chiu et al., 2009)", "paper": {"corpus_id": 2862121, "title": "Optimizing Language Model Information Retrieval System with Expectation Maximization Algorithm", "year": 2009, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "J. Chiu", "authorId": "2990581"}, {"name": "Jyun-Wei Huang", "authorId": "2684491"}], "n_citations": 0}, "snippets": ["Statistical language modeling (SLM) has been used in many different domains for decades and has also been applied to information retrieval (IR) recently. Documents retrieved using this approach are ranked according their probability of generating the given query."], "score": 0.9375}, {"id": "(Petersen, 2019)", "paper": {"corpus_id": 90238494, "title": "On the Estimation and Use of Statistical Modelling in Information Retrieval", "year": 2019, "venue": "arXiv.org", "authors": [{"name": "Casper Petersen", "authorId": "8304471"}], "n_citations": 0}, "snippets": ["A language model (LM) is, in its most basic form, a probability distribution over terms in a language where each term, t is assigned a non-zero probability denoting its probability of occurrence in the \"language\". A \"language\" here is defined as a non-empty finite sequence of symbols or terms. Given a query q and document d \u2208C for some collection C , d 's LM, \u03b8 d , is a probabilistic model that estimates the probability that q was generated by d . In other words, each document is viewed as a sample from the language, and its relevance to q is estimated as the probability that q was generated from this sample."], "score": 0.98828125}, {"id": "(Cummins et al., 2015)", "paper": {"corpus_id": 9519540, "title": "A P\u00f3lya Urn Document Language Model for Improved Information Retrieval", "year": 2015, "venue": "ACM Trans. Inf. Syst.", "authors": [{"name": "Ronan Cummins", "authorId": "3161572"}, {"name": "Jiaul H. Paik", "authorId": "2071527"}, {"name": "Yuanhua Lv", "authorId": "40282698"}], "n_citations": 35}, "snippets": ["The multinomial language model has been one of the most effective models of retrieval for more than a decade. However, the multinomial distribution does not model one important linguistic phenomenon relating to term dependency\u2014that is, the tendency of a term to repeat itself within a document (i.e., word burstiness)."], "score": 0.96630859375}, {"id": "(Wintrode, 2015)", "paper": {"corpus_id": 10844118, "title": "Cache-Augmented Latent Topic Language Models for Speech Retrieval", "year": 2015, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jonathan Wintrode", "authorId": "3045640"}], "n_citations": 0}, "snippets": ["Cache-based models assume the probability of a word in a document d is influenced both by the global frequency of that word and N-gram context as well as by the N-gram frequencies of d (or preceding cache of K words). Although most words are rare at the corpus level, when they do occur, they occur in bursts. Thus a local estimate, from the cache, may be more reliable than the global estimate."], "score": 0.93359375}, {"id": "(Hiemstra et al., 2005)", "paper": {"corpus_id": 644854, "title": "A database approach to information retrieval: The remarkable relationship between language models and region models", "year": 2005, "venue": "arXiv.org", "authors": [{"name": "D. Hiemstra", "authorId": "1691929"}, {"name": "V. Mihajlovi\u0107", "authorId": "144779604"}], "n_citations": 8}, "snippets": ["The right-hand side of the equation corresponds to the following region expression. \n\n(<doc> CONTAINING db) AND (<doc> CONTAINING ir) \n\nThis can be shown as follows: The region expression (<doc> CONTAINING db) produces all documents ranked according to P (T = db|D), i.e., all regions tagged as <doc>, ranked by the number of occurrences of db in those regions. Similarly, (<doc> CONTAINING ir) produces all documents ranked according to P (T = ir|D). Finally, the operator AND results in the regions tagged as <doc> that are in both operand sets. The score of the result regions is defined as the product of the scores of the same regions in the operands."], "score": 0.931640625}, {"id": "(Lee et al., 2008)", "paper": {"corpus_id": 7742063, "title": "Bridging Lexical Gaps between Queries and Questions on Large Online Q&A Collections with Compact Translation Models", "year": 2008, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jung-Tae Lee", "authorId": "143703455"}, {"name": "Sang-Bum Kim", "authorId": "2109561184"}, {"name": "Young-In Song", "authorId": "1693485"}, {"name": "Hae-Chang Rim", "authorId": "2326357"}], "n_citations": 68}, "snippets": ["The basic language modeling framework does not address the issue of lexical gaps between queries and question. (Berger et al., 1999) viewed information retrieval as statistical document-query translation and introduced translation models to map query words to document words. Assuming that a translation model can be represented by a conditional probability distribution of translation T (\u2022|\u2022) between words, we can model P (q|M D ) in Equation 3 as: \n\nwhere w represents a document word. 3 he translation probability T (q|w) virtually represents the degree of relationship between query word q and document word w captured in a different, machine translation setting. Then, in the traditional information retrieval viewpoint, the use of translation models produce an implicit query expansion effect, since query words not in a document are mapped to related words in the document. This implies that translation-based retrieval models would make positive contributions to retrieval performance only when the pre-constructed translation models have reliable translation probability distributions."], "score": 0.97265625}, {"id": "(Kraaij et al., 2003)", "paper": {"corpus_id": 5764728, "title": "Embedding Web-Based Statistical Translation Models in Cross-Language Information Retrieval", "year": 2003, "venue": "International Conference on Computational Logic", "authors": [{"name": "Wessel Kraaij", "authorId": "1740640"}, {"name": "Jian-Yun Nie", "authorId": "143619007"}, {"name": "Michel Simard", "authorId": "144866028"}], "n_citations": 123}, "snippets": ["Instead of translating a query before estimating a query model (the external approach), we propose to directly estimate the query model in the target language. We will do this by decomposing the problem into two components that are easier to estimate: \n\nwhere L is the size of the source vocabulary. Thus, P (t i |M Qs ) can be approximated by combining the translation model P (t i |s j ), which we can estimate on the parallel Web corpus, and the familiar P (s j |M Qs ) which can be estimated using relative frequencies. This simplified model, from which we have dropped the dependency of P (t i |s j ) on Q, can be interpreted as a way of mapping the probability distribution function in the source language event space P (s j |M Qs ) onto the event space of the target language vocabulary."], "score": 0.92626953125}], "table": null}], "cost": 0.46020300000000003}}
