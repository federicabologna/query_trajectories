{"original_query": "What are the differences between encoder-decoder and causal/non-causal decoder-only LMs?", "input": "are the differences encoder decoder causal non causal decoder only LMs", "output": {"sections": [{"title": "Introduction to Language Model Architectures", "tldr": "Language models have different architectures that determine how they process and generate text. The main architectures include encoder-only, decoder-only, and encoder-decoder models, with variations in whether they process text causally (one direction) or non-causally (bidirectionally). (LLM Memory)", "text": "\nLanguage models (LMs) are neural network architectures designed to understand, process, and generate human language. These models differ primarily in their architectural design and how they handle information flow, which directly impacts their capabilities and applications.\n\nThere are three fundamental architecture types for language models:\n\n1. **Encoder-only models**: These models process input text by transforming it into contextual representations. They excel at understanding text but aren't designed for text generation. Encoders typically use bidirectional (non-causal) attention, allowing them to consider context from both before and after each word.\n\n2. **Decoder-only models**: These are specialized for text generation. They can be either causal (autoregressive), where they only look at previous tokens when generating the next one, or non-causal, where they can look in both directions. Most large language models today use causal decoder-only architectures.\n\n3. **Encoder-decoder models**: These combine both components - an encoder to understand the input and a decoder to generate output. This architecture is particularly effective for tasks requiring transformation between text sequences, such as translation or summarization.\n\nThe key distinction between causal and non-causal models is the directionality of information flow. Causal models only look at previous tokens when predicting the next one (left-to-right processing), while non-causal models can access the entire context bidirectionally. This fundamental difference shapes what tasks each architecture excels at. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Encoder Architectures", "tldr": "Encoder-only models use bidirectional attention mechanisms to understand input text comprehensively by processing context from both directions. They excel at tasks requiring deep semantic understanding but are not designed for text generation. (8 sources)", "text": "\nEncoder-only models, exemplified by BERT (Bidirectional Encoder Representations from Transformers), process input sequences through bidirectional self-attention mechanisms, allowing them to capture context from both before and after each token in a sequence <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>. This bidirectional processing is a defining characteristic that fundamentally differentiates encoders from decoder architectures <Paper corpusId=\"269148552\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269982953\" paperTitle=\"(Leemann et al., 2024)\" isShortName></Paper>.\n\nThe training objective for encoder models typically involves masked language modeling (MLM), where random tokens in the input are masked, and the model must predict these masked tokens based on their surrounding context <Paper corpusId=\"269148552\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. This denoising approach teaches the model to develop rich contextual representations by understanding the relationships between words in both directions <Paper corpusId=\"267211690\" paperTitle=\"(Uludougan et al., 2024)\" isShortName></Paper>.\n\nEncoder architectures are particularly well-suited for tasks that require deep semantic understanding of input text rather than generation <Paper corpusId=\"267211690\" paperTitle=\"(Uludougan et al., 2024)\" isShortName></Paper>. These include:\n\n- Classification and regression tasks <Paper corpusId=\"277626915\" paperTitle=\"(Rankovi'c et al., 2025)\" isShortName></Paper>\n- Feature extraction and sentiment analysis <Paper corpusId=\"268247581\" paperTitle=\"(Pei et al., 2024)\" isShortName></Paper>\n- Understanding complex semantic relationships within text <Paper corpusId=\"277857043\" paperTitle=\"(Beiranvand et al., 2025)\" isShortName></Paper>\n\nUnlike decoder models, encoders do not generate text sequentially or employ causal masking. Instead, they develop holistic representations where each token has access to the complete context, making them powerful tools for comprehension tasks <Paper corpusId=\"277857043\" paperTitle=\"(Beiranvand et al., 2025)\" isShortName></Paper>. This bidirectional attention capability is so valuable that researchers have explored adapting causal decoder-only models to include bidirectional attention for encoder-centric tasks, resulting in significant performance improvements <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper>.\n\nIn implementation, encoder-only models like BERT typically use a special classification token () at the beginning of the input sequence, which aggregates information from the entire sequence and serves as the basis for classification decisions <Paper corpusId=\"269982953\" paperTitle=\"(Leemann et al., 2024)\" isShortName></Paper>. This architectural choice further emphasizes their focus on understanding rather than generation.", "citations": [{"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 269148552, "title": "Aligning the Objective of LLM-based Program Repair", "year": 2024, "venue": "", "authors": [{"name": "Junjielong Xu", "authorId": "2186630124"}, {"name": "Ying Fu", "authorId": "2296726489"}, {"name": "Shin Hwei Tan", "authorId": "2297427913"}, {"name": "Pinjia He", "authorId": "2265706586"}], "n_citations": 8}, "snippets": ["Encoder-only LLMs, such as BERT [23] and its variants like CodeBERT [46], have a bidirectional transformer encoder structure. They are typically trained on the masked language modeling objective (i.e., MLM), aiming to denoise and reconstruct the masked tokens via understanding the surrounding context (Fig. 1). As shown in Eq. 1, the loss of MLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the masked token when all the unmasked tokens are known.\n\nDecoder-only LLMs, including GPT series [13], [27] and LLaMA series [47], have an autoregressive transformer decoder structure. They are mainly trained on the causal language modeling objective (i.e., CLM), aiming to predict and complete next tokens via following the prefix context (Fig. 1). As shown in Eq. 2, the loss of CLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the next token when all the preceeding tokens are known.\n\nEncoder-decoder LLMs, such as T5 [24] and its variants like CodeT5 [48], have a complete transformer structure."], "score": 0.71240234375}, {"id": "(Leemann et al., 2024)", "paper": {"corpus_id": 269982953, "title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Tobias Leemann", "authorId": "2130899453"}, {"name": "Alina Fastowski", "authorId": "2302795616"}, {"name": "Felix Pfeiffer", "authorId": "2317114785"}, {"name": "Gjergji Kasneci", "authorId": "1686448"}], "n_citations": 5}, "snippets": ["Practical implementations introduce subtle modifications into the process described previously.The most relevant distinction is made between encoder-only models, that include BERT [15] and its variants, and decoder-only models such as the GPT models [38,(Radford et al., 2019).\n\nEncoder-only models.Considering BERT as an example of an encoder-only model, the first token is used for the classification, i.e, r = 1.Usually, a special token [CLS] is prepended to the text at position 1, however this is not strictly necessary for the functioning of the model.\n\nDecoder-only models.In contrast, decoder-models like GPT-2 (Radford et al., 2019) add the classification head on top of the last token for classification, i.e., r = |t|.A key difference is that in GPT-2 and other decoder-only models, a causal mask is laid over the attention matrix, resulting in \u03b1 i,j = 0 for j > i.This encodes the constraint that tokens can only attend to themselves or to previous ones."], "score": 0.69287109375}, {"id": "(Uludougan et al., 2024)", "paper": {"corpus_id": 267211690, "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Gokcce Uludougan", "authorId": "2281033147"}, {"name": "Zeynep Yirmibecsouglu Balal", "authorId": "2281033141"}, {"name": "Furkan Akkurt", "authorId": "2174736343"}, {"name": "Melikcsah Turker", "authorId": "2281033264"}, {"name": "Onur Gungor", "authorId": "9179697"}, {"name": "S. Uskudarli", "authorId": "66493576"}], "n_citations": 12}, "snippets": ["Models that exclusively employ encoders, typically trained with denoising objectives, are geared toward understanding tasks, as exemplified in works such as (Devlin et al., 2019) and (208229926). Conversely, models that exclusively use decoders are designed for generation tasks, employing causal language modeling, as demonstrated in various studies (Radford et al., 2019)Brown et al., 2020;Touvron et al., 2023). The Text-to-Text Transformer (T5) (Raffel et al., 2019), on the other hand, employs an encoder-decoder architecture and undergoes pretraining with a denoising objective referred to as span corruption. UniLM (Dong et al., 2019) is also an encoder-decoder model, but pre-trained using unidirectional, bidirectional, and sequence-to-sequence language modeling. This can be seen as a combination of causal and denoising objectives."], "score": 0.64208984375}, {"id": "(Rankovi'c et al., 2025)", "paper": {"corpus_id": 277626915, "title": "GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Bojana Rankovi'c", "authorId": "2219925647"}, {"name": "Philippe Schwaller", "authorId": "2239074343"}], "n_citations": 1}, "snippets": ["LLMs can follow different architectural designs: encoder-only (e.g., BERT 2), decoder-only (e.g., Qwen 52), and encoder-decoder (e.g., T5 49). Encoder-based models process the full input bidirectionally and are suited for classification and regression. Decoder-only models generate text autoregressively with causal masking. Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks."], "score": 0.703125}, {"id": "(Pei et al., 2024)", "paper": {"corpus_id": 268247581, "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qizhi Pei", "authorId": "2171652249"}, {"name": "Lijun Wu", "authorId": "47767791"}, {"name": "Kaiyuan Gao", "authorId": "1944690382"}, {"name": "Jinhua Zhu", "authorId": "151068900"}, {"name": "Yue Wang", "authorId": "2290062348"}, {"name": "Zun Wang", "authorId": "2290024261"}, {"name": "Tao Qin", "authorId": "2267250090"}, {"name": "Rui Yan", "authorId": "2257028545"}], "n_citations": 18}, "snippets": ["Encoder-only models (Devlin et al., 2019) specialize in processing input sequences of biomolecules and text through bi-directional self-attention, making them highly effective for tasks that require an in-depth understanding of the input, such as sentiment analysis and feature extraction in NLP. Thereby in biomolecule domain, encoder-only models establish a bi-directional association between biotokens and text tokens for predictive tasks (Zeng et al., 2022)", "In contrast, decoder-only models [18], [138] employ causal attention to focus on the sequence of previous tokens. This architecture is typically utilized in generative tasks, such as generating text descriptions that match the given molecule or for the reverse task (Liu et al., 2023). Thanks to the autoregressive generation property, decoder-only models are well suitable for instruction following and assistant/agent objectives."], "score": 0.64990234375}, {"id": "(Beiranvand et al., 2025)", "paper": {"corpus_id": 277857043, "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Azadeh Beiranvand", "authorId": "66841167"}, {"name": "S. M. Vahidipour", "authorId": "35409259"}], "n_citations": 0}, "snippets": ["Encoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input.\n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling.\n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step.\n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation."], "score": 0.810546875}, {"id": "(Suganthan et al., 2025)", "paper": {"corpus_id": 276771845, "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "P. Suganthan", "authorId": "1658871094"}, {"name": "Fedor Moiseev", "authorId": "2165469946"}, {"name": "Le Yan", "authorId": "2348489099"}, {"name": "Junru Wu", "authorId": "2261361394"}, {"name": "Jianmo Ni", "authorId": "2348507846"}, {"name": "Jay Han", "authorId": "2348488953"}, {"name": "I. Zitouni", "authorId": "1954563"}, {"name": "Enrique Alfonseca", "authorId": "1727837"}, {"name": "Xuanhui Wang", "authorId": "2348422460"}, {"name": "Zhe Dong", "authorId": "2349772191"}], "n_citations": 1}, "snippets": ["A key question thus arises: can we effectively adapt the powerful knowledge embedded in decoder-only models to excel in these encoder-centric tasks?", "Gemma's causal attention, ideal for generative tasks, inherently limits its applicability to encoder-based tasks. We demonstrate that simply enabling bidirectional attention during fine-tuning dramatically improves performance."], "score": 0.63134765625}], "table": null}, {"title": "Decoder Architectures", "tldr": "Decoder architectures are designed primarily for text generation and come in two main variants: causal decoders that only attend to past tokens (unidirectional) and prefix decoders that allow bidirectional attention over prefix tokens while maintaining unidirectional attention for generation. (13 sources)", "text": "\nDecoder architectures form a fundamental class of language models that specialize in sequential text generation. Unlike encoders, which focus on understanding input text, decoders are designed to generate new tokens based on previous context. There are two principal types of decoder architectures: causal decoders and prefix decoders.\n\nCausal decoder models, exemplified by GPT-series models, employ unidirectional (left-to-right) attention mechanisms where each token can only attend to itself and preceding tokens in the sequence <Paper corpusId=\"258947629\" paperTitle=\"(Roberts, 2023)\" isShortName></Paper>. This causal masking is what enables these models to predict the next token based on previously observed tokens, making them suitable for autoregressive text generation <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. The unidirectional nature of causal decoders is their defining characteristic, as it preserves the causality needed for coherent text generation <Paper corpusId=\"277128409\" paperTitle=\"(Ghosh et al., 2024)\" isShortName></Paper>.\n\nIn contrast, prefix decoder models represent a hybrid approach. They allow bidirectional attention for a portion of the input sequence (the prefix) while maintaining unidirectional attention for the tokens being generated <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>. This configuration enables prefix decoders to bidirectionally encode the input context while still generating output tokens autoregressively, combining the comprehensive understanding capabilities of encoders with the generative abilities of decoders <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nA key distinction between decoder architectures and encoder architectures lies in their attention mechanisms and optimization objectives. While encoders typically use masked language modeling with bidirectional attention, decoders are usually trained with next-token prediction using causal attention <Paper corpusId=\"270560675\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277626724\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>. This difference in attention mechanisms leads to cumulative quantization errors in causal attention, particularly affecting tokens appearing later in the sequence <Paper corpusId=\"260886785\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>.\n\nDecoder-only models have gained popularity due to several advantages over encoder-decoder models: they feature a simpler structure, faster training and inference speeds, and are particularly well-suited for pure generation tasks <Paper corpusId=\"267402678\" paperTitle=\"(Li, 2024)\" isShortName></Paper>. While encoder-decoder models process input and output sequences separately, decoder-only models typically concatenate input and target sequences, applying appropriate masking mechanisms throughout <Paper corpusId=\"258049081\" paperTitle=\"(Fu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nIn implementation, decoder models differ from encoders in how they process information. Encoders compute attention from scratch for each token prediction, allowing all tokens to attend to each other <Paper corpusId=\"273025546\" paperTitle=\"(Ewer et al., 2024)\" isShortName></Paper>. Decoders, however, use sequential processing where previously generated tokens serve as context for future generations <Paper corpusId=\"263729712\" paperTitle=\"(Alomari et al., 2023)\" isShortName></Paper>. This fundamental difference in information flow shapes the applications each architecture excels at, with decoders being particularly effective for natural language generation tasks <Paper corpusId=\"276423946\" paperTitle=\"(Busto-Castineira et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Roberts, 2023)", "paper": {"corpus_id": 258947629, "title": "How Powerful are Decoder-Only Transformer Neural Models?", "year": 2023, "venue": "IEEE International Joint Conference on Neural Network", "authors": [{"name": "Jesse Roberts", "authorId": "2115904887"}], "n_citations": 19}, "snippets": ["In this article we prove that the general transformer neural model undergirding modern large language models (LLMs) is Turing complete under reasonable assumptions", "Differentiating Encoder-only and Decoder-only Models: Decoder-only models have three necessary characteristics which are derived from their function in the vanilla transformer. The decoder must (1) provide a means of autoregressively predicting the next token based on the tokens generated so far given the encoder input as contextualization. In 2 this is shown as the recursive red connection mapping the output vector back into the last element of the input sequence of vectors. To be suited to this task, decoder-only models must (2) not see future values when evaluating a query on the input sequence of vectors. This is why decoder-only models are often referred to as causal language models (CLM). In 2, we refer to the decoder attention heads as causal attention heads rather than masked attention heads as they are called in (Vaswani et al., 2017). The model must be (3) trained to predict the next token given the current input sequence of vectors. This training method coupled with recursion allows decoder-only models to autoregressively generate arbitrarily long (up to the max size of the input vector sequence) sequences."], "score": 0.8037109375}, {"id": "(Yin et al., 2024)", "paper": {"corpus_id": 270702559, "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Kai Yin", "authorId": "2265383225"}, {"name": "Chengkai Liu", "authorId": "2308073678"}, {"name": "Ali Mostafavi", "authorId": "2258714985"}, {"name": "Xia Hu", "authorId": "2308068627"}], "n_citations": 12}, "snippets": ["Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."], "score": 0.74267578125}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 266755678, "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference", "year": 2024, "venue": "Neurocomputing", "authors": [{"name": "Yi-Hsueh Liu", "authorId": "2116426849"}, {"name": "Haoyang He", "authorId": "2155082967"}, {"name": "Tianle Han", "authorId": "2184719751"}, {"name": "Xu Zhang", "authorId": "2273584640"}, {"name": "Mengyuan Liu", "authorId": "2210636248"}, {"name": "Jiaming Tian", "authorId": "2257433902"}, {"name": "Yutong Zhang", "authorId": "2257095790"}, {"name": "Jiaqi Wang", "authorId": "2110238778"}, {"name": "Xiaohui Gao", "authorId": "2277869261"}, {"name": "Tianyang Zhong", "authorId": "2215167446"}, {"name": "Yi Pan", "authorId": "2221032216"}, {"name": "Shaochen Xu", "authorId": "2211904452"}, {"name": "Zihao Wu", "authorId": "2263593041"}, {"name": "Zheng Liu", "authorId": "2145977326"}, {"name": "Xin Zhang", "authorId": "2257586495"}, {"name": "Shu Zhang", "authorId": "2277750447"}, {"name": "Xintao Hu", "authorId": "1742535"}, {"name": "Tuo Zhang", "authorId": "49104946"}, {"name": "Ning Qiang", "authorId": "2251076040"}, {"name": "Tianming Liu", "authorId": "2254792886"}, {"name": "Bao Ge", "authorId": "2257302793"}], "n_citations": 74}, "snippets": ["LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."], "score": 0.8115234375}, {"id": "(Ghosh et al., 2024)", "paper": {"corpus_id": 277128409, "title": "A review on the applications of Transformer-based language models for nucleotide sequence analysis", "year": 2024, "venue": "Computational and Structural Biotechnology Journal", "authors": [{"name": "Nimisha Ghosh", "authorId": "2343636534"}, {"name": "Daniele Santoni", "authorId": "2299143904"}, {"name": "I. Saha", "authorId": "2105376090"}, {"name": "Giovanni Felici", "authorId": "2261390733"}], "n_citations": 0}, "snippets": ["The functioning of the Decoder has many parts in common with the Encoder, yet there are some major differences. In a nutshell, the Decoder processes the output component of the input-output pairs of the training data; encodes such information and produces its own embedding using a multi-head attention scheme similar to the Encoder, but: \n\n1. once the output has been encoded, it is combined with the  and  matrices coming from the Encoder. This is the step where Transformers learn the relation between the input of the training (e.g., the question) and the output (e.g., the answer to the question); considering the question-answer example, the Decoder performs cross-attention on the Encoder output (which represents the question) while processing the answer; 2. at the end of the process a linear layer has a number of output neurons equal to the size of the vocabulary; such network uses a softmax function to produce likelihood for each term in the vocabulary. Then, the term with the highest likelihood is the output of the Decoder i.e. the predicted next word; 3. The Decoder produces the output one word at a time; in the training phase, knowing the correct word, the error is computed and used to drive the backpropagation step and the correction of the weights to reduce the error", ".For example, the weights obtained in pretraining for Encoder-only model like BERT, are based on randomly masked words; on the other hand, for Decoder-only model like GPT, causal masking is used, where only the future tokens are masked, and the model predicts the next token given past tokens."], "score": 0.83349609375}, {"id": "(Patil et al., 2024)", "paper": {"corpus_id": 268157336, "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)", "year": 2024, "venue": "Applied Sciences", "authors": [{"name": "Rajvardhan Patil", "authorId": "2289385425"}, {"name": "Venkat Gudivada", "authorId": "117730513"}], "n_citations": 80}, "snippets": ["Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens", "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence", "In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."], "score": 0.77587890625}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 270560675, "title": "Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Bingqi Ma", "authorId": "2261489892"}, {"name": "Zhuofan Zong", "authorId": "1571400317"}, {"name": "Guanglu Song", "authorId": "12920342"}, {"name": "Hongsheng Li", "authorId": "2261394248"}, {"name": "Yu Liu", "authorId": "2261417717"}], "n_citations": 23}, "snippets": ["As outlined in Sec. 1, we observe two discrepancies between decoder-only LLMs and encoder-decoder models: optimization objective and model architecture. Specifically, the decoder-only LLMs are typically optimized using the next token prediction task while the encoder-decoder models are trained with the masked language modeling task. Besides, the former tokens in a sequence cannot attend the latter tokens in decoder-only LLMs while every token in the sequence can attend each other in the encoder models."], "score": 0.7783203125}, {"id": "(Zhang et al., 2025)", "paper": {"corpus_id": 277626724, "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Biao Zhang", "authorId": "2354284757"}, {"name": "Fedor Moiseev", "authorId": "2165469946"}, {"name": "Joshua Ainslie", "authorId": "2343748926"}, {"name": "P. Suganthan", "authorId": "1658871094"}, {"name": "Min Ma", "authorId": "2352024723"}, {"name": "Surya Bhupatiraju", "authorId": "9692128"}, {"name": "Federico Lebron", "authorId": "2275184616"}, {"name": "Orhan Firat", "authorId": "2273534960"}, {"name": "Armand Joulin", "authorId": "2319608"}, {"name": "Zhe Dong", "authorId": "2349772191"}], "n_citations": 0}, "snippets": ["Decoder-only pretraining often adopts causal language modeling on a single sequence. In contrast, encoder-decoder adaptation requires separate input and target sequences to be fed to the encoder and decoder separately."], "score": 0.8017578125}, {"id": "(Kim et al., 2023)", "paper": {"corpus_id": 260886785, "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Minsoo Kim", "authorId": "2141320070"}, {"name": "Sihwa Lee", "authorId": "2144376191"}, {"name": "Janghwan Lee", "authorId": "2265920992"}, {"name": "S. Hong", "authorId": "2158125346"}, {"name": "Duhyeuk Chang", "authorId": "2180828053"}, {"name": "Wonyong Sung", "authorId": "66936521"}, {"name": "Jungwook Choi", "authorId": "2506452"}], "n_citations": 15}, "snippets": ["Cumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss. Fig. 1(b) illustrates the differences. In NLU tasks, encoder models leverage the representation of a unique special token, transformed into a logit vector, for calculating cross-entropy loss, with the number of classes usually being a few [12]. Conversely, decoder models in NLG tasks predict the subsequent token for each input token, transforming each token representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k [25]. Hence, while encoder models gain ground truth loss from a single special token's logit, decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information."], "score": 0.93896484375}, {"id": "(Li, 2024)", "paper": {"corpus_id": 267402678, "title": "The evolution, applications, and future prospects of large language models: An in-depth overview", "year": 2024, "venue": "Applied and Computational Engineering", "authors": [{"name": "Jiayin Li", "authorId": "2282449950"}], "n_citations": 2}, "snippets": ["As shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision."], "score": 0.78076171875}, {"id": "(Fu et al., 2023)", "paper": {"corpus_id": 258049081, "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Z. Fu", "authorId": "1646716323"}, {"name": "W. Lam", "authorId": "1380007189"}, {"name": "Qian Yu", "authorId": "144873019"}, {"name": "A. M. So", "authorId": "1734000"}, {"name": "Shengding Hu", "authorId": "1576223501"}, {"name": "Zhiyuan Liu", "authorId": null}, {"name": "Nigel Collier", "authorId": "50638196"}], "n_citations": 43}, "snippets": ["Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture.\n\nThe main difference between the ED framework and the LM is how the input source information is merged into the decoder. As illustrated in Figure 2, the ED framework first uses multiple Transformer blocks to extract features H E \u22121 from the source sequence s. Afterwards, it utilizes a self attention ATT D l to get the feature matrix G D l . It then uses an encoder attention ATT J l to take G D l as query and uses the encoder's final output H E \u22121 as the key and value to calculate Q D l . On the other hand, an LM uses an unidirectional attention to handle the concatenated features."], "score": 0.70263671875}, {"id": "(Ewer et al., 2024)", "paper": {"corpus_id": 273025546, "title": "ENTP: Encoder-only Next Token Prediction", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ethan Ewer", "authorId": "2323781863"}, {"name": "Daewon Chae", "authorId": "2253659910"}, {"name": "Thomas Zeng", "authorId": "2323820473"}, {"name": "Jinkyu Kim", "authorId": "2323851531"}, {"name": "Kangwook Lee", "authorId": "2323790154"}], "n_citations": 4}, "snippets": ["Decoders use a causal attention, ensuring that each token attends only to the preceding tokens. In contrast, encoders allow all tokens to attend to each other by performing attention computation from scratch for each token prediction."], "score": 0.904296875}, {"id": "(Alomari et al., 2023)", "paper": {"corpus_id": 263729712, "title": "Warm-Starting for Improving the Novelty of Abstractive Summarization", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Ayham Alomari", "authorId": "2130042263"}, {"name": "A. S. Al-Shamayleh", "authorId": "1403466899"}, {"name": "N. Idris", "authorId": "36826893"}, {"name": "Aznul Qalid Md Sabri", "authorId": "2049063550"}, {"name": "I. Alsmadi", "authorId": "1770016"}, {"name": "Danah Omary", "authorId": "2182454665"}], "n_citations": 1}, "snippets": ["For the decoder part, we leverage GPT2, BERT, RoBERTa, and Ernie 2.0. GPT2 is trained on open-ended autoregressive generation with the objective of predicting the next word in a sequence of a few token starters (causal language modeling) utilizing only the Transformer's unidirectional ''left-to-right'' self-attention decoder. Theoretically, GPT2 is claimed to be the optimal design for use as a decoder. However, with a few adjustments, bi-directional encoderonly pretrained models such as BERT, RoBERTa, and Ernie 2.0 may simply be adapted as decoders. To compare the encoder and decoder architectures of the Transformer, encoder blocks consist solely of a bi-directional self-attention layer and two feed-forward layers. By contrast, decoder blocks have a unidirectional self-attention layer, a crossattention layer, and two feed-forward layers."], "score": 0.6669921875}, {"id": "(Busto-Castineira et al., 2025)", "paper": {"corpus_id": 276423946, "title": "Optimal word order for non-causal text generation with Large Language Models: The Spanish case", "year": 2025, "venue": "Pattern Recognition Letters", "authors": [{"name": "Andrea Busto-Casti\u00f1eira", "authorId": "2222734467"}, {"name": "Silvia Garc\u00eda-M\u00e9ndez", "authorId": "1405165681"}, {"name": "Francisco de Arriba-P\u00e9rez", "authorId": "2326130687"}, {"name": "F. Gonz\u00e1lez-Casta\u00f1o", "authorId": "1395988865"}], "n_citations": 0}, "snippets": ["Transformers are unsupervised learners thanks to their selfattention mechanism (Vaswani et al., 2017), which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder. \n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation (Kawara et al., 2021)(Nguyen et al., 2021), other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems."], "score": 0.71875}], "table": null}, {"title": "Causal Decoder Models", "tldr": "Causal decoder models use unidirectional attention mechanisms where each token can only attend to itself and preceding tokens, making them ideal for autoregressive text generation. Most state-of-the-art large language models use this architecture due to its effectiveness in zero-shot generalization, simpler structure, and efficient training. (12 sources)", "text": "\nCausal decoder models represent the dominant architecture in contemporary large language models (LLMs). These models implement unidirectional (left-to-right) attention mechanisms where each token can only attend to itself and previous tokens in the sequence <Paper corpusId=\"258947629\" paperTitle=\"(Roberts, 2023)\" isShortName></Paper> <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>. This causal masking pattern is the defining feature that enables these models to predict the next token based on previously observed tokens, making them naturally suited for autoregressive text generation <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\nThe training objective for causal decoder models is typically next-token prediction, also known as causal language modeling (CLM) <Paper corpusId=\"277857043\" paperTitle=\"(Beiranvand et al., 2025)\" isShortName></Paper>. This objective is formulated as:\n\n$$L(\\theta) = -\\sum_{t=1}^{T} \\log p_\\theta(x_t|x_{<t})$$\n\nwhere each token $x_t$ is predicted based solely on the sequence of preceding tokens $x_{<t}$ <Paper corpusId=\"277857043\" paperTitle=\"(Beiranvand et al., 2025)\" isShortName></Paper>.\n\nCausal decoder-only models have become the architecture of choice for most modern LLMs, including prominent examples like the GPT series <Paper corpusId=\"277349741\" paperTitle=\"(Nie et al., 2025)\" isShortName></Paper> <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper>. Despite the original Transformer being an encoder-decoder architecture, nearly all state-of-the-art language models exceeding 100 billion parameters are now causal decoder-only models <Paper corpusId=\"253420279\" paperTitle=\"(Scao et al., 2022)\" isShortName></Paper>.\n\nThe popularity of causal decoder models stems from several advantages:\n\n1. **Simplicity**: Their architecture is simpler than encoder-decoder models, with a more straightforward structure <Paper corpusId=\"267402678\" paperTitle=\"(Li, 2024)\" isShortName></Paper>.\n\n2. **Training and inference efficiency**: They feature faster training and inference speeds compared to encoder-decoder models <Paper corpusId=\"267402678\" paperTitle=\"(Li, 2024)\" isShortName></Paper>.\n\n3. **Natural fit for generation**: Their autoregressive nature makes them particularly well-suited for pure generation tasks <Paper corpusId=\"267402678\" paperTitle=\"(Li, 2024)\" isShortName></Paper>.\n\n4. **Zero-shot generalization**: Causal decoder-only models have demonstrated superior zero-shot generalization capabilities immediately after pretraining <Paper corpusId=\"253420279\" paperTitle=\"(Scao et al., 2022)\" isShortName></Paper>.\n\nDespite these advantages, causal decoder models do have certain limitations. The unidirectional nature of their attention mechanism means they cannot build as rich representations of input text as bidirectional models <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. Additionally, they suffer from cumulative quantization errors in their attention mechanisms, particularly affecting tokens appearing later in the sequence <Paper corpusId=\"260886785\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>.\n\nIn implementation, causal decoder models process information sequentially, where previously generated tokens serve as context for future generations <Paper corpusId=\"273025546\" paperTitle=\"(Ewer et al., 2024)\" isShortName></Paper>. This sequential processing, enforced by the causal mask applied to the attention matrix, ensures that tokens can only attend to themselves or previous ones <Paper corpusId=\"269982953\" paperTitle=\"(Leemann et al., 2024)\" isShortName></Paper>. The mask is implemented by setting the lower triangular part of the attention matrix to 0 and the remaining elements to \u2212\u221e, preventing tokens from attending to future positions <Paper corpusId=\"270214176\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Roberts, 2023)", "paper": {"corpus_id": 258947629, "title": "How Powerful are Decoder-Only Transformer Neural Models?", "year": 2023, "venue": "IEEE International Joint Conference on Neural Network", "authors": [{"name": "Jesse Roberts", "authorId": "2115904887"}], "n_citations": 19}, "snippets": ["In this article we prove that the general transformer neural model undergirding modern large language models (LLMs) is Turing complete under reasonable assumptions", "Differentiating Encoder-only and Decoder-only Models: Decoder-only models have three necessary characteristics which are derived from their function in the vanilla transformer. The decoder must (1) provide a means of autoregressively predicting the next token based on the tokens generated so far given the encoder input as contextualization. In 2 this is shown as the recursive red connection mapping the output vector back into the last element of the input sequence of vectors. To be suited to this task, decoder-only models must (2) not see future values when evaluating a query on the input sequence of vectors. This is why decoder-only models are often referred to as causal language models (CLM). In 2, we refer to the decoder attention heads as causal attention heads rather than masked attention heads as they are called in (Vaswani et al., 2017). The model must be (3) trained to predict the next token given the current input sequence of vectors. This training method coupled with recursion allows decoder-only models to autoregressively generate arbitrarily long (up to the max size of the input vector sequence) sequences."], "score": 0.8037109375}, {"id": "(Zheng et al., 2023)", "paper": {"corpus_id": 261064777, "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks", "year": 2023, "venue": "Empirical Software Engineering", "authors": [{"name": "Zibin Zheng", "authorId": "2148256392"}, {"name": "Kai-Chun Ning", "authorId": "2115304"}, {"name": "Jiachi Chen", "authorId": "2254800142"}, {"name": "Yanlin Wang", "authorId": "2214155529"}, {"name": "Wenqing Chen", "authorId": "2274095496"}, {"name": "Lianghong Guo", "authorId": "2217902484"}, {"name": "Weicheng Wang", "authorId": "2233023641"}], "n_citations": 76}, "snippets": ["Encoder-decoder Architecture: The vanilla Transformer proposed in (Vaswani et al., 2017) is based on encoder-decoder architecture, which comprises separate encoder and decoder components. The encoder processes the input sequence and captures its latent representation, which is then used by the decoder to generate the output sequence autoregressively. This architecture is well-suited for tasks involving sequence-to-sequence mapping, such as machine translation, text summarization, and dialogue generation.\n\nCausal Decoder Architecture: The causal decoder architecture is commonly implemented as a stack of decoder layers. It utilizes a diagonal mask matrix, allowing each token to only have access to information from preceding tokens. This constraint ensures a unidirectional and autoregressive generation process."], "score": 0.64697265625}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 248118752, "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Thomas Wang", "authorId": "2135734748"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Daniel Hesslow", "authorId": "80424302"}, {"name": "Teven Le Scao", "authorId": "1379806208"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Iz Beltagy", "authorId": "46181066"}, {"name": "Julien Launay", "authorId": "143945447"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 175}, "snippets": ["Causal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) .\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) .\n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target."], "score": 0.84423828125}, {"id": "(Beiranvand et al., 2025)", "paper": {"corpus_id": 277857043, "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Azadeh Beiranvand", "authorId": "66841167"}, {"name": "S. M. Vahidipour", "authorId": "35409259"}], "n_citations": 0}, "snippets": ["Encoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input.\n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling.\n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step.\n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation."], "score": 0.810546875}, {"id": "(Nie et al., 2025)", "paper": {"corpus_id": 277349741, "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tong Nie", "authorId": "94168461"}, {"name": "Jiangming Sun", "authorId": "2028643500"}, {"name": "Wei Ma", "authorId": "2277421553"}], "n_citations": 4}, "snippets": ["\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022)."], "score": 0.658203125}, {"id": "(Brown et al., 2020)", "paper": {"corpus_id": 218971783, "title": "Language Models are Few-Shot Learners", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tom B. Brown", "authorId": "31035595"}, {"name": "Benjamin Mann", "authorId": "2056658938"}, {"name": "Nick Ryder", "authorId": "39849748"}, {"name": "Melanie Subbiah", "authorId": "2065894334"}, {"name": "J. Kaplan", "authorId": "152724169"}, {"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Arvind Neelakantan", "authorId": "2072676"}, {"name": "Pranav Shyam", "authorId": "67311962"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Ariel Herbert-Voss", "authorId": "1404060687"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "T. Henighan", "authorId": "103143311"}, {"name": "R. Child", "authorId": "48422824"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Daniel M. Ziegler", "authorId": "2052152920"}, {"name": "Jeff Wu", "authorId": "49387725"}, {"name": "Clemens Winter", "authorId": "2059411355"}, {"name": "Christopher Hesse", "authorId": "144239765"}, {"name": "Mark Chen", "authorId": "2108828435"}, {"name": "Eric Sigler", "authorId": "2064673055"}, {"name": "Ma-teusz Litwin", "authorId": "1380985420"}, {"name": "Scott Gray", "authorId": "145565184"}, {"name": "Benjamin Chess", "authorId": "1490681878"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Christopher Berner", "authorId": "133740015"}, {"name": "Sam McCandlish", "authorId": "52238703"}, {"name": "Alec Radford", "authorId": "38909097"}, {"name": "I. Sutskever", "authorId": "1701686"}, {"name": "Dario Amodei", "authorId": "2698777"}], "n_citations": 42437}, "snippets": ["Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."], "score": 0.0}, {"id": "(Scao et al., 2022)", "paper": {"corpus_id": 253420279, "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Teven Le Scao", "authorId": "1379806208"}, {"name": "Angela Fan", "authorId": "144270981"}, {"name": "Christopher Akiki", "authorId": "2003696840"}, {"name": "Ellie Pavlick", "authorId": "2949185"}, {"name": "Suzana Ili'c", "authorId": "2066663381"}, {"name": "Daniel Hesslow", "authorId": "80424302"}, {"name": "Roman Castagn'e", "authorId": "2190282134"}, {"name": "A. Luccioni", "authorId": "2993731"}, {"name": "Fran\u00e7ois Yvon", "authorId": "1846431"}, {"name": "Matthias Gall\u00e9", "authorId": "2907260"}, {"name": "J. Tow", "authorId": "50195579"}, {"name": "Alexander M. Rush", "authorId": "2531268"}, {"name": "Stella Biderman", "authorId": "103476203"}, {"name": "Albert Webson", "authorId": "1991019030"}, {"name": "Pawan Sasanka Ammanamanchi", "authorId": "1451644426"}, {"name": "Thomas Wang", "authorId": "2135734748"}, {"name": "Beno\u00eet Sagot", "authorId": "68990982"}, {"name": "Niklas Muennighoff", "authorId": "2037383772"}, {"name": "Albert Villanova del Moral", "authorId": "46219923"}, {"name": "Olatunji Ruwase", "authorId": "2537545"}, {"name": "Rachel Bawden", "authorId": "48983885"}, {"name": "Stas Bekman", "authorId": "32136590"}, {"name": "Angelina McMillan-Major", "authorId": "1584940075"}, {"name": "Iz Beltagy", "authorId": "46181066"}, {"name": "Huu Nguyen", "authorId": "2168170616"}, {"name": "Lucile Saulnier", "authorId": "2113836860"}, {"name": "Samson Tan", "authorId": "145814654"}, {"name": "Pedro Ortiz Suarez", "authorId": "147846651"}, {"name": "Victor Sanh", "authorId": "2285868436"}, {"name": "Hugo Laurenccon", "authorId": "2172404846"}, {"name": "Yacine Jernite", "authorId": "2262249"}, {"name": "Julien Launay", "authorId": "143945447"}, {"name": "Margaret Mitchell", "authorId": "49501003"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Aaron Gokaslan", "authorId": "2273789852"}, {"name": "Adi Simhi", "authorId": "2183598223"}, {"name": "Aitor Soroa Etxabe", "authorId": "2078619062"}, {"name": "Alham Fikri Aji", "authorId": "8129718"}, {"name": "Amit Alfassy", "authorId": "73769093"}, {"name": "Anna Rogers", "authorId": "145046059"}, {"name": "Ariel Kreisberg Nitzav", "authorId": "2190281124"}, {"name": "Canwen Xu", "authorId": "66247317"}, {"name": "Chenghao Mou", "authorId": "35966970"}, {"name": "Chris C. Emezue", "authorId": "1591176064"}, {"name": "Christopher Klamm", "authorId": "2261291789"}, {"name": "Colin Leong", "authorId": "89269402"}, {"name": "Daniel Alexander van Strien", "authorId": "71075073"}, {"name": "David Ifeoluwa Adelani", "authorId": "2518906"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}, {"name": "E. G. Ponferrada", "authorId": "79512668"}, {"name": "Efrat Levkovizh", "authorId": "2190281122"}, {"name": "Ethan Kim", "authorId": "2047591327"}, {"name": "Eyal Natan", "authorId": "2088048322"}, {"name": "F. Toni", "authorId": "2067891070"}, {"name": "G\u00e9rard Dupont", "authorId": "13656138"}, {"name": "Germ\u00e1n Kruszewski", "authorId": "2067996"}, {"name": "Giada Pistilli", "authorId": "2158858559"}, {"name": "Hady ElSahar", "authorId": "2218938"}, {"name": "Hamza Benyamina", "authorId": "90563027"}, {"name": "H. Tran", "authorId": "2057078797"}, {"name": "Ian Yu", "authorId": "47948569"}, {"name": "Idris Abdulmumin", "authorId": "1429833598"}, {"name": "Isaac Johnson", "authorId": "2060080508"}, {"name": "Itziar Gonzalez-Dios", "authorId": "1404791152"}, {"name": "Javier de la Rosa", "authorId": "144979591"}, {"name": "Jenny Chim", "authorId": "2164872258"}, {"name": "Jesse Dodge", "authorId": "34176020"}, {"name": "Jian Zhu", "authorId": "144549416"}, {"name": "Jonathan Chang", "authorId": "2116123009"}, {"name": "Jorg Frohberg", "authorId": "2146695800"}, {"name": "Josephine Tobing", "authorId": "2094755167"}, {"name": "J. Bhattacharjee", "authorId": "143779690"}, {"name": "Khalid Almubarak", "authorId": "90615055"}, {"name": "Kimbo Chen", "authorId": "2157630500"}, {"name": "Kyle Lo", "authorId": "46258841"}, {"name": "L. V. Werra", "authorId": "51128119"}, {"name": "Leon Weber", "authorId": "20308468"}, {"name": "Long Phan", "authorId": null}, {"name": "Loubna Ben Allal", "authorId": "2190281230"}, {"name": "Ludovic Tanguy", "authorId": "77970446"}, {"name": "Manan Dey", "authorId": "1879591269"}, {"name": "M. Mu\u00f1oz", "authorId": "115568186"}, {"name": "Maraim Masoud", "authorId": "153528116"}, {"name": "Mar\u00eda Grandury", "authorId": "2176184513"}, {"name": "Mario vSavsko", "authorId": "2125821515"}, {"name": "Max Huang", "authorId": "2112504552"}, {"name": "Maximin Coavoux", "authorId": "3443469"}, {"name": "Mayank Singh", "authorId": "145431050"}, {"name": "Mike Tian-Jian Jiang", "authorId": "5745221"}, {"name": "Minh Chien Vu", "authorId": "1484109150"}, {"name": "M. A. Jauhar", "authorId": "2097304324"}, {"name": "Mustafa Ghaleb", "authorId": "2721586"}, {"name": "Nishant Subramani", "authorId": "34202134"}, {"name": "Nora Kassner", "authorId": "9529535"}, {"name": "Nurulaqilla Khamis", "authorId": "37441312"}, {"name": "Olivier Nguyen", "authorId": "2089233725"}, {"name": "Omar Espejel", "authorId": "2190280842"}, {"name": "Ona de Gibert", "authorId": "51436367"}, {"name": "Paulo Villegas", "authorId": "2176184659"}, {"name": "Peter Henderson", "authorId": "2071773966"}, {"name": "Pierre Colombo", "authorId": "46985469"}, {"name": "Priscilla Amuok", "authorId": "2190281321"}, {"name": "Quentin Lhoest", "authorId": "2113836945"}, {"name": "Rheza Harliman", "authorId": "80858030"}, {"name": "Rishi Bommasani", "authorId": "150272855"}, {"name": "R. L'opez", "authorId": "116000979"}, {"name": "Rui Ribeiro", "authorId": null}, {"name": "Salomey Osei", "authorId": "1486204986"}, {"name": "S. Pyysalo", "authorId": "1708916"}, {"name": "Sebastian Nagel", "authorId": "47351277"}, {"name": "Shamik Bose", "authorId": "2795685"}, {"name": "Shamsuddeen Hassan Muhammad", "authorId": "7744881"}, {"name": "S. Sharma", "authorId": "1409842673"}, {"name": "S. Longpre", "authorId": "29909347"}, {"name": "Somaieh Nikpoor", "authorId": "2099315138"}, {"name": "S. Silberberg", "authorId": "82674724"}, {"name": "S. Pai", "authorId": "2053516473"}, {"name": "S. Zink", "authorId": "2074482488"}, {"name": "Tiago Timponi Torrent", "authorId": "46308692"}, {"name": "Timo Schick", "authorId": "32246932"}, {"name": "Tristan Thrush", "authorId": "1500242049"}, {"name": "V. Danchev", "authorId": "3382327"}, {"name": "Vassilina Nikoulina", "authorId": "2841761"}, {"name": "Veronika Laippala", "authorId": "1796619"}, {"name": "Violette Lepercq", "authorId": "2190280574"}, {"name": "V. Prabhu", "authorId": "2059767242"}, {"name": "Zaid Alyafeai", "authorId": "25098419"}, {"name": "Zeerak Talat", "authorId": "2138053020"}, {"name": "Arun Raja", "authorId": "2048082186"}, {"name": "Benjamin Heinzerling", "authorId": "2266692"}, {"name": "Chenglei Si", "authorId": "152358188"}, {"name": "Elizabeth Salesky", "authorId": "3448427"}, {"name": "Sabrina J. Mielke", "authorId": "27689253"}, {"name": "Wilson Y. Lee", "authorId": "2183377987"}, {"name": "Abheesht Sharma", "authorId": "2051500420"}, {"name": "Andrea Santilli", "authorId": "2065039862"}, {"name": "Antoine Chaffin", "authorId": "2129106958"}, {"name": "Arnaud Stiegler", "authorId": "114762823"}, {"name": "Debajyoti Datta", "authorId": "2852125"}, {"name": "Eliza Szczechla", "authorId": "50812522"}, {"name": "Gunjan Chhablani", "authorId": "1509809381"}, {"name": "Han Wang", "authorId": "144407394"}, {"name": "Harshit Pandey", "authorId": "144834468"}, {"name": "Hendrik Strobelt", "authorId": "2879705"}, {"name": "Jason Alan Fries", "authorId": "31592365"}, {"name": "Jos Rozen", "authorId": "120419790"}, {"name": "Leo Gao", "authorId": "2027599537"}, {"name": "Lintang Sutawika", "authorId": "35566806"}, {"name": "M Saiful Bari", "authorId": "31773000"}, {"name": "Maged S. Al-Shaibani", "authorId": "1752627730"}, {"name": "Matteo Manica", "authorId": "35904689"}, {"name": "Nihal V. Nayak", "authorId": "22209084"}, {"name": "Ryan Teehan", "authorId": "2131107966"}, {"name": "Samuel Albanie", "authorId": "7641268"}, {"name": "Sheng Shen", "authorId": "2191455"}, {"name": "Srulik Ben-David", "authorId": "2152318619"}, {"name": "Stephen H. Bach", "authorId": "2870504"}, {"name": "Taewoon Kim", "authorId": "2111181991"}, {"name": "T. Bers", "authorId": "94251255"}, {"name": "Thibault F\u00e9vry", "authorId": "79215748"}, {"name": "Trishala Neeraj", "authorId": "10729963"}, {"name": "Urmish Thakker", "authorId": "70296695"}, {"name": "Vikas Raunak", "authorId": "24025563"}, {"name": "Xiang Tang", "authorId": "2118488348"}, {"name": "Zheng-Xin Yong", "authorId": "1725420331"}, {"name": "Zhiqing Sun", "authorId": "48064856"}, {"name": "Shaked Brody", "authorId": "1720739223"}, {"name": "Y. Uri", "authorId": "2101395835"}, {"name": "Hadar Tojarieh", "authorId": "2190280874"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Jaesung Tae", "authorId": "2112211652"}, {"name": "Jason Phang", "authorId": "80842917"}, {"name": "Ofir Press", "authorId": "40170001"}, {"name": "Conglong Li", "authorId": "2609325"}, {"name": "D. Narayanan", "authorId": "22252150"}, {"name": "Hatim Bourfoune", "authorId": "2190280830"}, {"name": "J. Casper", "authorId": "48991386"}, {"name": "Jeff Rasley", "authorId": "3299496"}, {"name": "Max Ryabinin", "authorId": "1491753352"}, {"name": "Mayank Mishra", "authorId": "1381446720"}, {"name": "Minjia Zhang", "authorId": "67016465"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "Myriam Peyrounette", "authorId": "31758637"}, {"name": "N. Patry", "authorId": "31614549"}, {"name": "Nouamane Tazi", "authorId": "2179884903"}, {"name": "Omar Sanseviero", "authorId": "2186979509"}, {"name": "Patrick von Platen", "authorId": "138609838"}, {"name": "Pierre Cornette", "authorId": "2190281218"}, {"name": "Pierre Franccois Lavall'ee", "authorId": "2190280981"}, {"name": "R. Lacroix", "authorId": "31734741"}, {"name": "Samyam Rajbhandari", "authorId": "32817044"}, {"name": "Sanchit Gandhi", "authorId": "2188737826"}, {"name": "Shaden Smith", "authorId": "2110486618"}, {"name": "S. Requena", "authorId": "2293408"}, {"name": "Suraj Patil", "authorId": "2147312210"}, {"name": "Tim Dettmers", "authorId": "3239480"}, {"name": "Ahmed Baruwa", "authorId": "114850513"}, {"name": "Amanpreet Singh", "authorId": null}, {"name": "Anastasia Cheveleva", "authorId": "2190281235"}, {"name": "Anne-Laure Ligozat", "authorId": "1769176"}, {"name": "Arjun Subramonian", "authorId": "1677386832"}, {"name": "Aur'elie N'ev'eol", "authorId": "2190281078"}, {"name": "Charles Lovering", "authorId": "10727711"}, {"name": "Dan Garrette", "authorId": "2758616"}, {"name": "D. Tunuguntla", "authorId": "70209311"}, {"name": "Ehud Reiter", "authorId": "144568312"}, {"name": "Ekaterina Taktasheva", "authorId": "2051713939"}, {"name": "E. Voloshina", "authorId": "2135526571"}, {"name": "Eli Bogdanov", "authorId": "2158860079"}, {"name": "Genta Indra Winata", "authorId": "9162688"}, {"name": "Hailey Schoelkopf", "authorId": "2184031883"}, {"name": "Jan-Christoph Kalo", "authorId": "3245041"}, {"name": "Jekaterina Novikova", "authorId": "2848048"}, {"name": "J. Forde", "authorId": "39774809"}, {"name": "Xiangru Tang", "authorId": "47274259"}, {"name": "Jungo Kasai", "authorId": "11348687"}, {"name": "Ken Kawamura", "authorId": "50106621"}, {"name": "Liam Hazan", "authorId": "2047711867"}, {"name": "Marine Carpuat", "authorId": "2954727"}, {"name": "Miruna Clinciu", "authorId": "2029314697"}, {"name": "Najoung Kim", "authorId": "8756748"}, {"name": "Newton Cheng", "authorId": "15590401"}, {"name": "Oleg Serikov", "authorId": "1799401599"}, {"name": "Omer Antverg", "authorId": "2132545395"}, {"name": "Oskar van der Wal", "authorId": "1986356851"}, {"name": "Rui Zhang", "authorId": "15176410"}, {"name": "Ruochen Zhang", "authorId": "49775305"}, {"name": "Sebastian Gehrmann", "authorId": "3159346"}, {"name": "Shachar Mirkin", "authorId": "8963527"}, {"name": "S. Pais", "authorId": "3097741"}, {"name": "Tatiana Shavrina", "authorId": "2134610800"}, {"name": "Thomas Scialom", "authorId": "90745780"}, {"name": "Tian Yun", "authorId": "2127600348"}, {"name": "Tomasz Limisiewicz", "authorId": "1666636295"}, {"name": "Verena Rieser", "authorId": "1681799"}, {"name": "Vitaly Protasov", "authorId": "2135362820"}, {"name": "V. Mikhailov", "authorId": "51259225"}, {"name": "Yada Pruksachatkun", "authorId": "100984698"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}, {"name": "Zachary Bamberger", "authorId": "2190281071"}, {"name": "Zden\u02c7ek Kasner", "authorId": "2343772132"}, {"name": "Zden\u011bk Kasner", "authorId": "1805991958"}, {"name": "A. Pestana", "authorId": "2190281954"}, {"name": "A. Feizpour", "authorId": "15845853"}, {"name": "Ammar Khan", "authorId": "2190399370"}, {"name": "Amy Faranak", "authorId": "2190281566"}, {"name": "A. Santos", "authorId": "2148971654"}, {"name": "Anthony Hevia", "authorId": "1739149407"}, {"name": "Antigona Unldreaj", "authorId": "2190281069"}, {"name": "Arash Aghagol", "authorId": "115638227"}, {"name": "Arezoo Abdollahi", "authorId": "2361305"}, {"name": "A. Tammour", "authorId": "101302626"}, {"name": "A. HajiHosseini", "authorId": "3110645"}, {"name": "Bahareh Behroozi", "authorId": "2190281564"}, {"name": "Benjamin Ayoade Ajibade", "authorId": "83263885"}, {"name": "B. Saxena", "authorId": "31577522"}, {"name": "Carlos Mu\u00f1oz Ferrandis", "authorId": "2005399190"}, {"name": "Danish Contractor", "authorId": "2075459"}, {"name": "D. Lansky", "authorId": "144635557"}, {"name": "Davis David", "authorId": "2058260775"}, {"name": "Douwe Kiela", "authorId": "2111313627"}, {"name": "D. A. Nguyen", "authorId": "5943347"}, {"name": "Edward Tan", "authorId": "47654100"}, {"name": "Emi Baylor", "authorId": "2026649806"}, {"name": "Ezinwanne Ozoani", "authorId": "2190281502"}, {"name": "F. Mirza", "authorId": "35330153"}, {"name": "Frankline Ononiwu", "authorId": "2190281922"}, {"name": "Habib Rezanejad", "authorId": "123343513"}, {"name": "H.A. Jones", "authorId": "2119822136"}, {"name": "Indrani Bhattacharya", "authorId": "2105001416"}, {"name": "Irene Solaiman", "authorId": "1404060690"}, {"name": "Irina Sedenko", "authorId": "2190281379"}, {"name": "Isar Nejadgholi", "authorId": "3163125"}, {"name": "J. Passmore", "authorId": "145629075"}, {"name": "Joshua Seltzer", "authorId": "150162316"}, {"name": "Julio Bonis Sanz", "authorId": "97979993"}, {"name": "Karen Fort", "authorId": null}, {"name": "L\u00edvia Dutra", "authorId": "3530609"}, {"name": "Mairon Samagaio", "authorId": "2190281373"}, {"name": "Maraim Elbadri", "authorId": "2190281500"}, {"name": "Margot Mieskes", "authorId": "2921990"}, {"name": "Marissa Gerchick", "authorId": "151492708"}, {"name": "Martha Akinlolu", "authorId": "2190281205"}, {"name": "Michael McKenna", "authorId": "2060092577"}, {"name": "Mike Qiu", "authorId": "2056851511"}, {"name": "M. Ghauri", "authorId": "144449938"}, {"name": "Mykola Burynok", "authorId": "2190281203"}, {"name": "Nafis Abrar", "authorId": "1401945312"}, {"name": "Nazneen Rajani", "authorId": "8937909"}, {"name": "Nour Elkott", "authorId": "2190281555"}, {"name": "N. Fahmy", "authorId": "1992948200"}, {"name": "Olanrewaju Samuel", "authorId": "2164156047"}, {"name": "Ran An", "authorId": "2061141169"}, {"name": "R. Kromann", "authorId": "9294251"}, {"name": "Ryan Hao", "authorId": "2137183106"}, {"name": "S. Alizadeh", "authorId": "4279554"}, {"name": "Sarmad Shubber", "authorId": "2190281531"}, {"name": "Silas L. Wang", "authorId": "2116420702"}, {"name": "Sourav Roy", "authorId": "2109853801"}, {"name": "S. Viguier", "authorId": "10726201"}, {"name": "Thanh-Cong Le", "authorId": "2153620715"}, {"name": "Tobi Oyebade", "authorId": "2190281729"}, {"name": "T. Le", "authorId": "2153620985"}, {"name": "Yoyo Yang", "authorId": "2190429590"}, {"name": "Zach Nguyen", "authorId": "2297189567"}, {"name": "Abhinav Ramesh Kashyap", "authorId": "41124383"}, {"name": "A. Palasciano", "authorId": "2318515251"}, {"name": "A. Callahan", "authorId": "2840689"}, {"name": "Anima Shukla", "authorId": "2042747208"}, {"name": "Antonio Miranda-Escalada", "authorId": "1414073449"}, {"name": "A. Singh", "authorId": "2110183222"}, {"name": "Benjamin Beilharz", "authorId": "1379935164"}, {"name": "Bo Wang", "authorId": "2165371942"}, {"name": "C. Brito", "authorId": "144972524"}, {"name": "Chenxi Zhou", "authorId": "2111169784"}, {"name": "Chirag Jain", "authorId": "50732716"}, {"name": "Chuxin Xu", "authorId": "2158158973"}, {"name": "Cl\u00e9mentine Fourrier", "authorId": "2080941785"}, {"name": "Daniel Le'on Perin'an", "authorId": "2174177869"}, {"name": "Daniel Molano", "authorId": "2082057793"}, {"name": "Dian Yu", "authorId": "150978762"}, {"name": "Enrique Manjavacas", "authorId": "24907368"}, {"name": "Fabio Barth", "authorId": "2139792578"}, {"name": "Florian Fuhrimann", "authorId": "2190281754"}, {"name": "Gabriel Altay", "authorId": "2165227550"}, {"name": "Giyaseddin Bayrak", "authorId": "2166224123"}, {"name": "Gully Burns", "authorId": null}, {"name": "Helena U. Vrabec", "authorId": "88811067"}, {"name": "I. Bello", "authorId": "121044523"}, {"name": "Isha Dash", "authorId": "93460753"}, {"name": "J. Kang", "authorId": "72725318"}, {"name": "John Giorgi", "authorId": "37585306"}, {"name": "Jonas Golde", "authorId": "144983077"}, {"name": "J. Posada", "authorId": "2066514466"}, {"name": "Karthi Sivaraman", "authorId": "1601562797"}, {"name": "Lokesh Bulchandani", "authorId": "2190281314"}, {"name": "Lu Liu", "authorId": "2145287083"}, {"name": "Luisa Shinzato", "authorId": "2100596120"}, {"name": "Madeleine Hahn de Bykhovetz", "authorId": "2190281960"}, {"name": "Maiko Takeuchi", "authorId": "2068853922"}, {"name": "Marc P\u00e0mies", "authorId": "1850527789"}, {"name": "M. A. Castillo", "authorId": "87956698"}, {"name": "Marianna Nezhurina", "authorId": "2174178585"}, {"name": "Mario Sanger", "authorId": "1879523878"}, {"name": "M. Samwald", "authorId": "3004898"}, {"name": "Michael Cullan", "authorId": "120397552"}, {"name": "Michael Weinberg", "authorId": "50564168"}, {"name": "M. Wolf", "authorId": "2072502429"}, {"name": "Mina Mihaljcic", "authorId": "2190280864"}, {"name": "Minna Liu", "authorId": "2112211627"}, {"name": "M. Freidank", "authorId": "1397064923"}, {"name": "Myungsun Kang", "authorId": "4981508"}, {"name": "Natasha Seelam", "authorId": "12046785"}, {"name": "N. Dahlberg", "authorId": "48948105"}, {"name": "N. Broad", "authorId": "40208102"}, {"name": "N. Muellner", "authorId": "70256289"}, {"name": "Pascale Fung", "authorId": "40539650"}, {"name": "Patricia Haller", "authorId": "2097023671"}, {"name": "Patrick Haller", "authorId": "2298902857"}, {"name": "R. Eisenberg", "authorId": "115525190"}, {"name": "Robert Martin", "authorId": "2111138678"}, {"name": "Rodrigo Canalli", "authorId": "2291171257"}, {"name": "Rosaline Su", "authorId": "2190282202"}, {"name": "Ruisi Su", "authorId": "153083809"}, {"name": "Samuel Cahyawijaya", "authorId": "66986482"}, {"name": "Samuele Garda", "authorId": "51878929"}, {"name": "Shlok S Deshmukh", "authorId": "2174177330"}, {"name": "Shubhanshu Mishra", "authorId": "2112134590"}, {"name": "Sid Kiblawi", "authorId": "39620434"}, {"name": "Simon Ott", "authorId": "119994729"}, {"name": "Sinee Sang-aroonsiri", "authorId": "2190281679"}, {"name": "Srishti Kumar", "authorId": "120438284"}, {"name": "Stefan Schweter", "authorId": "134757625"}, {"name": "S. Bharati", "authorId": "8723233"}, {"name": "Tanmay Laud", "authorId": "103242455"}, {"name": "Th\u00e9o Gigant", "authorId": "2174176862"}, {"name": "Tomoya Kainuma", "authorId": "2190281376"}, {"name": "Wojciech Kusa", "authorId": "50320098"}, {"name": "Yanis Labrak", "authorId": "2139767217"}, {"name": "Yashasvi Bajaj", "authorId": "1572961212"}, {"name": "Y. Venkatraman", "authorId": "2051879548"}, {"name": "Yifan Xu", "authorId": "2110154622"}, {"name": "Ying Xu", "authorId": "2118670234"}, {"name": "Yu Xu", "authorId": "2142717873"}, {"name": "Z. Tan", "authorId": "1643680733"}, {"name": "Zhongli Xie", "authorId": "79110285"}, {"name": "Zifan Ye", "authorId": "2114134227"}, {"name": "M. Bras", "authorId": "2065370401"}, {"name": "Younes Belkada", "authorId": "2037496520"}, {"name": "Thomas Wolf", "authorId": "50335211"}], "n_citations": 2393}, "snippets": ["Notably, while the original Transformer is based on an encoder-decoder architecture, many popular models have opted for encoder-only (e.g. BERT, (Devlin et al., 2019)) or decoder-only (e.g. GPT, (Radford et al., 2018)) approaches. Currently, all state-of-the-art language models over 100 billion parameters are causal decoder-only models Rae et al., 2021;Chowdhery et al., 2022). This is in opposition to the findings of (Raffel et al., 2019), in which encoderdecoder models significantly outperform decoder-only models for transfer learning.\n\nPrior to our work, the literature was lacking a systematic evaluation of the zero-shot generalization capabilities of different architectures and pretraining objectives. We explored this question in (Wang et al., 2022) where we evaluated encoder-decoder and decoder-only architectures and their interactions with causal, prefix, and masked language modeling pretraining objectives. Our results show that immediately after pretraining, causal decoderonly models performed best -validating the choice of state-of-the-art LLMs."], "score": 0.74072265625}, {"id": "(Li, 2024)", "paper": {"corpus_id": 267402678, "title": "The evolution, applications, and future prospects of large language models: An in-depth overview", "year": 2024, "venue": "Applied and Computational Engineering", "authors": [{"name": "Jiayin Li", "authorId": "2282449950"}], "n_citations": 2}, "snippets": ["As shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision."], "score": 0.78076171875}, {"id": "(Kim et al., 2023)", "paper": {"corpus_id": 260886785, "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Minsoo Kim", "authorId": "2141320070"}, {"name": "Sihwa Lee", "authorId": "2144376191"}, {"name": "Janghwan Lee", "authorId": "2265920992"}, {"name": "S. Hong", "authorId": "2158125346"}, {"name": "Duhyeuk Chang", "authorId": "2180828053"}, {"name": "Wonyong Sung", "authorId": "66936521"}, {"name": "Jungwook Choi", "authorId": "2506452"}], "n_citations": 15}, "snippets": ["Cumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss. Fig. 1(b) illustrates the differences. In NLU tasks, encoder models leverage the representation of a unique special token, transformed into a logit vector, for calculating cross-entropy loss, with the number of classes usually being a few [12]. Conversely, decoder models in NLG tasks predict the subsequent token for each input token, transforming each token representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k [25]. Hence, while encoder models gain ground truth loss from a single special token's logit, decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information."], "score": 0.93896484375}, {"id": "(Ewer et al., 2024)", "paper": {"corpus_id": 273025546, "title": "ENTP: Encoder-only Next Token Prediction", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ethan Ewer", "authorId": "2323781863"}, {"name": "Daewon Chae", "authorId": "2253659910"}, {"name": "Thomas Zeng", "authorId": "2323820473"}, {"name": "Jinkyu Kim", "authorId": "2323851531"}, {"name": "Kangwook Lee", "authorId": "2323790154"}], "n_citations": 4}, "snippets": ["Decoders use a causal attention, ensuring that each token attends only to the preceding tokens. In contrast, encoders allow all tokens to attend to each other by performing attention computation from scratch for each token prediction."], "score": 0.904296875}, {"id": "(Leemann et al., 2024)", "paper": {"corpus_id": 269982953, "title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Tobias Leemann", "authorId": "2130899453"}, {"name": "Alina Fastowski", "authorId": "2302795616"}, {"name": "Felix Pfeiffer", "authorId": "2317114785"}, {"name": "Gjergji Kasneci", "authorId": "1686448"}], "n_citations": 5}, "snippets": ["Practical implementations introduce subtle modifications into the process described previously.The most relevant distinction is made between encoder-only models, that include BERT [15] and its variants, and decoder-only models such as the GPT models [38,(Radford et al., 2019).\n\nEncoder-only models.Considering BERT as an example of an encoder-only model, the first token is used for the classification, i.e, r = 1.Usually, a special token [CLS] is prepended to the text at position 1, however this is not strictly necessary for the functioning of the model.\n\nDecoder-only models.In contrast, decoder-models like GPT-2 (Radford et al., 2019) add the classification head on top of the last token for classification, i.e., r = |t|.A key difference is that in GPT-2 and other decoder-only models, a causal mask is laid over the attention matrix, resulting in \u03b1 i,j = 0 for j > i.This encodes the constraint that tokens can only attend to themselves or to previous ones."], "score": 0.69287109375}, {"id": "(Jiang et al., 2024)", "paper": {"corpus_id": 270214176, "title": "A Survey on Large Language Models for Code Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Juyong Jiang", "authorId": "2294682530"}, {"name": "Fan Wang", "authorId": "2304542351"}, {"name": "Jiasi Shen", "authorId": "2305041631"}, {"name": "Sungju Kim", "authorId": "2304525068"}, {"name": "Sunghun Kim", "authorId": "2257349580"}], "n_citations": 197}, "snippets": ["In encoder-decoder LLMs, a pivot token is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x = {1, . . ., } of the encoder and the sequence after it as the target output x = {+1, . . ., } of decoder.Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows:\n\nwhere x\u2264 is the source sequence input and x< denotes the target sequence autoregressively generated so far", "The conditional probability (|x<)) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token attends only to its predecessors and itself."], "score": 0.66064453125}], "table": null}, {"title": "Non-Causal Decoder Models", "tldr": "Non-causal decoder models, also known as prefix decoder models, allow bidirectional attention over prefix tokens while maintaining unidirectional attention for generated tokens. This hybrid approach combines the comprehensive understanding capabilities of encoders with the generative abilities of decoders, making them effective for tasks requiring both deep context understanding and text generation. (14 sources)", "text": "\nNon-causal decoder models represent an architectural variation that addresses some of the limitations of standard causal decoder models. Unlike causal decoders that employ strictly unidirectional attention, non-causal decoders modify the attention masking pattern to allow bidirectional attention for a portion of the input sequence (the prefix) while maintaining unidirectional attention for the tokens being generated <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nThis architecture is also commonly referred to as a \"prefix decoder\" or \"prefix language model\" (PrefixLM) approach <Paper corpusId=\"247625205\" paperTitle=\"(Scao et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"147704286\" paperTitle=\"(Dong et al., 2019)\" isShortName></Paper> <Paper corpusId=\"3608234\" paperTitle=\"(Liu et al., 2018)\" isShortName></Paper>. The key distinction lies in how the self-attention mask is configured: the prefix portion uses a fully visible (non-causal) mask that allows tokens to attend to all positions in the prefix, while the generation portion maintains the traditional causal mask that restricts attention to previous tokens only <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nThis hybrid approach offers several advantages:\n\n1. **Rich bidirectional representations**: By allowing bidirectional attention in the prefix portion, non-causal decoder models can build richer representations of the input context, similar to how encoders process information <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\n2. **Combined capabilities**: Non-causal decoders effectively combine the comprehensive understanding capabilities of encoders with the generative abilities of decoders within a single model architecture <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\n3. **Parameter efficiency**: Unlike encoder-decoder models that require separate parameters for encoding and decoding, non-causal decoder models share the same parameters for both processes, potentially making them more parameter-efficient <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\nIn implementation, the non-causal decoder architecture replaces the standard causal mask with a non-causal mask in the self-attention module, allowing tokens to attend to all positions rather than only previous positions <Paper corpusId=\"257050658\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This approach bears similarities to the attention mechanisms used in non-autoregressive translation (NAT) models, which adopt unmasked self-attention over all target tokens <Paper corpusId=\"248266379\" paperTitle=\"(Xiao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"13756489\" paperTitle=\"(Vaswani et al., 2017)\" isShortName></Paper>.\n\nNotable examples of models employing non-causal decoder architectures include GLM-130B <Paper corpusId=\"277349741\" paperTitle=\"(Nie et al., 2025)\" isShortName></Paper> and U-PaLM <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>. Additionally, research has shown that causal decoder-only models can be efficiently adapted into non-causal decoders by extending pretraining with span corruption objectives, potentially improving zero-shot generalization after multitask fine-tuning <Paper corpusId=\"247625205\" paperTitle=\"(Scao et al._1, 2022)\" isShortName></Paper>.\n\nIt's worth noting that while the distinction between decoder-only and encoder-decoder models can sometimes blur, there are still fundamental differences in how they process information. Encoder-decoder models process input and target sequences independently with different parameters and include a cross-attention component, while decoder-only models (including prefix decoders) process inputs and targets by concatenating them <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper> <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>.", "citations": [{"id": "(Wang et al., 2022)", "paper": {"corpus_id": 248118752, "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Thomas Wang", "authorId": "2135734748"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Daniel Hesslow", "authorId": "80424302"}, {"name": "Teven Le Scao", "authorId": "1379806208"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Iz Beltagy", "authorId": "46181066"}, {"name": "Julien Launay", "authorId": "143945447"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 175}, "snippets": ["Causal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) .\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) .\n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target."], "score": 0.84423828125}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 266755678, "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference", "year": 2024, "venue": "Neurocomputing", "authors": [{"name": "Yi-Hsueh Liu", "authorId": "2116426849"}, {"name": "Haoyang He", "authorId": "2155082967"}, {"name": "Tianle Han", "authorId": "2184719751"}, {"name": "Xu Zhang", "authorId": "2273584640"}, {"name": "Mengyuan Liu", "authorId": "2210636248"}, {"name": "Jiaming Tian", "authorId": "2257433902"}, {"name": "Yutong Zhang", "authorId": "2257095790"}, {"name": "Jiaqi Wang", "authorId": "2110238778"}, {"name": "Xiaohui Gao", "authorId": "2277869261"}, {"name": "Tianyang Zhong", "authorId": "2215167446"}, {"name": "Yi Pan", "authorId": "2221032216"}, {"name": "Shaochen Xu", "authorId": "2211904452"}, {"name": "Zihao Wu", "authorId": "2263593041"}, {"name": "Zheng Liu", "authorId": "2145977326"}, {"name": "Xin Zhang", "authorId": "2257586495"}, {"name": "Shu Zhang", "authorId": "2277750447"}, {"name": "Xintao Hu", "authorId": "1742535"}, {"name": "Tuo Zhang", "authorId": "49104946"}, {"name": "Ning Qiang", "authorId": "2251076040"}, {"name": "Tianming Liu", "authorId": "2254792886"}, {"name": "Bao Ge", "authorId": "2257302793"}], "n_citations": 74}, "snippets": ["LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."], "score": 0.8115234375}, {"id": "(Scao et al._1, 2022)", "paper": {"corpus_id": 247625205, "title": "What Language Model to Train if You Have One Million GPU Hours?", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Teven Le Scao", "authorId": "1379806208"}, {"name": "Thomas Wang", "authorId": "2135734748"}, {"name": "Daniel Hesslow", "authorId": "80424302"}, {"name": "Lucile Saulnier", "authorId": "2113836860"}, {"name": "Stas Bekman", "authorId": "32136590"}, {"name": "Saiful Bari", "authorId": "2054179756"}, {"name": "Stella Biderman", "authorId": "103476203"}, {"name": "Hady ElSahar", "authorId": "2218938"}, {"name": "Niklas Muennighoff", "authorId": "2037383772"}, {"name": "Jason Phang", "authorId": "80842917"}, {"name": "Ofir Press", "authorId": "40170001"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Victor Sanh", "authorId": "2285868436"}, {"name": "Sheng Shen", "authorId": "2191455"}, {"name": "Lintang Sutawika", "authorId": "35566806"}, {"name": "Jaesung Tae", "authorId": "2112211652"}, {"name": "Zheng-Xin Yong", "authorId": "1725420331"}, {"name": "Julien Launay", "authorId": "143945447"}, {"name": "Iz Beltagy", "authorId": "46181066"}], "n_citations": 109}, "snippets": ["Alternatives include encoder-decoder models trained with a span-corruption objective (e.g., T5 Raffel et al. (2019)), as well as non-causal decoders models with visibility over a prefix (so-called Prefix LMs, (Liu et al., 2018); (Dong et al., 2019))", "Following autoregressive pretraining, decoder-only models can be efficiently adapted into non-causal decoders, simply by extending pretraining with span corruption. This adaptation produces a second model, which can provide excellent zero-shot generalization after multitask finetuning."], "score": 0.6279296875}, {"id": "(Dong et al., 2019)", "paper": {"corpus_id": 147704286, "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Li Dong", "authorId": "145307652"}, {"name": "Nan Yang", "authorId": "144610884"}, {"name": "Wenhui Wang", "authorId": "51456429"}, {"name": "Furu Wei", "authorId": "49807919"}, {"name": "Xiaodong Liu", "authorId": "46522098"}, {"name": "Yu Wang", "authorId": "72682749"}, {"name": "Jianfeng Gao", "authorId": "1800422"}, {"name": "M. Zhou", "authorId": "143849609"}, {"name": "H. Hon", "authorId": "145058181"}], "n_citations": 1560}, "snippets": ["This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL."], "score": 0.0}, {"id": "(Liu et al., 2018)", "paper": {"corpus_id": 3608234, "title": "Generating Wikipedia by Summarizing Long Sequences", "year": 2018, "venue": "International Conference on Learning Representations", "authors": [{"name": "Peter J. Liu", "authorId": "35025299"}, {"name": "Mohammad Saleh", "authorId": "144413479"}, {"name": "Etienne Pot", "authorId": "38627717"}, {"name": "Ben Goodrich", "authorId": "2065067542"}, {"name": "Ryan Sepassi", "authorId": "35474601"}, {"name": "Lukasz Kaiser", "authorId": "40527594"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}], "n_citations": 801}, "snippets": ["We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations."], "score": 0.0}, {"id": "(Patil et al., 2024)", "paper": {"corpus_id": 268157336, "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)", "year": 2024, "venue": "Applied Sciences", "authors": [{"name": "Rajvardhan Patil", "authorId": "2289385425"}, {"name": "Venkat Gudivada", "authorId": "117730513"}], "n_citations": 80}, "snippets": ["Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens", "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence", "In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."], "score": 0.77587890625}, {"id": "(Yin et al., 2024)", "paper": {"corpus_id": 270702559, "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Kai Yin", "authorId": "2265383225"}, {"name": "Chengkai Liu", "authorId": "2308073678"}, {"name": "Ali Mostafavi", "authorId": "2258714985"}, {"name": "Xia Hu", "authorId": "2308068627"}], "n_citations": 12}, "snippets": ["Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."], "score": 0.74267578125}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 257050658, "title": "Parallel Sentence-Level Explanation Generation for Real-World Low-Resource Scenarios", "year": 2023, "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "authors": [{"name": "Y. Liu", "authorId": "1679704"}, {"name": "Xiaokang Chen", "authorId": "2292210488"}, {"name": "Qianwen Dai", "authorId": "2072900666"}], "n_citations": 4}, "snippets": ["For the modification of the self-attention mask, because the decoder input is the copied sequence of encoder input, the self-attention module is allowed to attend all positions, rather than only left positions in the conventional Transformer decoder. Therefore, the self-attention mask is replaced with a non-causal mask in our non-autoregressive decoder."], "score": 0.638671875}, {"id": "(Xiao et al., 2022)", "paper": {"corpus_id": 248266379, "title": "A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond", "year": 2022, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Yisheng Xiao", "authorId": "152678922"}, {"name": "Lijun Wu", "authorId": "47767550"}, {"name": "Junliang Guo", "authorId": "13838086"}, {"name": "Juntao Li", "authorId": "2109013629"}, {"name": "M. Zhang", "authorId": "39767557"}, {"name": "Tao Qin", "authorId": "143826491"}, {"name": "Tie-Yan Liu", "authorId": "2110264337"}], "n_citations": 87}, "snippets": ["(1) Specifically, AT models need to prevent earlier decoding steps from peeking at information from later steps. Therefore, the constraint of an autoregressive factorization of the output distribution is required, and they adopt the strict causal mask by applying a lower triangular matrix in the self-attention module of the conventional Transformer decoder (Vaswani et al., 2017). (2) However, for NAT models, including the iteration-based NAT models, this constraint is no longer necessary, so they adopt the unmasked self-attention over all target tokens [16]."], "score": 0.75}, {"id": "(Vaswani et al., 2017)", "paper": {"corpus_id": 13756489, "title": "Attention is All you Need", "year": 2017, "venue": "Neural Information Processing Systems", "authors": [{"name": "Ashish Vaswani", "authorId": "40348417"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Niki Parmar", "authorId": "3877127"}, {"name": "Jakob Uszkoreit", "authorId": "39328010"}, {"name": "Llion Jones", "authorId": "145024664"}, {"name": "Aidan N. Gomez", "authorId": "19177000"}, {"name": "Lukasz Kaiser", "authorId": "40527594"}, {"name": "I. Polosukhin", "authorId": "3443442"}], "n_citations": 132444}, "snippets": ["The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."], "score": 0.0}, {"id": "(Nie et al., 2025)", "paper": {"corpus_id": 277349741, "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tong Nie", "authorId": "94168461"}, {"name": "Jiangming Sun", "authorId": "2028643500"}, {"name": "Wei Ma", "authorId": "2277421553"}], "n_citations": 4}, "snippets": ["\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022)."], "score": 0.658203125}, {"id": "(Tay et al., 2022)", "paper": {"corpus_id": 252780443, "title": "UL2: Unifying Language Learning Paradigms", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yi Tay", "authorId": "144447820"}, {"name": "Mostafa Dehghani", "authorId": "3226635"}, {"name": "Vinh Q. Tran", "authorId": "2057663102"}, {"name": "Xavier Garc\u00eda", "authorId": "143936294"}, {"name": "Jason Wei", "authorId": "119640649"}, {"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Dara Bahri", "authorId": "2119725651"}, {"name": "Tal Schuster", "authorId": "32303439"}, {"name": "H. Zheng", "authorId": "2115689465"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "N. Houlsby", "authorId": "2815290"}, {"name": "Donald Metzler", "authorId": "1680617"}], "n_citations": 313}, "snippets": ["The line between decoder-only and encoder-decoder models is less clear. PrefixLM models are almost encoder-decoder models with shared parameters (but not quite). From an inductive bias point of view, there are multiple differences. Encoder-Decoder models process input and targets independently with a different set of parameters. This is a form of sparsity where different set of parameters are used for different tokens. Encoder-Decoder models also have a cross attention component that connects input tokens to target tokens. Meanwhile, decoder-only models process inputs and targets by concatenating them. Hence, the representations of inputs and targets are concurrently build layer by layer as the input/targets propagate up the network. Conversely, the decoder in Encoder-decoder models generally only looks at the fully processed encoder input. Overall, the inductive bias of PrefixLM decoder-only models and Encoder-Decoder models could be pretty similar modulo the subtle differences stated above. The distinct property is that Encoder-Decoder models are generally approximately 2x parameters of a decoder-only model when compute-matched."], "score": 0.84814453125}, {"id": "(Saha et al., 2023)", "paper": {"corpus_id": 263829839, "title": "LLM for SoC Security: A Paradigm Shift", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Dipayan Saha", "authorId": "2256992493"}, {"name": "Shams Tarek", "authorId": "2114625129"}, {"name": "Katayoon Yahyaei", "authorId": "2256991081"}, {"name": "Sujan Kumar Saha", "authorId": "2231854143"}, {"name": "Jingbo Zhou", "authorId": "2257235852"}, {"name": "M. Tehranipoor", "authorId": "145954982"}, {"name": "Farimah Farahmandi", "authorId": "1997019"}], "n_citations": 54}, "snippets": ["The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named", ".In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens."], "score": 0.79833984375}, {"id": "(Raffel et al., 2019)", "paper": {"corpus_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019, "venue": "Journal of machine learning research", "authors": [{"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Wei Li", "authorId": "2157338362"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 20336}, "snippets": ["Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."], "score": 0.0}], "table": null}, {"title": "Encoder-Decoder Models", "tldr": "Encoder-decoder models combine an encoder that processes input using bidirectional attention with a decoder that generates output using causal attention. This architecture excels at sequence-to-sequence tasks like translation and summarization by leveraging cross-attention mechanisms to connect the encoded input to the generated output. (20 sources)", "text": "\nEncoder-decoder models represent the original architecture introduced in the vanilla Transformer <Paper corpusId=\"13756489\" paperTitle=\"(Vaswani et al., 2017)\" isShortName></Paper>. This architecture consists of two distinct components: an encoder that processes the input sequence and a decoder that generates the output sequence <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\nThe encoder transforms input tokens into contextual representations using bidirectional self-attention, allowing each token to attend to all positions in the input sequence <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"221702858\" paperTitle=\"(Tay et al., 2020)\" isShortName></Paper>. This bidirectional processing enables comprehensive understanding of the input context from both directions.\n\nIn contrast, the decoder component employs causal self-attention, where each token can only attend to itself and previous tokens, ensuring autoregressive generation <Paper corpusId=\"263729712\" paperTitle=\"(Alomari et al., 2023)\" isShortName></Paper> <Paper corpusId=\"261101164\" paperTitle=\"(Kuang et al., 2023)\" isShortName></Paper>. Crucially, the decoder also contains a cross-attention mechanism that connects the decoder states with the encoder outputs, allowing the model to align generated tokens with the input sequence <Paper corpusId=\"277128409\" paperTitle=\"(Ghosh et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nThis architecture is particularly well-suited for sequence-to-sequence tasks that involve transforming an input sequence into a different output sequence <Paper corpusId=\"267402678\" paperTitle=\"(Li, 2024)\" isShortName></Paper>. Examples include:\n\n- Machine translation <Paper corpusId=\"231645376\" paperTitle=\"(Kawara et al., 2021)\" isShortName></Paper> <Paper corpusId=\"234785837\" paperTitle=\"(Nguyen et al., 2021)\" isShortName></Paper>\n- Text summarization <Paper corpusId=\"267402678\" paperTitle=\"(Li, 2024)\" isShortName></Paper>\n- Dialogue generation <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>\n\nPopular encoder-decoder models include T5 <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>, BART <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>, and UniLM <Paper corpusId=\"147704286\" paperTitle=\"(Dong et al., 2019)\" isShortName></Paper>. These models are typically trained with objectives like span corruption or masked span prediction, where the model learns to recover masked portions of text <Paper corpusId=\"261494010\" paperTitle=\"(Wei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"267211690\" paperTitle=\"(Uludougan et al., 2024)\" isShortName></Paper>.\n\nA key distinction between encoder-decoder models and decoder-only models is that the former process input and target sequences independently with different parameters, while the latter concatenate inputs and targets <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>. This separation in encoder-decoder models creates a form of parameter sparsity, with different parameters handling different aspects of the task <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>.\n\nHowever, encoder-decoder models generally require approximately twice as many parameters as decoder-only models with equivalent compute, which has contributed to the recent popularity of decoder-only architectures for large language models <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>. Despite this, encoder-decoder models remain valuable for tasks requiring structured transformation between distinct input and output sequences <Paper corpusId=\"277626915\" paperTitle=\"(Rankovi'c et al., 2025)\" isShortName></Paper>.\n\nA specialized variant is the cascaded encoder model, which combines a causal encoder for streaming processing with a non-causal encoder for more comprehensive analysis, allowing the system to operate in both streaming and non-streaming modes <Paper corpusId=\"248157514\" paperTitle=\"(Ding et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Vaswani et al., 2017)", "paper": {"corpus_id": 13756489, "title": "Attention is All you Need", "year": 2017, "venue": "Neural Information Processing Systems", "authors": [{"name": "Ashish Vaswani", "authorId": "40348417"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Niki Parmar", "authorId": "3877127"}, {"name": "Jakob Uszkoreit", "authorId": "39328010"}, {"name": "Llion Jones", "authorId": "145024664"}, {"name": "Aidan N. Gomez", "authorId": "19177000"}, {"name": "Lukasz Kaiser", "authorId": "40527594"}, {"name": "I. Polosukhin", "authorId": "3443442"}], "n_citations": 132444}, "snippets": ["The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."], "score": 0.0}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 266755678, "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference", "year": 2024, "venue": "Neurocomputing", "authors": [{"name": "Yi-Hsueh Liu", "authorId": "2116426849"}, {"name": "Haoyang He", "authorId": "2155082967"}, {"name": "Tianle Han", "authorId": "2184719751"}, {"name": "Xu Zhang", "authorId": "2273584640"}, {"name": "Mengyuan Liu", "authorId": "2210636248"}, {"name": "Jiaming Tian", "authorId": "2257433902"}, {"name": "Yutong Zhang", "authorId": "2257095790"}, {"name": "Jiaqi Wang", "authorId": "2110238778"}, {"name": "Xiaohui Gao", "authorId": "2277869261"}, {"name": "Tianyang Zhong", "authorId": "2215167446"}, {"name": "Yi Pan", "authorId": "2221032216"}, {"name": "Shaochen Xu", "authorId": "2211904452"}, {"name": "Zihao Wu", "authorId": "2263593041"}, {"name": "Zheng Liu", "authorId": "2145977326"}, {"name": "Xin Zhang", "authorId": "2257586495"}, {"name": "Shu Zhang", "authorId": "2277750447"}, {"name": "Xintao Hu", "authorId": "1742535"}, {"name": "Tuo Zhang", "authorId": "49104946"}, {"name": "Ning Qiang", "authorId": "2251076040"}, {"name": "Tianming Liu", "authorId": "2254792886"}, {"name": "Bao Ge", "authorId": "2257302793"}], "n_citations": 74}, "snippets": ["LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."], "score": 0.8115234375}, {"id": "(Yin et al., 2024)", "paper": {"corpus_id": 270702559, "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Kai Yin", "authorId": "2265383225"}, {"name": "Chengkai Liu", "authorId": "2308073678"}, {"name": "Ali Mostafavi", "authorId": "2258714985"}, {"name": "Xia Hu", "authorId": "2308068627"}], "n_citations": 12}, "snippets": ["Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."], "score": 0.74267578125}, {"id": "(Zheng et al., 2023)", "paper": {"corpus_id": 261064777, "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks", "year": 2023, "venue": "Empirical Software Engineering", "authors": [{"name": "Zibin Zheng", "authorId": "2148256392"}, {"name": "Kai-Chun Ning", "authorId": "2115304"}, {"name": "Jiachi Chen", "authorId": "2254800142"}, {"name": "Yanlin Wang", "authorId": "2214155529"}, {"name": "Wenqing Chen", "authorId": "2274095496"}, {"name": "Lianghong Guo", "authorId": "2217902484"}, {"name": "Weicheng Wang", "authorId": "2233023641"}], "n_citations": 76}, "snippets": ["Encoder-decoder Architecture: The vanilla Transformer proposed in (Vaswani et al., 2017) is based on encoder-decoder architecture, which comprises separate encoder and decoder components. The encoder processes the input sequence and captures its latent representation, which is then used by the decoder to generate the output sequence autoregressively. This architecture is well-suited for tasks involving sequence-to-sequence mapping, such as machine translation, text summarization, and dialogue generation.\n\nCausal Decoder Architecture: The causal decoder architecture is commonly implemented as a stack of decoder layers. It utilizes a diagonal mask matrix, allowing each token to only have access to information from preceding tokens. This constraint ensures a unidirectional and autoregressive generation process."], "score": 0.64697265625}, {"id": "(Tay et al., 2020)", "paper": {"corpus_id": 221702858, "title": "Efficient Transformers: A Survey", "year": 2020, "venue": "ACM Computing Surveys", "authors": [{"name": "Yi Tay", "authorId": "144447820"}, {"name": "Mostafa Dehghani", "authorId": "3226635"}, {"name": "Dara Bahri", "authorId": "11774695"}, {"name": "Donald Metzler", "authorId": "1680617"}], "n_citations": 1128}, "snippets": ["In encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the selfattention used in the encoder need not. Fulfilling this requirement can prove challenging for many efficient self-attention designs."], "score": 0.806640625}, {"id": "(Alomari et al., 2023)", "paper": {"corpus_id": 263729712, "title": "Warm-Starting for Improving the Novelty of Abstractive Summarization", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Ayham Alomari", "authorId": "2130042263"}, {"name": "A. S. Al-Shamayleh", "authorId": "1403466899"}, {"name": "N. Idris", "authorId": "36826893"}, {"name": "Aznul Qalid Md Sabri", "authorId": "2049063550"}, {"name": "I. Alsmadi", "authorId": "1770016"}, {"name": "Danah Omary", "authorId": "2182454665"}], "n_citations": 1}, "snippets": ["For the decoder part, we leverage GPT2, BERT, RoBERTa, and Ernie 2.0. GPT2 is trained on open-ended autoregressive generation with the objective of predicting the next word in a sequence of a few token starters (causal language modeling) utilizing only the Transformer's unidirectional ''left-to-right'' self-attention decoder. Theoretically, GPT2 is claimed to be the optimal design for use as a decoder. However, with a few adjustments, bi-directional encoderonly pretrained models such as BERT, RoBERTa, and Ernie 2.0 may simply be adapted as decoders. To compare the encoder and decoder architectures of the Transformer, encoder blocks consist solely of a bi-directional self-attention layer and two feed-forward layers. By contrast, decoder blocks have a unidirectional self-attention layer, a crossattention layer, and two feed-forward layers."], "score": 0.6669921875}, {"id": "(Kuang et al., 2023)", "paper": {"corpus_id": 261101164, "title": "DLIP: Distilling Language-Image Pre-training", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Huafeng Kuang", "authorId": "1380215283"}, {"name": "Jie Wu", "authorId": "2118432533"}, {"name": "Xiawu Zheng", "authorId": "51056401"}, {"name": "Ming Li", "authorId": "2150654378"}, {"name": "Xuefeng Xiao", "authorId": "2118724465"}, {"name": "Rui Wang", "authorId": "39077217"}, {"name": "Minghang Zheng", "authorId": "2026322565"}, {"name": "Rongrong Ji", "authorId": "1572139630"}], "n_citations": 4}, "snippets": ["The structure of the decoder is similar to the text encoder, and the difference is that it replaces the bidirectional self-attention layers with causal self-attention layers."], "score": 0.650390625}, {"id": "(Ghosh et al., 2024)", "paper": {"corpus_id": 277128409, "title": "A review on the applications of Transformer-based language models for nucleotide sequence analysis", "year": 2024, "venue": "Computational and Structural Biotechnology Journal", "authors": [{"name": "Nimisha Ghosh", "authorId": "2343636534"}, {"name": "Daniele Santoni", "authorId": "2299143904"}, {"name": "I. Saha", "authorId": "2105376090"}, {"name": "Giovanni Felici", "authorId": "2261390733"}], "n_citations": 0}, "snippets": ["The functioning of the Decoder has many parts in common with the Encoder, yet there are some major differences. In a nutshell, the Decoder processes the output component of the input-output pairs of the training data; encodes such information and produces its own embedding using a multi-head attention scheme similar to the Encoder, but: \n\n1. once the output has been encoded, it is combined with the  and  matrices coming from the Encoder. This is the step where Transformers learn the relation between the input of the training (e.g., the question) and the output (e.g., the answer to the question); considering the question-answer example, the Decoder performs cross-attention on the Encoder output (which represents the question) while processing the answer; 2. at the end of the process a linear layer has a number of output neurons equal to the size of the vocabulary; such network uses a softmax function to produce likelihood for each term in the vocabulary. Then, the term with the highest likelihood is the output of the Decoder i.e. the predicted next word; 3. The Decoder produces the output one word at a time; in the training phase, knowing the correct word, the error is computed and used to drive the backpropagation step and the correction of the weights to reduce the error", ".For example, the weights obtained in pretraining for Encoder-only model like BERT, are based on randomly masked words; on the other hand, for Decoder-only model like GPT, causal masking is used, where only the future tokens are masked, and the model predicts the next token given past tokens."], "score": 0.83349609375}, {"id": "(Patil et al., 2024)", "paper": {"corpus_id": 268157336, "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)", "year": 2024, "venue": "Applied Sciences", "authors": [{"name": "Rajvardhan Patil", "authorId": "2289385425"}, {"name": "Venkat Gudivada", "authorId": "117730513"}], "n_citations": 80}, "snippets": ["Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens", "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence", "In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."], "score": 0.77587890625}, {"id": "(Li, 2024)", "paper": {"corpus_id": 267402678, "title": "The evolution, applications, and future prospects of large language models: An in-depth overview", "year": 2024, "venue": "Applied and Computational Engineering", "authors": [{"name": "Jiayin Li", "authorId": "2282449950"}], "n_citations": 2}, "snippets": ["As shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision."], "score": 0.78076171875}, {"id": "(Kawara et al., 2021)", "paper": {"corpus_id": 231645376, "title": "Preordering Encoding on Transformer for Translation", "year": 2021, "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "authors": [{"name": "Yuki Kawara", "authorId": "46178138"}, {"name": "Chenhui Chu", "authorId": "2427516"}, {"name": "Yuki Arase", "authorId": "3043844"}], "n_citations": 17}, "snippets": ["The difference in word orders between source and target languages is a serious hurdle for machine translation. Preordering methods, which reorder the words in a source sentence before translation to obtain a similar word ordering with a target language, significantly improve the quality in statistical machine translation. While the information on the preordering position improved the translation quality in recurrent neural network-based models, questions such as how to use preordering information and whether it is helpful for the Transformer model remain unaddressed. In this article, we successfully employed preordering techniques in the Transformer-based neural machine translation. Specifically, we proposed a novel <italic>preordering encoding</italic> that exploits the reordering information of the source and target sentences as positional encoding in the Transformer model. Experimental results on ASPEC Japanese\u2013English and WMT 2015 English\u2013German, English\u2013Czech, and English\u2013Russian translation tasks confirmed that the proposed method significantly improved the translation quality evaluated by the BLEU scores of the Transformer model by <inline-formula><tex-math notation=\"LaTeX\">${\\text{1.34}}$</tex-math></inline-formula> points in the Japanese\u2013to\u2013English task, <inline-formula><tex-math notation=\"LaTeX\">${\\text{2.19}}$</tex-math></inline-formula> points in the English\u2013to\u2013German task, <inline-formula><tex-math notation=\"LaTeX\">${\\text{0.15}}$</tex-math></inline-formula> points in the Czech\u2013to\u2013English task, and <inline-formula><tex-math notation=\"LaTeX\">${\\text {1.48}}$</tex-math></inline-formula> points in the English\u2013to\u2013Russian task."], "score": 0.0}, {"id": "(Nguyen et al., 2021)", "paper": {"corpus_id": 234785837, "title": "Improving Transformer-Based Neural Machine Translation with Prior Alignments", "year": 2021, "venue": "Complex", "authors": [{"name": "Thien Nguyen", "authorId": "33775047"}, {"name": "Lam Nguyen", "authorId": "2151126066"}, {"name": "Phuoc Tran", "authorId": "153460631"}, {"name": "Huu Nguyen", "authorId": "122619186"}], "n_citations": 22}, "snippets": ["Transformer is a neural machine translation model which revolutionizes machine translation. Compared with traditional statistical machine translation models and other neural machine translation models, the recently proposed transformer model radically and fundamentally changes machine translation with its self-attention and cross-attention mechanisms. These mechanisms effectively model token alignments between source and target sentences. It has been reported that the transformer model provides accurate posterior alignments. In this work, we empirically prove the reverse effect, showing that prior alignments help transformer models produce better translations. Experiment results on Vietnamese-English news translation task show not only the positive effect of manually annotated alignments on transformer models but also the surprising outperformance of statistically constructed alignments reinforced with the flexibility of token-type selection over manual alignments in improving transformer models. Statistically constructed word-to-lemma alignments are used to train a word-to-word transformer model. The novel hybrid transformer model improves the baseline transformer model and transformer model trained with manual alignments by 2.53 and 0.79 BLEU, respectively. In addition to BLEU score, we make limited human judgment on translation results. Strong correlation between human and machine judgment confirms our findings."], "score": 0.0}, {"id": "(Raffel et al., 2019)", "paper": {"corpus_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019, "venue": "Journal of machine learning research", "authors": [{"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Wei Li", "authorId": "2157338362"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 20336}, "snippets": ["Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."], "score": 0.0}, {"id": "(Saha et al., 2023)", "paper": {"corpus_id": 263829839, "title": "LLM for SoC Security: A Paradigm Shift", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Dipayan Saha", "authorId": "2256992493"}, {"name": "Shams Tarek", "authorId": "2114625129"}, {"name": "Katayoon Yahyaei", "authorId": "2256991081"}, {"name": "Sujan Kumar Saha", "authorId": "2231854143"}, {"name": "Jingbo Zhou", "authorId": "2257235852"}, {"name": "M. Tehranipoor", "authorId": "145954982"}, {"name": "Farimah Farahmandi", "authorId": "1997019"}], "n_citations": 54}, "snippets": ["The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named", ".In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens."], "score": 0.79833984375}, {"id": "(Dong et al., 2019)", "paper": {"corpus_id": 147704286, "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Li Dong", "authorId": "145307652"}, {"name": "Nan Yang", "authorId": "144610884"}, {"name": "Wenhui Wang", "authorId": "51456429"}, {"name": "Furu Wei", "authorId": "49807919"}, {"name": "Xiaodong Liu", "authorId": "46522098"}, {"name": "Yu Wang", "authorId": "72682749"}, {"name": "Jianfeng Gao", "authorId": "1800422"}, {"name": "M. Zhou", "authorId": "143849609"}, {"name": "H. Hon", "authorId": "145058181"}], "n_citations": 1560}, "snippets": ["This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL."], "score": 0.0}, {"id": "(Wei et al., 2023)", "paper": {"corpus_id": 261494010, "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair", "year": 2023, "venue": "ESEC/SIGSOFT FSE", "authors": [{"name": "Yuxiang Wei", "authorId": "2237736409"}, {"name": "Chun Xia", "authorId": "145349987"}, {"name": "Lingming Zhang", "authorId": "2237429253"}], "n_citations": 100}, "snippets": ["Encoder-only models use only the encoder component by training using Masked Language Modeling (MLM) [14] objective where a small percentage (e.g., 15%) of the tokens are masked on. The goal of MLM is to recover these masked tokens given the surrounding context. Encoder-only models such as CodeBERT [15] and GraphCodeBERT [22] are designed to provide a representation of the input code to be used for downstream tasks such as code classification [72]. Decoder-only models, on the other hand, aim to autoregressively generate tokens based on all previously generated tokens. CodeGEN [51,52], Codex [10] and PolyCoder [71] are examples of decoder-only LLMs where they can be used for code autocompletion tasks. Different from encoder-and decoder-only LLMs, encoder-decoder models (e.g., CodeT5 [62,63] and PLBART [3]) combine both encoder and decoder together and jointly train both components together. A commonly used pre-training objective for encoder-decoder models is Masked Span Prediction (MSP) where random spans (multiple consecutive tokens) are replaced with single masked tokens and the models learn to fill in the masked span with the correct sequence of tokens. Furthermore, decoder-only models like InCoder [17] can also perform infilling by training through causal language modeling [2] objective."], "score": 0.75537109375}, {"id": "(Uludougan et al., 2024)", "paper": {"corpus_id": 267211690, "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Gokcce Uludougan", "authorId": "2281033147"}, {"name": "Zeynep Yirmibecsouglu Balal", "authorId": "2281033141"}, {"name": "Furkan Akkurt", "authorId": "2174736343"}, {"name": "Melikcsah Turker", "authorId": "2281033264"}, {"name": "Onur Gungor", "authorId": "9179697"}, {"name": "S. Uskudarli", "authorId": "66493576"}], "n_citations": 12}, "snippets": ["Models that exclusively employ encoders, typically trained with denoising objectives, are geared toward understanding tasks, as exemplified in works such as (Devlin et al., 2019) and (208229926). Conversely, models that exclusively use decoders are designed for generation tasks, employing causal language modeling, as demonstrated in various studies (Radford et al., 2019)Brown et al., 2020;Touvron et al., 2023). The Text-to-Text Transformer (T5) (Raffel et al., 2019), on the other hand, employs an encoder-decoder architecture and undergoes pretraining with a denoising objective referred to as span corruption. UniLM (Dong et al., 2019) is also an encoder-decoder model, but pre-trained using unidirectional, bidirectional, and sequence-to-sequence language modeling. This can be seen as a combination of causal and denoising objectives."], "score": 0.64208984375}, {"id": "(Tay et al., 2022)", "paper": {"corpus_id": 252780443, "title": "UL2: Unifying Language Learning Paradigms", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yi Tay", "authorId": "144447820"}, {"name": "Mostafa Dehghani", "authorId": "3226635"}, {"name": "Vinh Q. Tran", "authorId": "2057663102"}, {"name": "Xavier Garc\u00eda", "authorId": "143936294"}, {"name": "Jason Wei", "authorId": "119640649"}, {"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Dara Bahri", "authorId": "2119725651"}, {"name": "Tal Schuster", "authorId": "32303439"}, {"name": "H. Zheng", "authorId": "2115689465"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "N. Houlsby", "authorId": "2815290"}, {"name": "Donald Metzler", "authorId": "1680617"}], "n_citations": 313}, "snippets": ["The line between decoder-only and encoder-decoder models is less clear. PrefixLM models are almost encoder-decoder models with shared parameters (but not quite). From an inductive bias point of view, there are multiple differences. Encoder-Decoder models process input and targets independently with a different set of parameters. This is a form of sparsity where different set of parameters are used for different tokens. Encoder-Decoder models also have a cross attention component that connects input tokens to target tokens. Meanwhile, decoder-only models process inputs and targets by concatenating them. Hence, the representations of inputs and targets are concurrently build layer by layer as the input/targets propagate up the network. Conversely, the decoder in Encoder-decoder models generally only looks at the fully processed encoder input. Overall, the inductive bias of PrefixLM decoder-only models and Encoder-Decoder models could be pretty similar modulo the subtle differences stated above. The distinct property is that Encoder-Decoder models are generally approximately 2x parameters of a decoder-only model when compute-matched."], "score": 0.84814453125}, {"id": "(Rankovi'c et al., 2025)", "paper": {"corpus_id": 277626915, "title": "GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Bojana Rankovi'c", "authorId": "2219925647"}, {"name": "Philippe Schwaller", "authorId": "2239074343"}], "n_citations": 1}, "snippets": ["LLMs can follow different architectural designs: encoder-only (e.g., BERT 2), decoder-only (e.g., Qwen 52), and encoder-decoder (e.g., T5 49). Encoder-based models process the full input bidirectionally and are suited for classification and regression. Decoder-only models generate text autoregressively with causal masking. Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks."], "score": 0.703125}, {"id": "(Ding et al., 2022)", "paper": {"corpus_id": 248157514, "title": "A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes", "year": 2022, "venue": "Interspeech", "authors": [{"name": "Shaojin Ding", "authorId": "51267247"}, {"name": "Weiran Wang", "authorId": "2117928344"}, {"name": "Ding Zhao", "authorId": "47783130"}, {"name": "Tara N. Sainath", "authorId": "1784851"}, {"name": "Yanzhang He", "authorId": "2145999837"}, {"name": "R. David", "authorId": "2061545932"}, {"name": "Rami Botros", "authorId": "2004995"}, {"name": "Xin Wang", "authorId": "2153688851"}, {"name": "R. Panigrahy", "authorId": "1679451"}, {"name": "Qiao Liang", "authorId": "2055746849"}, {"name": "Dongseong Hwang", "authorId": "2241835900"}, {"name": "Ian McGraw", "authorId": "143685627"}, {"name": "Rohit Prabhavalkar", "authorId": "2557391"}, {"name": "Trevor Strohman", "authorId": "2985957"}], "n_citations": 17}, "snippets": ["The original cascaded encoder model (Narayanan et al., 2020) uses a shared RNN-T decoder. The decoder works with a causal encoder in the first pass to provide streaming recognition results, and works with an additional non-causal encoder that sits on top of the causal encoder to provide more accurate final results, leveraging audio right context extracted by the noncausal encoder."], "score": 0.66259765625}], "table": null}, {"title": "Comparison of Model Architectures", "tldr": "The three main language model architectures (encoder-only, decoder-only, and encoder-decoder) have distinct strengths and limitations based on their attention mechanisms and training objectives. Encoder-only models excel at understanding tasks through bidirectional attention, decoder-only models dominate text generation through causal attention, and encoder-decoder models offer versatility for sequence-to-sequence tasks through their combined approach. (19 sources)", "text": "\nWhen comparing language model architectures, the key distinctions emerge from their attention mechanisms, training objectives, and the tasks they excel at. These differences directly impact their performance characteristics and application domains.\n\nEncoder-only models like BERT utilize bidirectional attention mechanisms that allow each token to attend to all positions in the input sequence <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper> <Paper corpusId=\"257496129\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>. This bidirectional processing enables them to build richer contextual representations than causal models, making them particularly effective for natural language understanding (NLU) tasks such as classification, feature extraction, and sentiment analysis <Paper corpusId=\"268247581\" paperTitle=\"(Pei et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277857043\" paperTitle=\"(Beiranvand et al., 2025)\" isShortName></Paper>. Their typical training objective is masked language modeling (MLM), where they learn to recover masked tokens based on surrounding context <Paper corpusId=\"261494010\" paperTitle=\"(Wei et al., 2023)\" isShortName></Paper>.\n\nDecoder-only models, exemplified by GPT series, employ causal attention where tokens can only attend to themselves and preceding tokens <Paper corpusId=\"263729712\" paperTitle=\"(Alomari et al., 2023)\" isShortName></Paper> <Paper corpusId=\"257496129\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>. These models are trained with next-token prediction objectives (causal language modeling) and excel at text generation tasks <Paper corpusId=\"277857043\" paperTitle=\"(Beiranvand et al., 2025)\" isShortName></Paper>. Despite their limitations in building rich bidirectional representations, causal decoder-only models have become the dominant architecture for state-of-the-art large language models, demonstrating superior zero-shot generalization immediately after pretraining <Paper corpusId=\"253420279\" paperTitle=\"(Scao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\nNon-causal decoder models (prefix decoders) represent a hybrid approach that combines bidirectional attention for input processing with unidirectional attention for generation <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>. This architecture allows models to build richer representations of input text while maintaining generative capabilities <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. Recent research has demonstrated that causal decoder models can be effectively adapted to incorporate bidirectional attention, significantly improving their performance on encoder-centric tasks <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper>.\n\nEncoder-decoder models like T5 and BART combine both components, using bidirectional attention in the encoder and causal attention in the decoder, connected by a cross-attention mechanism <Paper corpusId=\"270832367\" paperTitle=\"(Busto-Castineira et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>. This architecture is especially effective for sequence-to-sequence tasks such as translation and summarization <Paper corpusId=\"277626915\" paperTitle=\"(Rankovi'c et al., 2025)\" isShortName></Paper> <Paper corpusId=\"267211690\" paperTitle=\"(Uludougan et al., 2024)\" isShortName></Paper>. However, encoder-decoder models typically require approximately twice as many parameters as decoder-only models with equivalent compute, contributing to the recent popularity of decoder-only architectures <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>.\n\nA crucial distinction between architectures lies in how they process information. In encoder-decoder models, the encoder processes input and the decoder processes targets independently with separate parameters <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>. In contrast, decoder-only models concatenate inputs and targets, processing them together with shared parameters <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>. This fundamental difference affects their parameter efficiency and inductive biases.\n\nAnother significant distinction is in the error accumulation patterns. Causal decoder models suffer from cumulative quantization errors in their attention mechanisms, particularly affecting tokens appearing later in the sequence <Paper corpusId=\"260886785\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>. This is because each token's representation depends on a different number of attention probabilities affected by quantization errors, creating an uneven distribution of errors throughout the sequence.\n\nWhen choosing between architectures, the task requirements should guide the decision. For tasks requiring deep semantic understanding without generation, encoder-only models offer the strongest contextual representations through bidirectional attention <Paper corpusId=\"258461112\" paperTitle=\"(Jain et al., 2022)\" isShortName></Paper>. For pure generation tasks, causal decoder-only models provide the most natural fit with their autoregressive nature <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>. For tasks requiring both understanding and transformation between sequences, encoder-decoder models offer the most balanced approach <Paper corpusId=\"270560675\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. The distinction between these architectures is not merely technical but reflects fundamental differences in how they process information and what capabilities they prioritize <Paper corpusId=\"221702858\" paperTitle=\"(Tay et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(He et al., 2023)", "paper": {"corpus_id": 257496129, "title": "Representation Learning for Stack Overflow Posts: How Far Are We?", "year": 2023, "venue": "ACM Transactions on Software Engineering and Methodology", "authors": [{"name": "Junda He", "authorId": "2158107537"}, {"name": "Zhou Xin", "authorId": "2211431558"}, {"name": "Bowen Xu", "authorId": "2203459"}, {"name": "Ting Zhang", "authorId": "2146322053"}, {"name": "Kisub Kim", "authorId": "35276441"}, {"name": "Zhou Yang", "authorId": "2139059234"}, {"name": "Ferdian Thung", "authorId": "2121315"}, {"name": "I. Irsan", "authorId": "9223952"}, {"name": "David Lo", "authorId": "2150912791"}], "n_citations": 20}, "snippets": ["Transformer-based language models can be categorized into three types: encoder-only, decoder-only, and encoderdecoder models. \n\nEncoder-only models exclusively leverage the encoder stacks of the vanilla transformer [48] architecture. BERT [10] stands as a prominent encoder-only representation model, which learns a bidirectional contextual representation of text.\n\nIn contrast, Decoder-only models consist solely of the decoder components of the original transformer architecture. A notable instance of such models is the GPT [35], GPT operates under a causal language modeling (CLM) framework during its training phase. CLM is a strategy where the model predicts the next token in a sequence while only considering preceding tokens. In other words, this design restricts the model from accessing future tokens in the sequence. \n\nBridging the above approaches, textitEncoder-decoder models integrate both the encoder and decoder components of the transformer architecture. Popular encoder-decoder models involve T5 [37] and BART [23]."], "score": 0.66015625}, {"id": "(Pei et al., 2024)", "paper": {"corpus_id": 268247581, "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qizhi Pei", "authorId": "2171652249"}, {"name": "Lijun Wu", "authorId": "47767791"}, {"name": "Kaiyuan Gao", "authorId": "1944690382"}, {"name": "Jinhua Zhu", "authorId": "151068900"}, {"name": "Yue Wang", "authorId": "2290062348"}, {"name": "Zun Wang", "authorId": "2290024261"}, {"name": "Tao Qin", "authorId": "2267250090"}, {"name": "Rui Yan", "authorId": "2257028545"}], "n_citations": 18}, "snippets": ["Encoder-only models (Devlin et al., 2019) specialize in processing input sequences of biomolecules and text through bi-directional self-attention, making them highly effective for tasks that require an in-depth understanding of the input, such as sentiment analysis and feature extraction in NLP. Thereby in biomolecule domain, encoder-only models establish a bi-directional association between biotokens and text tokens for predictive tasks (Zeng et al., 2022)", "In contrast, decoder-only models [18], [138] employ causal attention to focus on the sequence of previous tokens. This architecture is typically utilized in generative tasks, such as generating text descriptions that match the given molecule or for the reverse task (Liu et al., 2023). Thanks to the autoregressive generation property, decoder-only models are well suitable for instruction following and assistant/agent objectives."], "score": 0.64990234375}, {"id": "(Beiranvand et al., 2025)", "paper": {"corpus_id": 277857043, "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Azadeh Beiranvand", "authorId": "66841167"}, {"name": "S. M. Vahidipour", "authorId": "35409259"}], "n_citations": 0}, "snippets": ["Encoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input.\n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling.\n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step.\n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation."], "score": 0.810546875}, {"id": "(Wei et al., 2023)", "paper": {"corpus_id": 261494010, "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair", "year": 2023, "venue": "ESEC/SIGSOFT FSE", "authors": [{"name": "Yuxiang Wei", "authorId": "2237736409"}, {"name": "Chun Xia", "authorId": "145349987"}, {"name": "Lingming Zhang", "authorId": "2237429253"}], "n_citations": 100}, "snippets": ["Encoder-only models use only the encoder component by training using Masked Language Modeling (MLM) [14] objective where a small percentage (e.g., 15%) of the tokens are masked on. The goal of MLM is to recover these masked tokens given the surrounding context. Encoder-only models such as CodeBERT [15] and GraphCodeBERT [22] are designed to provide a representation of the input code to be used for downstream tasks such as code classification [72]. Decoder-only models, on the other hand, aim to autoregressively generate tokens based on all previously generated tokens. CodeGEN [51,52], Codex [10] and PolyCoder [71] are examples of decoder-only LLMs where they can be used for code autocompletion tasks. Different from encoder-and decoder-only LLMs, encoder-decoder models (e.g., CodeT5 [62,63] and PLBART [3]) combine both encoder and decoder together and jointly train both components together. A commonly used pre-training objective for encoder-decoder models is Masked Span Prediction (MSP) where random spans (multiple consecutive tokens) are replaced with single masked tokens and the models learn to fill in the masked span with the correct sequence of tokens. Furthermore, decoder-only models like InCoder [17] can also perform infilling by training through causal language modeling [2] objective."], "score": 0.75537109375}, {"id": "(Alomari et al., 2023)", "paper": {"corpus_id": 263729712, "title": "Warm-Starting for Improving the Novelty of Abstractive Summarization", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Ayham Alomari", "authorId": "2130042263"}, {"name": "A. S. Al-Shamayleh", "authorId": "1403466899"}, {"name": "N. Idris", "authorId": "36826893"}, {"name": "Aznul Qalid Md Sabri", "authorId": "2049063550"}, {"name": "I. Alsmadi", "authorId": "1770016"}, {"name": "Danah Omary", "authorId": "2182454665"}], "n_citations": 1}, "snippets": ["For the decoder part, we leverage GPT2, BERT, RoBERTa, and Ernie 2.0. GPT2 is trained on open-ended autoregressive generation with the objective of predicting the next word in a sequence of a few token starters (causal language modeling) utilizing only the Transformer's unidirectional ''left-to-right'' self-attention decoder. Theoretically, GPT2 is claimed to be the optimal design for use as a decoder. However, with a few adjustments, bi-directional encoderonly pretrained models such as BERT, RoBERTa, and Ernie 2.0 may simply be adapted as decoders. To compare the encoder and decoder architectures of the Transformer, encoder blocks consist solely of a bi-directional self-attention layer and two feed-forward layers. By contrast, decoder blocks have a unidirectional self-attention layer, a crossattention layer, and two feed-forward layers."], "score": 0.6669921875}, {"id": "(Scao et al., 2022)", "paper": {"corpus_id": 253420279, "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Teven Le Scao", "authorId": "1379806208"}, {"name": "Angela Fan", "authorId": "144270981"}, {"name": "Christopher Akiki", "authorId": "2003696840"}, {"name": "Ellie Pavlick", "authorId": "2949185"}, {"name": "Suzana Ili'c", "authorId": "2066663381"}, {"name": "Daniel Hesslow", "authorId": "80424302"}, {"name": "Roman Castagn'e", "authorId": "2190282134"}, {"name": "A. Luccioni", "authorId": "2993731"}, {"name": "Fran\u00e7ois Yvon", "authorId": "1846431"}, {"name": "Matthias Gall\u00e9", "authorId": "2907260"}, {"name": "J. Tow", "authorId": "50195579"}, {"name": "Alexander M. Rush", "authorId": "2531268"}, {"name": "Stella Biderman", "authorId": "103476203"}, {"name": "Albert Webson", "authorId": "1991019030"}, {"name": "Pawan Sasanka Ammanamanchi", "authorId": "1451644426"}, {"name": "Thomas Wang", "authorId": "2135734748"}, {"name": "Beno\u00eet Sagot", "authorId": "68990982"}, {"name": "Niklas Muennighoff", "authorId": "2037383772"}, {"name": "Albert Villanova del Moral", "authorId": "46219923"}, {"name": "Olatunji Ruwase", "authorId": "2537545"}, {"name": "Rachel Bawden", "authorId": "48983885"}, {"name": "Stas Bekman", "authorId": "32136590"}, {"name": "Angelina McMillan-Major", "authorId": "1584940075"}, {"name": "Iz Beltagy", "authorId": "46181066"}, {"name": "Huu Nguyen", "authorId": "2168170616"}, {"name": "Lucile Saulnier", "authorId": "2113836860"}, {"name": "Samson Tan", "authorId": "145814654"}, {"name": "Pedro Ortiz Suarez", "authorId": "147846651"}, {"name": "Victor Sanh", "authorId": "2285868436"}, {"name": "Hugo Laurenccon", "authorId": "2172404846"}, {"name": "Yacine Jernite", "authorId": "2262249"}, {"name": "Julien Launay", "authorId": "143945447"}, {"name": "Margaret Mitchell", "authorId": "49501003"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Aaron Gokaslan", "authorId": "2273789852"}, {"name": "Adi Simhi", "authorId": "2183598223"}, {"name": "Aitor Soroa Etxabe", "authorId": "2078619062"}, {"name": "Alham Fikri Aji", "authorId": "8129718"}, {"name": "Amit Alfassy", "authorId": "73769093"}, {"name": "Anna Rogers", "authorId": "145046059"}, {"name": "Ariel Kreisberg Nitzav", "authorId": "2190281124"}, {"name": "Canwen Xu", "authorId": "66247317"}, {"name": "Chenghao Mou", "authorId": "35966970"}, {"name": "Chris C. Emezue", "authorId": "1591176064"}, {"name": "Christopher Klamm", "authorId": "2261291789"}, {"name": "Colin Leong", "authorId": "89269402"}, {"name": "Daniel Alexander van Strien", "authorId": "71075073"}, {"name": "David Ifeoluwa Adelani", "authorId": "2518906"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}, {"name": "E. G. Ponferrada", "authorId": "79512668"}, {"name": "Efrat Levkovizh", "authorId": "2190281122"}, {"name": "Ethan Kim", "authorId": "2047591327"}, {"name": "Eyal Natan", "authorId": "2088048322"}, {"name": "F. Toni", "authorId": "2067891070"}, {"name": "G\u00e9rard Dupont", "authorId": "13656138"}, {"name": "Germ\u00e1n Kruszewski", "authorId": "2067996"}, {"name": "Giada Pistilli", "authorId": "2158858559"}, {"name": "Hady ElSahar", "authorId": "2218938"}, {"name": "Hamza Benyamina", "authorId": "90563027"}, {"name": "H. Tran", "authorId": "2057078797"}, {"name": "Ian Yu", "authorId": "47948569"}, {"name": "Idris Abdulmumin", "authorId": "1429833598"}, {"name": "Isaac Johnson", "authorId": "2060080508"}, {"name": "Itziar Gonzalez-Dios", "authorId": "1404791152"}, {"name": "Javier de la Rosa", "authorId": "144979591"}, {"name": "Jenny Chim", "authorId": "2164872258"}, {"name": "Jesse Dodge", "authorId": "34176020"}, {"name": "Jian Zhu", "authorId": "144549416"}, {"name": "Jonathan Chang", "authorId": "2116123009"}, {"name": "Jorg Frohberg", "authorId": "2146695800"}, {"name": "Josephine Tobing", "authorId": "2094755167"}, {"name": "J. Bhattacharjee", "authorId": "143779690"}, {"name": "Khalid Almubarak", "authorId": "90615055"}, {"name": "Kimbo Chen", "authorId": "2157630500"}, {"name": "Kyle Lo", "authorId": "46258841"}, {"name": "L. V. Werra", "authorId": "51128119"}, {"name": "Leon Weber", "authorId": "20308468"}, {"name": "Long Phan", "authorId": null}, {"name": "Loubna Ben Allal", "authorId": "2190281230"}, {"name": "Ludovic Tanguy", "authorId": "77970446"}, {"name": "Manan Dey", "authorId": "1879591269"}, {"name": "M. Mu\u00f1oz", "authorId": "115568186"}, {"name": "Maraim Masoud", "authorId": "153528116"}, {"name": "Mar\u00eda Grandury", "authorId": "2176184513"}, {"name": "Mario vSavsko", "authorId": "2125821515"}, {"name": "Max Huang", "authorId": "2112504552"}, {"name": "Maximin Coavoux", "authorId": "3443469"}, {"name": "Mayank Singh", "authorId": "145431050"}, {"name": "Mike Tian-Jian Jiang", "authorId": "5745221"}, {"name": "Minh Chien Vu", "authorId": "1484109150"}, {"name": "M. A. Jauhar", "authorId": "2097304324"}, {"name": "Mustafa Ghaleb", "authorId": "2721586"}, {"name": "Nishant Subramani", "authorId": "34202134"}, {"name": "Nora Kassner", "authorId": "9529535"}, {"name": "Nurulaqilla Khamis", "authorId": "37441312"}, {"name": "Olivier Nguyen", "authorId": "2089233725"}, {"name": "Omar Espejel", "authorId": "2190280842"}, {"name": "Ona de Gibert", "authorId": "51436367"}, {"name": "Paulo Villegas", "authorId": "2176184659"}, {"name": "Peter Henderson", "authorId": "2071773966"}, {"name": "Pierre Colombo", "authorId": "46985469"}, {"name": "Priscilla Amuok", "authorId": "2190281321"}, {"name": "Quentin Lhoest", "authorId": "2113836945"}, {"name": "Rheza Harliman", "authorId": "80858030"}, {"name": "Rishi Bommasani", "authorId": "150272855"}, {"name": "R. L'opez", "authorId": "116000979"}, {"name": "Rui Ribeiro", "authorId": null}, {"name": "Salomey Osei", "authorId": "1486204986"}, {"name": "S. Pyysalo", "authorId": "1708916"}, {"name": "Sebastian Nagel", "authorId": "47351277"}, {"name": "Shamik Bose", "authorId": "2795685"}, {"name": "Shamsuddeen Hassan Muhammad", "authorId": "7744881"}, {"name": "S. Sharma", "authorId": "1409842673"}, {"name": "S. Longpre", "authorId": "29909347"}, {"name": "Somaieh Nikpoor", "authorId": "2099315138"}, {"name": "S. Silberberg", "authorId": "82674724"}, {"name": "S. Pai", "authorId": "2053516473"}, {"name": "S. Zink", "authorId": "2074482488"}, {"name": "Tiago Timponi Torrent", "authorId": "46308692"}, {"name": "Timo Schick", "authorId": "32246932"}, {"name": "Tristan Thrush", "authorId": "1500242049"}, {"name": "V. Danchev", "authorId": "3382327"}, {"name": "Vassilina Nikoulina", "authorId": "2841761"}, {"name": "Veronika Laippala", "authorId": "1796619"}, {"name": "Violette Lepercq", "authorId": "2190280574"}, {"name": "V. Prabhu", "authorId": "2059767242"}, {"name": "Zaid Alyafeai", "authorId": "25098419"}, {"name": "Zeerak Talat", "authorId": "2138053020"}, {"name": "Arun Raja", "authorId": "2048082186"}, {"name": "Benjamin Heinzerling", "authorId": "2266692"}, {"name": "Chenglei Si", "authorId": "152358188"}, {"name": "Elizabeth Salesky", "authorId": "3448427"}, {"name": "Sabrina J. Mielke", "authorId": "27689253"}, {"name": "Wilson Y. Lee", "authorId": "2183377987"}, {"name": "Abheesht Sharma", "authorId": "2051500420"}, {"name": "Andrea Santilli", "authorId": "2065039862"}, {"name": "Antoine Chaffin", "authorId": "2129106958"}, {"name": "Arnaud Stiegler", "authorId": "114762823"}, {"name": "Debajyoti Datta", "authorId": "2852125"}, {"name": "Eliza Szczechla", "authorId": "50812522"}, {"name": "Gunjan Chhablani", "authorId": "1509809381"}, {"name": "Han Wang", "authorId": "144407394"}, {"name": "Harshit Pandey", "authorId": "144834468"}, {"name": "Hendrik Strobelt", "authorId": "2879705"}, {"name": "Jason Alan Fries", "authorId": "31592365"}, {"name": "Jos Rozen", "authorId": "120419790"}, {"name": "Leo Gao", "authorId": "2027599537"}, {"name": "Lintang Sutawika", "authorId": "35566806"}, {"name": "M Saiful Bari", "authorId": "31773000"}, {"name": "Maged S. Al-Shaibani", "authorId": "1752627730"}, {"name": "Matteo Manica", "authorId": "35904689"}, {"name": "Nihal V. Nayak", "authorId": "22209084"}, {"name": "Ryan Teehan", "authorId": "2131107966"}, {"name": "Samuel Albanie", "authorId": "7641268"}, {"name": "Sheng Shen", "authorId": "2191455"}, {"name": "Srulik Ben-David", "authorId": "2152318619"}, {"name": "Stephen H. Bach", "authorId": "2870504"}, {"name": "Taewoon Kim", "authorId": "2111181991"}, {"name": "T. Bers", "authorId": "94251255"}, {"name": "Thibault F\u00e9vry", "authorId": "79215748"}, {"name": "Trishala Neeraj", "authorId": "10729963"}, {"name": "Urmish Thakker", "authorId": "70296695"}, {"name": "Vikas Raunak", "authorId": "24025563"}, {"name": "Xiang Tang", "authorId": "2118488348"}, {"name": "Zheng-Xin Yong", "authorId": "1725420331"}, {"name": "Zhiqing Sun", "authorId": "48064856"}, {"name": "Shaked Brody", "authorId": "1720739223"}, {"name": "Y. Uri", "authorId": "2101395835"}, {"name": "Hadar Tojarieh", "authorId": "2190280874"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Jaesung Tae", "authorId": "2112211652"}, {"name": "Jason Phang", "authorId": "80842917"}, {"name": "Ofir Press", "authorId": "40170001"}, {"name": "Conglong Li", "authorId": "2609325"}, {"name": "D. Narayanan", "authorId": "22252150"}, {"name": "Hatim Bourfoune", "authorId": "2190280830"}, {"name": "J. Casper", "authorId": "48991386"}, {"name": "Jeff Rasley", "authorId": "3299496"}, {"name": "Max Ryabinin", "authorId": "1491753352"}, {"name": "Mayank Mishra", "authorId": "1381446720"}, {"name": "Minjia Zhang", "authorId": "67016465"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "Myriam Peyrounette", "authorId": "31758637"}, {"name": "N. Patry", "authorId": "31614549"}, {"name": "Nouamane Tazi", "authorId": "2179884903"}, {"name": "Omar Sanseviero", "authorId": "2186979509"}, {"name": "Patrick von Platen", "authorId": "138609838"}, {"name": "Pierre Cornette", "authorId": "2190281218"}, {"name": "Pierre Franccois Lavall'ee", "authorId": "2190280981"}, {"name": "R. Lacroix", "authorId": "31734741"}, {"name": "Samyam Rajbhandari", "authorId": "32817044"}, {"name": "Sanchit Gandhi", "authorId": "2188737826"}, {"name": "Shaden Smith", "authorId": "2110486618"}, {"name": "S. Requena", "authorId": "2293408"}, {"name": "Suraj Patil", "authorId": "2147312210"}, {"name": "Tim Dettmers", "authorId": "3239480"}, {"name": "Ahmed Baruwa", "authorId": "114850513"}, {"name": "Amanpreet Singh", "authorId": null}, {"name": "Anastasia Cheveleva", "authorId": "2190281235"}, {"name": "Anne-Laure Ligozat", "authorId": "1769176"}, {"name": "Arjun Subramonian", "authorId": "1677386832"}, {"name": "Aur'elie N'ev'eol", "authorId": "2190281078"}, {"name": "Charles Lovering", "authorId": "10727711"}, {"name": "Dan Garrette", "authorId": "2758616"}, {"name": "D. Tunuguntla", "authorId": "70209311"}, {"name": "Ehud Reiter", "authorId": "144568312"}, {"name": "Ekaterina Taktasheva", "authorId": "2051713939"}, {"name": "E. Voloshina", "authorId": "2135526571"}, {"name": "Eli Bogdanov", "authorId": "2158860079"}, {"name": "Genta Indra Winata", "authorId": "9162688"}, {"name": "Hailey Schoelkopf", "authorId": "2184031883"}, {"name": "Jan-Christoph Kalo", "authorId": "3245041"}, {"name": "Jekaterina Novikova", "authorId": "2848048"}, {"name": "J. Forde", "authorId": "39774809"}, {"name": "Xiangru Tang", "authorId": "47274259"}, {"name": "Jungo Kasai", "authorId": "11348687"}, {"name": "Ken Kawamura", "authorId": "50106621"}, {"name": "Liam Hazan", "authorId": "2047711867"}, {"name": "Marine Carpuat", "authorId": "2954727"}, {"name": "Miruna Clinciu", "authorId": "2029314697"}, {"name": "Najoung Kim", "authorId": "8756748"}, {"name": "Newton Cheng", "authorId": "15590401"}, {"name": "Oleg Serikov", "authorId": "1799401599"}, {"name": "Omer Antverg", "authorId": "2132545395"}, {"name": "Oskar van der Wal", "authorId": "1986356851"}, {"name": "Rui Zhang", "authorId": "15176410"}, {"name": "Ruochen Zhang", "authorId": "49775305"}, {"name": "Sebastian Gehrmann", "authorId": "3159346"}, {"name": "Shachar Mirkin", "authorId": "8963527"}, {"name": "S. Pais", "authorId": "3097741"}, {"name": "Tatiana Shavrina", "authorId": "2134610800"}, {"name": "Thomas Scialom", "authorId": "90745780"}, {"name": "Tian Yun", "authorId": "2127600348"}, {"name": "Tomasz Limisiewicz", "authorId": "1666636295"}, {"name": "Verena Rieser", "authorId": "1681799"}, {"name": "Vitaly Protasov", "authorId": "2135362820"}, {"name": "V. Mikhailov", "authorId": "51259225"}, {"name": "Yada Pruksachatkun", "authorId": "100984698"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}, {"name": "Zachary Bamberger", "authorId": "2190281071"}, {"name": "Zden\u02c7ek Kasner", "authorId": "2343772132"}, {"name": "Zden\u011bk Kasner", "authorId": "1805991958"}, {"name": "A. Pestana", "authorId": "2190281954"}, {"name": "A. Feizpour", "authorId": "15845853"}, {"name": "Ammar Khan", "authorId": "2190399370"}, {"name": "Amy Faranak", "authorId": "2190281566"}, {"name": "A. Santos", "authorId": "2148971654"}, {"name": "Anthony Hevia", "authorId": "1739149407"}, {"name": "Antigona Unldreaj", "authorId": "2190281069"}, {"name": "Arash Aghagol", "authorId": "115638227"}, {"name": "Arezoo Abdollahi", "authorId": "2361305"}, {"name": "A. Tammour", "authorId": "101302626"}, {"name": "A. HajiHosseini", "authorId": "3110645"}, {"name": "Bahareh Behroozi", "authorId": "2190281564"}, {"name": "Benjamin Ayoade Ajibade", "authorId": "83263885"}, {"name": "B. Saxena", "authorId": "31577522"}, {"name": "Carlos Mu\u00f1oz Ferrandis", "authorId": "2005399190"}, {"name": "Danish Contractor", "authorId": "2075459"}, {"name": "D. Lansky", "authorId": "144635557"}, {"name": "Davis David", "authorId": "2058260775"}, {"name": "Douwe Kiela", "authorId": "2111313627"}, {"name": "D. A. Nguyen", "authorId": "5943347"}, {"name": "Edward Tan", "authorId": "47654100"}, {"name": "Emi Baylor", "authorId": "2026649806"}, {"name": "Ezinwanne Ozoani", "authorId": "2190281502"}, {"name": "F. Mirza", "authorId": "35330153"}, {"name": "Frankline Ononiwu", "authorId": "2190281922"}, {"name": "Habib Rezanejad", "authorId": "123343513"}, {"name": "H.A. Jones", "authorId": "2119822136"}, {"name": "Indrani Bhattacharya", "authorId": "2105001416"}, {"name": "Irene Solaiman", "authorId": "1404060690"}, {"name": "Irina Sedenko", "authorId": "2190281379"}, {"name": "Isar Nejadgholi", "authorId": "3163125"}, {"name": "J. Passmore", "authorId": "145629075"}, {"name": "Joshua Seltzer", "authorId": "150162316"}, {"name": "Julio Bonis Sanz", "authorId": "97979993"}, {"name": "Karen Fort", "authorId": null}, {"name": "L\u00edvia Dutra", "authorId": "3530609"}, {"name": "Mairon Samagaio", "authorId": "2190281373"}, {"name": "Maraim Elbadri", "authorId": "2190281500"}, {"name": "Margot Mieskes", "authorId": "2921990"}, {"name": "Marissa Gerchick", "authorId": "151492708"}, {"name": "Martha Akinlolu", "authorId": "2190281205"}, {"name": "Michael McKenna", "authorId": "2060092577"}, {"name": "Mike Qiu", "authorId": "2056851511"}, {"name": "M. Ghauri", "authorId": "144449938"}, {"name": "Mykola Burynok", "authorId": "2190281203"}, {"name": "Nafis Abrar", "authorId": "1401945312"}, {"name": "Nazneen Rajani", "authorId": "8937909"}, {"name": "Nour Elkott", "authorId": "2190281555"}, {"name": "N. Fahmy", "authorId": "1992948200"}, {"name": "Olanrewaju Samuel", "authorId": "2164156047"}, {"name": "Ran An", "authorId": "2061141169"}, {"name": "R. Kromann", "authorId": "9294251"}, {"name": "Ryan Hao", "authorId": "2137183106"}, {"name": "S. Alizadeh", "authorId": "4279554"}, {"name": "Sarmad Shubber", "authorId": "2190281531"}, {"name": "Silas L. Wang", "authorId": "2116420702"}, {"name": "Sourav Roy", "authorId": "2109853801"}, {"name": "S. Viguier", "authorId": "10726201"}, {"name": "Thanh-Cong Le", "authorId": "2153620715"}, {"name": "Tobi Oyebade", "authorId": "2190281729"}, {"name": "T. Le", "authorId": "2153620985"}, {"name": "Yoyo Yang", "authorId": "2190429590"}, {"name": "Zach Nguyen", "authorId": "2297189567"}, {"name": "Abhinav Ramesh Kashyap", "authorId": "41124383"}, {"name": "A. Palasciano", "authorId": "2318515251"}, {"name": "A. Callahan", "authorId": "2840689"}, {"name": "Anima Shukla", "authorId": "2042747208"}, {"name": "Antonio Miranda-Escalada", "authorId": "1414073449"}, {"name": "A. Singh", "authorId": "2110183222"}, {"name": "Benjamin Beilharz", "authorId": "1379935164"}, {"name": "Bo Wang", "authorId": "2165371942"}, {"name": "C. Brito", "authorId": "144972524"}, {"name": "Chenxi Zhou", "authorId": "2111169784"}, {"name": "Chirag Jain", "authorId": "50732716"}, {"name": "Chuxin Xu", "authorId": "2158158973"}, {"name": "Cl\u00e9mentine Fourrier", "authorId": "2080941785"}, {"name": "Daniel Le'on Perin'an", "authorId": "2174177869"}, {"name": "Daniel Molano", "authorId": "2082057793"}, {"name": "Dian Yu", "authorId": "150978762"}, {"name": "Enrique Manjavacas", "authorId": "24907368"}, {"name": "Fabio Barth", "authorId": "2139792578"}, {"name": "Florian Fuhrimann", "authorId": "2190281754"}, {"name": "Gabriel Altay", "authorId": "2165227550"}, {"name": "Giyaseddin Bayrak", "authorId": "2166224123"}, {"name": "Gully Burns", "authorId": null}, {"name": "Helena U. Vrabec", "authorId": "88811067"}, {"name": "I. Bello", "authorId": "121044523"}, {"name": "Isha Dash", "authorId": "93460753"}, {"name": "J. Kang", "authorId": "72725318"}, {"name": "John Giorgi", "authorId": "37585306"}, {"name": "Jonas Golde", "authorId": "144983077"}, {"name": "J. Posada", "authorId": "2066514466"}, {"name": "Karthi Sivaraman", "authorId": "1601562797"}, {"name": "Lokesh Bulchandani", "authorId": "2190281314"}, {"name": "Lu Liu", "authorId": "2145287083"}, {"name": "Luisa Shinzato", "authorId": "2100596120"}, {"name": "Madeleine Hahn de Bykhovetz", "authorId": "2190281960"}, {"name": "Maiko Takeuchi", "authorId": "2068853922"}, {"name": "Marc P\u00e0mies", "authorId": "1850527789"}, {"name": "M. A. Castillo", "authorId": "87956698"}, {"name": "Marianna Nezhurina", "authorId": "2174178585"}, {"name": "Mario Sanger", "authorId": "1879523878"}, {"name": "M. Samwald", "authorId": "3004898"}, {"name": "Michael Cullan", "authorId": "120397552"}, {"name": "Michael Weinberg", "authorId": "50564168"}, {"name": "M. Wolf", "authorId": "2072502429"}, {"name": "Mina Mihaljcic", "authorId": "2190280864"}, {"name": "Minna Liu", "authorId": "2112211627"}, {"name": "M. Freidank", "authorId": "1397064923"}, {"name": "Myungsun Kang", "authorId": "4981508"}, {"name": "Natasha Seelam", "authorId": "12046785"}, {"name": "N. Dahlberg", "authorId": "48948105"}, {"name": "N. Broad", "authorId": "40208102"}, {"name": "N. Muellner", "authorId": "70256289"}, {"name": "Pascale Fung", "authorId": "40539650"}, {"name": "Patricia Haller", "authorId": "2097023671"}, {"name": "Patrick Haller", "authorId": "2298902857"}, {"name": "R. Eisenberg", "authorId": "115525190"}, {"name": "Robert Martin", "authorId": "2111138678"}, {"name": "Rodrigo Canalli", "authorId": "2291171257"}, {"name": "Rosaline Su", "authorId": "2190282202"}, {"name": "Ruisi Su", "authorId": "153083809"}, {"name": "Samuel Cahyawijaya", "authorId": "66986482"}, {"name": "Samuele Garda", "authorId": "51878929"}, {"name": "Shlok S Deshmukh", "authorId": "2174177330"}, {"name": "Shubhanshu Mishra", "authorId": "2112134590"}, {"name": "Sid Kiblawi", "authorId": "39620434"}, {"name": "Simon Ott", "authorId": "119994729"}, {"name": "Sinee Sang-aroonsiri", "authorId": "2190281679"}, {"name": "Srishti Kumar", "authorId": "120438284"}, {"name": "Stefan Schweter", "authorId": "134757625"}, {"name": "S. Bharati", "authorId": "8723233"}, {"name": "Tanmay Laud", "authorId": "103242455"}, {"name": "Th\u00e9o Gigant", "authorId": "2174176862"}, {"name": "Tomoya Kainuma", "authorId": "2190281376"}, {"name": "Wojciech Kusa", "authorId": "50320098"}, {"name": "Yanis Labrak", "authorId": "2139767217"}, {"name": "Yashasvi Bajaj", "authorId": "1572961212"}, {"name": "Y. Venkatraman", "authorId": "2051879548"}, {"name": "Yifan Xu", "authorId": "2110154622"}, {"name": "Ying Xu", "authorId": "2118670234"}, {"name": "Yu Xu", "authorId": "2142717873"}, {"name": "Z. Tan", "authorId": "1643680733"}, {"name": "Zhongli Xie", "authorId": "79110285"}, {"name": "Zifan Ye", "authorId": "2114134227"}, {"name": "M. Bras", "authorId": "2065370401"}, {"name": "Younes Belkada", "authorId": "2037496520"}, {"name": "Thomas Wolf", "authorId": "50335211"}], "n_citations": 2393}, "snippets": ["Notably, while the original Transformer is based on an encoder-decoder architecture, many popular models have opted for encoder-only (e.g. BERT, (Devlin et al., 2019)) or decoder-only (e.g. GPT, (Radford et al., 2018)) approaches. Currently, all state-of-the-art language models over 100 billion parameters are causal decoder-only models Rae et al., 2021;Chowdhery et al., 2022). This is in opposition to the findings of (Raffel et al., 2019), in which encoderdecoder models significantly outperform decoder-only models for transfer learning.\n\nPrior to our work, the literature was lacking a systematic evaluation of the zero-shot generalization capabilities of different architectures and pretraining objectives. We explored this question in (Wang et al., 2022) where we evaluated encoder-decoder and decoder-only architectures and their interactions with causal, prefix, and masked language modeling pretraining objectives. Our results show that immediately after pretraining, causal decoderonly models performed best -validating the choice of state-of-the-art LLMs."], "score": 0.74072265625}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 248118752, "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Thomas Wang", "authorId": "2135734748"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Daniel Hesslow", "authorId": "80424302"}, {"name": "Teven Le Scao", "authorId": "1379806208"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Iz Beltagy", "authorId": "46181066"}, {"name": "Julien Launay", "authorId": "143945447"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 175}, "snippets": ["Causal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) .\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) .\n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target."], "score": 0.84423828125}, {"id": "(Patil et al., 2024)", "paper": {"corpus_id": 268157336, "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)", "year": 2024, "venue": "Applied Sciences", "authors": [{"name": "Rajvardhan Patil", "authorId": "2289385425"}, {"name": "Venkat Gudivada", "authorId": "117730513"}], "n_citations": 80}, "snippets": ["Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens", "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence", "In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."], "score": 0.77587890625}, {"id": "(Suganthan et al., 2025)", "paper": {"corpus_id": 276771845, "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "P. Suganthan", "authorId": "1658871094"}, {"name": "Fedor Moiseev", "authorId": "2165469946"}, {"name": "Le Yan", "authorId": "2348489099"}, {"name": "Junru Wu", "authorId": "2261361394"}, {"name": "Jianmo Ni", "authorId": "2348507846"}, {"name": "Jay Han", "authorId": "2348488953"}, {"name": "I. Zitouni", "authorId": "1954563"}, {"name": "Enrique Alfonseca", "authorId": "1727837"}, {"name": "Xuanhui Wang", "authorId": "2348422460"}, {"name": "Zhe Dong", "authorId": "2349772191"}], "n_citations": 1}, "snippets": ["A key question thus arises: can we effectively adapt the powerful knowledge embedded in decoder-only models to excel in these encoder-centric tasks?", "Gemma's causal attention, ideal for generative tasks, inherently limits its applicability to encoder-based tasks. We demonstrate that simply enabling bidirectional attention during fine-tuning dramatically improves performance."], "score": 0.63134765625}, {"id": "(Busto-Castineira et al., 2024)", "paper": {"corpus_id": 270832367, "title": "Predictability and Causality in Spanish and English Natural Language Generation", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Andrea Busto-Casti\u00f1eira", "authorId": "2222734467"}, {"name": "Francisco Javier Gonz\u00e1lez-Casta\u00f1o", "authorId": "2323809078"}, {"name": "Silvia Garc\u00eda-M\u00e9ndez", "authorId": "1405165681"}, {"name": "Francisco de Arriba-P\u00e9rez", "authorId": "2034282614"}], "n_citations": 1}, "snippets": ["While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder", "By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence (Zeng et al., 2021)."], "score": 0.69140625}, {"id": "(Saha et al., 2023)", "paper": {"corpus_id": 263829839, "title": "LLM for SoC Security: A Paradigm Shift", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Dipayan Saha", "authorId": "2256992493"}, {"name": "Shams Tarek", "authorId": "2114625129"}, {"name": "Katayoon Yahyaei", "authorId": "2256991081"}, {"name": "Sujan Kumar Saha", "authorId": "2231854143"}, {"name": "Jingbo Zhou", "authorId": "2257235852"}, {"name": "M. Tehranipoor", "authorId": "145954982"}, {"name": "Farimah Farahmandi", "authorId": "1997019"}], "n_citations": 54}, "snippets": ["The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named", ".In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens."], "score": 0.79833984375}, {"id": "(Rankovi'c et al., 2025)", "paper": {"corpus_id": 277626915, "title": "GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Bojana Rankovi'c", "authorId": "2219925647"}, {"name": "Philippe Schwaller", "authorId": "2239074343"}], "n_citations": 1}, "snippets": ["LLMs can follow different architectural designs: encoder-only (e.g., BERT 2), decoder-only (e.g., Qwen 52), and encoder-decoder (e.g., T5 49). Encoder-based models process the full input bidirectionally and are suited for classification and regression. Decoder-only models generate text autoregressively with causal masking. Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks."], "score": 0.703125}, {"id": "(Uludougan et al., 2024)", "paper": {"corpus_id": 267211690, "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Gokcce Uludougan", "authorId": "2281033147"}, {"name": "Zeynep Yirmibecsouglu Balal", "authorId": "2281033141"}, {"name": "Furkan Akkurt", "authorId": "2174736343"}, {"name": "Melikcsah Turker", "authorId": "2281033264"}, {"name": "Onur Gungor", "authorId": "9179697"}, {"name": "S. Uskudarli", "authorId": "66493576"}], "n_citations": 12}, "snippets": ["Models that exclusively employ encoders, typically trained with denoising objectives, are geared toward understanding tasks, as exemplified in works such as (Devlin et al., 2019) and (208229926). Conversely, models that exclusively use decoders are designed for generation tasks, employing causal language modeling, as demonstrated in various studies (Radford et al., 2019)Brown et al., 2020;Touvron et al., 2023). The Text-to-Text Transformer (T5) (Raffel et al., 2019), on the other hand, employs an encoder-decoder architecture and undergoes pretraining with a denoising objective referred to as span corruption. UniLM (Dong et al., 2019) is also an encoder-decoder model, but pre-trained using unidirectional, bidirectional, and sequence-to-sequence language modeling. This can be seen as a combination of causal and denoising objectives."], "score": 0.64208984375}, {"id": "(Tay et al., 2022)", "paper": {"corpus_id": 252780443, "title": "UL2: Unifying Language Learning Paradigms", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yi Tay", "authorId": "144447820"}, {"name": "Mostafa Dehghani", "authorId": "3226635"}, {"name": "Vinh Q. Tran", "authorId": "2057663102"}, {"name": "Xavier Garc\u00eda", "authorId": "143936294"}, {"name": "Jason Wei", "authorId": "119640649"}, {"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Dara Bahri", "authorId": "2119725651"}, {"name": "Tal Schuster", "authorId": "32303439"}, {"name": "H. Zheng", "authorId": "2115689465"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "N. Houlsby", "authorId": "2815290"}, {"name": "Donald Metzler", "authorId": "1680617"}], "n_citations": 313}, "snippets": ["The line between decoder-only and encoder-decoder models is less clear. PrefixLM models are almost encoder-decoder models with shared parameters (but not quite). From an inductive bias point of view, there are multiple differences. Encoder-Decoder models process input and targets independently with a different set of parameters. This is a form of sparsity where different set of parameters are used for different tokens. Encoder-Decoder models also have a cross attention component that connects input tokens to target tokens. Meanwhile, decoder-only models process inputs and targets by concatenating them. Hence, the representations of inputs and targets are concurrently build layer by layer as the input/targets propagate up the network. Conversely, the decoder in Encoder-decoder models generally only looks at the fully processed encoder input. Overall, the inductive bias of PrefixLM decoder-only models and Encoder-Decoder models could be pretty similar modulo the subtle differences stated above. The distinct property is that Encoder-Decoder models are generally approximately 2x parameters of a decoder-only model when compute-matched."], "score": 0.84814453125}, {"id": "(Kim et al., 2023)", "paper": {"corpus_id": 260886785, "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Minsoo Kim", "authorId": "2141320070"}, {"name": "Sihwa Lee", "authorId": "2144376191"}, {"name": "Janghwan Lee", "authorId": "2265920992"}, {"name": "S. Hong", "authorId": "2158125346"}, {"name": "Duhyeuk Chang", "authorId": "2180828053"}, {"name": "Wonyong Sung", "authorId": "66936521"}, {"name": "Jungwook Choi", "authorId": "2506452"}], "n_citations": 15}, "snippets": ["Cumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss. Fig. 1(b) illustrates the differences. In NLU tasks, encoder models leverage the representation of a unique special token, transformed into a logit vector, for calculating cross-entropy loss, with the number of classes usually being a few [12]. Conversely, decoder models in NLG tasks predict the subsequent token for each input token, transforming each token representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k [25]. Hence, while encoder models gain ground truth loss from a single special token's logit, decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information."], "score": 0.93896484375}, {"id": "(Jain et al., 2022)", "paper": {"corpus_id": 258461112, "title": "ContraCLM: Contrastive Learning For Causal Language Model", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Nihal Jain", "authorId": "2146677401"}, {"name": "Dejiao Zhang", "authorId": "2358258"}, {"name": "Wasi Uddin Ahmad", "authorId": "38123220"}, {"name": "Zijian Wang", "authorId": "50219006"}, {"name": "Feng Nan", "authorId": "144647318"}, {"name": "Xiaopeng Li", "authorId": "2187045812"}, {"name": "Ming Tan", "authorId": "144745483"}, {"name": "Ramesh Nallapati", "authorId": "1701451"}, {"name": "Baishakhi Ray", "authorId": "31631000"}, {"name": "Parminder Bhatia", "authorId": "50339091"}, {"name": "Xiaofei Ma", "authorId": "47646605"}, {"name": "Bing Xiang", "authorId": "144028698"}], "n_citations": 16}, "snippets": ["Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations."], "score": 0.705078125}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 270560675, "title": "Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Bingqi Ma", "authorId": "2261489892"}, {"name": "Zhuofan Zong", "authorId": "1571400317"}, {"name": "Guanglu Song", "authorId": "12920342"}, {"name": "Hongsheng Li", "authorId": "2261394248"}, {"name": "Yu Liu", "authorId": "2261417717"}], "n_citations": 23}, "snippets": ["As outlined in Sec. 1, we observe two discrepancies between decoder-only LLMs and encoder-decoder models: optimization objective and model architecture. Specifically, the decoder-only LLMs are typically optimized using the next token prediction task while the encoder-decoder models are trained with the masked language modeling task. Besides, the former tokens in a sequence cannot attend the latter tokens in decoder-only LLMs while every token in the sequence can attend each other in the encoder models."], "score": 0.7783203125}, {"id": "(Tay et al., 2020)", "paper": {"corpus_id": 221702858, "title": "Efficient Transformers: A Survey", "year": 2020, "venue": "ACM Computing Surveys", "authors": [{"name": "Yi Tay", "authorId": "144447820"}, {"name": "Mostafa Dehghani", "authorId": "3226635"}, {"name": "Dara Bahri", "authorId": "11774695"}, {"name": "Donald Metzler", "authorId": "1680617"}], "n_citations": 1128}, "snippets": ["In encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the selfattention used in the encoder need not. Fulfilling this requirement can prove challenging for many efficient self-attention designs."], "score": 0.806640625}], "table": null}], "cost": 0.6176639999999999}}
