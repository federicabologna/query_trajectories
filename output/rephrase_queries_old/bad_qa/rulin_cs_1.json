{"original_query": "What are the latest works on finetuning an auto-regressive LM for dense passage retrieval? How are their performance compared with bi-directional encoders?", "input": "latest works on auto regressive LM dense passage retrieval their performance compared with bi directional encoders", "output": {"sections": [{"title": "Introduction/Background", "tldr": "Dense passage retrieval has evolved from traditional bi-directional encoder-based methods to incorporating autoregressive language models. This shift represents a significant advancement in information retrieval, with various approaches showing different performance characteristics. (LLM Memory)", "text": "\nInformation retrieval systems have traditionally relied on bi-directional encoder architectures like BERT to create dense vector representations of text for matching queries with relevant documents. These bi-directional encoders process text in both directions simultaneously, allowing them to capture contextual relationships effectively. In recent years, however, the field has seen growing interest in using autoregressive language models (LMs) for retrieval tasks. Unlike bi-directional models, autoregressive LMs process text sequentially, predicting each token based on previous tokens, which enables them to generate coherent text but presents different challenges for retrieval tasks. The exploration of autoregressive models for retrieval is motivated by their impressive performance in natural language understanding and generation tasks, as well as their potential to unify various NLP tasks within a single framework. This shift has opened new avenues for research in dense passage retrieval, with different approaches exhibiting various tradeoffs in terms of effectiveness, efficiency, and computational requirements. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Traditional Dense Retrieval Approaches", "tldr": "Traditional dense retrieval systems typically employ dual-encoder architectures that transform queries and documents into vector representations, with the goal of calculating relevance through similarity metrics like cosine similarity or dot products. These systems have evolved through innovations like improved negative sampling strategies, knowledge distillation, and specialized pre-training objectives to enhance retrieval performance. (7 sources)", "text": "\nDense retrieval models represent a significant advancement over traditional term-matching methods like BM25, particularly in addressing the vocabulary mismatch problem. At their core, these systems use pre-trained language models with siamese or dual-encoder architectures to encode queries and documents into low-dimensional vector spaces, enabling effective semantic search <Paper corpusId=\"251594591\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"257985191\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233231706\" paperTitle=\"(Hofstatter et al., 2021)\" isShortName></Paper>. The relevance between queries and documents is calculated using cosine similarity or dot-product functions, making high-quality text representations crucial for performance <Paper corpusId=\"251594591\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper>.\n\nThe Dense Passage Retriever (DPR) model was a breakthrough that demonstrated dense retrieval could outperform traditional BM25 methods <Paper corpusId=\"251594591\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper>. Since this breakthrough, researchers have pursued various strategies to enhance dense retrieval performance. One significant line of research focuses on improving the pre-training process specifically for retrieval tasks. Some approaches incorporate auxiliary self-supervised reconstruction tasks that force encoders to generate better text representations <Paper corpusId=\"251594591\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"243865399\" paperTitle=\"(Lu et al., 2021)\" isShortName></Paper>. Other methods have explored contextual-supervised learning and masked auto-encoding to improve the quality of dense vectors <Paper corpusId=\"257985191\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>.\n\nThe training methodology for dense retrievers has also received considerable attention. Effective negative sampling strategies have proven crucial for performance, with many systems employing hard negative sampling techniques <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper> <Paper corpusId=\"233289894\" paperTitle=\"(Zhan et al., 2021)\" isShortName></Paper>. Research has shown that while hard negative sampling outperforms random sampling, it also carries potential risks when implemented statically. This has led to the development of more sophisticated approaches like the Stable Training Algorithm for dense Retrieval (STAR) and the Algorithm for Directly Optimizing Ranking Performance (ADORE), which improve training stability and optimize ranking performance directly <Paper corpusId=\"233289894\" paperTitle=\"(Zhan et al., 2021)\" isShortName></Paper>.\n\nAdditional enhancement strategies include knowledge distillation from more powerful cross-encoder models, data augmentation techniques, and the development of tailored pre-trained models specifically designed for retrieval tasks <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper>. These bi-directional encoder-based approaches have been widely adopted in real-world applications like open-domain question answering, conversational systems, and web search <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper> <Paper corpusId=\"222310837\" paperTitle=\"(Lin et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Wu et al., 2022)", "paper": {"corpus_id": 251594591, "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval", "year": 2022, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Xing Wu", "authorId": "2155226596"}, {"name": "Guangyuan Ma", "authorId": "2068996632"}, {"name": "Meng Lin", "authorId": "2156805995"}, {"name": "Zijia Lin", "authorId": "1818920"}, {"name": "Zhongyuan Wang", "authorId": "2135394423"}, {"name": "Songlin Hu", "authorId": "40845069"}], "n_citations": 26}, "snippets": ["Dense retrieval models are generally based on pre-trained language models with a siamese or dual-encoder architecture to encode queries and documents into low-dimensional vector space for effective search (Hofst\u00e4tter et al., 2021)Humeau et al. 2019;Xiong et al. 2020;(Zhan et al., 2021)Zhan et al. , 2020. The relevances between queries and documents are calculated with cosine similarity or dot-product function in the vector space. Therefore, high-quality text representation based on PLM is crucial for dense passage retrieval.\n\nDPR (Karpukhin et al. 2020) successfully shows that dense retrieval models can outperform BM25 methods. Since then, some works have emerged to boost dense retrieval performance by improving the pre-training process tailored for dense retrieval. (Lu et al., 2021)Gao and Callan 2021a;Liu and Shao 2022) encourage the encoder to improve the text representation modeling ability through auxiliary self-supervised reconstruction tasks. Auxiliary tasks usually utilize a weak decoder to reconstruct the masked text with the assistance of the text's vector from the encoder, which forces the encoder to provide better text representations. Although these works have been shown to be very effective and achieved some improvements in dense retrieval, they mainly focus on single-text internal modeling without considering contextual information. (Chang et al. 2020;Gao and Callan 2021b;Ma et al. 2022) proposes multi-source and multi-granularity contrastive span prediction"], "score": 0.548828125}, {"id": "(Wu et al., 2023)", "paper": {"corpus_id": 257985191, "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xing Wu", "authorId": "2155226596"}, {"name": "Guangyuan Ma", "authorId": "2068996632"}, {"name": "Peng Wang", "authorId": "144282672"}, {"name": "Meng Lin", "authorId": "2156805995"}, {"name": "Zijia Lin", "authorId": "1818920"}, {"name": "Fuzheng Zhang", "authorId": "2642200"}, {"name": "Songlin Hu", "authorId": "40845069"}], "n_citations": 8}, "snippets": ["PLM-based dense retrieval typically employs a siamese or dual-encoder architecture to convert queries and documents into a low-dimensional vector space (Hofst\u00e4tter et al., 2021)Humeau et al., 2019;Xiong et al., 2020;(Zhan et al., 2021)Zhan et al., , 2020)). Relevance between queries and documents is calculated using cosine similarity or dot products. This low-dimension vector is called the dense vector and is trained to capture the sentence semantics at the passage level. Recent efforts aim to improve dense retrieval performance by adding auxiliary tasks to pre-training, like contextual-supervised learning (Gao and Callan, 2021b;Wu et al., 2022  ( Wu et al., 2022), expands on this by incorporating contextual masked auto-encoding, leading to better dense retrieval performances."], "score": 0.52099609375}, {"id": "(Hofstatter et al., 2021)", "paper": {"corpus_id": 233231706, "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling", "year": 2021, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Sebastian Hofst\u00e4tter", "authorId": "97393346"}, {"name": "Sheng-Chieh Lin", "authorId": "122045993"}, {"name": "Jheng-Hong Yang", "authorId": "2109723027"}, {"name": "Jimmy J. Lin", "authorId": "145580839"}, {"name": "A. Hanbury", "authorId": "1699657"}], "n_citations": 402}, "snippets": ["A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further."], "score": 0.0}, {"id": "(Long et al., 2022)", "paper": {"corpus_id": 253157959, "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Dingkun Long", "authorId": "8427191"}, {"name": "Yanzhao Zhang", "authorId": "2107949588"}, {"name": "Guangwei Xu", "authorId": "2149131512"}, {"name": "Pengjun Xie", "authorId": "35930962"}], "n_citations": 4}, "snippets": ["Dense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair.\n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022)."], "score": 0.52783203125}, {"id": "(Lu et al., 2021)", "paper": {"corpus_id": 243865399, "title": "Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Shuqi Lu", "authorId": "1830381674"}, {"name": "Di He", "authorId": "1391126980"}, {"name": "Chenyan Xiong", "authorId": "2139787803"}, {"name": "Guolin Ke", "authorId": "35286545"}, {"name": "Waleed Malik", "authorId": "2060300532"}, {"name": "Zhicheng Dou", "authorId": "1897235"}, {"name": "Paul N. Bennett", "authorId": "144609235"}, {"name": "Tie-Yan Liu", "authorId": "2110264337"}, {"name": "Arnold Overwijk", "authorId": "2734525"}], "n_citations": 74}, "snippets": ["Dense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding that can reconstruct the input texts. However, in this paper, we provide theoretical analyses and show empirically that an autoencoder language model with a low reconstruction loss may not provide good sequence representations because the decoder may take shortcuts by exploiting language patterns. To address this, we propose a new self-learning method that pre-trains the autoencoder using a weak decoder, with restricted capacity and attention flexibility to push the encoder to provide better text representations. Our experiments on web search, news recommendation, and open domain question answering show that our pre-trained model significantly boosts the effectiveness and few-shot ability of dense retrieval models. Our code is available at https://github.com/microsoft/SEED-Encoder/."], "score": 0.0}, {"id": "(Zhan et al., 2021)", "paper": {"corpus_id": 233289894, "title": "Optimizing Dense Retrieval Model Training with Hard Negatives", "year": 2021, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Jingtao Zhan", "authorId": "1643961315"}, {"name": "Jiaxin Mao", "authorId": "1644047628"}, {"name": "Yiqun Liu", "authorId": "1783406"}, {"name": "Jiafeng Guo", "authorId": "70414094"}, {"name": "M. Zhang", "authorId": "39767557"}, {"name": "Shaoping Ma", "authorId": "8093158"}], "n_citations": 276}, "snippets": ["Ranking has always been one of the top concerns in information retrieval researches. For decades, the lexical matching signal has dominated the ad-hoc retrieval process, but solely using this signal in retrieval may cause the vocabulary mismatch problem. In recent years, with the development of representation learning techniques, many researchers turn to Dense Retrieval (DR) models for better ranking performance. Although several existing DR models have already obtained promising results, their performance improvement heavily relies on the sampling of training examples. Many effective sampling strategies are not efficient enough for practical usage, and for most of them, there still lacks theoretical analysis in how and why performance improvement happens. To shed light on these research questions, we theoretically investigate different training strategies for DR models and try to explain why hard negative sampling performs better than random sampling. Through the analysis, we also find that there are many potential risks in static hard negative sampling, which is employed by many existing training methods. Therefore, we propose two training strategies named a Stable Training Algorithm for dense Retrieval (STAR) and a query-side training Algorithm for Directly Optimizing Ranking pErformance (ADORE), respectively. STAR improves the stability of DR training process by introducing random negatives. ADORE replaces the widely-adopted static hard negative sampling method with a dynamic one to directly optimize the ranking performance. Experimental results on two publicly available retrieval benchmark datasets show that either strategy gains significant improvements over existing competitive baselines and a combination of them leads to the best performance."], "score": 0.0}, {"id": "(Lin et al., 2020)", "paper": {"corpus_id": 222310837, "title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "year": 2020, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jimmy J. Lin", "authorId": "145580839"}, {"name": "Rodrigo Nogueira", "authorId": "143744603"}, {"name": "Andrew Yates", "authorId": "144115896"}], "n_citations": 627}, "snippets": ["The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is based on the preprint of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly."], "score": 0.0}], "table": null}, {"title": "Autoregressive Language Models for Retrieval", "tldr": "Autoregressive LMs address limitations of dual-encoder approaches by enabling deep token-level interactions between queries and documents, often generating document identifiers directly instead of computing similarity scores between embeddings. These models demonstrate strong generalization abilities and can effectively combine retrieval with reranking through sequence generation capabilities. (9 sources)", "text": "\nAutoregressive language models represent a significant shift from traditional dual-encoder approaches to dense passage retrieval. While dual-encoder systems like DPR independently encode queries and documents, they suffer from two key limitations: shallow interactions between query and document representations, and potential information loss when compressing documents into single dense vectors <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>. To address these limitations, autoregressive search engines have emerged as a promising alternative approach.\n\nUnlike traditional retrievers that compute similarity between query and document embeddings, autoregressive models directly generate document identifiers which are then mapped to complete documents in a candidate pool <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. This approach has gained significant traction in information retrieval research due to several advantages. First, autoregressive generation models perform deep token-level cross-attention during the generation process, enabling more nuanced query-document interactions compared to the shallow interactions in dual-encoder models <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. Second, these models have demonstrated strong generalization capabilities, outperforming traditional BM25 methods even in zero-shot settings <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>.\n\nThe retrieval-as-generation paradigm effectively addresses the bottleneck of limited interactions in dual-encoder models <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>. In the reranking context, sequence generation approaches have shown particular promise by leveraging autoregressive capabilities to perform tasks such as listwise sorting or generating rationales for relevance decisions <Paper corpusId=\"267938301\" paperTitle=\"(Yoon et al., 2024)\" isShortName></Paper>. Models that jointly process query and passage information at inference time have proven effective for zero-shot retrieval scenarios <Paper corpusId=\"267938301\" paperTitle=\"(Yoon et al., 2024)\" isShortName></Paper> <Paper corpusId=\"212725651\" paperTitle=\"(Nogueira et al., 2020)\" isShortName></Paper>.\n\nNotable implementations of this approach include FiD-Light, which employs a supervised learning method to re-rank passages effectively using source pointers during autoregressive text generation <Paper corpusId=\"269188036\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"252568176\" paperTitle=\"(Hofstatter et al., 2022)\" isShortName></Paper>. This model uses a listwise auto-regressive re-ranking mechanism trained to identify and prioritize relevant passages based on the output generated during the text generation process <Paper corpusId=\"269188036\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. The Neural Corpus Indexer (NCI) represents another significant advancement, functioning as a sequence-to-sequence network that directly generates relevant document identifiers for queries, achieving substantial improvements in recall performance compared to traditional methods <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249395549\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Ziems et al., 2023)", "paper": {"corpus_id": 258714822, "title": "Large Language Models are Built-in Autoregressive Search Engines", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Noah Ziems", "authorId": "2264184691"}, {"name": "W. Yu", "authorId": "38767143"}, {"name": "Zhihan Zhang", "authorId": "72871419"}, {"name": "Meng Jiang", "authorId": "2152153656"}], "n_citations": 42}, "snippets": ["Along with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020).\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better estimation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022)."], "score": 0.54931640625}, {"id": "(Karpukhin et al., 2020)", "paper": {"corpus_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ledell Yu Wu", "authorId": "51183248"}, {"name": "Sergey Edunov", "authorId": "2068070"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 3794}, "snippets": ["Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."], "score": 0.0}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 268031876, "title": "Generative Retrieval with Large Language Models", "year": 2024, "venue": "", "authors": [{"name": "Ye Wang", "authorId": "2185022832"}, {"name": "Xinrun Xu", "authorId": "2290204960"}, {"name": "Rui Xie", "authorId": "2143721734"}, {"name": "Wenxin Hu", "authorId": "2288018918"}, {"name": "Wei Ye", "authorId": "2052980435"}], "n_citations": 1}, "snippets": ["Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020).\n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models."], "score": 0.55419921875}, {"id": "(Khattab et al., 2020)", "paper": {"corpus_id": 216553223, "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "year": 2020, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "O. Khattab", "authorId": "144112155"}, {"name": "M. Zaharia", "authorId": "143834867"}], "n_citations": 1377}, "snippets": ["Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."], "score": 0.0}, {"id": "(Yoon et al., 2024)", "paper": {"corpus_id": 267938301, "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Soyoung Yoon", "authorId": "2287336807"}, {"name": "Eunbi Choi", "authorId": "2287970016"}, {"name": "Jiyeon Kim", "authorId": "2287064006"}, {"name": "Yireun Kim", "authorId": "2181032855"}, {"name": "Hyeongu Yun", "authorId": "2286896884"}, {"name": "Seung-won Hwang", "authorId": "2287694374"}], "n_citations": 16}, "snippets": ["In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;(Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities. Specifically, a series of studies that use the encoder-decoder architecture of T5 (Sec. 2.2), and applying zero-shot reranking with LLMs (Sec. 2.3), or viewing reranking as autoregressive text generation problem (Wang et al., 2024) has been successful."], "score": 0.54296875}, {"id": "(Nogueira et al., 2020)", "paper": {"corpus_id": 212725651, "title": "Document Ranking with a Pretrained Sequence-to-Sequence Model", "year": 2020, "venue": "Findings", "authors": [{"name": "Rodrigo Nogueira", "authorId": "143744603"}, {"name": "Zhiying Jiang", "authorId": "2197574931"}, {"name": "Ronak Pradeep", "authorId": "1816753042"}, {"name": "Jimmy J. Lin", "authorId": "145580839"}], "n_citations": 584}, "snippets": ["This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as \"target tokens\", and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model\u2019s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai."], "score": 0.0}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 269188036, "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yizheng Huang", "authorId": "2260272949"}, {"name": "Jimmy X. Huang", "authorId": "2259653248"}], "n_citations": 51}, "snippets": ["FiD-Light (Hofst\u00e4tter et al., 2022) employs a supervised approach where the model is fine-tuned on specific datasets to learn how to re-rank passages effectively using source pointers during autoregressive text generation. The model uses a listwise auto-regressive re-ranking mechanism, trained to identify and re-rank relevant passages based on the output generated during the text generation process."], "score": 0.82275390625}, {"id": "(Hofstatter et al., 2022)", "paper": {"corpus_id": 252568176, "title": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Sebastian Hofst\u00e4tter", "authorId": "97393346"}, {"name": "Jiecao Chen", "authorId": "2809410"}, {"name": "K. Raman", "authorId": "2062947723"}, {"name": "Hamed Zamani", "authorId": "2499986"}], "n_citations": 82}, "snippets": ["Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency."], "score": 0.0}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 249395549, "title": "A Neural Corpus Indexer for Document Retrieval", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yujing Wang", "authorId": "2115659209"}, {"name": "Ying Hou", "authorId": "2149103804"}, {"name": "Hong Wang", "authorId": "49528487"}, {"name": "Ziming Miao", "authorId": "40793591"}, {"name": "Shibin Wu", "authorId": null}, {"name": "Hao Sun", "authorId": "2118180377"}, {"name": "Qi Chen", "authorId": "1819450790"}, {"name": "Yuqing Xia", "authorId": "2111274567"}, {"name": "Chengmin Chi", "authorId": "2168109575"}, {"name": "Guoshuai Zhao", "authorId": "2905509"}, {"name": "Zheng Liu", "authorId": "2145976175"}, {"name": "Xing Xie", "authorId": "2110972758"}, {"name": "Hao Sun", "authorId": "2118180377"}, {"name": "Weiwei Deng", "authorId": "2066621592"}, {"name": "Qi Zhang", "authorId": null}, {"name": "Mao Yang", "authorId": "2110611349"}], "n_citations": 146}, "snippets": ["Current state-of-the-art document retrieval solutions mainly follow an index-retrieve paradigm, where the index is hard to be directly optimized for the final retrieval target. In this paper, we aim to show that an end-to-end deep neural network unifying training and indexing stages can significantly improve the recall performance of traditional methods. To this end, we propose Neural Corpus Indexer (NCI), a sequence-to-sequence network that generates relevant document identifiers directly for a designated query. To optimize the recall performance of NCI, we invent a prefix-aware weight-adaptive decoder architecture, and leverage tailored techniques including query generation, semantic document identifiers, and consistency-based regularization. Empirical studies demonstrated the superiority of NCI on two commonly used academic benchmarks, achieving +21.4% and +16.8% relative enhancement for Recall@1 on NQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to the best baseline method."], "score": 0.0}], "table": null}, {"title": "Performance Comparison", "tldr": "When comparing traditional bi-directional encoders with autoregressive language models for retrieval, autoregressive approaches demonstrate significant performance improvements by enabling deeper query-document interactions. These newer models show particular strength in generalization capabilities and zero-shot settings, addressing fundamental limitations of dual-encoder architectures. (6 sources)", "text": "\nRecent studies have highlighted the performance advantages of autoregressive language models over traditional bi-directional encoder approaches for retrieval tasks. The fundamental limitations of dual-encoder systems like DPR include shallow interactions between query and document representations and potential information loss when compressing documents into single dense vectors <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper> <Paper corpusId=\"220302658\" paperTitle=\"(Khattab et al._1, 2020)\" isShortName></Paper>. These limitations create a performance ceiling that autoregressive approaches have been able to surpass.\n\nAutoregressive search engines have demonstrated strong generalization abilities, outperforming traditional BM25 methods even in zero-shot settings where models have not been specifically trained on the target domain <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. This is particularly significant as it suggests that autoregressive models can maintain effectiveness across diverse domains without requiring domain-specific fine-tuning.\n\nThe Neural Corpus Indexer (NCI) represents a concrete example of performance improvements, achieving +21.4% relative enhancement for Recall@1 on the NQ320k dataset and +16.8% improvement for R-Precision on the TriviaQA dataset compared to the best baseline methods <Paper corpusId=\"249395549\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. These substantial improvements highlight the effectiveness of the end-to-end retrieval paradigm that autoregressive models enable.\n\nThe performance gains can be largely attributed to the deeper token-level cross-attention mechanisms employed by autoregressive models, which allow for more nuanced query-document interactions compared to the shallow interactions in dual-encoder systems <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper> <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>. By directly generating document identifiers rather than computing similarity between independently derived vector representations, autoregressive approaches effectively address the bottleneck of limited interactions in dual-encoder models <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nWhile autoregressive models demonstrate superior retrieval performance, it's worth noting that they typically require more computational resources during inference compared to dual-encoder approaches. However, the performance improvements, particularly in challenging retrieval scenarios, often justify this additional computational cost <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Ziems et al., 2023)", "paper": {"corpus_id": 258714822, "title": "Large Language Models are Built-in Autoregressive Search Engines", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Noah Ziems", "authorId": "2264184691"}, {"name": "W. Yu", "authorId": "38767143"}, {"name": "Zhihan Zhang", "authorId": "72871419"}, {"name": "Meng Jiang", "authorId": "2152153656"}], "n_citations": 42}, "snippets": ["Along with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020).\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better estimation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022)."], "score": 0.54931640625}, {"id": "(Karpukhin et al., 2020)", "paper": {"corpus_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ledell Yu Wu", "authorId": "51183248"}, {"name": "Sergey Edunov", "authorId": "2068070"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 3794}, "snippets": ["Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."], "score": 0.0}, {"id": "(Khattab et al._1, 2020)", "paper": {"corpus_id": 220302658, "title": "Relevance-guided Supervision for OpenQA with ColBERT", "year": 2020, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "O. Khattab", "authorId": "144112155"}, {"name": "Christopher Potts", "authorId": "144922861"}, {"name": "M. Zaharia", "authorId": "143834867"}], "n_citations": 100}, "snippets": ["Abstract Systems for Open-Domain Question Answering (OpenQA) generally depend on a retriever for finding candidate passages in a large corpus and a reader for extracting answers from those passages. In much recent work, the retriever is a learned component that uses coarse-grained vector representations of questions and passages. We argue that this modeling choice is insufficiently expressive for dealing with the complexity of natural language questions. To address this, we define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT to OpenQA. ColBERT creates fine-grained interactions between questions and passages. We propose an efficient weak supervision strategy that iteratively uses ColBERT to create its own training data. This greatly improves OpenQA retrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system attains state-of-the-art extractive OpenQA performance on all three datasets."], "score": 0.0}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 249395549, "title": "A Neural Corpus Indexer for Document Retrieval", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yujing Wang", "authorId": "2115659209"}, {"name": "Ying Hou", "authorId": "2149103804"}, {"name": "Hong Wang", "authorId": "49528487"}, {"name": "Ziming Miao", "authorId": "40793591"}, {"name": "Shibin Wu", "authorId": null}, {"name": "Hao Sun", "authorId": "2118180377"}, {"name": "Qi Chen", "authorId": "1819450790"}, {"name": "Yuqing Xia", "authorId": "2111274567"}, {"name": "Chengmin Chi", "authorId": "2168109575"}, {"name": "Guoshuai Zhao", "authorId": "2905509"}, {"name": "Zheng Liu", "authorId": "2145976175"}, {"name": "Xing Xie", "authorId": "2110972758"}, {"name": "Hao Sun", "authorId": "2118180377"}, {"name": "Weiwei Deng", "authorId": "2066621592"}, {"name": "Qi Zhang", "authorId": null}, {"name": "Mao Yang", "authorId": "2110611349"}], "n_citations": 146}, "snippets": ["Current state-of-the-art document retrieval solutions mainly follow an index-retrieve paradigm, where the index is hard to be directly optimized for the final retrieval target. In this paper, we aim to show that an end-to-end deep neural network unifying training and indexing stages can significantly improve the recall performance of traditional methods. To this end, we propose Neural Corpus Indexer (NCI), a sequence-to-sequence network that generates relevant document identifiers directly for a designated query. To optimize the recall performance of NCI, we invent a prefix-aware weight-adaptive decoder architecture, and leverage tailored techniques including query generation, semantic document identifiers, and consistency-based regularization. Empirical studies demonstrated the superiority of NCI on two commonly used academic benchmarks, achieving +21.4% and +16.8% relative enhancement for Recall@1 on NQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to the best baseline method."], "score": 0.0}, {"id": "(Khattab et al., 2020)", "paper": {"corpus_id": 216553223, "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "year": 2020, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "O. Khattab", "authorId": "144112155"}, {"name": "M. Zaharia", "authorId": "143834867"}], "n_citations": 1377}, "snippets": ["Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."], "score": 0.0}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 268031876, "title": "Generative Retrieval with Large Language Models", "year": 2024, "venue": "", "authors": [{"name": "Ye Wang", "authorId": "2185022832"}, {"name": "Xinrun Xu", "authorId": "2290204960"}, {"name": "Rui Xie", "authorId": "2143721734"}, {"name": "Wenxin Hu", "authorId": "2288018918"}, {"name": "Wei Ye", "authorId": "2052980435"}], "n_citations": 1}, "snippets": ["Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020).\n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models."], "score": 0.55419921875}], "table": null}, {"title": "Recent Innovations and Enhancements", "tldr": "Recent innovations in dense passage retrieval include specialized pre-training strategies, balanced training algorithms, and retrieval augmentation frameworks that enable more effective query-document matching and better generalization capabilities. (7 sources)", "text": "\n* **Context-aware pre-training**: Researchers have developed specialized pre-training techniques for dense retrieval models that incorporate contextual information rather than focusing solely on single-text internal modeling. These approaches include contextual-supervised learning and contextual masked auto-encoding, which significantly improve dense retrieval performance by creating more robust text representations. <Paper corpusId=\"251594591\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"257985191\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>\n\n* **Masked auto-encoder architectures**: Masked auto-encoder (MAE) pre-training has emerged as one of the most promising approaches for dense passage retrieval. These architectures enhance the text representation ability of encoders by creating more challenging reconstruction tasks for the decoder, effectively pushing the encoder to develop better semantic understanding capabilities. <Paper corpusId=\"258833383\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>\n\n* **Balanced training algorithms**: To address instability issues in dense retrieval training, researchers have developed algorithms like STAR (Stable Training Algorithm for dense Retrieval) that improve training stability by introducing random negatives alongside hard negatives. Complementary approaches like ADORE (Algorithm for Directly Optimizing Ranking Performance) replace static hard negative sampling with dynamic methods to directly optimize ranking performance. <Paper corpusId=\"251594591\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"233289894\" paperTitle=\"(Zhan et al., 2021)\" isShortName></Paper>\n\n* **Topic-aware sampling**: Efficient training techniques like TAS-Balanced (Topic-Aware query and balanced margin Sampling) have been developed to reduce computational requirements while maintaining performance. This approach clusters queries before training and samples from these clusters, enabling effective model training on consumer-grade hardware while still achieving competitive results. <Paper corpusId=\"257985191\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233231706\" paperTitle=\"(Hofstatter et al., 2021)\" isShortName></Paper>\n\n* **Generic retrieval plug-ins**: Moving beyond tightly coupled systems, researchers have begun exploring generic retrieval plug-in frameworks that can assist target language models without requiring joint fine-tuning. This approach allows retrievers to support various language models that may not be known in advance or cannot be fine-tuned together with the retriever. <Paper corpusId=\"258960666\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>\n\n* **Cross-domain applications**: Dense passage retrieval techniques have been successfully applied across multiple domains, including open-domain question answering, conversational systems, and web search, demonstrating their versatility and effectiveness in diverse information retrieval scenarios. <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper>", "citations": [{"id": "(Wu et al., 2022)", "paper": {"corpus_id": 251594591, "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval", "year": 2022, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Xing Wu", "authorId": "2155226596"}, {"name": "Guangyuan Ma", "authorId": "2068996632"}, {"name": "Meng Lin", "authorId": "2156805995"}, {"name": "Zijia Lin", "authorId": "1818920"}, {"name": "Zhongyuan Wang", "authorId": "2135394423"}, {"name": "Songlin Hu", "authorId": "40845069"}], "n_citations": 26}, "snippets": ["Dense retrieval models are generally based on pre-trained language models with a siamese or dual-encoder architecture to encode queries and documents into low-dimensional vector space for effective search (Hofst\u00e4tter et al., 2021)Humeau et al. 2019;Xiong et al. 2020;(Zhan et al., 2021)Zhan et al. , 2020. The relevances between queries and documents are calculated with cosine similarity or dot-product function in the vector space. Therefore, high-quality text representation based on PLM is crucial for dense passage retrieval.\n\nDPR (Karpukhin et al. 2020) successfully shows that dense retrieval models can outperform BM25 methods. Since then, some works have emerged to boost dense retrieval performance by improving the pre-training process tailored for dense retrieval. (Lu et al., 2021)Gao and Callan 2021a;Liu and Shao 2022) encourage the encoder to improve the text representation modeling ability through auxiliary self-supervised reconstruction tasks. Auxiliary tasks usually utilize a weak decoder to reconstruct the masked text with the assistance of the text's vector from the encoder, which forces the encoder to provide better text representations. Although these works have been shown to be very effective and achieved some improvements in dense retrieval, they mainly focus on single-text internal modeling without considering contextual information. (Chang et al. 2020;Gao and Callan 2021b;Ma et al. 2022) proposes multi-source and multi-granularity contrastive span prediction"], "score": 0.548828125}, {"id": "(Wu et al., 2023)", "paper": {"corpus_id": 257985191, "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xing Wu", "authorId": "2155226596"}, {"name": "Guangyuan Ma", "authorId": "2068996632"}, {"name": "Peng Wang", "authorId": "144282672"}, {"name": "Meng Lin", "authorId": "2156805995"}, {"name": "Zijia Lin", "authorId": "1818920"}, {"name": "Fuzheng Zhang", "authorId": "2642200"}, {"name": "Songlin Hu", "authorId": "40845069"}], "n_citations": 8}, "snippets": ["PLM-based dense retrieval typically employs a siamese or dual-encoder architecture to convert queries and documents into a low-dimensional vector space (Hofst\u00e4tter et al., 2021)Humeau et al., 2019;Xiong et al., 2020;(Zhan et al., 2021)Zhan et al., , 2020)). Relevance between queries and documents is calculated using cosine similarity or dot products. This low-dimension vector is called the dense vector and is trained to capture the sentence semantics at the passage level. Recent efforts aim to improve dense retrieval performance by adding auxiliary tasks to pre-training, like contextual-supervised learning (Gao and Callan, 2021b;Wu et al., 2022  ( Wu et al., 2022), expands on this by incorporating contextual masked auto-encoding, leading to better dense retrieval performances."], "score": 0.52099609375}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 258833383, "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Zehan Li", "authorId": "2109967721"}, {"name": "Yanzhao Zhang", "authorId": "2107949588"}, {"name": "Dingkun Long", "authorId": "8427191"}, {"name": "Pengjun Xie", "authorId": "35930962"}], "n_citations": 3}, "snippets": ["Recently, various studies have been directed towards exploring dense passage retrieval techniques employing pre-trained language models, among which the masked auto-encoder (MAE) pre-training architecture has emerged as the most promising. The conventional MAE framework relies on leveraging the passage reconstruction of decoder to bolster the text representation ability of encoder, thereby enhancing the performance of resulting dense retrieval systems. Within the context of building the representation ability of the encoder through passage reconstruction of decoder, it is reasonable to postulate that a ``more demanding'' decoder will necessitate a corresponding increase in the encoder's ability."], "score": 0.60302734375}, {"id": "(Zhan et al., 2021)", "paper": {"corpus_id": 233289894, "title": "Optimizing Dense Retrieval Model Training with Hard Negatives", "year": 2021, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Jingtao Zhan", "authorId": "1643961315"}, {"name": "Jiaxin Mao", "authorId": "1644047628"}, {"name": "Yiqun Liu", "authorId": "1783406"}, {"name": "Jiafeng Guo", "authorId": "70414094"}, {"name": "M. Zhang", "authorId": "39767557"}, {"name": "Shaoping Ma", "authorId": "8093158"}], "n_citations": 276}, "snippets": ["Ranking has always been one of the top concerns in information retrieval researches. For decades, the lexical matching signal has dominated the ad-hoc retrieval process, but solely using this signal in retrieval may cause the vocabulary mismatch problem. In recent years, with the development of representation learning techniques, many researchers turn to Dense Retrieval (DR) models for better ranking performance. Although several existing DR models have already obtained promising results, their performance improvement heavily relies on the sampling of training examples. Many effective sampling strategies are not efficient enough for practical usage, and for most of them, there still lacks theoretical analysis in how and why performance improvement happens. To shed light on these research questions, we theoretically investigate different training strategies for DR models and try to explain why hard negative sampling performs better than random sampling. Through the analysis, we also find that there are many potential risks in static hard negative sampling, which is employed by many existing training methods. Therefore, we propose two training strategies named a Stable Training Algorithm for dense Retrieval (STAR) and a query-side training Algorithm for Directly Optimizing Ranking pErformance (ADORE), respectively. STAR improves the stability of DR training process by introducing random negatives. ADORE replaces the widely-adopted static hard negative sampling method with a dynamic one to directly optimize the ranking performance. Experimental results on two publicly available retrieval benchmark datasets show that either strategy gains significant improvements over existing competitive baselines and a combination of them leads to the best performance."], "score": 0.0}, {"id": "(Hofstatter et al., 2021)", "paper": {"corpus_id": 233231706, "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling", "year": 2021, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Sebastian Hofst\u00e4tter", "authorId": "97393346"}, {"name": "Sheng-Chieh Lin", "authorId": "122045993"}, {"name": "Jheng-Hong Yang", "authorId": "2109723027"}, {"name": "Jimmy J. Lin", "authorId": "145580839"}, {"name": "A. Hanbury", "authorId": "1699657"}], "n_citations": 402}, "snippets": ["A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further."], "score": 0.0}, {"id": "(Yu et al., 2023)", "paper": {"corpus_id": 258960666, "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zichun Yu", "authorId": "2275526493"}, {"name": "Chenyan Xiong", "authorId": "2139787803"}, {"name": "S. Yu", "authorId": "150311558"}, {"name": "Zhiyuan Liu", "authorId": "2109232579"}], "n_citations": 69}, "snippets": ["Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together."], "score": 0.521484375}, {"id": "(Long et al., 2022)", "paper": {"corpus_id": 253157959, "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Dingkun Long", "authorId": "8427191"}, {"name": "Yanzhao Zhang", "authorId": "2107949588"}, {"name": "Guangwei Xu", "authorId": "2149131512"}, {"name": "Pengjun Xie", "authorId": "35930962"}], "n_citations": 4}, "snippets": ["Dense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair.\n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022)."], "score": 0.52783203125}], "table": null}], "cost": 0.21107399999999998}}
