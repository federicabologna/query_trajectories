{"original_query": "Can classifier-free guidance be extended beyond conditional diffusion models in image generation to NLP tasks?", "input": "classifier free guidance beyond conditional diffusion models in image generation NLP tasks", "output": {"sections": [{"title": "Introduction to Classifier-Free Guidance", "tldr": "Classifier-Free Guidance (CFG) is a technique that improves conditional generation in diffusion models without requiring a separate classifier. It works by simultaneously training conditional and unconditional models, then combining their outputs during sampling to balance quality and diversity. (6 sources)", "text": "\nClassifier-Free Guidance (CFG) represents a significant advancement in the field of generative modeling, particularly for diffusion models. Introduced by Ho et al., this technique provides a method to enhance the quality of generated outputs without relying on external classifiers <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>. Unlike its predecessor, classifier guidance, CFG achieves a similar trade-off between sample quality and diversity by jointly training conditional and unconditional diffusion models and combining their score estimates during the sampling process.\n\nThe development of CFG addressed limitations in previous approaches to guided generation. Classifier guidance, introduced by Dhariwal et al., improved sample quality by modifying the diffusion score using gradients from an external classifier <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>. While effective, this method was constrained by its dependency on classifier gradients, which limited the variability of generated images <Paper corpusId=\"272690217\" paperTitle=\"(Buchanan et al., 2024)\" isShortName></Paper>. Building on the insight from Salimans et al. that well-classified data tends to exhibit high perceptual quality <Paper corpusId=\"1687220\" paperTitle=\"(Salimans et al., 2016)\" isShortName></Paper>, CFG sought to achieve similar benefits without the need for a separate classifier.\n\nThe implementation of CFG involves training a single neural network that serves both as an unconditional and conditional model. During training, conditioning is randomly set to a null token based on a probability hyperparameter, allowing the model to learn both conditional and unconditional generations simultaneously <Paper corpusId=\"272690217\" paperTitle=\"(Buchanan et al., 2024)\" isShortName></Paper>. This approach has become widely adopted due to its simplicity and effectiveness compared to classifier guidance <Paper corpusId=\"258615416\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>.\n\nCFG operates by amplifying the directional difference between conditional and unconditional predictions in the model's latent space. By enhancing this directional shift, the model is effectively steered toward outputs that better align with given conditions <Paper corpusId=\"276774646\" paperTitle=\"(Jacobi et al., 2025)\" isShortName></Paper>. This technique has proven particularly valuable in text-to-image generation models, where it improves adherence to text prompts without requiring external classifiers, making it a foundational component in many state-of-the-art generative systems <Paper corpusId=\"258615416\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Ho, 2022)", "paper": {"corpus_id": 249145348, "title": "Classifier-Free Diffusion Guidance", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Jonathan Ho", "authorId": "2126278"}], "n_citations": 3970}, "snippets": ["We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."], "score": 0.9599609375}, {"id": "(Dhariwal et al., 2021)", "paper": {"corpus_id": 234357997, "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Alex Nichol", "authorId": "38967461"}], "n_citations": 7951}, "snippets": ["We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"], "score": 0.0}, {"id": "(Buchanan et al., 2024)", "paper": {"corpus_id": 272690217, "title": "Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation", "year": 2024, "venue": "2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)", "authors": [{"name": "Noah Buchanan", "authorId": "2321408469"}, {"name": "Susan Gauch", "authorId": "2268404815"}, {"name": "Quan Mai", "authorId": "2308097654"}], "n_citations": 1}, "snippets": ["Classifier guidance (Dhariwal et al., 2021) modifies the diffusion score, specifically the gradient of the log probability density function, which is more tractable to learn than directly modeling the data distribution. This approach employs a classifier to guide the generation process by increasing the probability of data that the classifier assigns a high likelihood to the correct label. As demonstrated by (Salimans et al., 2016), data that is well-classified tends to exhibit high perceptual quality, contributing to superior image generation outcomes. \n\nDespite its effectiveness in balancing precision and recall, classifier guidance depends on the gradients of an image classifier, limiting the variability of generated images. This dependency raises the question of whether it is possible to achieve comparable or superior guidance without relying on a classifier. \n\nClassifier-free guidance, introduced by [7], addresses this issue by eliminating the need for a dedicated classifier. Instead, it involves training an unconditional denoising diffusion model, parameterized by a score estimator, alongside a conditional denoising diffusion model, parameterized by a conditional score estimator. These are parameterized using a single neural network. For the unconditional model, a null token is used for the class identifier. The models are jointly trained, with the conditioning randomly set to a null token based on a hyperparameter probability p-uncond. This methodology not only simplifies the model architecture but also enhances guidance capabilities without the dependence on an external classifier."], "score": 0.9296875}, {"id": "(Salimans et al., 2016)", "paper": {"corpus_id": 1687220, "title": "Improved Techniques for Training GANs", "year": 2016, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tim Salimans", "authorId": "2887364"}, {"name": "I. Goodfellow", "authorId": "153440022"}, {"name": "Wojciech Zaremba", "authorId": "2563432"}, {"name": "Vicki Cheung", "authorId": "34415167"}, {"name": "Alec Radford", "authorId": "38909097"}, {"name": "Xi Chen", "authorId": "41192764"}], "n_citations": 9072}, "snippets": ["We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes."], "score": 0.0}, {"id": "(Zhao et al., 2023)", "paper": {"corpus_id": 258615416, "title": "Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator", "year": 2023, "venue": "ACM Multimedia", "authors": [{"name": "Jing Zhao", "authorId": "46509200"}, {"name": "Heliang Zheng", "authorId": "28331771"}, {"name": "Chaoyue Wang", "authorId": "2518211"}, {"name": "Long Lan", "authorId": "2156125124"}, {"name": "Wanrong Huang", "authorId": "3441469"}, {"name": "Wenjing Yang", "authorId": "2120811655"}], "n_citations": 10}, "snippets": ["Classifier-free guidance [13] is a powerful sampling technique as it directs the model towards text guidance and away from null-text guidance by introducing a null-text guidance term. Compared to the previous study, classifier guidance (Dhariwal et al., 2021), which utilizes a separate classifier to trade off Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) via truncation or low-temperature sampling, classifierfree guidance can be easily implemented and applied. Specifically, classifier-free guidance trains an unconditional denoising diffusion model together with the conditional model and updates the prediction of noise by increasing the distance between target noise and null-text noise. The utilization of classifier-free guidance has greatly enhanced the caliber of generated images and has become ubiquitous in subsequent studies [7] 8, 10, 11, 15-17, 20, 25, 26, 33, 39]."], "score": 0.91796875}, {"id": "(Jacobi et al., 2025)", "paper": {"corpus_id": 276774646, "title": "Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jonathan Jacobi", "authorId": "2348485713"}, {"name": "Gal Niv", "authorId": "2333352"}], "n_citations": 0}, "snippets": ["Inspired by the\"features as directions\"perspective and the Classifier-Free Guidance (CFG) approach from diffusion models, Superscopes amplifies weak but meaningful features, enabling the interpretation of internal representations that previous methods failed to explain-all without requiring additional training.\n\nIn diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)). \n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier."], "score": 0.92529296875}], "table": null}, {"title": "Classifier-Free Guidance in Diffusion Models", "tldr": "Classifier-Free Guidance (CFG) has become the dominant conditioning approach in diffusion models, particularly for text-to-image generation, as it eliminates the need for external classifiers while providing superior control over quality-diversity trade-offs. CFG works by combining conditional and unconditional score estimates during sampling with a guidance scale parameter that amplifies the directional shift toward the desired condition. (10 sources)", "text": "\nClassifier-Free Guidance (CFG) represents a significant advancement in diffusion models, specifically addressing limitations in earlier conditioning approaches. Prior to CFG, conditional diffusion models primarily relied on classifier guidance, introduced by Dhariwal et al., which used gradients from an external classifier to modify the diffusion score <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>. While effective, this approach was constrained by its dependency on separate classifier models and limited conditioning capabilities <Paper corpusId=\"252438737\" paperTitle=\"(Zbinden, 2022)\" isShortName></Paper>.\n\nHo and Salimans introduced CFG to overcome these limitations, demonstrating that guidance could be achieved with a pure generative model without requiring an external classifier <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>. The approach works by jointly training a conditional and unconditional diffusion model using a single neural network. During training, the conditioning information is randomly dropped with a fixed probability, enabling the model to learn both conditional and unconditional objectives simultaneously <Paper corpusId=\"253420366\" paperTitle=\"(Schramowski et al., 2022)\" isShortName></Paper>. This dual-purpose training creates what is effectively a unified model that can generate both conditional and unconditional outputs.\n\nThe core mechanism of CFG involves a linear interpolation between conditional and unconditional predictions during the sampling process. This is typically expressed as:\n\n\u03b8\u0302(xt, t, c) = (1 + \u03c4)\u03b8(xt, t, c) - \u03c4\u03b8(xt, t)\n\nwhere \u03c4 is the guidance scale parameter <Paper corpusId=\"258059755\" paperTitle=\"(Armandpour et al., 2023)\" isShortName></Paper>. Intuitively, this pushes the unconditional prediction in the direction of the conditional prediction, with the guidance scale determining the magnitude of this directional shift <Paper corpusId=\"253420366\" paperTitle=\"(Schramowski et al., 2022)\" isShortName></Paper>. This parameter allows for a controlled trade-off between sample quality and diversity, similar to what classifier guidance achieves <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>.\n\nThe impact of CFG has been particularly profound in text-to-image generation. Nichol et al. demonstrated through their GLIDE model that CFG can effectively generate text-conditional images with remarkable quality <Paper corpusId=\"245335086\" paperTitle=\"(Nichol et al., 2021)\" isShortName></Paper>. When evaluated by human judges, GLIDE's samples using classifier-free guidance were preferred over those from DALL-E for both photorealism and caption similarity <Paper corpusId=\"245335086\" paperTitle=\"(Nichol et al., 2021)\" isShortName></Paper>. This demonstrated that CFG works more favorably than other guidance methods like CLIP guidance for text-conditional image generation <Paper corpusId=\"248097655\" paperTitle=\"(Ramesh et al., 2022)\" isShortName></Paper>.\n\nBeyond its core functionality, CFG has enabled more versatile conditioning capabilities. Unlike classifier guidance, which is limited by a fixed set of classes, CFG can condition on complex information such as detailed text descriptions, allowing for more elaborate image compositions <Paper corpusId=\"252438737\" paperTitle=\"(Zbinden, 2022)\" isShortName></Paper>. This flexibility has made CFG the foundation for numerous advanced generative systems, including those incorporating multiple types of conditioning <Paper corpusId=\"253581213\" paperTitle=\"(Brooks et al., 2022)\" isShortName></Paper>.\n\nThe widespread adoption of CFG in modern diffusion models is evident in its implementation across numerous state-of-the-art systems. It forms the key basis of modern text-guided generation with diffusion models <Paper corpusId=\"270391454\" paperTitle=\"(Chung et al., 2024)\" isShortName></Paper> and has been incorporated into prominent models like Stable Diffusion <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper>. By enabling efficient and high-quality conditional generation without external classifiers, CFG has become a cornerstone technique in the evolution of diffusion-based image generation.", "citations": [{"id": "(Dhariwal et al., 2021)", "paper": {"corpus_id": 234357997, "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Alex Nichol", "authorId": "38967461"}], "n_citations": 7951}, "snippets": ["We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"], "score": 0.0}, {"id": "(Zbinden, 2022)", "paper": {"corpus_id": 252438737, "title": "Implementing and Experimenting with Diffusion Models for Text-to-Image Generation", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Robin Zbinden", "authorId": "1431226552"}], "n_citations": 3}, "snippets": ["Depending on a separate model is inconvenient and it complicates the training pipeline. Moreover, for classifier guidance, the number of classes is limited, preventing us from conditioning on complex information to generate more elaborated image compositions. Thus,  proposed classifier-free guidance, which only relies on a single diffusion model. Classifierfree guidance considers a conditional diffusion model \u03b8 (x t , t|y) that can be made unconditional by replacing occasionally during training the condition y by an empty condition \u2205, e.g., by setting the caption to an empty string. The model \u03b8 (x t , t|\u2205) can then be used to generate unconditional images. Classifier-free guidance therefore consists in updating the model output using a linear combination between \u03b8 (x t , t|\u2205) and \u03b8 (x t , t|y) in the following way:\n\nwith s \u2265 1 being the guidance scale. This update can be understood as an attempt to move further in the direction of the conditional model, while moving away from the unconditional model. It is performed at each diffusion step when sampling and Ho and Salimans (2021) have shown that it improves sample quality."], "score": 0.95166015625}, {"id": "(Ho, 2022)", "paper": {"corpus_id": 249145348, "title": "Classifier-Free Diffusion Guidance", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Jonathan Ho", "authorId": "2126278"}], "n_citations": 3970}, "snippets": ["We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."], "score": 0.9599609375}, {"id": "(Schramowski et al., 2022)", "paper": {"corpus_id": 253420366, "title": "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "P. Schramowski", "authorId": "40896023"}, {"name": "Manuel Brack", "authorId": "2166299958"}, {"name": "Bjorn Deiseroth", "authorId": "2905059"}, {"name": "K. Kersting", "authorId": "2066493115"}], "n_citations": 309}, "snippets": ["Classifier-free guidance [17] is a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. The approach randomly drops the text conditioning c p with a fixed probability during training, resulting in a joint model for unconditional and conditional objectives. During inference the score estimates for the x-prediction are adjusted so that:\n\nwith guidance scale s g which is typically chosen as s g \u2208 (0, 20] and \u03b8 defining the noise estimate with parameters \u03b8. Intuitively, the unconditioned -prediction \u03b8 (z t ) is pushed in the direction of the conditioned \u03b8 (z t , c p ) to yield an image faithful to prompt p. Lastly, s g determines the magnitude of the influence"], "score": 0.94091796875}, {"id": "(Armandpour et al., 2023)", "paper": {"corpus_id": 258059755, "title": "Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Mohammadreza Armandpour", "authorId": "66139883"}, {"name": "A. Sadeghian", "authorId": "145321315"}, {"name": "Huangjie Zheng", "authorId": "8158616"}, {"name": "Amir Sadeghian", "authorId": "145759966"}, {"name": "Mingyuan Zhou", "authorId": "2152175923"}], "n_citations": 128}, "snippets": ["To generate photo-realistic images given text prompts, the diffusion models can further take advantage of classifier guidance [9] or classifier-free guidance [15] to improve the image quality. Especially, in the context of text-to-image generation, classifier-free guidance is more widely used, which is usually expressed as a linear interpolation between the conditional and unconditional prediction \u02c6 \u03b8 (x t , t, c) = (1 + \u03c4 ) \u03b8 (x t , t, c) \u2212 \u03c4 \u03b8 (x t , t) at each timestep t with a guidance scale parameter \u03c4."], "score": 0.9296875}, {"id": "(Nichol et al., 2021)", "paper": {"corpus_id": 245335086, "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alex Nichol", "authorId": "38967461"}, {"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Pranav Shyam", "authorId": "67311962"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Bob McGrew", "authorId": "39593364"}, {"name": "I. Sutskever", "authorId": "1701686"}, {"name": "Mark Chen", "authorId": "2108828435"}], "n_citations": 3629}, "snippets": ["Ho & Salimans (2021) achieved similar results without a separately trained classifier through the use of classifier-free guidance, a form of guidance that interpolates between predictions from a diffusion model with and without labels.\n\nNext, we compare two techniques for guiding diffusion models towards text prompts: CLIP guidance and classifier-free guidance. Using human and automated evaluations, we find that classifier-free guidance yields higherquality images.\n\nWe find that samples from our model generated with classifier-free guidance are both photorealistic and reflect a wide breadth of world knowledge. When evaluated by human judges, our samples are preferred to those from DALL-E (Ramesh et al., 2021) 87% of the time when evaluated for photorealism, and 69% of the time when evaluated for caption similarity."], "score": 0.9560546875}, {"id": "(Ramesh et al., 2022)", "paper": {"corpus_id": 248097655, "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Alex Nichol", "authorId": "38967461"}, {"name": "Casey Chu", "authorId": "30414789"}, {"name": "Mark Chen", "authorId": "2108828435"}], "n_citations": 6915}, "snippets": ["Ho and Salimans (Ho, 2022) introduced classifier-free guidance and showed that one can perform guidance implictly from the predictions of the model with and without the conditioning information, thus removing the need for a classifier. Nichol et al. [35] showed classifier-free guidance works more favorably than CLIP guidance for text conditional image generation."], "score": 0.95263671875}, {"id": "(Brooks et al., 2022)", "paper": {"corpus_id": 253581213, "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Tim Brooks", "authorId": "2679394"}, {"name": "Aleksander Holynski", "authorId": "2248172435"}, {"name": "Alexei A. Efros", "authorId": "1763086"}], "n_citations": 1833}, "snippets": ["Classifier-free diffusion guidance [20] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifierfree guidance effectively shifts probability mass toward data where an implicit classifier p \u03b8 (c|z t ) assigns high likelihood to the conditioning c. Our model is therefore capable of conditional or unconditional denoising with respect to both or either conditional inputs. We introduce two guidance scales, s I and s T , which can be adjusted to trade off how strongly the generated samples correspond with the input image and how strongly they correspond with the edit instruction."], "score": 0.9296875}, {"id": "(Chung et al., 2024)", "paper": {"corpus_id": 270391454, "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hyungjin Chung", "authorId": "2110872233"}, {"name": "Jeongsol Kim", "authorId": "2109216792"}, {"name": "Geon Yeong Park", "authorId": "153118937"}, {"name": "Hyelin Nam", "authorId": "2268758810"}, {"name": "Jong Chul Ye", "authorId": "2254155658"}], "n_citations": 35}, "snippets": ["Classifier-free guidance (CFG) (Ho, 2022) forms the key basis of modern text-guided generation with diffusion models (Dhariwal et al., 2021)(Rombach et al., 2021). Nowadays, it is common practice to train a diffusion model with large-scale paired text-image data (Schuhmann et al., 2022), so that sampling (i.e. generating) a signal (e.g. image, video) from a diffusion model can either be done unconditionally from p \u03b8 (x|\u2205) \u2261 p \u03b8 (x), or conditionally from p \u03b8 (x|c), where c is the text conditioning. Once trained, it seems natural that one would acquire samples from the conditional distribution by simply solving the probability-flow ODE or SDE sampling (Song et al., 2020)a;(Karras et al., 2022) with the conditional score function. In practice, however, it is observed that the conditioning signal is insufficient when used naively. To emphasize the guidance, one uses the guidance scale \u03c9 > 1, where the direction can be defined by the direction from the unconditional score to the conditional score (Ho, 2022)."], "score": 0.92919921875}, {"id": "(Rombach et al., 2021)", "paper": {"corpus_id": 245335280, "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Robin Rombach", "authorId": "1660819540"}, {"name": "A. Blattmann", "authorId": "119843260"}, {"name": "Dominik Lorenz", "authorId": "2053482699"}, {"name": "Patrick Esser", "authorId": "35175531"}, {"name": "B. Ommer", "authorId": "1796707"}], "n_citations": 15768}, "snippets": ["By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."], "score": 0.0}], "table": null}, {"title": "Technical Implementation of Classifier-Free Guidance", "tldr": "Classifier-Free Guidance is implemented by training a single model that serves both conditional and unconditional purposes by randomly dropping conditioning information during training. During sampling, outputs from conditional and unconditional paths are combined using a guidance scale parameter that controls the quality-diversity trade-off. (14 sources)", "text": "\nClassifier-Free Guidance (CFG) offers an elegant implementation approach that eliminates the need for separate conditional and unconditional models. The technical implementation consists of two primary phases: the training phase, where the model learns both conditional and unconditional generation capabilities, and the sampling phase, where these capabilities are combined to achieve guided generation.\n\nDuring the training phase, a single model is trained to handle both conditional and unconditional generation simultaneously. This is achieved by randomly dropping out the conditioning information (such as text prompts or class labels) with a predetermined probability <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>. When the condition is dropped, it is typically replaced with a null token, padding tokens, or a learned embedding, effectively teaching the model to generate content without conditioning <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>. This approach enables the model to learn both conditional distribution p(x|c) and unconditional distribution p(x) within a unified architecture <Paper corpusId=\"270923987\" paperTitle=\"(Sadat et al., 2024)\" isShortName></Paper>.\n\nThe sampling phase is where the true power of CFG emerges. The model combines predictions from both conditional and unconditional paths using a linear interpolation formula. This formula is typically expressed as:\n\n\u03b5(zt, c, t) = \u03b5\u03b8(zt, c, t) + \u03b1(\u03b5\u03b8(zt, c, t) - \u03b5\u03b8(zt, t))\n\nwhere \u03b5\u03b8(zt, c, t) is the conditional score estimate, \u03b5\u03b8(zt, t) is the unconditional score estimate, and \u03b1 (also called \u03c9, \u03c4, \u03b3, or w in different implementations) is the guidance scale parameter <Paper corpusId=\"269502576\" paperTitle=\"(Basu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258556958\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper> <Paper corpusId=\"260886966\" paperTitle=\"(Ye et al., 2023)\" isShortName></Paper>. This formula effectively amplifies the directional difference between conditional and unconditional predictions, steering the generation toward outputs that better align with the given condition.\n\nFrom a theoretical perspective, CFG can be understood through the lens of Bayesian inference. Ho and Salimans demonstrated that the difference between conditional and unconditional score estimates implicitly represents the gradient of the classifier <Paper corpusId=\"259341599\" paperTitle=\"(Baykal et al., 2023)\" isShortName></Paper>. This insight reveals that CFG effectively approximates classifier guidance without requiring a separate classifier, as expressed in the equation:\n\n\u2207x log p(y|x) \u2248 \u2207x log p(x|y) - \u2207x log p(x)\n\nwhere the right side represents the difference between conditional and unconditional score functions <Paper corpusId=\"260886956\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>.\n\nThe guidance scale parameter (\u03b1) plays a crucial role in controlling the balance between sample quality and diversity. Higher values of \u03b1 increase fidelity to the conditioning information but may reduce diversity and introduce artifacts, while lower values preserve diversity at the cost of weaker conditioning <Paper corpusId=\"277633776\" paperTitle=\"(Jagpal et al., 2025)\" isShortName></Paper>. This parameter provides a convenient knob for adjusting the output characteristics without retraining the model.\n\nOne of the notable advantages of CFG is its flexibility in handling multiple types of conditions. The approach can be extended to compose multiple guidance signals by introducing separate guidance scales for each condition type <Paper corpusId=\"258564566\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. For example, in models that condition on both text and reference images, different guidance scales can be applied to each condition, allowing fine-grained control over how strongly each condition influences the generation <Paper corpusId=\"258564566\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249375227\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.\n\nCFG has been successfully implemented across various model architectures. In latent diffusion models (LDMs), it is applied to the noise prediction in latent space <Paper corpusId=\"254408758\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper>. For text-to-image generation, the technique has been shown to significantly improve image-text alignment, especially for challenging text prompts <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260886966\" paperTitle=\"(Ye et al., 2023)\" isShortName></Paper>. Recent research has also shown that CFG-like behavior can be achieved without additional training of an unconditional model by using a conditioning vector independent of the input data <Paper corpusId=\"270923987\" paperTitle=\"(Sadat et al., 2024)\" isShortName></Paper>.\n\nIn practice, the sampling process typically employs CFG in conjunction with methods like DDIM (Denoising Diffusion Implicit Models) sampling to update the latent representation at each time step <Paper corpusId=\"269502576\" paperTitle=\"(Basu et al., 2024)\" isShortName></Paper>. This combination has become the standard approach in most state-of-the-art diffusion-based generative systems, thanks to its computational efficiency and effectiveness in improving conditional generation quality <Paper corpusId=\"268351323\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265466084\" paperTitle=\"(Sueyoshi et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Yu et al., 2022)", "paper": {"corpus_id": 249926846, "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation", "year": 2022, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Jiahui Yu", "authorId": "2338016295"}, {"name": "Yuanzhong Xu", "authorId": "2145139570"}, {"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "Thang Luong", "authorId": "1821711"}, {"name": "Gunjan Baid", "authorId": "1396954703"}, {"name": "Zirui Wang", "authorId": "2331539"}, {"name": "Vijay Vasudevan", "authorId": "2053781980"}, {"name": "Alexander Ku", "authorId": "31702389"}, {"name": "Yinfei Yang", "authorId": "2118771180"}, {"name": "Burcu Karagol Ayan", "authorId": "143990191"}, {"name": "Ben Hutchinson", "authorId": "2044655623"}, {"name": "Wei Han", "authorId": "143911112"}, {"name": "Zarana Parekh", "authorId": "27456119"}, {"name": "Xin Li", "authorId": "2158973314"}, {"name": "Han Zhang", "authorId": null}, {"name": "Jason Baldridge", "authorId": "1387994164"}, {"name": "Yonghui Wu", "authorId": "48607963"}], "n_citations": 1133}, "snippets": ["Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability", "Classifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."], "score": 0.95263671875}, {"id": "(Sadat et al., 2024)", "paper": {"corpus_id": 270923987, "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Seyedmorteza Sadat", "authorId": "2261742393"}, {"name": "Manuel Kansy", "authorId": "2204861903"}, {"name": "Otmar Hilliges", "authorId": "1466533438"}, {"name": "Romann M. Weber", "authorId": "145848224"}], "n_citations": 14}, "snippets": ["In this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model. The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score."], "score": 0.93798828125}, {"id": "(Basu et al., 2024)", "paper": {"corpus_id": 269502576, "title": "On Mechanistic Knowledge Localization in Text-to-Image Generative Models", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Samyadeep Basu", "authorId": "2114710333"}, {"name": "Keivan Rezaei", "authorId": "2204576892"}, {"name": "Priyatham Kattakinda", "authorId": "1962835975"}, {"name": "Ryan A. Rossi", "authorId": "2317012495"}, {"name": "Cherry Zhao", "authorId": "2299942084"}, {"name": "V. Morariu", "authorId": "2061209811"}, {"name": "Varun Manjunatha", "authorId": "1977256"}, {"name": "S. Feizi", "authorId": "34389431"}], "n_citations": 15}, "snippets": ["During the inference process, the regulation of image generation involves the utilization of classifier-free guidance, as outlined in (Ho, 2022) which incorporates scores from both the conditional and unconditional diffusion models at each time-step.Specifically, the classifier-free guidance is applied at each time-step to combine the conditional (\u03f5 \u03b8 (z t , c, t)) and unconditional score estimates (\u03f5 \u03b8 (z t , t)).\n\nThe result is a combined score denoted as \u03b5(z t , c, t).\n\n\u03b5(z t , c, t) = \u03f5 \u03b8 (z t , c, t) + \u03b1 (\u03f5 \u03b8 (z t , c, t) \u2212 \u03f5 \u03b8 (z t , t)) , \u2200t \u2208 [T, 1] .\n\n(1) This combined score is used to update the latent z t using DDIM sampling (Song et al., 2020) at each time-step to obtain the final latent code z 0 ."], "score": 0.93798828125}, {"id": "(Dong et al., 2023)", "paper": {"corpus_id": 258556958, "title": "Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models", "year": 2023, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Wenkai Dong", "authorId": "23677993"}, {"name": "Song Xue", "authorId": "2216675591"}, {"name": "Xiaoyue Duan", "authorId": "2067781481"}, {"name": "Shumin Han", "authorId": "1488666685"}], "n_citations": 62}, "snippets": ["To this end, the classifier-free guidance technique is proposed, where the prediction for each step is a combination of conditional and unconditional predictions. Formally, let c = \u03c4 \u03b8 (P) be the conditional embedding vector and \u2205 = \u03c4 \u03b8 (\"\") be the unconditional one, the classifierfree guidance prediction is calculated by: \n\nwhere \u03c9 is the guidance scale parameter."], "score": 0.93017578125}, {"id": "(Ye et al., 2023)", "paper": {"corpus_id": 260886966, "title": "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Hu Ye", "authorId": "2145877255"}, {"name": "Jun Zhang", "authorId": "2157209270"}, {"name": "Siyi Liu", "authorId": "150301258"}, {"name": "Xiao Han", "authorId": null}, {"name": "Wei Yang", "authorId": "2150081263"}], "n_citations": 807}, "snippets": ["For the conditional diffusion models, classifier guidance (Dhariwal et al., 2021) is a straightforward technique used to balance image fidelity and sample diversity by utilizing gradients from a separately trained classifier. To eliminate the need for training a classifier independently, classifier-free guidance [39] is often employed as an alternative method. In this approach, the conditional and unconditional diffusion models are jointly trained by randomly dropping c during training. In the sampling stage, the predicted noise is calculated based on the prediction of both the conditional model \u03f5 \u03b8 (x t , c, t) and unconditional model \u03f5 \u03b8 (x t , t): \n\nhere, w, often named guidance scale or guidance weight, is a scalar value that adjusts the alignment with condition c. \n\nFor text-to-image diffusion models, classifier-free guidance plays a crucial role in enhancing the image-text alignment of generated samples."], "score": 0.93212890625}, {"id": "(Baykal et al., 2023)", "paper": {"corpus_id": 259341599, "title": "ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning", "year": 2023, "venue": "Asian Conference on Machine Learning", "authors": [{"name": "Gulcin Baykal", "authorId": "51222946"}, {"name": "Halil Faruk Karagoz", "authorId": "2213299635"}, {"name": "T. Binhuraib", "authorId": "2092548838"}, {"name": "Gozde Unal", "authorId": "2256988285"}], "n_citations": 3}, "snippets": ["Ho and Salimans (2021) propose classifier-free guidance method that does not require a separate classifier. The conditioning information y is periodically utilized while it is dropped out at the remaining time. Therefore, a single model can be used for both unconditional and conditional generation. Ho and Salimans (2021) derive that the unconditional \u03b8 (x t , t) and conditional \u03b8 (x t , t, y) estimations can be used to represent the gradients of the classifier as: \n\nEquation 7 implies that an implicit classifier can eliminate the need for an explicit classifier, and Ho and Salimans (2021) report better results with the classifier-free guidance compared to the explicit classifier guidance."], "score": 0.95849609375}, {"id": "(Yang et al., 2023)", "paper": {"corpus_id": 260886956, "title": "LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts", "year": 2023, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Binbin Yang", "authorId": "2118583061"}, {"name": "Yinzheng Luo", "authorId": "2157838841"}, {"name": "Ziliang Chen", "authorId": "49865638"}, {"name": "Guangrun Wang", "authorId": "2749191"}, {"name": "Xiaodan Liang", "authorId": "40250403"}, {"name": "Liang Lin", "authorId": "2148303324"}], "n_citations": 15}, "snippets": ["Classifier-guidance (Dhariwal et al., 2021) provides a way for diffusion model to achieve conditional generation by using the gradient of a separately trained classifier p(y|x t ) during sampling. As a more efficient technique, classifier-free guidance [14, 24] replaces the noise estimator by a combination of conditional and unconditional model, without requirement of p(y|x t ): \n\nwhere y is the class label or text embedding from language model [24], \u03c9 \u2265 1 denotes the guidance scale and trivially increasing \u03c9 will amplify the effect of conditional input."], "score": 0.951171875}, {"id": "(Dhariwal et al., 2021)", "paper": {"corpus_id": 234357997, "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Alex Nichol", "authorId": "38967461"}], "n_citations": 7951}, "snippets": ["We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"], "score": 0.0}, {"id": "(Jagpal et al., 2025)", "paper": {"corpus_id": 277633776, "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Diljeet Jagpal", "authorId": "2354333652"}, {"name": "Xi Chen", "authorId": "2355422362"}, {"name": "Vinay P. Namboodiri", "authorId": "145460361"}], "n_citations": 0}, "snippets": ["Classifier-free guidance (Ho, 2022) enables conditioning diffusion models on text without relying on an external classifier. \n\nDuring training, the model alternates between a specific condition y (e.g., text) and a null condition \u2205, learning both the conditional s \u03b8 (x t , t | y) and unconditional s \u03b8 (x t , t | \u2205) score functions. The final conditional score function at sampling is given by: \n\nwhere \u03b3 controls the influence of the text condition. Adjusting the guidance scale, \u03b3, allows the diffusion process to produce images that align closely with the text prompt."], "score": 0.9326171875}, {"id": "(Huang et al., 2023)", "paper": {"corpus_id": 258564566, "title": "Style-A-Video: Agile Diffusion for Arbitrary Text-Based Video Style Transfer", "year": 2023, "venue": "IEEE Signal Processing Letters", "authors": [{"name": "Nisha Huang", "authorId": "2186281333"}, {"name": "Yu-xin Zhang", "authorId": "2108078624"}, {"name": "Weiming Dong", "authorId": "40441149"}], "n_citations": 17}, "snippets": ["Classifier-free guidance [17] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifier-free guidance effectively shifts probability mass toward", "Guidance for conditions. For our task, the scoring network \u03b5  (  ,   ,  T ) has three conditions: the input image   , text prompt  T , and self-attention map   . We find it beneficial to leverage classifier-free guidance:", "concerning each condition. Liu et al. (Liu et al., 2022) demonstrate that a conditional diffusion model can compose score estimates from multiple different conditioning values. We introduce three guidance scales,   ,  T , and   , which can be adjusted to trade off how strongly the generated samples correspond with the conditions."], "score": 0.94091796875}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 249375227, "title": "Compositional Visual Generation with Composable Diffusion Models", "year": 2022, "venue": "European Conference on Computer Vision", "authors": [{"name": "Nan Liu", "authorId": "2087010550"}, {"name": "Shuang Li", "authorId": "145015904"}, {"name": "Yilun Du", "authorId": "15394275"}, {"name": "A. Torralba", "authorId": "143805211"}, {"name": "J. Tenenbaum", "authorId": "1763295"}], "n_citations": 528}, "snippets": ["Large text-guided diffusion models, such as DALLE-2, are able to generate stunning photorealistic images given natural language descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper, we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. To do this, we interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation. Project page: https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/"], "score": 0.0}, {"id": "(Zhang et al., 2022)", "paper": {"corpus_id": 254408758, "title": "SINE: SINgle Image Editing with Text-to-Image Diffusion Models", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Zhixing Zhang", "authorId": "2128662401"}, {"name": "Ligong Han", "authorId": "3471102"}, {"name": "Arna Ghosh", "authorId": "2461629"}, {"name": "Dimitris N. Metaxas", "authorId": "1711560"}, {"name": "Jian Ren", "authorId": "2111473627"}], "n_citations": 160}, "snippets": ["Model-Based Classifier-Free Guidance. With the above-presented LDMs, we introduce our approach, inspired by classifier-free guidance, to overcome overfitting when fine-tuning LDMs with one image. Classifier-free guidance [15] is a technique widely adopted by prior text-to-image diffusion models [32,35]. A single diffusion model is trained using conditional and unconditional objectives by randomly dropping the condition during training. When sampling, a linear combination of the conditional and unconditional score estimation is used: where \u03f5 \u03b8 (z t , c) and \u03f5 \u03b8 (z t ) are the conditional and unconditional \u03f5-predictions, c is the conditioning vector generated by \u03c4 \u03b8 , and w is the weight for the guidance. The predication is performed using the Tweedie's formula [7], namely, z t \u2212 \u221a 1 \u2212 \u1fb1t \u03b5\u03b8 / \u221a \u1fb1t , where \u1fb1t is a function of t that affects the sampling quality."], "score": 0.9267578125}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 268351323, "title": "Active Generation for Image Classification", "year": 2024, "venue": "European Conference on Computer Vision", "authors": [{"name": "Tao Huang", "authorId": "2265957484"}, {"name": "Jiaqi Liu", "authorId": "2290839014"}, {"name": "Shan You", "authorId": "2111867716"}, {"name": "Chang Xu", "authorId": "2155590441"}], "n_citations": 5}, "snippets": ["Text-to-image diffusion models incorporate additional text conditional variables c in the noise prediction model to predict the conditional noise \u03f5 \u03b8 (x t , c, t) and guide the generation.A classifier-free guidance technique (Ho, 2022) is typically adopted, enabling the utilization of text-conditioned guidance during training without the need for a classifier.The denoising model is trained to handle both conditioned input, where a text prompt is provided, and unconditioned input, where the prompt is replaced with \u2205.This allows for the representation of the guidance direction as the translation from the conditioned input \u03f5 \u03b8 (x t , c, t) to the unconditioned input \u03f5 \u03b8 (x t , \u2205, t).The guidance is performed with a guidance scale s by"], "score": 0.93115234375}, {"id": "(Sueyoshi et al., 2023)", "paper": {"corpus_id": 265466084, "title": "Predicated Diffusion: Predicate Logic-Based Attention Guidance for Text-to-Image Diffusion Models", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Kota Sueyoshi", "authorId": "2268492302"}, {"name": "Takashi Matsubara", "authorId": "2268495650"}], "n_citations": 8}, "snippets": ["Training-Free Guidance Even when a diffusion model is designed without condition c, it can reproduce the conditional probability p(x|c) without retraining. This is because, from a diffusion model p(x) and a separate classifier p(c|x) for class label c, one can obtain the gradient of the conditional log-probability, \u2207 x log p(x|c) = \u2207 x log p(c|x) + \u2207 x log p(x). Although grounded in probability theory, what it practically offers is additional guidance \u2207 x log p(c|x) for updating images, which is generalized as classifier guidance (Dhariwal et al., 2021)", "When the diffusion model is conditioned on c, the difference between conditional and unconditional updates serves as classifier-free guidance, which can adjust the fidelity of the generated image to condition c (Ho, 2022)."], "score": 0.9189453125}], "table": null}, {"title": "Extensions Beyond Conditional Diffusion Models", "tldr": "Classifier-Free Guidance has expanded beyond diffusion models to enhance other generative frameworks including Flow Matching, language models, and one-step generators. These extensions introduce novel guidance mechanisms like contrastive guidance, negative guidance, and hierarchical category guidance that enable more precise control across diverse modalities. (16 sources)", "text": "\nWhile Classifier-Free Guidance (CFG) was originally developed for diffusion models, its utility has expanded significantly to other generative frameworks. Zheng et al. demonstrated that CFG can be integrated into Flow Matching (FM) models, which represent an alternative simulation-free approach using Continuous Normalizing Flows (CNFs) <Paper corpusId=\"265351587\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>. This expansion beyond diffusion models indicates the versatility of the CFG methodology as a general conditioning approach for generative tasks.\n\nRecent research has also introduced variations that extend the basic CFG concept in novel directions. Hierarchical CFG approaches have emerged, where Pan et al. proposed a fine-grained CFG sampling method that incorporates hierarchical category label information <Paper corpusId=\"268041325\" paperTitle=\"(Pan et al., 2024)\" isShortName></Paper>. This approach replaces the unconditional model with a superclass conditional model, allowing the system to leverage both specific category knowledge and broader class relationships, resulting in enhanced control over image generation.\n\nAnother significant extension is negative guidance, where Li et al. demonstrated that CFG can be adapted to selectively \"forget\" specific concepts during image editing while maintaining desired attributes <Paper corpusId=\"270123253\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This implementation combines positive guidance for learning text prompts with negative guidance that allows the model to gradually remove unwanted concepts from the original image <Paper corpusId=\"261276613\" paperTitle=\"(Gandikota et al., 2023)\" isShortName></Paper>. The balance between positive and negative guidance signals can be controlled through weighting parameters, offering fine-grained manipulation of the generation process.\n\nContrastive Guidance represents another innovative extension, where Wu et al. characterized intended factors using two carefully crafted prompts that differ in minimal tokens <Paper corpusId=\"267770589\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. This approach provides continuous, rig-like controls for text-to-image generation and has shown benefits in guiding domain-specific diffusion models and improving zero-shot image editors.\n\nFor one-step diffusion models, Nguyen et al. introduced SNOOPI, which enhances guidance during both training and inference <Paper corpusId=\"274446026\" paperTitle=\"(Nguyen et al., 2024)\" isShortName></Paper>. Their approach includes Proper Guidance-SwiftBrush (PG-SB), which employs random-scale classifier-free guidance, and Negative-Away Steer Attention (NASA), which integrates negative prompts via cross-attention to suppress undesired elements in generated images.\n\nThe concept of spatially varying guidance has also emerged, with Azarian et al. proposing \"segmentation-free guidance\" that dynamically adjusts negative prompts for each image patch based on attention maps <Paper corpusId=\"271051241\" paperTitle=\"(Azarian et al., 2024)\" isShortName></Paper>. This approach enables different parts of an image to receive different guidance without requiring explicit segmentation, resulting in more coherent and controlled generation.\n\nIn large language model (LLM) applications, Zhuang et al. adapted CFG for image token generation by subtracting the probability of unconditional generation from the logits distribution of conditional generation <Paper corpusId=\"275787953\" paperTitle=\"(Zhuang et al., 2025)\" isShortName></Paper>. This implementation uses Gaussian noise features as conditional input to simulate unconditional generation, demonstrating how CFG principles can transfer to different architectures.\n\nCFG has also been extended to handle various conditioning inputs beyond text, enabling models to control synthesized samples with multiple modalities such as edge maps, human pose skeletons, segmentation maps, depth, and normals <Paper corpusId=\"273811150\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper> <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper> <Paper corpusId=\"256827727\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. This multimodal conditioning capability has greatly expanded the application scope of generative models.\n\nRecent work by Kwon et al. has introduced a geometric perspective on enhancing CFG performance by filtering the singular vectors of both conditional and unconditional scores using singular value decomposition <Paper corpusId=\"277271753\" paperTitle=\"(Kwon et al., 2025)\" isShortName></Paper>. This process aligns the unconditional score with the conditional score, refining the sampling trajectory to stay closer to the data manifold.\n\nFor scenarios requiring efficient generation, Ohayon et al. developed Compressed CFG (CCFG), which allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models while controlling the trade-off between generation quality and input fidelity <Paper corpusId=\"276094842\" paperTitle=\"(Ohayon et al., 2025)\" isShortName></Paper>.\n\nAddressing limitations in standard CFG's static unconditional input, Li et al. proposed a dynamic approach that adapts to model uncertainty during iterative generation processes <Paper corpusId=\"278910862\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. This approach recognizes that model uncertainty varies throughout the generation process and adjusts guidance accordingly.\n\nThese extensions demonstrate that the fundamental principles of CFG have proven valuable beyond their original context, influencing a broad spectrum of generative modeling approaches across different architectures and tasks <Paper corpusId=\"277955619\" paperTitle=\"(Ifriqi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276422090\" paperTitle=\"(Zheng et al., 2025)\" isShortName></Paper>. The continued evolution of CFG variants reflects its flexibility as a core technique for enhancing conditional generation quality and control across the generative AI landscape.", "citations": [{"id": "(Zheng et al., 2023)", "paper": {"corpus_id": 265351587, "title": "Guided Flows for Generative Modeling and Decision Making", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Qinqing Zheng", "authorId": "2166847"}, {"name": "Matt Le", "authorId": "2267723599"}, {"name": "Neta Shaul", "authorId": "2219927868"}, {"name": "Y. Lipman", "authorId": "3232072"}, {"name": "Aditya Grover", "authorId": "2267723293"}, {"name": "Ricky T. Q. Chen", "authorId": "2253976277"}], "n_citations": 46}, "snippets": ["Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields."], "score": 0.96533203125}, {"id": "(Pan et al., 2024)", "paper": {"corpus_id": 268041325, "title": "FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10, 000 Classes", "year": 2024, "venue": "Applied intelligence (Boston)", "authors": [{"name": "Ziying Pan", "authorId": "2288865816"}, {"name": "Kun Wang", "authorId": "2288886271"}, {"name": "Gang Li", "authorId": "2243959855"}, {"name": "Feihong He", "authorId": "2242676809"}, {"name": "Xiwang Li", "authorId": "2288032594"}, {"name": "Yongxuan Lai", "authorId": "2289803819"}], "n_citations": 1}, "snippets": ["Classifier-free guidance has supplanted classifier guided sampling to achieve a trade-off between the diversity and quality of sampling. While reducing training complexity, this approach secures high-quality image generation. Currently, classifier-free guidance is widely utilized in various diffusion models. We attempt to introduce hierarchical category label information because knowing which superclass each category belongs to is helpful for capturing intricate details and enhancing the overall quality of the generated images. Therefore, we introduce a fine-grained classifier-free guidance sampling method, yielding improved results under specific conditions. Integrated with TieredEmbedder, this approach leverages superclass conditional information to enhance control over generated images. Conventional classifierfree guidance involves training both conditional and unconditional generative models. In our approach, a superclass conditional generative model is introduced to replace the unconditional model. During training, we use the same neural network to parameterize both the conditional model and the superclass conditional model, leveraging a score estimator \u03f5 \u03b8 for optimization guidance. Subclass labels c are probabilistically replaced with their corresponding superclass labels c p to achieve the learning of superclass embeddings. During sampling, trained superclass and subclass embeddings are input into the DiT network to obtain predictive noise. Our method combines conditional scores estimation \u03f5 \u03b8 (x t | c) and superclass conditional scores estimation \u03f5 \u03b8 (x t | c p ) to guide specific category image generation, with the specific formula provided below: \n\nwhere \u03c9 is the guidance scale, and \u03b5\u03b8 is the modified score. \n\nBy thoroughly exploiting the distributional characteristics of superclasses and subclasses, our sampling method achieves a finer control than classifier-free guidance. We can capture commonalities among subclasses within the same superclass and simultaneously capture distinct distributions between different superclasses, thereby making the learned distributions of each category more closely approximate real-world situations."], "score": 0.958984375}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 270123253, "title": "Text Guided Image Editing with Automatic Concept Locating and Forgetting", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jia Li", "authorId": "2268721096"}, {"name": "Lijie Hu", "authorId": "2153121378"}, {"name": "Zhixian He", "authorId": "2304013931"}, {"name": "Jingfeng Zhang", "authorId": "2253808698"}, {"name": "Tianhang Zheng", "authorId": "2268675026"}, {"name": "Di Wang", "authorId": "2268815109"}], "n_citations": 9}, "snippets": ["Classifier-free diffusion guidance is commonly employed in class-conditional and text-conditional image generation tasks to enhance the visual quality of the generated images and to ensure that the sampled outputs better correspond to their respective conditioning factors.Typically, diffusion models use a guidance signal, where the model is conditioned on the desired concept or attribute to generate samples aligned with that concept (Gandikota et al., 2023).\n\nOur approach employs negative guidance to allow the diffusion model to gradually forget a specified concept in the original image while applying positive guidance for learning text prompts.Combining with Eq 1 and Eq 2, we compute the following score estimate using Classifier-Free Guidance during inference:\n\nwhere c p and c n are input prompt and forgetting concepts, w and \u03b7 are weights for controlling the balance between the positive and negative guidance signals in the final CFG score estimate.A higher value of eta will give more weight to the negative guidance, allowing tuning the level of concept forgetting during the diffusion process."], "score": 0.93017578125}, {"id": "(Gandikota et al., 2023)", "paper": {"corpus_id": 261276613, "title": "Unified Concept Editing in Diffusion Models", "year": 2023, "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "authors": [{"name": "Rohit Gandikota", "authorId": "52017367"}, {"name": "Hadas Orgad", "authorId": "1398583303"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}, {"name": "Joanna Materzy'nska", "authorId": "2235064038"}, {"name": "David Bau", "authorId": "144159726"}], "n_citations": 192}, "snippets": ["Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Unified Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models.We present scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and perform extensive experiments demonstrating improved efficacy and scalability over prior work. Our code is available at unified.baulab.info."], "score": 0.0}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 267770589, "title": "Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "C. Wu", "authorId": "2270764731"}, {"name": "Fernando De la Torre", "authorId": "2239102325"}], "n_citations": 2}, "snippets": ["The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a\"baseline\"that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors."], "score": 0.95654296875}, {"id": "(Nguyen et al., 2024)", "paper": {"corpus_id": 274446026, "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Viet Nguyen", "authorId": "2275329323"}, {"name": "Anh Aengus Nguyen", "authorId": "2333424276"}, {"name": "T. Dao", "authorId": "2276606039"}, {"name": "Khoi Nguyen", "authorId": "2261741144"}, {"name": "Cuong Pham", "authorId": "2269462407"}, {"name": "Toan Tran", "authorId": "2275127531"}, {"name": "Anh Tran", "authorId": "2327046351"}], "n_citations": 2}, "snippets": ["Classifier-free Guidance (CFG) [15] is an inference technique designed to enhance the quality of generated images by blending the predictions from both a conditional and an unconditional model. At each sampling step, CFG adjusts the denoiser's output using a control parameter \u03ba > 1, allowing for controlled guidance that aligns more closely with the desired conditions", "Negative Prompts provide enhanced control by suppressing unwanted features in the generated content. Instead of producing an unconditional output, the model generates an output conditioned on the negative prompt y \u2212 , as follows:", "This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images."], "score": 0.94482421875}, {"id": "(Azarian et al., 2024)", "paper": {"corpus_id": 271051241, "title": "Segmentation-Free Guidance for Text-to-Image Diffusion Models", "year": 2024, "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "authors": [{"name": "K. Azarian", "authorId": "2075053"}, {"name": "Debasmit Das", "authorId": "49950690"}, {"name": "Qiqi Hou", "authorId": "2293594635"}, {"name": "F. Porikli", "authorId": "2253777162"}], "n_citations": 0}, "snippets": ["Our objective is to dynamically adjust the negative prompt for each image patch. We examine attention maps within the diffusion model, specifically where it interacts with the text prompt embeddings. For each patch of the attention map, we aim to find the object in the positive prompt with the highest correlation. Subsequently, this selected object is excluded from the negative prompt interacting with that specific patch. Accordingly, the forward pass of the diffusion model carries out as if each patch cross-attends dynamically with a different negative prompt. Furthermore, the corresponding attention weight is adjusted to account for self-attention interactions. Since this proposed method of guidance does not involve any segmentation network as a guidance function, we term this method as segmentationfree guidance."], "score": 0.97900390625}, {"id": "(Zhuang et al., 2025)", "paper": {"corpus_id": 275787953, "title": "VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xianwei Zhuang", "authorId": "2293439758"}, {"name": "Yuxin Xie", "authorId": "2306871988"}, {"name": "Yufan Deng", "authorId": "2276775533"}, {"name": "Liming Liang", "authorId": "2307892931"}, {"name": "Jinghan Ru", "authorId": "2341529875"}, {"name": "Yuguo Yin", "authorId": "2342470685"}, {"name": "Yuexian Zou", "authorId": "2260859476"}], "n_citations": 11}, "snippets": ["Classifier-free guidance (CFG) significantly enhances the capability of generative diffusion models to produce samples of exceptionally high fidelity. This approach integrates conditional generative models with the distribution estimation of unconditional models trained concurrently, thereby improving the overall quality of generation. Inspired by DALL-E 2 [68], VAR [84] and VAR-CLIP [100], we employ Gaussian noise features as conditional input to simulate unconditional generation. Subsequently, we derive the final distribution of image token outputs by subtracting the probability of uncon- ditional generation from the logits distribution of conditional generation. Specifically, assuming that the image features obtained through LLMs are denoted as H i , we derive the translated visual generation features through a mapping layer as H g = Projector(H i ). Subsequently, we employ the Classifier-Free Guidance (CFG) strategy to obtain the final features of the visual generation tokens, which can be expressed as: \n\nwhere H e represents randomly initialized Gaussian noise, serving as the unconditional feature, and \u03bb denotes the scale hyperparameter for the CFG strategy."], "score": 0.9375}, {"id": "(Luo et al., 2023)", "paper": {"corpus_id": 273811150, "title": "ReactFace: Online Multiple Appropriate Facial Reaction Generation in Dyadic Interactions.", "year": 2023, "venue": "IEEE Transactions on Visualization and Computer Graphics", "authors": [{"name": "Cheng Luo", "authorId": "2153561173"}, {"name": "Siyang Song", "authorId": "2275025172"}, {"name": "Weicheng Xie", "authorId": "34181727"}, {"name": "Micol Spitale", "authorId": "73772115"}, {"name": "Zongyuan Ge", "authorId": "2325909107"}, {"name": "Linlin Shen", "authorId": "2121272943"}, {"name": "Hatice Gunes", "authorId": "2256439823"}], "n_citations": 4}, "snippets": ["Further research has shown that guidance can be derived from the generative model itself without a classifier, a method known as classifier-free guidance. This advancement unlocks various forms of conditional signals, leading to conditional generative models (Zhang et al., 2023), (Rombach et al., 2021) that control synthesized samples with multiple modalities, such as text, edge maps, human pose skeletons, segmentation maps, depth, and normals."], "score": 0.92578125}, {"id": "(Rombach et al., 2021)", "paper": {"corpus_id": 245335280, "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Robin Rombach", "authorId": "1660819540"}, {"name": "A. Blattmann", "authorId": "119843260"}, {"name": "Dominik Lorenz", "authorId": "2053482699"}, {"name": "Patrick Esser", "authorId": "35175531"}, {"name": "B. Ommer", "authorId": "1796707"}], "n_citations": 15768}, "snippets": ["By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."], "score": 0.0}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 256827727, "title": "Adding Conditional Control to Text-to-Image Diffusion Models", "year": 2023, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Lvmin Zhang", "authorId": "17744884"}, {"name": "Anyi Rao", "authorId": "36290866"}, {"name": "Maneesh Agrawala", "authorId": "1820412"}], "n_citations": 4175}, "snippets": ["We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models."], "score": 0.0}, {"id": "(Kwon et al., 2025)", "paper": {"corpus_id": 277271753, "title": "TCFG: Tangential Damping Classifier-free Guidance", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mingi Kwon", "authorId": "2182293854"}, {"name": "Shin seong Kim", "authorId": "2352073121"}, {"name": "Jaeseok Jeong. Yi Ting Hsiao", "authorId": "2351806103"}, {"name": "Youngjung Uh", "authorId": "2253393666"}], "n_citations": 0}, "snippets": ["In this work, we introduce a novel approach that leverages a geometric perspective on the unconditional score to enhance CFG performance when conditional scores are available. Specifically, we propose a method that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process aligns the unconditional score with the conditional score, thereby refining the sampling trajectory to stay closer to the manifold."], "score": 0.9677734375}, {"id": "(Ohayon et al., 2025)", "paper": {"corpus_id": 276094842, "title": "Compressed Image Generation with Denoising Diffusion Codebook Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Guy Ohayon", "authorId": "51228065"}, {"name": "Hila Manor", "authorId": "2245392181"}, {"name": "T. Michaeli", "authorId": "1880407"}, {"name": "Michael Elad", "authorId": "2266840963"}], "n_citations": 0}, "snippets": ["Here, we introduce a new CFG method that allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models, while controlling the tradeoff between generation quality and the fidelity to the inputs", "We coin our method Compressed CFG (CCFG)."], "score": 0.94775390625}, {"id": "(Li et al., 2025)", "paper": {"corpus_id": 278910862, "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking", "year": 2025, "venue": "", "authors": [{"name": "Pengxiang Li", "authorId": "2363590342"}, {"name": "Shilin Yan", "authorId": "2362879323"}, {"name": "Joey Tsai", "authorId": "2362728158"}, {"name": "Renrui Zhang", "authorId": "2291314199"}, {"name": "Ruichuan An", "authorId": "2363570661"}, {"name": "Ziyu Guo", "authorId": "145490494"}, {"name": "Xiaowei Gao", "authorId": "2303650253"}], "n_citations": 1}, "snippets": ["Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically."], "score": 0.9267578125}, {"id": "(Ifriqi et al., 2025)", "paper": {"corpus_id": 277955619, "title": "Entropy Rectifying Guidance for Diffusion and Flow Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tariq Berrada Ifriqi", "authorId": "2329186126"}, {"name": "Adriana Romero-Soriano", "authorId": "1456285042"}, {"name": "M. Drozdzal", "authorId": "3325894"}, {"name": "Jakob Verbeek", "authorId": "2281637540"}, {"name": "Alahari Karteek", "authorId": "72492981"}], "n_citations": 0}, "snippets": ["Guidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others."], "score": 0.93359375}, {"id": "(Zheng et al., 2025)", "paper": {"corpus_id": 276422090, "title": "RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Chenxi Zheng", "authorId": "2220800052"}, {"name": "Yihong Lin", "authorId": "2294183727"}, {"name": "Bangzhen Liu", "authorId": "2220596660"}, {"name": "Xuemiao Xu", "authorId": "2281155649"}, {"name": "Yongwei Nie", "authorId": "2273558537"}, {"name": "Shengfeng He", "authorId": "2257314718"}], "n_citations": 3}, "snippets": ["A major challenge in text-to-image generation using diffusion models (Yu et al., 2024)(Liu et al., 2024)(Xu et al., 2024) is guiding the generative process to reflect the input text accurately. A widely adopted solution is classifier-free guidance (CFG, Ho & Salimans (2022)), which eliminates the need for external classifiers by training a unified model for both unconditional and conditional image generation. During inference, CFG achieves conditional generation by interpolating between the conditional and unconditional scores, effectively guiding the model to match the input text. This method has shown significant success in various text-to-image tasks (Balaji et al., 2022;Nichol et al., 2021;Ramesh et al., 2022)."], "score": 0.93359375}], "table": null}, {"title": "Applications in NLP Tasks", "tldr": "Classifier-Free Guidance has been successfully extended from image generation to various NLP tasks, enabling more flexible conditioning for text generation, language modeling, and multimodal applications. These implementations leverage the same core principles while adapting to the unique requirements of language processing tasks. (9 sources)", "text": "\nWhile Classifier-Free Guidance (CFG) was initially developed for image generation, its principles have been effectively adapted to Natural Language Processing (NLP) tasks, demonstrating its versatility across different modalities. In text-to-image generation systems like Parti, CFG has been implemented by randomly replacing text prompts with padded tokens during training, which significantly improves image-text alignment, especially for challenging text prompts <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>.\n\nThe adaptation of CFG to NLP tasks stems from its ability to handle more versatile conditioning signals beyond simple class labels. Unlike earlier approaches that relied on external classifiers for guidance <Paper corpusId=\"235619773\" paperTitle=\"(Ho et al., 2021)\" isShortName></Paper> <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>, CFG's architecture allows for more flexible conditioning with text and other modalities <Paper corpusId=\"268033671\" paperTitle=\"(Huang et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>. This flexibility has proven particularly valuable in multimodal applications where language and visual elements intersect.\n\nRecent advances in text-to-image diffusion models have faced the challenge of accurately reflecting input text in generated images. CFG has emerged as a widely adopted solution for guiding the generative process to better match text inputs, showing significant success across various text-to-image tasks <Paper corpusId=\"276422090\" paperTitle=\"(Zheng et al., 2025)\" isShortName></Paper>. By interpolating between conditional and unconditional scores during inference, these models achieve better alignment between generated content and textual descriptions.\n\nThe expansion of CFG to handle multiple modalities has enabled conditional generative models to control synthesized content through various input forms, including text, edge maps, human pose skeletons, segmentation maps, depth information, and normal maps <Paper corpusId=\"273811150\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper> <Paper corpusId=\"256827727\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper>. This multimodal conditioning capability has created new possibilities for integrating language guidance with other forms of structural and semantic information.\n\nIn autoregressive text generation models, CFG has been applied by combining logits from conditional and unconditional models during inference, similar to its implementation in diffusion models <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>. This approach allows language models to benefit from the same quality-diversity trade-off mechanisms that have proven effective in image generation.\n\nThe integration of CFG into NLP frameworks demonstrates that its fundamental principles\u2014combining conditional and unconditional generation paths to enhance alignment with desired conditions\u2014transcend specific model architectures or data modalities. By adapting the core CFG methodology to language-specific representations and generation processes, researchers have extended its benefits to a wide range of text generation and multimodal language tasks.", "citations": [{"id": "(Yu et al., 2022)", "paper": {"corpus_id": 249926846, "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation", "year": 2022, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Jiahui Yu", "authorId": "2338016295"}, {"name": "Yuanzhong Xu", "authorId": "2145139570"}, {"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "Thang Luong", "authorId": "1821711"}, {"name": "Gunjan Baid", "authorId": "1396954703"}, {"name": "Zirui Wang", "authorId": "2331539"}, {"name": "Vijay Vasudevan", "authorId": "2053781980"}, {"name": "Alexander Ku", "authorId": "31702389"}, {"name": "Yinfei Yang", "authorId": "2118771180"}, {"name": "Burcu Karagol Ayan", "authorId": "143990191"}, {"name": "Ben Hutchinson", "authorId": "2044655623"}, {"name": "Wei Han", "authorId": "143911112"}, {"name": "Zarana Parekh", "authorId": "27456119"}, {"name": "Xin Li", "authorId": "2158973314"}, {"name": "Han Zhang", "authorId": null}, {"name": "Jason Baldridge", "authorId": "1387994164"}, {"name": "Yonghui Wu", "authorId": "48607963"}], "n_citations": 1133}, "snippets": ["Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability", "Classifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."], "score": 0.95263671875}, {"id": "(Ho et al., 2021)", "paper": {"corpus_id": 235619773, "title": "Cascaded Diffusion Models for High Fidelity Image Generation", "year": 2021, "venue": "Journal of machine learning research", "authors": [{"name": "Jonathan Ho", "authorId": "2126278"}, {"name": "Chitwan Saharia", "authorId": "2314850972"}, {"name": "William Chan", "authorId": "144333684"}, {"name": "David J. Fleet", "authorId": "1793739"}, {"name": "Mohammad Norouzi", "authorId": "144739074"}, {"name": "Tim Salimans", "authorId": "2887364"}], "n_citations": 1235}, "snippets": ["We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256, outperforming VQ-VAE-2."], "score": 0.0}, {"id": "(Dhariwal et al., 2021)", "paper": {"corpus_id": 234357997, "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Alex Nichol", "authorId": "38967461"}], "n_citations": 7951}, "snippets": ["We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"], "score": 0.0}, {"id": "(Huang et al._1, 2024)", "paper": {"corpus_id": 268033671, "title": "Diffusion Model-Based Image Editing: A Survey", "year": 2024, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Yi Huang", "authorId": "2249841594"}, {"name": "Jiancheng Huang", "authorId": "2194958029"}, {"name": "Yifan Liu", "authorId": "2247959941"}, {"name": "Mingfu Yan", "authorId": "2267694301"}, {"name": "Jiaxi Lv", "authorId": "2154657214"}, {"name": "Jianzhuang Liu", "authorId": "2267504760"}, {"name": "Wei Xiong", "authorId": "2273646978"}, {"name": "He Zhang", "authorId": "2274091009"}, {"name": "Shifeng Chen", "authorId": "2247480051"}, {"name": "Liangliang Cao", "authorId": "2288206453"}], "n_citations": 102}, "snippets": ["Early efforts (Ho et al., 2021), (Dhariwal et al., 2021), (Chao et al., 2022)- (246442182) usually incorporate the class-induced gradients via an additional pretrained classifier during sampling. However, Ho et al. (Ho, 2022) introduce the classifier-free guidance, which does not rely on an external classifier and allows for more versatile conditions, e.g., text, as guidance."], "score": 0.95068359375}, {"id": "(Ho, 2022)", "paper": {"corpus_id": 249145348, "title": "Classifier-Free Diffusion Guidance", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Jonathan Ho", "authorId": "2126278"}], "n_citations": 3970}, "snippets": ["We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."], "score": 0.9599609375}, {"id": "(Zheng et al., 2025)", "paper": {"corpus_id": 276422090, "title": "RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Chenxi Zheng", "authorId": "2220800052"}, {"name": "Yihong Lin", "authorId": "2294183727"}, {"name": "Bangzhen Liu", "authorId": "2220596660"}, {"name": "Xuemiao Xu", "authorId": "2281155649"}, {"name": "Yongwei Nie", "authorId": "2273558537"}, {"name": "Shengfeng He", "authorId": "2257314718"}], "n_citations": 3}, "snippets": ["A major challenge in text-to-image generation using diffusion models (Yu et al., 2024)(Liu et al., 2024)(Xu et al., 2024) is guiding the generative process to reflect the input text accurately. A widely adopted solution is classifier-free guidance (CFG, Ho & Salimans (2022)), which eliminates the need for external classifiers by training a unified model for both unconditional and conditional image generation. During inference, CFG achieves conditional generation by interpolating between the conditional and unconditional scores, effectively guiding the model to match the input text. This method has shown significant success in various text-to-image tasks (Balaji et al., 2022;Nichol et al., 2021;Ramesh et al., 2022)."], "score": 0.93359375}, {"id": "(Luo et al., 2023)", "paper": {"corpus_id": 273811150, "title": "ReactFace: Online Multiple Appropriate Facial Reaction Generation in Dyadic Interactions.", "year": 2023, "venue": "IEEE Transactions on Visualization and Computer Graphics", "authors": [{"name": "Cheng Luo", "authorId": "2153561173"}, {"name": "Siyang Song", "authorId": "2275025172"}, {"name": "Weicheng Xie", "authorId": "34181727"}, {"name": "Micol Spitale", "authorId": "73772115"}, {"name": "Zongyuan Ge", "authorId": "2325909107"}, {"name": "Linlin Shen", "authorId": "2121272943"}, {"name": "Hatice Gunes", "authorId": "2256439823"}], "n_citations": 4}, "snippets": ["Further research has shown that guidance can be derived from the generative model itself without a classifier, a method known as classifier-free guidance. This advancement unlocks various forms of conditional signals, leading to conditional generative models (Zhang et al., 2023), (Rombach et al., 2021) that control synthesized samples with multiple modalities, such as text, edge maps, human pose skeletons, segmentation maps, depth, and normals."], "score": 0.92578125}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 256827727, "title": "Adding Conditional Control to Text-to-Image Diffusion Models", "year": 2023, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Lvmin Zhang", "authorId": "17744884"}, {"name": "Anyi Rao", "authorId": "36290866"}, {"name": "Maneesh Agrawala", "authorId": "1820412"}], "n_citations": 4175}, "snippets": ["We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models."], "score": 0.0}, {"id": "(Rombach et al., 2021)", "paper": {"corpus_id": 245335280, "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Robin Rombach", "authorId": "1660819540"}, {"name": "A. Blattmann", "authorId": "119843260"}, {"name": "Dominik Lorenz", "authorId": "2053482699"}, {"name": "Patrick Esser", "authorId": "35175531"}, {"name": "B. Ommer", "authorId": "1796707"}], "n_citations": 15768}, "snippets": ["By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."], "score": 0.0}], "table": null}, {"title": "Recent Innovations and Advanced Techniques", "tldr": "Recent innovations in Classifier-Free Guidance have expanded its capabilities through techniques like Compressed CFG, geometric filtering of singular vectors, and integration with negative prompts. These advancements address specific limitations in the original approach while enhancing control and quality across diverse applications. (10 sources)", "text": "\n- **Compressed Classifier-Free Guidance (CCFG)**: Ohayon et al. introduced Compressed CFG, a novel approach that enables generating compressed conditional samples using any pair of conditional and unconditional diffusion models. This technique allows precise control over the trade-off between generation quality and fidelity to inputs. <Paper corpusId=\"276094842\" paperTitle=\"(Ohayon et al., 2025)\" isShortName></Paper>\n\n- **Geometric Filtering**: Kwon et al. developed a geometric enhancement for CFG that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process better aligns the unconditional score with the conditional score, refining the sampling trajectory to stay closer to the data manifold and improving overall CFG performance. <Paper corpusId=\"277271753\" paperTitle=\"(Kwon et al., 2025)\" isShortName></Paper>\n\n- **SNOOPI Framework**: For one-step diffusion models, Nguyen et al. created SNOOPI, which enhances guidance during both training and inference. It implements Proper Guidance-SwiftBrush (PG-SB), using random-scale classifier-free guidance to improve training stability, and Negative-Away Steer Attention (NASA), which integrates negative prompts via cross-attention to suppress unwanted elements in generated images. <Paper corpusId=\"274446026\" paperTitle=\"(Nguyen et al., 2024)\" isShortName></Paper>\n\n- **Guidance-Based Diffusion**: Viola et al. highlighted how guidance-based diffusion incorporates external supervision alongside original conditioning, using a guidance function that measures whether certain criteria are met. This approach enables fine-grained control over outputs in various applications. <Paper corpusId=\"274823034\" paperTitle=\"(Viola et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>\n\n- **Extension to Inverse Problems**: The CFG approach has been extended to handle general noisy linear and non-linear inverse problems through approximation of posterior sampling. This creates a blended version of diffusion sampling with manifold constrained gradient, yielding more desirable generative paths in noisy settings. <Paper corpusId=\"274823034\" paperTitle=\"(Viola et al., 2024)\" isShortName></Paper> <Paper corpusId=\"252596252\" paperTitle=\"(Chung et al., 2022)\" isShortName></Paper>\n\n- **Unified Network Training**: Advancements in the training methodology now allow conditional diffusion models (\u03b5_\u03b8(x_t|y)) and unconditional models (\u03b5_\u03b8(x_t|y=0)) to be trained as a single neural network. This streamlined approach offers advantages over earlier methods by training a single model to guide the diffusion process and accommodating different types of conditional data such as text embeddings. <Paper corpusId=\"276725462\" paperTitle=\"(Sordo et al., 2025)\" isShortName></Paper> <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>\n\n- **Image-Conditional Diffusion Models**: Building on Ho et al.'s classifier-free diffusion guidance, Saharia et al. developed image-conditional diffusion models for super-resolution and image-to-image translation tasks. This work demonstrated how the guidance scale hyperparameter could be interpreted within the classifier-free diffusion model framework. <Paper corpusId=\"258714952\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"243938678\" paperTitle=\"(Saharia et al., 2021)\" isShortName></Paper>\n\n- **Multi-Modal Adaptation**: Recent innovations have enabled CFG to better handle multi-modal data, making it particularly effective when training on diverse data types. This has expanded the applicability of CFG beyond its original context to more complex generative tasks. <Paper corpusId=\"276725462\" paperTitle=\"(Sordo et al., 2025)\" isShortName></Paper>", "citations": [{"id": "(Ohayon et al., 2025)", "paper": {"corpus_id": 276094842, "title": "Compressed Image Generation with Denoising Diffusion Codebook Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Guy Ohayon", "authorId": "51228065"}, {"name": "Hila Manor", "authorId": "2245392181"}, {"name": "T. Michaeli", "authorId": "1880407"}, {"name": "Michael Elad", "authorId": "2266840963"}], "n_citations": 0}, "snippets": ["Here, we introduce a new CFG method that allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models, while controlling the tradeoff between generation quality and the fidelity to the inputs", "We coin our method Compressed CFG (CCFG)."], "score": 0.94775390625}, {"id": "(Kwon et al., 2025)", "paper": {"corpus_id": 277271753, "title": "TCFG: Tangential Damping Classifier-free Guidance", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mingi Kwon", "authorId": "2182293854"}, {"name": "Shin seong Kim", "authorId": "2352073121"}, {"name": "Jaeseok Jeong. Yi Ting Hsiao", "authorId": "2351806103"}, {"name": "Youngjung Uh", "authorId": "2253393666"}], "n_citations": 0}, "snippets": ["In this work, we introduce a novel approach that leverages a geometric perspective on the unconditional score to enhance CFG performance when conditional scores are available. Specifically, we propose a method that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process aligns the unconditional score with the conditional score, thereby refining the sampling trajectory to stay closer to the manifold."], "score": 0.9677734375}, {"id": "(Nguyen et al., 2024)", "paper": {"corpus_id": 274446026, "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Viet Nguyen", "authorId": "2275329323"}, {"name": "Anh Aengus Nguyen", "authorId": "2333424276"}, {"name": "T. Dao", "authorId": "2276606039"}, {"name": "Khoi Nguyen", "authorId": "2261741144"}, {"name": "Cuong Pham", "authorId": "2269462407"}, {"name": "Toan Tran", "authorId": "2275127531"}, {"name": "Anh Tran", "authorId": "2327046351"}], "n_citations": 2}, "snippets": ["Classifier-free Guidance (CFG) [15] is an inference technique designed to enhance the quality of generated images by blending the predictions from both a conditional and an unconditional model. At each sampling step, CFG adjusts the denoiser's output using a control parameter \u03ba > 1, allowing for controlled guidance that aligns more closely with the desired conditions", "Negative Prompts provide enhanced control by suppressing unwanted features in the generated content. Instead of producing an unconditional output, the model generates an output conditioned on the negative prompt y \u2212 , as follows:", "This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images."], "score": 0.94482421875}, {"id": "(Viola et al., 2024)", "paper": {"corpus_id": 274823034, "title": "Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Massimiliano Viola", "authorId": "2335870109"}, {"name": "Kevin Qu", "authorId": "2335870504"}, {"name": "Nando Metzger", "authorId": "2031912818"}, {"name": "Bingxin Ke", "authorId": "34926212"}, {"name": "Alexander Becker", "authorId": "2078701909"}, {"name": "Konrad Schindler", "authorId": "2243003715"}, {"name": "Anton Obukhov", "authorId": "4366091"}], "n_citations": 6}, "snippets": ["To allow fine-grained control over the output, guidance-based diffusion [13] incorporates external supervision alongside the original conditioning, using a guidance function that measures whether certain criteria are met. In guided image generation, classifier guidance [14] enables class-conditional outputs from a pretrained, unconditional diffusion model, via gradients from a classifier trained on ImageNet [58] im-ages at different noise scales. Similarly, gradients from a CLIP model [51] trained on noisy images can guide generation toward a user-defined text caption [47]. An alternative, classifier-free guidance [26,47], achieves similar control without training a separate classifier, by parameterizing both conditional and unconditional diffusion models within the same network. The approach is further extended to handle general nonlinear inverse problems [9], using gradients calculated on the expected denoised images."], "score": 0.93115234375}, {"id": "(Ho, 2022)", "paper": {"corpus_id": 249145348, "title": "Classifier-Free Diffusion Guidance", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Jonathan Ho", "authorId": "2126278"}], "n_citations": 3970}, "snippets": ["We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."], "score": 0.9599609375}, {"id": "(Chung et al., 2022)", "paper": {"corpus_id": 252596252, "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Hyungjin Chung", "authorId": "2110872233"}, {"name": "Jeongsol Kim", "authorId": "2109216792"}, {"name": "Michael T. McCann", "authorId": "2179304"}, {"name": "M. Klasky", "authorId": "83695651"}, {"name": "J. C. Ye", "authorId": "2998762"}], "n_citations": 861}, "snippets": ["Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring. Code available at https://github.com/DPS2022/diffusion-posterior-sampling"], "score": 0.0}, {"id": "(Sordo et al., 2025)", "paper": {"corpus_id": 276725462, "title": "A Review on Generative AI For Text-To-Image and Image-To-Image Generation and Implications To Scientific Images", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zineb Sordo", "authorId": "2267907323"}, {"name": "Eric Chagnon", "authorId": "2271183137"}, {"name": "Daniela Ushizima", "authorId": "2276224795"}], "n_citations": 1}, "snippets": ["Classifier-free guidance, proposed by Ho et al. [12], allows for enhanced control in diffusion models by eliminating the need for separate classifiers. Instead of relying on a separate classifier, which increases training complexity and introduces bias potential, classifier-free guidance trains the diffusion model to directly learn and combine conditional and unconditional distributions during inference, streamlining the process. In other words, the authors train a conditional diffusion model \u03f5 \u03b8 (x t |y) and an unconditional model \u03f5 \u03b8 (x t |y = 0) as a single neural network as follows:\n\nThis approach is advantageous compared to the previous one as it trains a single model to guide the diffusion process and can take different types of conditional data such as text embeddings. We will see that many models rely on classifier free-guidance especially when training on multi-modal data."], "score": 0.9560546875}, {"id": "(Dhariwal et al., 2021)", "paper": {"corpus_id": 234357997, "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Alex Nichol", "authorId": "38967461"}], "n_citations": 7951}, "snippets": ["We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"], "score": 0.0}, {"id": "(Zhu et al., 2023)", "paper": {"corpus_id": 258714952, "title": "Denoising Diffusion Models for Plug-and-Play Image Restoration", "year": 2023, "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "authors": [{"name": "Yuanzhi Zhu", "authorId": null}, {"name": "K. Zhang", "authorId": "144110274"}, {"name": "Jingyun Liang", "authorId": "145270228"}, {"name": "Jiezhang Cao", "authorId": "32879676"}, {"name": "B. Wen", "authorId": "1766554"}, {"name": "R. Timofte", "authorId": "1732855"}, {"name": "L. Gool", "authorId": "1681236"}], "n_citations": 219}, "snippets": ["Ho et al. [25] introduced the classifier-free diffusion guidance with s \u03b8 (x, t, y) = \u2207 x log p t (x|y) the imageconditional diffusion models. With the same idea, Saharia et al. (Saharia et al., 2021)[49] trained image-conditional diffusion models for SR and image-to-image translation in concurrent work. Nichol et al. [40] proposed to use text-guided diffusion models to generate photo-realistic images with classifierfree guidance. The hyperparameter \u03bb in (1) can be interpreted as the guidance scale in classifier-free diffusion models."], "score": 0.9228515625}, {"id": "(Saharia et al., 2021)", "paper": {"corpus_id": 243938678, "title": "Palette: Image-to-Image Diffusion Models", "year": 2021, "venue": "International Conference on Computer Graphics and Interactive Techniques", "authors": [{"name": "Chitwan Saharia", "authorId": "2139922989"}, {"name": "William Chan", "authorId": "2150198218"}, {"name": "Huiwen Chang", "authorId": "2140690517"}, {"name": "Chris A. Lee", "authorId": "2143770178"}, {"name": "Jonathan Ho", "authorId": "2112615253"}, {"name": "Tim Salimans", "authorId": "2117261512"}, {"name": "David J. Fleet", "authorId": "1793739"}, {"name": "Mohammad Norouzi", "authorId": "2138892698"}], "n_citations": 1647}, "snippets": ["This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io/ for an overview of the results and code."], "score": 0.0}], "table": null}, {"title": "Limitations and Future Directions", "tldr": "Despite its widespread adoption, Classifier-Free Guidance faces several limitations including quality-diversity trade-offs, computational inefficiencies, and challenges with complex multi-modal conditioning. Future research directions focus on dynamic guidance mechanisms, more efficient implementations, and better theoretical frameworks to overcome these limitations. (5 sources)", "text": "\nWhile Classifier-Free Guidance (CFG) has become a fundamental technique in conditional generative models, it presents several important limitations that current and future research aims to address. One significant limitation is the inherent trade-off between sample quality, diversity, and consistency with conditioning. As noted by Ifriqi et al., standard CFG often improves some of these aspects at the expense of others <Paper corpusId=\"277955619\" paperTitle=\"(Ifriqi et al., 2025)\" isShortName></Paper>. This trade-off remains a fundamental challenge that requires more sophisticated solutions beyond simple guidance scaling.\n\nThe static nature of conventional CFG implementation poses another limitation. Li et al. highlight that standard CFG employs a fixed unconditional input throughout the generation process, which can be suboptimal for iterative generation where model uncertainty varies dynamically <Paper corpusId=\"278910862\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. This static approach fails to adapt to the changing requirements at different stages of the generation process, potentially limiting the quality of the final output.\n\nWhen extending CFG to complex multi-modal conditioning scenarios, additional challenges emerge. Kant et al. observed that applying classifier-free guidance beyond text conditioning can lead to undesirable artifacts such as over-saturated generations <Paper corpusId=\"267547881\" paperTitle=\"(Kant et al., 2024)\" isShortName></Paper>. This suggests that while CFG works well with single modality conditioning, its effectiveness diminishes with multiple conditioning types, indicating a need for more sophisticated multi-modal guidance techniques.\n\nThe computational overhead of CFG also presents a practical limitation. The need to compute both conditional and unconditional predictions at each step increases inference time and resource requirements. This becomes particularly problematic for real-time applications or deployment on resource-constrained devices <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nCurrent research is exploring several promising directions to address these limitations. Sadat et al. demonstrated theoretically that CFG-like behavior can be achieved without additional training of an unconditional model by using a conditioning vector independent of the input data <Paper corpusId=\"270923987\" paperTitle=\"(Sadat et al., 2024)\" isShortName></Paper>. This approach could potentially reduce the computational overhead of CFG while maintaining its benefits.\n\nAnother promising direction involves dynamic guidance mechanisms that adapt to the model's uncertainty throughout the generation process. Li et al. proposed approaches that modify the guidance strength based on the model's confidence at different stages <Paper corpusId=\"278910862\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. Such adaptive techniques could potentially optimize the quality-diversity trade-off more effectively than static approaches.\n\nThe integration of negative guidance with positive guidance represents another avenue for enhancing CFG's capabilities. Li et al. demonstrated that combining positive guidance for learning text prompts with negative guidance allows diffusion models to selectively \"forget\" specified concepts while preserving desired attributes <Paper corpusId=\"270123253\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This balanced approach, controlled through weighting parameters, enables more nuanced control over the generation process.\n\nFuture research will likely focus on developing unified theoretical frameworks that better explain and predict the effects of different guidance techniques. Additionally, more efficient implementations of CFG that reduce computational requirements without sacrificing effectiveness will be crucial for broader adoption. As generative models continue to evolve, we can expect more sophisticated guidance mechanisms that dynamically balance multiple objectives and better handle complex multi-modal conditioning scenarios <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Ifriqi et al., 2025)", "paper": {"corpus_id": 277955619, "title": "Entropy Rectifying Guidance for Diffusion and Flow Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tariq Berrada Ifriqi", "authorId": "2329186126"}, {"name": "Adriana Romero-Soriano", "authorId": "1456285042"}, {"name": "M. Drozdzal", "authorId": "3325894"}, {"name": "Jakob Verbeek", "authorId": "2281637540"}, {"name": "Alahari Karteek", "authorId": "72492981"}], "n_citations": 0}, "snippets": ["Guidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others."], "score": 0.93359375}, {"id": "(Li et al., 2025)", "paper": {"corpus_id": 278910862, "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking", "year": 2025, "venue": "", "authors": [{"name": "Pengxiang Li", "authorId": "2363590342"}, {"name": "Shilin Yan", "authorId": "2362879323"}, {"name": "Joey Tsai", "authorId": "2362728158"}, {"name": "Renrui Zhang", "authorId": "2291314199"}, {"name": "Ruichuan An", "authorId": "2363570661"}, {"name": "Ziyu Guo", "authorId": "145490494"}, {"name": "Xiaowei Gao", "authorId": "2303650253"}], "n_citations": 1}, "snippets": ["Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically."], "score": 0.9267578125}, {"id": "(Kant et al., 2024)", "paper": {"corpus_id": 267547881, "title": "SPAD: Spatially Aware Multi-View Diffusers", "year": 2024, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Yash Kant", "authorId": "66536530"}, {"name": "Ziyi Wu", "authorId": "2253894469"}, {"name": "Michael Vasilkovsky", "authorId": "2261673978"}, {"name": "Guocheng Qian", "authorId": "2279023722"}, {"name": "Jian Ren", "authorId": "2258296012"}, {"name": "R. A. Guler", "authorId": "134642679"}, {"name": "Bernard Ghanem", "authorId": "2279742224"}, {"name": "S. Tulyakov", "authorId": "145582202"}, {"name": "Igor Gilitschenski", "authorId": "2262216913"}, {"name": "Aliaksandr Siarohin", "authorId": "10753214"}], "n_citations": 38}, "snippets": ["Classifier-free diffusion guidance (Ho, 2022) is a technique used to balance the quality and diversity of images produced by diffusion models. This method is particularly effective in class-conditional and text-conditional image generation, enhancing both the visual quality of images and their alignment with given conditions. Inspired by (Brooks et al., 2022) we explore the integration of classifier-free guidance with Epipolar Attention and Pl\u00fccker Embedding. Implementing classifierfree guidance involves simultaneous training of the diffusion model for both conditional and unconditional denoising tasks. During inference, these models' score estimates are merged. We have four different types of conditioning injected into our system: Outcome: As shown in Fig. 13, we find that classifier-free guidance beyond text conditioning does not provide additional benefits, and rather leads to over-saturated generations. This also aligns with our observations on MVDream."], "score": 0.9384765625}, {"id": "(Sadat et al., 2024)", "paper": {"corpus_id": 270923987, "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Seyedmorteza Sadat", "authorId": "2261742393"}, {"name": "Manuel Kansy", "authorId": "2204861903"}, {"name": "Otmar Hilliges", "authorId": "1466533438"}, {"name": "Romann M. Weber", "authorId": "145848224"}], "n_citations": 14}, "snippets": ["In this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model. The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score."], "score": 0.93798828125}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 270123253, "title": "Text Guided Image Editing with Automatic Concept Locating and Forgetting", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jia Li", "authorId": "2268721096"}, {"name": "Lijie Hu", "authorId": "2153121378"}, {"name": "Zhixian He", "authorId": "2304013931"}, {"name": "Jingfeng Zhang", "authorId": "2253808698"}, {"name": "Tianhang Zheng", "authorId": "2268675026"}, {"name": "Di Wang", "authorId": "2268815109"}], "n_citations": 9}, "snippets": ["Classifier-free diffusion guidance is commonly employed in class-conditional and text-conditional image generation tasks to enhance the visual quality of the generated images and to ensure that the sampled outputs better correspond to their respective conditioning factors.Typically, diffusion models use a guidance signal, where the model is conditioned on the desired concept or attribute to generate samples aligned with that concept (Gandikota et al., 2023).\n\nOur approach employs negative guidance to allow the diffusion model to gradually forget a specified concept in the original image while applying positive guidance for learning text prompts.Combining with Eq 1 and Eq 2, we compute the following score estimate using Classifier-Free Guidance during inference:\n\nwhere c p and c n are input prompt and forgetting concepts, w and \u03b7 are weights for controlling the balance between the positive and negative guidance signals in the final CFG score estimate.A higher value of eta will give more weight to the negative guidance, allowing tuning the level of concept forgetting during the diffusion process."], "score": 0.93017578125}], "table": null}], "cost": 0.57288}}
