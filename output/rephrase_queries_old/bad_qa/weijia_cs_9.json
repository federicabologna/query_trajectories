{"original_query": "How should we analyze the training dynamics of language models?", "input": "analyze training dynamics of language models", "output": {"sections": [{"title": "Introduction: What are Training Dynamics?", "tldr": "Training dynamics refer to the complex patterns of how language models learn and evolve during the training process. Understanding these dynamics reveals how models acquire knowledge progressively, develop capabilities, and potentially encounter challenges like memorization or forgetting. (LLM Memory)", "text": "\nTraining dynamics encompass the entire spectrum of behaviors, patterns, and phenomena that occur as language models learn from data over time. At its core, training dynamics analyze how neural networks evolve through their learning trajectory - from random initialization to a fully trained model capable of sophisticated language understanding and generation. This includes examining how loss functions decrease, how representations form within the network, and how performance on various tasks develops at different stages of training.\n\nThe study of training dynamics reveals critical insights about the learning process itself. For example, language models don't learn all skills simultaneously but instead develop capabilities in a systematic progression - often mastering simple syntactic patterns before complex semantic relationships. This progressive acquisition of knowledge follows distinct phases where different types of information are internalized at varying rates.\n\nAnalyzing training dynamics also illuminates important practical concerns such as optimization challenges, the emergence of capabilities at specific training thresholds, and potential issues like catastrophic forgetting or overfitting. Researchers track metrics like gradient norms, attention patterns, and activation distributions to gain visibility into these internal processes. Such analysis can help determine optimal training schedules, architecture choices, and data requirements.\n\nFor large language models specifically, training dynamics are particularly complex due to the billions of parameters and massive datasets involved. The scale introduces emergent behaviors where certain capabilities appear only after specific training thresholds are crossed. Understanding these dynamics helps explain why some models exhibit surprising capabilities that weren't explicitly programmed but emerge organically through the training process. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Stages and Phases of Learning", "tldr": "Language models follow consistent developmental trajectories with distinct learning phases, progressing from simple pattern recognition to complex linguistic understanding. These phases include early acquisition of syntactic knowledge, development of intermediate capabilities like attention mechanisms, and later refinement of semantic and world knowledge. (16 sources)", "text": "\nResearch has consistently demonstrated that language models progress through distinct stages during training, with certain linguistic capabilities emerging in a remarkably consistent order. Studies have found that language models learn grammatical phenomena in a predictable sequence regardless of architecture, initialization, or data shuffling <Paper corpusId=\"237491997\" paperTitle=\"(Choshen et al., 2021)\" isShortName></Paper> <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> . This consistency suggests that training dynamics follow fundamental learning principles rather than random pathways.\n\nThe early stages of learning are characterized by the acquisition of basic syntactic patterns. Models initially rely on local cues rather than word order, resembling bag-of-words models before gradually developing sensitivity to structural relationships <Paper corpusId=\"237491997\" paperTitle=\"(Choshen et al., 2021)\" isShortName></Paper>. Research on both Transformer and LSTM architectures shows that local syntactic information, such as parts of speech, is learned earlier than information encoding long-distance dependencies <Paper corpusId=\"247656607\" paperTitle=\"(Teehan et al., 2022)\" isShortName></Paper>. Function and content words also show different learning patterns, with finer distinctions appearing for specific parts of speech and verb forms <Paper corpusId=\"247656607\" paperTitle=\"(Teehan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>.\n\nThe intermediate training phase often features a critical transition where models develop key computational mechanisms. For instance, in a synthetic factual recall task, researchers identified a performance plateau that coincides with the formation of attention-based circuits supporting recall <Paper corpusId=\"277349236\" paperTitle=\"(Zucchet et al., 2025)\" isShortName></Paper>. Similarly, \"induction heads\" for in-context learning have been observed to appear at specific inflection points during pre-training <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>. These transitions can be abrupt and dramatic - often characterized as breakthroughs, emergence, or phase transitions in the literature <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253117181\" paperTitle=\"(Caballero et al., 2022)\" isShortName></Paper>.\n\nAs training progresses, language models enter later stages where they refine their capabilities and develop more complex understanding. Studies of ALBERT show that reconstruction and prediction of different parts of speech occur at different learning speeds during pretraining <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>. World knowledge and reasoning abilities are typically learned later and less stably than syntactic rules <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. This progression from syntax to semantics reflects a developmental trajectory somewhat similar to human language acquisition .\n\nThe learning trajectory can also be divided into specific temporal phases based on training steps. Research on language model pretraining has identified that linguistic information begins to be encoded between 10\u00b3-10\u2074 steps (approximately 2-20 billion tokens), followed by a \"critical\" learning phase between 10\u2074-10\u2075 steps where most improvements occur across various metrics <Paper corpusId=\"276937763\" paperTitle=\"(Wal et al., 2025)\" isShortName></Paper>. This pattern remains consistent across model sizes.\n\nInterestingly, learning dynamics don't always follow a simple monotonic improvement pattern. Some capabilities show a \"double-descent\" trend where performance initially degrades before improving <Paper corpusId=\"254877112\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper>. Other linguistic abilities exhibit more complex patterns, with some deteriorating in early phases before recovering, and others showing continuous improvement or oscillation throughout training <Paper corpusId=\"269921716\" paperTitle=\"(Jung et al., 2024)\" isShortName></Paper>. These non-monotonic learning trajectories suggest complex interactions between different capabilities during training.\n\nScale plays an important role in determining learning phases. Larger models often require more data to reach critical learning thresholds <Paper corpusId=\"267061159\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. However, research comparing models of different sizes (from 125M to 175B parameters) has found that perplexity is a stronger predictor of model behaviors than model size or training computation alone <Paper corpusId=\"254877112\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper>.\n\nTraining dynamics can also reveal when models transition from memorization to generalization. Recent work has formalized the concept of a \"critical data size\" that marks a shift from quick memorization to slow generalization, identifying data insufficiency, sufficiency, and surplus regimes in language model training <Paper corpusId=\"267061159\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. Similarly, researchers have observed that the order in which rules are learned appears to be governed by their relative simplicity, with models first learning simpler patterns before more complex ones <Paper corpusId=\"272826680\" paperTitle=\"(M'esz'aros et al., 2024)\" isShortName></Paper>.\n\nUnderstanding these developmental stages has practical implications for training and fine-tuning language models. Some studies have identified bifurcations in weight dynamics that mark transitions to stationary states, suggesting optimal points to terminate training <Paper corpusId=\"268379408\" paperTitle=\"(Nicolini et al., 2024)\" isShortName></Paper>. Others have found that model capabilities can be transient, with some abilities emerging and then disappearing during training despite continuous improvement in loss metrics <Paper corpusId=\"273025704\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265157721\" paperTitle=\"(Singh et al., 2023)\" isShortName></Paper>. These insights highlight the importance of monitoring multiple metrics beyond just loss when evaluating model development.", "citations": [{"id": "(Choshen et al., 2021)", "paper": {"corpus_id": 237491997, "title": "The Grammar-Learning Trajectories of Neural Language Models", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Leshem Choshen", "authorId": "41019330"}, {"name": "Guy Hacohen", "authorId": "94064232"}, {"name": "D. Weinshall", "authorId": "1789171"}, {"name": "Omri Abend", "authorId": "2769805"}], "n_citations": 29}, "snippets": ["We begin ( \u00a73) by establishing that NLMs learn grammatical phenomena in a consistent order. We evaluate NLMs at different time points along their training, showing that the performance on linguistic phenomena across initializations is highly correlated. We further find many similarities in the set of examples that they correctly classify.\n\nStill, models of different architectures learn at a different pace, and hence cannot be directly compared at identical time points. In \u00a73.3, we overcome this by re-scaling the timeline. We then show that despite architectural differences, NLMs present highly correlated performance trajectories. In \u00a73.4, we further demonstrate that even the choice of training data has minor influence on the results. Finally, in \u00a73.5 we show that the learning dynamics essentially follows a single dimension. Namely, where the average performance is similar, success on linguistic phenomena is also similar.\n\nWe proceed by analyzing the early stages of learning in \u00a74. We find that, at first, NLMs rely mostly on local cues and not on word order. They thus resemble bag-of-words models over a window of the preceding tokens. Later stages seem to drift further away from bag-of-words models toward n-gram models, and with time seem to be more sensitive to structural cues. We also find evidence that some latent features that the model learns may not be related to linguistic phenomena."], "score": 0.99267578125}, {"id": "(Chang et al., 2023)", "paper": {"corpus_id": 261277016, "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "Z. Tu", "authorId": "144035504"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 13}, "snippets": ["Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context", "Previous work has demonstrated that language models exhibit fine-grained learning patterns that are not captured by the corpus-level loss curve (related work in \u00a72). In particular, sudden increases and decreases in example loss ( \u00a75 and (Xia et al., 2022)) may be somewhat surprising given that the pre-training text is i.i.d. for all pre-training steps. By demonstrating that many of these sudden changes are consistent regardless of random initialization and data shuffling ( \u00a75.2), our work indicates that some instances of sudden learning and \"forgetting\" are not due to random chance or the specific examples observed in a given step."], "score": 0.994140625}, {"id": "(Teehan et al., 2022)", "paper": {"corpus_id": 247656607, "title": "Emergent Structures and Training Dynamics in Large Language Models", "year": 2022, "venue": "BIGSCIENCE", "authors": [{"name": "Ryan Teehan", "authorId": "2131107966"}, {"name": "Miruna Clinciu", "authorId": "2029314697"}, {"name": "Oleg Serikov", "authorId": "1799401599"}, {"name": "Eliza Szczechla", "authorId": "50812522"}, {"name": "Natasha Seelam", "authorId": "12046785"}, {"name": "Shachar Mirkin", "authorId": "8963527"}, {"name": "Aaron Gokaslan", "authorId": "2273789852"}], "n_citations": 11}, "snippets": ["Training dynamics is an emerging field of research, promising to improve our understanding of knowledge acquisition in neural networks and offering insights into the utility of pre-trained models and embedded representations for downstream tasks. Most studies of Transformers (e.g. RoBERTa (Liu et al., 2021)) and LSTMs (Hochreiter et al., 1997) agree that models acquire linguistic knowledge early in the learning process.\n\nLocal syntactic information, such as parts of speech, is learned earlier than information encoding long-distance dependencies (e.g. topic) Saphra, 2021). Exploration of AL-BERT (Lan et al., 2019) and LSTM-based networks reveals different learning patterns for function and content words with more fine-grained distinctions within these categories including part of speech and verb form (Saphra, 2021;(Chiang et al., 2020).\n\nDifferences in learning trajectory were also observed between layers. In LSTMs, recurrent layers become more task-independent over the course of training, while embeddings become more taskspecific (Saphra, 2021). In Transformer-based architectures, i.e.: ALBERT and ELECTRA, (Chiang et al., 2020) observe differences in performance patterns between the top and last layers."], "score": 0.99267578125}, {"id": "(Chiang et al., 2020)", "paper": {"corpus_id": 222140842, "title": "Pretrained Language Model Embryology: The Birth of ALBERT", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Cheng-Han Chiang", "authorId": "1992777064"}, {"name": "Sung-Feng Huang", "authorId": "2210669195"}, {"name": "Hung-yi Lee", "authorId": "1706104"}], "n_citations": 42}, "snippets": ["While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."], "score": 0.0}, {"id": "(Zucchet et al., 2025)", "paper": {"corpus_id": 277349236, "title": "How do language models learn facts? Dynamics, curricula and hallucinations", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nicolas Zucchet", "authorId": "1729494470"}, {"name": "J\u00f6rg Bornschein", "authorId": "2320771936"}, {"name": "Stephanie Chan", "authorId": "2316336431"}, {"name": "Andrew K. Lampinen", "authorId": "2270676283"}, {"name": "Razvan Pascanu", "authorId": "1996134"}, {"name": "Soham De", "authorId": "2289159449"}], "n_citations": 7}, "snippets": ["This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories."], "score": 0.99755859375}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 261822542, "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Angelica Chen", "authorId": "13336152"}, {"name": "Ravid Schwartz-Ziv", "authorId": "2240524527"}, {"name": "Kyunghyun Cho", "authorId": "1979489"}, {"name": "Matthew L. Leavitt", "authorId": "2240527814"}, {"name": "Naomi Saphra", "authorId": "2362960"}], "n_citations": 74}, "snippets": ["While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2022), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens."], "score": 0.99169921875}, {"id": "(Caballero et al., 2022)", "paper": {"corpus_id": 253117181, "title": "Broken Neural Scaling Laws", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Ethan Caballero", "authorId": "24130593"}, {"name": "Kshitij Gupta", "authorId": "2066789106"}, {"name": "I. Rish", "authorId": "2109771"}, {"name": "David Krueger", "authorId": "145055042"}], "n_citations": 76}, "snippets": ["We present a smoothly broken power law functional form (that we refer to as a Broken Neural Scaling Law (BNSL)) that accurately models&extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as amount of compute used for training (or inference), number of model parameters, training dataset size, model input size, number of training steps, or upstream performance varies) for various architectures&for each of various tasks within a large&diverse set of upstream&downstream tasks, in zero-shot, prompted,&finetuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems,\"emergent phase transitions\", arithmetic, supervised learning, unsupervised/self-supervised learning,&reinforcement learning (single agent&multi-agent). When compared to other functional forms for neural scaling, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models&extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent&the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws"], "score": 0.0}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 233289478, "title": "Probing Across Time: What Does RoBERTa Know and When?", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Leo Z. Liu", "authorId": null}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Jungo Kasai", "authorId": "11348687"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Noah A. Smith", "authorId": "144365875"}], "n_citations": 87}, "snippets": ["Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster."], "score": 0.0}, {"id": "(Wal et al., 2025)", "paper": {"corpus_id": 276937763, "title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs", "year": 2025, "venue": "International Conference on Learning Representations", "authors": [{"name": "Oskar van der Wal", "authorId": "1986356851"}, {"name": "Pietro Lesci", "authorId": "2325954375"}, {"name": "Max M\u00fcller-Eberstein", "authorId": "1416353805"}, {"name": "Naomi Saphra", "authorId": "2308101135"}, {"name": "Hailey Schoelkopf", "authorId": "2184031883"}, {"name": "Willem Zuidema", "authorId": "2254288138"}, {"name": "Stella Biderman", "authorId": "2273535086"}], "n_citations": 2}, "snippets": ["In this work, we define stability as the change in a metric of interest (e.g., validation loss) caused by changes in randomness factors and quantify it using the standard deviation of that metric (see (Du et al., 2023) for other approaches to quantify stability)", "using training maps constructed from statistics of the model parameters, we identify the characteristics of stable training runs and the early signals of instability.\n\nLanguage modelling is largely stable. Generally, we observe LM pre-training dynamics to follow consistent trajectories. Across seeds and model sizes, downstream performance and representational efficiency consistently increase during pre-training, and training maps are linear (except for the outlier seeds).\n\nLinguistic information is encoded in the initial learning phase (10^3-10^4 steps). Across all experiments, metrics start moving away from the initial random baseline around step 10^3 (2B tokens circa) and reach their convergence level around step 10^4 (20B tokens circa). In this initial phase, representational shift peaks and linguistic information begins to be encoded into the models' latent representations.\n\nMost improvements happen in the \"critical\" learning phase (10^4-10^5 steps). In the range of 10^3 to 10^4 steps, most learning occurs, as measured by all of our metrics."], "score": 0.99609375}, {"id": "(Xia et al., 2022)", "paper": {"corpus_id": 254877112, "title": "Training Trajectories of Language Models Across Scales", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mengzhou Xia", "authorId": "67284811"}, {"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Chunting Zhou", "authorId": "2384711"}, {"name": "Xi Victoria Lin", "authorId": "143724481"}, {"name": "Ramakanth Pasunuru", "authorId": "10721120"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Ves Stoyanov", "authorId": "1389924486"}], "n_citations": 64}, "snippets": ["We attempt to make progress to answer these questions by studying the training trajectories of differently-sized OPT models (Zhang et al., 2022) through analyzing their intermediate checkpoints.\n\nIn contrast to prior work, which studies the trajectories of small models with up to 300M parameters (Liu et al., 2021;Choshen et al., 2022;Blevins et al., 2022) or focuses on the language modeling objective alone (Kaplan et al., 2020;Hernandez et al., 2021Hernandez et al., , 2022, we are the first to comprehensively study the training trajectories of large-scale autoregressive language models with up to 175B parameters across a wide range of settings.\n\nRepeatedly across training and different model scales, we analyze three aspects of model performance: (i) next-token prediction on subsets of tokens (ii) sequence-level generation and (iii) downstream task performance. We use perplexity, which is closely tied to language model evaluation, as the major metric throughout the study.\n\nFor next-token prediction ( \u00a73), we study the trajectory by categorizing each token's prediction as stagnated, upward or downward according to its perplexity trend as training progresses. We find each category comprising a significant number of tokens: while a significant number of tokens' perplexity stagnate, a subset of tokens with an increasing perplexity in smaller models exhibit a doubledescent trend (Nakkiran et al., 2020) where perplexity increases and then decreases in larger models. These behaviors primarily emerge at a similar validation perplexity across model scales"], "score": 0.998046875}, {"id": "(Jung et al., 2024)", "paper": {"corpus_id": 269921716, "title": "Unveiling Key Aspects of Fine-Tuning in Sentence Embeddings: A Representation Rank Analysis", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Euna Jung", "authorId": "2053481633"}, {"name": "Jaeill Kim", "authorId": "2157223778"}, {"name": "Jungmin Ko", "authorId": "2244228799"}, {"name": "Jinwoo Park", "authorId": "2302470951"}, {"name": "Wonjong Rhee", "authorId": "2292408696"}], "n_citations": 0}, "snippets": ["To explore linguistic aspects, we followed the methodology of Conneau et al. (2018) and utilized SentEval toolkit \u2021 to investigate training dynamics of ten different linguistic abilities.Based on the observed trends in training dynamics, we grouped the ten probing tasks into three categories, as presented in Figure 4.\n\nThe first group (Figure 4a) consists of three tasks that exhibit deterioration in Phase 1 followed by recovery in Phase 2. They are Length (number of tokens), Depth (depth of sentence structure trees), and TopConstituents (the grammatical structure of sentences), and all three are closely related to the performance of sentence embedding.We emphasize that the worst performance occurs at or near the boundary between Phase 1 and Phase 2 for the three linguistic abilities, indicating a strong correlation with representation rank.The trend of deterioration followed by recovery was also observed for alignment, where the deterioration occurs while uniformity sharply improves.\n\nThe second group (Figure 4b) consists of three tasks that exhibit an upward performance trend in both Phase 1 and Phase 2. They are WordContent (deducing words from sentence representations), \u2021 https://github.com/facebookresearch/SentEval/tree/main/data/probing  SubjNumber, and ObjNumber (matching the number of subjects and objects in sentence clauses, respectively), and all three are intimately related to the sentence embedding task.These three linguistic abilities do not deteriorate in Phase 1 despite uniformity's sharp improvement, suggesting that the three do not form a trade-off relationship with uniformity.\n\nThe third group (Figure 4c) consists of the four remaining tasks.Their performance either deteriorates (BigramShift and CoordinationInversion) or oscillates (Tense and OddManOut) throughout the fine-tuning."], "score": 0.9873046875}, {"id": "(Zhu et al., 2024)", "paper": {"corpus_id": 267061159, "title": "Critical Data Size of Language Models from a Grokking Perspective", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xuekai Zhu", "authorId": "2145238612"}, {"name": "Yao Fu", "authorId": "2280103402"}, {"name": "Bowen Zhou", "authorId": "2218723159"}, {"name": "Zhouhan Lin", "authorId": "2280367391"}], "n_citations": 18}, "snippets": ["We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data."], "score": 0.99560546875}, {"id": "(M'esz'aros et al., 2024)", "paper": {"corpus_id": 272826680, "title": "Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Anna M'esz'aros", "authorId": "2214094238"}, {"name": "Szilvia Ujv'ary", "authorId": "2299943014"}, {"name": "Wieland Brendel", "authorId": "40634590"}, {"name": "Patrik Reizinger", "authorId": "1382657853"}, {"name": "Ferenc Husz'ar", "authorId": "2322443584"}], "n_citations": 0}, "snippets": ["We analize the dynamics of learning rule extrapolation. We report results on the Transformer (training dynamics of Mamba and the LSTM are in Appx. A), trained on the a n b n language-where high rule extrapolation ability is achieved. Fig. 3 shows that first, the model learns the sequences obeying (R2), then it learns the language (R1) \u2229 (R2) as its subset. We argue that the order in which rules are learnt is governed by the relative simplicity of the rules, quantified by Kolmogorov complexity. Given a formal language with rules (R1) and (R2), let p 1 , p 2 and p 1,2 be distributions defined by LMs that generate sequences that satisfy (R1), (R2) and (R1) \u2229 (R2), respectively. If, e.g., K(p 2 ) \u226a K(p 1,2 ), our normative algorithm will first learn (R2), and then learn the (R1) \u2229 (R2) as its subset. In the a n b n language, (R2) (a's before b's), is, on average, simpler to generate than (R1) (#a=#b) and (R1) \u2229 (R2). Therefore, we expect our normative algorithm to first learn (R2), and then learn (R1) \u2229 (R2) as its subset. Remarkably, our Transformer employs the same strategy ( Fig. 3), verifying the presence of simplicity bias. This result is matches past observations that Transformers are biased towards low Kolmogorov complexity [Goldblum et al., 2023]."], "score": 0.99365234375}, {"id": "(Nicolini et al., 2024)", "paper": {"corpus_id": 268379408, "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Carlo Nicolini", "authorId": "2256990455"}, {"name": "Jacopo Staiano", "authorId": "2256994086"}, {"name": "Bruno Lepri", "authorId": "2291065942"}, {"name": "Raffaele Marino", "authorId": "2291066066"}], "n_citations": 1}, "snippets": ["In this study we have analyzed both the temporal and spatial dimensions of training a large language model.As discussed above, our work is the first one dealing with distribution of network weights as a whole, by means of computational methods borrowed from statistical mechanics.More specifically, this work shows that a bifurcation occurs in the dynamics of the weights during the training process.Such transitions are observed across various models of different sizes trained with distinct datasets.We have conducted a thorough and meticulous analysis of this aspect and concluded that this bifurcation marks a transition to a stationary state, indicating that further training is unlikely to significantly alter the weight values.Thus, training can be efficiently terminated upon reaching such a stationary state.Moreover, our study has offered a possible interpretation of the bifurcation phenomenon in terms of model perplexity."], "score": 0.9951171875}, {"id": "(Lee et al., 2024)", "paper": {"corpus_id": 273025704, "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jin Hwa Lee", "authorId": "2323997802"}, {"name": "Thomas Jiralerspong", "authorId": "2187058673"}, {"name": "Lei Yu", "authorId": "2324060912"}, {"name": "Y. Bengio", "authorId": "1865800402"}, {"name": "Emily Cheng", "authorId": "2323783941"}], "n_citations": 4}, "snippets": ["Most research on LMs focuses on the final configuration of the model at the end of pre-training. Yet, recent work shows that learning dynamics can elucidate the behavior and computational mechanisms of LMs (Chen et al., 2023)(Singh et al., 2023)Tigges et al., 2024). It has been found that, over training, LMs' weight matrices become higher-rank (Boix-Adser\u00e0 et al., 2023), their representations higher dimensional (Cheng et al., 2024), and their gradients increasingly diffuse (Weber et al., 2024). Over finetuning, representational dimensionality has been found to change in concert with geometric properties like cluster reorganization (Doimo et al., 2024)."], "score": 0.98681640625}, {"id": "(Singh et al., 2023)", "paper": {"corpus_id": 265157721, "title": "The Transient Nature of Emergent In-Context Learning in Transformers", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Aaditya K. Singh", "authorId": "2257127712"}, {"name": "Stephanie C. Y. Chan", "authorId": "50328436"}, {"name": "Ted Moskovitz", "authorId": "2238206617"}, {"name": "Erin Grant", "authorId": "2266468307"}, {"name": "Andrew M. Saxe", "authorId": "2261233446"}, {"name": "Felix Hill", "authorId": "2265548290"}], "n_citations": 44}, "snippets": ["Transformer neural networks can exhibit a surprising capacity for in-context learning (ICL) despite not being explicitly trained for it. Prior work has provided a deeper understanding of how ICL emerges in transformers, e.g. through the lens of mechanistic interpretability, Bayesian inference, or by examining the distributional properties of training data. However, in each of these cases, ICL is treated largely as a persistent phenomenon; namely, once ICL emerges, it is assumed to persist asymptotically. Here, we show that the emergence of ICL during transformer training is, in fact, often transient. We train transformers on synthetic data designed so that both ICL and in-weights learning (IWL) strategies can lead to correct predictions. We find that ICL first emerges, then disappears and gives way to IWL, all while the training loss decreases, indicating an asymptotic preference for IWL. The transient nature of ICL is observed in transformers across a range of model sizes and datasets, raising the question of how much to\"overtrain\"transformers when seeking compact, cheaper-to-run models. We find that L2 regularization may offer a path to more persistent ICL that removes the need for early stopping based on ICL-style validation tasks. Finally, we present initial evidence that ICL transience may be caused by competition between ICL and IWL circuits."], "score": 0.0}], "table": null}, {"title": "Architectural Influence on Training Dynamics", "tldr": "The architecture of language models significantly impacts how they learn during training, with different designs showing varied learning speeds and patterns despite following similar overall trajectories. Transformers have become the dominant architecture due to their effective attention mechanisms, though the specific interactions between components like attention layers and MLPs evolve distinctly through training phases. (10 sources)", "text": "\nThe architecture of a language model substantially influences its training dynamics, though research suggests that fundamental learning patterns remain consistent across different designs. Studies have found that despite architectural differences, neural language models (NLMs) present highly correlated performance trajectories when their timelines are appropriately scaled <Paper corpusId=\"237491997\" paperTitle=\"(Choshen et al., 2021)\" isShortName></Paper>. This suggests that architecture affects the pace of learning rather than the fundamental order in which linguistic phenomena are acquired.\n\nDifferent architectures show distinct learning patterns for various linguistic elements. In both Transformer and LSTM architectures, local syntactic information like parts of speech is learned earlier than information encoding long-distance dependencies <Paper corpusId=\"247656607\" paperTitle=\"(Teehan et al., 2022)\" isShortName></Paper>. However, the specific learning trajectories can vary by architecture. For instance, in LSTMs, recurrent layers become more task-independent during training while embeddings become more task-specific. In contrast, Transformer-based models like ALBERT and ELECTRA show different performance patterns between their top and bottom layers <Paper corpusId=\"247656607\" paperTitle=\"(Teehan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>.\n\nThe Transformer architecture has become particularly dominant in recent years <Paper corpusId=\"272910946\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"13756489\" paperTitle=\"(Vaswani et al., 2017)\" isShortName></Paper>, revolutionizing natural language processing through its self-attention mechanism. This architecture enables models to attend to interactions between different elements in a sequence, which is crucial for capturing linguistic dependencies. Recent theoretical advances have begun to elucidate how the self-attention mechanism learns during training for next-token prediction tasks <Paper corpusId=\"272910946\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258947127\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268379753\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nAnalysis of training dynamics in Transformers has revealed a two-step process in which self-attention operates as a \"discriminative scanning algorithm.\" Starting from uniform attention, the model gradually attends more to distinct key tokens for specific next-token predictions while paying less attention to common tokens that appear across different contexts <Paper corpusId=\"258947127\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>. This process involves what researchers call a \"scan and snap\" dynamic, where the attention mechanism first scans for relevant tokens and then snaps to a stable configuration.\n\nFurther research has characterized this process as involving two distinct steps: hard retrieval, where self-attention selects high-priority input tokens associated with the last input token, followed by soft composition, where it creates a convex combination of these tokens from which the next token can be sampled <Paper corpusId=\"268379753\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This suggests that the attention mechanism implicitly discovers the structure of the language during training.\n\nStudies of shallow transformers have shown that gradient flow serves as an inherent mechanism dividing the training process into two phases: first, the linear MLP quickly aligns with target signals while softmax attention remains almost unchanged; then, attention matrices and the MLP evolve jointly to enlarge the classification margin <Paper corpusId=\"273346324\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This phased approach appears to be an inherent property of the architecture rather than a result of explicit design.\n\nModel size also affects training dynamics. Larger models show different stabilization patterns than smaller ones, with nearly all layers in larger models stabilizing early in training (within the first 20%), while smaller models exhibit slower and less stable convergence <Paper corpusId=\"273351173\" paperTitle=\"(Martinez et al., 2024)\" isShortName></Paper>. This difference appears to be related to the effective rank of the model parameters, with lower effective rank correlating with slower convergence.\n\nBeyond model size, research has identified other architectural elements that influence training dynamics. Circuits in language models appear causally relevant to emergent capabilities and can be ablated to reduce these capabilities <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>. Different layers also converge at different rates during training, with early network layers typically converging earlier than later layers <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>.\n\nThe overall picture that emerges is that while different architectures may learn at different rates and through somewhat different mechanisms, they tend to follow similar overall trajectories in terms of the order in which they acquire different types of linguistic knowledge. This suggests fundamental principles underlying language model training that transcend specific architectural choices.", "citations": [{"id": "(Choshen et al., 2021)", "paper": {"corpus_id": 237491997, "title": "The Grammar-Learning Trajectories of Neural Language Models", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Leshem Choshen", "authorId": "41019330"}, {"name": "Guy Hacohen", "authorId": "94064232"}, {"name": "D. Weinshall", "authorId": "1789171"}, {"name": "Omri Abend", "authorId": "2769805"}], "n_citations": 29}, "snippets": ["We begin ( \u00a73) by establishing that NLMs learn grammatical phenomena in a consistent order. We evaluate NLMs at different time points along their training, showing that the performance on linguistic phenomena across initializations is highly correlated. We further find many similarities in the set of examples that they correctly classify.\n\nStill, models of different architectures learn at a different pace, and hence cannot be directly compared at identical time points. In \u00a73.3, we overcome this by re-scaling the timeline. We then show that despite architectural differences, NLMs present highly correlated performance trajectories. In \u00a73.4, we further demonstrate that even the choice of training data has minor influence on the results. Finally, in \u00a73.5 we show that the learning dynamics essentially follows a single dimension. Namely, where the average performance is similar, success on linguistic phenomena is also similar.\n\nWe proceed by analyzing the early stages of learning in \u00a74. We find that, at first, NLMs rely mostly on local cues and not on word order. They thus resemble bag-of-words models over a window of the preceding tokens. Later stages seem to drift further away from bag-of-words models toward n-gram models, and with time seem to be more sensitive to structural cues. We also find evidence that some latent features that the model learns may not be related to linguistic phenomena."], "score": 0.99267578125}, {"id": "(Teehan et al., 2022)", "paper": {"corpus_id": 247656607, "title": "Emergent Structures and Training Dynamics in Large Language Models", "year": 2022, "venue": "BIGSCIENCE", "authors": [{"name": "Ryan Teehan", "authorId": "2131107966"}, {"name": "Miruna Clinciu", "authorId": "2029314697"}, {"name": "Oleg Serikov", "authorId": "1799401599"}, {"name": "Eliza Szczechla", "authorId": "50812522"}, {"name": "Natasha Seelam", "authorId": "12046785"}, {"name": "Shachar Mirkin", "authorId": "8963527"}, {"name": "Aaron Gokaslan", "authorId": "2273789852"}], "n_citations": 11}, "snippets": ["Training dynamics is an emerging field of research, promising to improve our understanding of knowledge acquisition in neural networks and offering insights into the utility of pre-trained models and embedded representations for downstream tasks. Most studies of Transformers (e.g. RoBERTa (Liu et al., 2021)) and LSTMs (Hochreiter et al., 1997) agree that models acquire linguistic knowledge early in the learning process.\n\nLocal syntactic information, such as parts of speech, is learned earlier than information encoding long-distance dependencies (e.g. topic) Saphra, 2021). Exploration of AL-BERT (Lan et al., 2019) and LSTM-based networks reveals different learning patterns for function and content words with more fine-grained distinctions within these categories including part of speech and verb form (Saphra, 2021;(Chiang et al., 2020).\n\nDifferences in learning trajectory were also observed between layers. In LSTMs, recurrent layers become more task-independent over the course of training, while embeddings become more taskspecific (Saphra, 2021). In Transformer-based architectures, i.e.: ALBERT and ELECTRA, (Chiang et al., 2020) observe differences in performance patterns between the top and last layers."], "score": 0.99267578125}, {"id": "(Chiang et al., 2020)", "paper": {"corpus_id": 222140842, "title": "Pretrained Language Model Embryology: The Birth of ALBERT", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Cheng-Han Chiang", "authorId": "1992777064"}, {"name": "Sung-Feng Huang", "authorId": "2210669195"}, {"name": "Hung-yi Lee", "authorId": "1706104"}], "n_citations": 42}, "snippets": ["While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."], "score": 0.0}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 272910946, "title": "Non-asymptotic Convergence of Training Transformers for Next-token Prediction", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Ruiquan Huang", "authorId": "2140386847"}, {"name": "Yingbin Liang", "authorId": "2294511987"}, {"name": "Jing Yang", "authorId": "2261298891"}], "n_citations": 7}, "snippets": ["The transformer architecture (Vaswani et al., 2017) has revolutionized the field of machine learning, establishing itself as a foundation model for numerous applications, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Dosovitskiy et al., 2020), and multi-modal signal processing (Tsai et al., 2019). In particular, transformers achieve tremendous empirical success in large language models (LLMs) such as GPT-3 (Brown et al., 2020). Despite the empirical success, limited theoretical understanding of transformers have caused a series of critical concerns about their robustness, interpretability, and bias issues (Bommasani et al., 2021;(Belkin, 2024). \n\nTo overcome these issues, recent advances in transformer theory have investigated the convergence of training transformers under theoretically amenable setting such as linear regression (Mahankali et al., 2023;Zhang et al., 2023;Huang et al., 2023) and binary classification (Tarzanagh et al., 2023b,a;Vasudeva et al., 2024;Li et al., 2023). Nevertheless, one of the fundamental task in LLMs and other generative models is next-token prediction (NTP), which involves predicting the next word or token in a sequence, given the previous tokens. In NTP, a few recent theoretical studies have started to investigate the training dynamics of transformers (Tian et al., 2023)(Li et al., 2024). However, those works lack of fine-grained non-asymptotic convergence analysis of the training process, posing the following open questions for further investigation: \n\nHow fast does the training of a transformer converge in NTP?"], "score": 0.98681640625}, {"id": "(Vaswani et al., 2017)", "paper": {"corpus_id": 13756489, "title": "Attention is All you Need", "year": 2017, "venue": "Neural Information Processing Systems", "authors": [{"name": "Ashish Vaswani", "authorId": "40348417"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Niki Parmar", "authorId": "3877127"}, {"name": "Jakob Uszkoreit", "authorId": "39328010"}, {"name": "Llion Jones", "authorId": "145024664"}, {"name": "Aidan N. Gomez", "authorId": "19177000"}, {"name": "Lukasz Kaiser", "authorId": "40527594"}, {"name": "I. Polosukhin", "authorId": "3443442"}], "n_citations": 132444}, "snippets": ["The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."], "score": 0.0}, {"id": "(Tian et al., 2023)", "paper": {"corpus_id": 258947127, "title": "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yuandong Tian", "authorId": "1932187449"}, {"name": "Yiping Wang", "authorId": "2167496459"}, {"name": "Beidi Chen", "authorId": "4319427"}, {"name": "S. Du", "authorId": "145697585"}], "n_citations": 79}, "snippets": ["Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \\emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \\emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends more to distinct key tokens for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens. Among distinct tokens, it progressively drops attention weights, following the order of low to high co-occurrence between the key and the query token in the training set. Interestingly, this procedure does not lead to winner-takes-all, but decelerates due to a \\emph{phase transition} that is controllable by the learning rates of the two layers, leaving (almost) fixed token combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on synthetic and real-world data (WikiText)."], "score": 0.0}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 268379753, "title": "Mechanics of Next Token Prediction with Self-Attention", "year": 2024, "venue": "International Conference on Artificial Intelligence and Statistics", "authors": [{"name": "Yingcong Li", "authorId": "1527089987"}, {"name": "Yixiao Huang", "authorId": "2284934923"}, {"name": "M. E. Ildiz", "authorId": "46214352"}, {"name": "A. Rawat", "authorId": "2241094"}, {"name": "Samet Oymak", "authorId": "3103394"}], "n_citations": 30}, "snippets": ["Transformer-based language models are trained on large datasets to predict the next token given an input sequence. Despite this simple training objective, they have led to revolutionary advances in natural language processing. Underlying this success is the self-attention mechanism. In this work, we ask: $\\textit{What}$ $\\textit{does}$ $\\textit{a}$ $\\textit{single}$ $\\textit{self-attention}$ $\\textit{layer}$ $\\textit{learn}$ $\\textit{from}$ $\\textit{next-token}$ $\\textit{prediction?}$ We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: $\\textbf{(1)}$ $\\textbf{Hard}$ $\\textbf{retrieval:}$ Given input sequence, self-attention precisely selects the $\\textit{high-priority}$ $\\textit{input}$ $\\textit{tokens}$ associated with the last input token. $\\textbf{(2)}$ $\\textbf{Soft}$ $\\textbf{composition:}$ It then creates a convex combination of the high-priority tokens from which the next token can be sampled. Under suitable conditions, we rigorously characterize these mechanics through a directed graph over tokens extracted from the training data. We prove that gradient descent implicitly discovers the strongly-connected components (SCC) of this graph and self-attention learns to retrieve the tokens that belong to the highest-priority SCC available in the context window. Our theory relies on decomposing the model weights into a directional component and a finite component that correspond to hard retrieval and soft composition steps respectively. This also formalizes a related implicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that these findings shed light on how self-attention processes sequential data and pave the path toward demystifying more complex architectures."], "score": 0.0}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 273346324, "title": "Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Hongru Yang", "authorId": "2118571035"}, {"name": "B. Kailkhura", "authorId": "1749353"}, {"name": "Zhangyang Wang", "authorId": "2269758970"}, {"name": "Yingbin Liang", "authorId": "2269703508"}], "n_citations": 3}, "snippets": ["In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system", "We discover that gradient flow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed \\textit{automatic balancing of gradients}, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss."], "score": 0.99658203125}, {"id": "(Martinez et al., 2024)", "paper": {"corpus_id": 273351173, "title": "Tending Towards Stability: Convergence Challenges in Small Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Richard Diehl Martinez", "authorId": "2266941716"}, {"name": "Pietro Lesci", "authorId": "2325954375"}, {"name": "P. Buttery", "authorId": "33490976"}], "n_citations": 4}, "snippets": ["We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models."], "score": 0.9931640625}, {"id": "(Quirke et al., 2023)", "paper": {"corpus_id": 264935245, "title": "Training Dynamics of Contextual N-Grams in Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Lucia Quirke", "authorId": "2243240285"}, {"name": "Lovis Heindrich", "authorId": "2047549746"}, {"name": "Wes Gurnee", "authorId": "2056771333"}, {"name": "Neel Nanda", "authorId": "2051128902"}], "n_citations": 5}, "snippets": ["Training dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020)Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model."], "score": 0.99755859375}], "table": null}, {"title": "Factors Affecting Training Dynamics", "tldr": "Multiple factors significantly influence language model training dynamics, including dataset size, model scale, learning rate schedules, and optimization techniques. These factors interact in complex ways, creating distinct learning regimes and phase transitions that affect memorization, generalization capabilities, and convergence patterns. (10 sources)", "text": "\nThe size and quality of training data play a fundamental role in shaping language model training dynamics. Research has identified a \"critical data size\" that marks a significant transition from quick memorization to slow generalization in language models, with data insufficiency, sufficiency, and surplus regimes each exhibiting distinct training patterns <Paper corpusId=\"267061159\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. As model sizes increase, this critical point also becomes larger, indicating that larger models require proportionally more data to reach their full potential <Paper corpusId=\"267061159\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>.\n\nModel scale itself directly impacts learning trajectories. Larger language models memorize training data faster across all settings and, counterintuitively, can memorize a larger portion of data before overfitting compared to smaller models <Paper corpusId=\"248986465\" paperTitle=\"(Tirumala et al., 2022)\" isShortName></Paper>. They also tend to forget less throughout the training process, suggesting that scale provides inherent advantages beyond just raw computational power <Paper corpusId=\"248986465\" paperTitle=\"(Tirumala et al., 2022)\" isShortName></Paper>. When analyzing different model scales (up to 175B parameters), researchers have observed that token prediction patterns vary across sizes, with some tokens showing a \"double-descent\" trend in larger models where perplexity initially increases before decreasing <Paper corpusId=\"254877112\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper>.\n\nLearning rate schedules significantly impact training dynamics and convergence patterns. Recent research has established that the cross-entropy loss curves of neural language models follow a scaling law with learning rate annealing over training steps, accounting for both power-law scaling over data size and additional loss reduction during learning rate annealing phases <Paper corpusId=\"271909320\" paperTitle=\"(Tissue et al., 2024)\" isShortName></Paper>. The technique of \"SkipLR\" - switching learning rates at predetermined times during training - has been shown to cause loss curves to contract toward each other, providing insights into how learning rate schedules affect overall training dynamics <Paper corpusId=\"274060474\" paperTitle=\"(Subramanian et al., 2024)\" isShortName></Paper>.\n\nOptimization techniques also play a crucial role in training dynamics. Studies comparing Sharpness-Aware Minimization (SAM) with standard Stochastic Gradient Descent (SGD) have shown that SAM significantly reduces the operator norm of the Hessian and produces different alignment patterns between gradients and the principal eigenvector of the Hessian <Paper corpusId=\"262217060\" paperTitle=\"(Long et al., 2023)\" isShortName></Paper>. While SAM takes longer to achieve certain loss values, it ultimately achieves training error similar to SGD but with less sharpness, suggesting different convergence properties <Paper corpusId=\"262217060\" paperTitle=\"(Long et al., 2023)\" isShortName></Paper>.\n\nFine-tuning approaches also demonstrate distinct training dynamics. Linear Probing followed by Fine-Tuning (LP-FT) for classification tasks has been analyzed using neural tangent kernel theory, revealing that the linear head norm alongside initial prediction accuracy significantly influences fine-tuning outcomes <Paper corpusId=\"270062468\" paperTitle=\"(Tomihari et al., 2024)\" isShortName></Paper>. The increase in linear head norm during linear probing reduces changes in learned features during subsequent fine-tuning <Paper corpusId=\"270062468\" paperTitle=\"(Tomihari et al., 2024)\" isShortName></Paper>.\n\nThe content of training data and model architecture components also affect how learning progresses. Analysis of memorization patterns has revealed that models memorize nouns and numbers first, as these may act as unique identifiers for individual training examples <Paper corpusId=\"248986465\" paperTitle=\"(Tirumala et al., 2022)\" isShortName></Paper>. Further research suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping learning biases early in training <Paper corpusId=\"276235362\" paperTitle=\"(Yao et al., 2025)\" isShortName></Paper>.\n\nRecent studies have proposed a \"Temporal Scaling Law\" to model how test loss evolves as training steps increase, breaking down the overall loss into fine-grained token position loss and developing a dynamic hyperbolic law that accurately predicts test loss across training steps for both in-distribution and out-of-distribution validation datasets <Paper corpusId=\"269449894\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper>. This approach provides a more nuanced understanding of loss progression than traditional scaling laws that focus only on final test loss.\n\nComprehensive analysis of various 7B-scale models has highlighted that training strategies significantly impact learning efficiency, especially in early training stages. Factors including dataset quality, learning rate adjustments, batch size, and regularization techniques all contribute to different training trajectories <Paper corpusId=\"268820276\" paperTitle=\"(Yang et al._1, 2024)\" isShortName></Paper>. Additionally, research has found that task dynamics within a domain can predict dynamics of unseen tasks, and that enhancement of abilities across domains progresses from basic to advanced levels, similar to curriculum learning in humans <Paper corpusId=\"268820276\" paperTitle=\"(Yang et al._1, 2024)\" isShortName></Paper>.", "citations": [{"id": "(Zhu et al., 2024)", "paper": {"corpus_id": 267061159, "title": "Critical Data Size of Language Models from a Grokking Perspective", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xuekai Zhu", "authorId": "2145238612"}, {"name": "Yao Fu", "authorId": "2280103402"}, {"name": "Bowen Zhou", "authorId": "2218723159"}, {"name": "Zhouhan Lin", "authorId": "2280367391"}], "n_citations": 18}, "snippets": ["We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data."], "score": 0.99560546875}, {"id": "(Tirumala et al., 2022)", "paper": {"corpus_id": 248986465, "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Kushal Tirumala", "authorId": "2551387"}, {"name": "Aram H. Markosyan", "authorId": "153608000"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Armen Aghajanyan", "authorId": "2201435"}], "n_citations": 197}, "snippets": ["We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples."], "score": 0.9990234375}, {"id": "(Xia et al., 2022)", "paper": {"corpus_id": 254877112, "title": "Training Trajectories of Language Models Across Scales", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mengzhou Xia", "authorId": "67284811"}, {"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Chunting Zhou", "authorId": "2384711"}, {"name": "Xi Victoria Lin", "authorId": "143724481"}, {"name": "Ramakanth Pasunuru", "authorId": "10721120"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Ves Stoyanov", "authorId": "1389924486"}], "n_citations": 64}, "snippets": ["We attempt to make progress to answer these questions by studying the training trajectories of differently-sized OPT models (Zhang et al., 2022) through analyzing their intermediate checkpoints.\n\nIn contrast to prior work, which studies the trajectories of small models with up to 300M parameters (Liu et al., 2021;Choshen et al., 2022;Blevins et al., 2022) or focuses on the language modeling objective alone (Kaplan et al., 2020;Hernandez et al., 2021Hernandez et al., , 2022, we are the first to comprehensively study the training trajectories of large-scale autoregressive language models with up to 175B parameters across a wide range of settings.\n\nRepeatedly across training and different model scales, we analyze three aspects of model performance: (i) next-token prediction on subsets of tokens (ii) sequence-level generation and (iii) downstream task performance. We use perplexity, which is closely tied to language model evaluation, as the major metric throughout the study.\n\nFor next-token prediction ( \u00a73), we study the trajectory by categorizing each token's prediction as stagnated, upward or downward according to its perplexity trend as training progresses. We find each category comprising a significant number of tokens: while a significant number of tokens' perplexity stagnate, a subset of tokens with an increasing perplexity in smaller models exhibit a doubledescent trend (Nakkiran et al., 2020) where perplexity increases and then decreases in larger models. These behaviors primarily emerge at a similar validation perplexity across model scales"], "score": 0.998046875}, {"id": "(Tissue et al., 2024)", "paper": {"corpus_id": 271909320, "title": "Scaling Law with Learning Rate Annealing", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Howe Tissue", "authorId": "2316486693"}, {"name": "Venus Wang", "authorId": "2316485233"}, {"name": "Lu Wang", "authorId": "2316501479"}], "n_citations": 9}, "snippets": ["We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are constant parameters. This formulation takes into account two factors: (1) power-law scaling over data size, and (2) the additional loss reduction during LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training."], "score": 0.98828125}, {"id": "(Subramanian et al., 2024)", "paper": {"corpus_id": 274060474, "title": "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Shreyas Vathul Subramanian", "authorId": "2237161748"}, {"name": "Vignesh Ganapathiraman", "authorId": "7194698"}, {"name": "Corey Barrett", "authorId": "2258574389"}], "n_citations": 0}, "snippets": ["We consider the effect of switching the learning rate at a predetermined time during training, which we refer to as \"SkipLR\". We model SGD as a stochastic gradient flow and show that when starting from the same initial parameters, switching the learning rate causes the loss curves to contract towards each other. We demonstrate this theoretically for some simple cases, and empirically on large language models. Our analysis provides insight into how learning rate schedules affect the training dynamics, and could inform the design of new schedules to accelerate convergence."], "score": 0.99658203125}, {"id": "(Long et al., 2023)", "paper": {"corpus_id": 262217060, "title": "Sharpness-Aware Minimization and the Edge of Stability", "year": 2023, "venue": "Journal of machine learning research", "authors": [{"name": "Philip M. Long", "authorId": "144007105"}, {"name": "Peter L. Bartlett", "authorId": "2244620501"}], "n_citations": 10}, "snippets": ["Next, we plot the same quantities when the network is trained with SAM, with \u03c1 = 0.3, in Figure 13. Here, the operator norm of the Hessian is significantly less than when SGD is used, and we see evidence that training in SAM operates at the edge of stability analyzed in Section 2. In Figure 14, we zoom in on the lower part of the curve, and plot the operator norm of the Hessian, to examine the relationship between this quantity and the SAM edge in more detail", "Figure 15 contains plots of the training loss, once again estimated per-minibatch. We included these mainly to motivate the combinations of hyperparameters where we examined other aspects of the dynamics of SAM. As expected, while SAM does take longer to achieve a certain loss, it ultimately achieves training error similar to SGD, but with less sharpness", "Figure 16 contains plots of the alignment, once again estimated per-minibatch. For the large learning rates, late in training, despite the sampling noise arising from the use of minibatches, we see a systematic tendency for the SAM gradients align more closely with the principal eigenvector of the Hessian than the gradients at the initial solution. However, for the smallest learning rates, the opposite holds."], "score": 0.98291015625}, {"id": "(Tomihari et al., 2024)", "paper": {"corpus_id": 270062468, "title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Akiyoshi Tomihari", "authorId": "2303396796"}, {"name": "Issei Sato", "authorId": "2303397982"}], "n_citations": 4}, "snippets": ["In this paper, we analyze the training dynamics of LP-FT for classification tasks on the basis of the neural tangent kernel (NTK) theory. Our analysis decomposes the NTK matrix into two components. This decomposition highlights the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage. We also observe a significant increase in the linear head norm during LP, which stems from training with the cross-entropy (CE) loss. This increase in the linear head norm effectively reduces changes in learned features."], "score": 0.99560546875}, {"id": "(Yao et al., 2025)", "paper": {"corpus_id": 276235362, "title": "An Analysis for Reasoning Bias of Language Models with Small Initialization", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Junjie Yao", "authorId": "2277847328"}, {"name": "Zhongwang Zhang", "authorId": "1953073920"}, {"name": "Z. Xu", "authorId": "2136355451"}], "n_citations": 2}, "snippets": ["Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena."], "score": 0.99072265625}, {"id": "(Xiong et al., 2024)", "paper": {"corpus_id": 269449894, "title": "Temporal Scaling Law for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yizhe Xiong", "authorId": "2249971338"}, {"name": "Xiansheng Chen", "authorId": "2298904872"}, {"name": "Xin Ye", "authorId": "2299108794"}, {"name": "Hui Chen", "authorId": "2298921971"}, {"name": "Zijia Lin", "authorId": "1818920"}, {"name": "Haoran Lian", "authorId": "2298903058"}, {"name": "Jianwei Niu", "authorId": "2293626051"}, {"name": "Guiguang Ding", "authorId": "2242661989"}], "n_citations": 10}, "snippets": ["Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps."], "score": 0.99365234375}, {"id": "(Yang et al._1, 2024)", "paper": {"corpus_id": 268820276, "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chenghao Yang", "authorId": "2345264293"}, {"name": "Junzhuo Li", "authorId": "2294388803"}, {"name": "Xinyao Niu", "authorId": "2290184043"}, {"name": "Xinrun Du", "authorId": "2279346001"}, {"name": "Songyang Gao", "authorId": "2294382707"}, {"name": "Haoran Zhang", "authorId": "2281020035"}, {"name": "Zhaoliang Chen", "authorId": "2294810112"}, {"name": "Xingwei Qu", "authorId": "2239104064"}, {"name": "Ruibin Yuan", "authorId": "2032236274"}, {"name": "Yizhi Li", "authorId": "2129449392"}, {"name": "Jiaheng Liu", "authorId": "2294523552"}, {"name": "Stephen W. Huang", "authorId": "2283188391"}, {"name": "Shawn Yue", "authorId": "2282979169"}, {"name": "Wenhu Chen", "authorId": "2249847177"}, {"name": "Jie Fu", "authorId": "2265967208"}, {"name": "Ge Zhang", "authorId": "2143853895"}], "n_citations": 2}, "snippets": ["In this work, we first investigate the dynamics of several open-sourced large language models (i.e., Baichuan-7B (Yang et al., 2023), DeepSeek-7B (DeepSeek-AI et al., 2024), Amber-7B (Liu et al., 2023), OpenLLaMA-7B (Geng & Liu, 2023;Touvron et al., 2023;Computer, 2023), Yi-34B (AI et al., 2024) and DeepSeek-67B), and analyze their performance results across diverse tasks using the corresponding intermediate checkpoints based on the number of pre-trained tokens. Then, based on the theoretical framework of the Scaling Law (Kaplan et al., 2020), we analyze the performance patterns of different models across various downstream tasks, and provide findings to facilitate further research on the training dynamics of the LLMs, where the findings are shown as follows: \n\n\u2022 Findings on task dynamic prediction: Within the training process of LLMs, we observe that the dynamics of existing downstream tasks within a domain can predict the dynamics of unseen tasks. This suggests that a model's performance on known tasks can inform us about how it might perform on similar, yet unseen tasks within the same domain. (Section 4.1.1) \n\n\u2022 Findings on cross-domain promotion: Similar to the human cognitive process, the enhancement of different abilities across various domains progresses from basic to advanced levels following curriculum learning. The curriculum between crossdomain tasks can guide the training direction of models and the insights gained from one domain can potentially promote the learning process of other domains. (Section 4.1.2) \n\n\u2022 Findings on the effect of training strategies, model architecture, etc. : Based on the results of several 7b-scale models (Baichuan2-7b, DeepSeek-7b, OpenLLaMA-7b, and Amber-7b), we comprehensively analyze the effect of training strategies, dataset quality, and model architecture in training LLMs. For example, we observe that the training dataset, learning rate adjustments, batch size, and regularization techniques play a significant role in the learning efficiency in the early training stage (Section 4.2.1)."], "score": 0.99169921875}], "table": null}, {"title": "Metrics and Methods for Analyzing Training Dynamics", "tldr": "Researchers employ a diverse array of metrics and methodologies to analyze language model training dynamics, from specialized model suites to mathematical frameworks that characterize learning patterns. These approaches reveal critical insights about how models learn, including phase transitions in capability acquisition, token-level learning patterns, and the stabilization of model components. (12 sources)", "text": "\n* **Model Suite Benchmarks**: The Pythia suite provides 16 language models of varying sizes (70M to 12B parameters) with 154 checkpoints each, allowing researchers to analyze how models develop across training while controlling for data ordering and other variables <Paper corpusId=\"257921893\" paperTitle=\"(Biderman et al., 2023)\" isShortName></Paper>.\n\n* **Token-Level Analysis**: Fine-grained examination of how individual tokens' losses evolve during training reveals that significant loss reduction occurs only for a select group of tokens, with many being \"easy tokens\" (already learned) or \"hard tokens\" that resist convergence <Paper corpusId=\"269042762\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>.\n\n* **Temporal Scaling Laws**: This approach models how test loss evolves over training steps by breaking down the overall loss into fine-grained token position loss and developing a dynamic hyperbolic law that accurately predicts test loss across training steps for both in-distribution and out-of-distribution validation datasets <Paper corpusId=\"269449894\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper>.\n\n* **Singular Vector Canonical Correlation Analysis (SVCCA)**: This tool allows different model layers and networks to be compared, demonstrating that early network layers typically converge earlier in training than later layers <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>.\n\n* **Learning Dynamics Framework**: The \"Learning Law\" theoretical framework optimizes language model learning by maximizing the data compression ratio in an \"LM-training-as-lossless-compression\" perspective, revealing properties of the dynamics in the optimal learning process <Paper corpusId=\"268041376\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>.\n\n* **Stability Metrics**: Researchers quantify stability during training using standard deviation of metrics like validation loss, and identify training phases by constructing training maps from model parameter statistics <Paper corpusId=\"276937763\" paperTitle=\"(Wal et al., 2025)\" isShortName></Paper>.\n\n* **Influence Analysis**: Step-wise decomposition analysis of how influence accumulates among different potential responses helps understand the learning dynamics during different types of fine-tuning <Paper corpusId=\"271213641\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>.\n\n* **Activation Convergence Analysis**: By examining how quickly Attention and MLP activations converge to their final state across different model sizes, researchers have found that larger models' layers stabilize much earlier in training (within the first 20%) compared to smaller models <Paper corpusId=\"273351173\" paperTitle=\"(Martinez et al., 2024)\" isShortName></Paper>.\n\n* **Feature Evolution Tracking**: Techniques like SAE-Track use sparse autoencoders to obtain a continual series of features throughout training, enabling analysis of semantic evolution, feature formation processes, and directional drift of feature vectors <Paper corpusId=\"274981612\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.\n\n* **Multi-Metric Correlation**: Analyzing how validation loss correlates with performance on downstream benchmarks (e.g., Hellaswag, ARC) across thousands of training steps provides insights into improvements in text fluency, coherence, and factual accuracy <Paper corpusId=\"274823085\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>.\n\n* **Phase Transition Detection**: Multiple metrics track when capabilities emerge rapidly during training, identifying inflection points that characterize phase transitions where discrete, additive capabilities emerge independently <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>.\n\n* **Circuit Analysis**: Techniques to identify and ablate neural circuits causally linked to specific capabilities help understand how these components develop during training and contribute to emergent abilities <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>.\n\n* **Developmental Timeline Mapping**: Research has identified consistent learning phases across model sizes, with linguistic information beginning to be encoded between 10\u00b3-10\u2074 steps (approximately 2-20 billion tokens), followed by a \"critical\" learning phase between 10\u2074-10\u2075 steps where most improvements occur <Paper corpusId=\"276937763\" paperTitle=\"(Wal et al., 2025)\" isShortName></Paper>.\n\n* **Cross-Model Consistency Analysis**: Comparing developmental patterns across different model architectures, initializations, and data shuffling reveals that many learning patterns are consistent regardless of these variations <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Biderman et al., 2023)", "paper": {"corpus_id": 257921893, "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Stella Biderman", "authorId": "103476203"}, {"name": "Hailey Schoelkopf", "authorId": "2184031883"}, {"name": "Quentin G. Anthony", "authorId": "1404060481"}, {"name": "Herbie Bradley", "authorId": "2070768742"}, {"name": "Kyle O'Brien", "authorId": "2212970046"}, {"name": "Eric Hallahan", "authorId": "2162462983"}, {"name": "Mohammad Aflah Khan", "authorId": "2168771748"}, {"name": "Shivanshu Purohit", "authorId": "2162467233"}, {"name": "USVSN Sai Prashanth", "authorId": "2162462141"}, {"name": "Edward Raff", "authorId": "34885007"}, {"name": "Aviya Skowron", "authorId": "2213349418"}, {"name": "Lintang Sutawika", "authorId": "35566806"}, {"name": "Oskar van der Wal", "authorId": "1986356851"}], "n_citations": 1306}, "snippets": ["How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics."], "score": 0.99267578125}, {"id": "(Lin et al., 2024)", "paper": {"corpus_id": 269042762, "title": "Rho-1: Not All Tokens Are What You Need", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zheng-Wen Lin", "authorId": "31113759"}, {"name": "Zhibin Gou", "authorId": "1797090"}, {"name": "Yeyun Gong", "authorId": "2254121650"}, {"name": "Xiao Liu", "authorId": "49544272"}, {"name": "Yelong Shen", "authorId": "2237948786"}, {"name": "Ruochen Xu", "authorId": "2266367743"}, {"name": "Chen Lin", "authorId": "2269773814"}, {"name": "Yujiu Yang", "authorId": "2284727148"}, {"name": "Jian Jiao", "authorId": "2143968416"}, {"name": "Nan Duan", "authorId": "2269471632"}, {"name": "Weizhu Chen", "authorId": "2249538838"}], "n_citations": 75}, "snippets": ["To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In \u00a72.1, we evaluated the model's token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are \"easy tokens\" that are already learned, and some are \"hard tokens\" that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates", ".Investigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations [Saphra and Lopez, 2018], the acquisition of linguistic knowledge [Choshen et al., 2021, Liu et al., 2021], and the phenomenon of grokking [Power et al., 2022]. The analysis by Xia et al. [2022] is the most related to ours, which examines token-level training trajectories in models of varying sizes. Our findings, however, diverge from those of Xia et al. [2022], who posit that tokens with little change in perplexity are \"already learned\". We identify a spectrum of token patterns, including \"easy tokens\" and \"hard tokens\" that resist convergence."], "score": 0.99169921875}, {"id": "(Xiong et al., 2024)", "paper": {"corpus_id": 269449894, "title": "Temporal Scaling Law for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yizhe Xiong", "authorId": "2249971338"}, {"name": "Xiansheng Chen", "authorId": "2298904872"}, {"name": "Xin Ye", "authorId": "2299108794"}, {"name": "Hui Chen", "authorId": "2298921971"}, {"name": "Zijia Lin", "authorId": "1818920"}, {"name": "Haoran Lian", "authorId": "2298903058"}, {"name": "Jianwei Niu", "authorId": "2293626051"}, {"name": "Guiguang Ding", "authorId": "2242661989"}], "n_citations": 10}, "snippets": ["Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps."], "score": 0.99365234375}, {"id": "(Quirke et al., 2023)", "paper": {"corpus_id": 264935245, "title": "Training Dynamics of Contextual N-Grams in Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Lucia Quirke", "authorId": "2243240285"}, {"name": "Lovis Heindrich", "authorId": "2047549746"}, {"name": "Wes Gurnee", "authorId": "2056771333"}, {"name": "Neel Nanda", "authorId": "2051128902"}], "n_citations": 5}, "snippets": ["Training dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020)Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model."], "score": 0.99755859375}, {"id": "(Gu et al., 2024)", "paper": {"corpus_id": 268041376, "title": "Towards Optimal Learning of Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yuxian Gu", "authorId": "2116405624"}, {"name": "Li Dong", "authorId": "2286153844"}, {"name": "Y. Hao", "authorId": "34128716"}, {"name": "Qingxiu Dong", "authorId": "2287927238"}, {"name": "Minlie Huang", "authorId": "2285704485"}, {"name": "Furu Wei", "authorId": "2253471545"}], "n_citations": 7}, "snippets": ["This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an\"LM-training-as-lossless-compression\"view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task."], "score": 0.986328125}, {"id": "(Wal et al., 2025)", "paper": {"corpus_id": 276937763, "title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs", "year": 2025, "venue": "International Conference on Learning Representations", "authors": [{"name": "Oskar van der Wal", "authorId": "1986356851"}, {"name": "Pietro Lesci", "authorId": "2325954375"}, {"name": "Max M\u00fcller-Eberstein", "authorId": "1416353805"}, {"name": "Naomi Saphra", "authorId": "2308101135"}, {"name": "Hailey Schoelkopf", "authorId": "2184031883"}, {"name": "Willem Zuidema", "authorId": "2254288138"}, {"name": "Stella Biderman", "authorId": "2273535086"}], "n_citations": 2}, "snippets": ["In this work, we define stability as the change in a metric of interest (e.g., validation loss) caused by changes in randomness factors and quantify it using the standard deviation of that metric (see (Du et al., 2023) for other approaches to quantify stability)", "using training maps constructed from statistics of the model parameters, we identify the characteristics of stable training runs and the early signals of instability.\n\nLanguage modelling is largely stable. Generally, we observe LM pre-training dynamics to follow consistent trajectories. Across seeds and model sizes, downstream performance and representational efficiency consistently increase during pre-training, and training maps are linear (except for the outlier seeds).\n\nLinguistic information is encoded in the initial learning phase (10^3-10^4 steps). Across all experiments, metrics start moving away from the initial random baseline around step 10^3 (2B tokens circa) and reach their convergence level around step 10^4 (20B tokens circa). In this initial phase, representational shift peaks and linguistic information begins to be encoded into the models' latent representations.\n\nMost improvements happen in the \"critical\" learning phase (10^4-10^5 steps). In the range of 10^3 to 10^4 steps, most learning occurs, as measured by all of our metrics."], "score": 0.99609375}, {"id": "(Ren et al., 2024)", "paper": {"corpus_id": 271213641, "title": "Learning Dynamics of LLM Finetuning", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yi Ren", "authorId": "2115242507"}, {"name": "Danica J. Sutherland", "authorId": "2262445067"}], "n_citations": 21}, "snippets": ["Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses."], "score": 0.98681640625}, {"id": "(Martinez et al., 2024)", "paper": {"corpus_id": 273351173, "title": "Tending Towards Stability: Convergence Challenges in Small Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Richard Diehl Martinez", "authorId": "2266941716"}, {"name": "Pietro Lesci", "authorId": "2325954375"}, {"name": "P. Buttery", "authorId": "33490976"}], "n_citations": 4}, "snippets": ["We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models."], "score": 0.9931640625}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 274981612, "title": "Tracking the Feature Dynamics in LLM Training: A Mechanistic Study", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yangfan Xu", "authorId": "2154895781"}, {"name": "Yi Wang", "authorId": "2226917099"}, {"name": "Hao Wang", "authorId": "2328190828"}], "n_citations": 4}, "snippets": ["Understanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we (1) introduce SAE-Track, a novel method for efficiently obtaining a continual series of SAEs, providing the foundation for a mechanistic study that covers (2) the semantic evolution of features, (3) the underlying processes of feature formation, and (4) the directional drift of feature vectors. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution."], "score": 0.99462890625}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 274823085, "title": "Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach", "year": 2024, "venue": "", "authors": [{"name": "Miles Q. Li", "authorId": "2139317485"}, {"name": "Benjamin C. M. Fung", "authorId": "2333234336"}, {"name": "Shih-Chia Huang", "authorId": "2333311177"}], "n_citations": 0}, "snippets": ["Training Dynamics: We analyze how validation loss and downstream benchmarks (e.g., Hellaswag, ARC) evolve over 40,000+ training steps and correlate these metrics with improvements in text fluency, coherence, and factual accuracy."], "score": 0.9951171875}, {"id": "(Chang et al., 2023)", "paper": {"corpus_id": 261277016, "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "Z. Tu", "authorId": "144035504"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 13}, "snippets": ["Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context", "Previous work has demonstrated that language models exhibit fine-grained learning patterns that are not captured by the corpus-level loss curve (related work in \u00a72). In particular, sudden increases and decreases in example loss ( \u00a75 and (Xia et al., 2022)) may be somewhat surprising given that the pre-training text is i.i.d. for all pre-training steps. By demonstrating that many of these sudden changes are consistent regardless of random initialization and data shuffling ( \u00a75.2), our work indicates that some instances of sudden learning and \"forgetting\" are not due to random chance or the specific examples observed in a given step."], "score": 0.994140625}, {"id": "(Chiang et al., 2020)", "paper": {"corpus_id": 222140842, "title": "Pretrained Language Model Embryology: The Birth of ALBERT", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Cheng-Han Chiang", "authorId": "1992777064"}, {"name": "Sung-Feng Huang", "authorId": "2210669195"}, {"name": "Hung-yi Lee", "authorId": "1706104"}], "n_citations": 42}, "snippets": ["While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."], "score": 0.0}], "table": null}, {"title": "Emergent Capabilities and Phenomena", "tldr": "Language models exhibit distinctive phase transitions during training where capabilities emerge abruptly rather than gradually, with different abilities appearing at specific thresholds. These emergent phenomena include memorization patterns that favor certain parts of speech, sudden improvements in factual recall abilities, and transitions from memorization to generalization that follow predictable patterns governed by data size and model complexity. (11 sources)", "text": "\nLanguage model training is characterized by non-uniform development of capabilities, with certain abilities emerging suddenly at specific points in training. These discontinuous dynamics are often described as \"breakthroughs,\" \"emergence,\" or \"phase transitions\" in the literature <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253117181\" paperTitle=\"(Caballero et al., 2022)\" isShortName></Paper>. Rather than smooth, continuous improvement, many capabilities exhibit abrupt shifts that mark fundamental changes in model behavior.\n\nResearchers have discovered that these phase transitions often follow predictable patterns. Specific model circuits causally linked to emergent capabilities develop at consistent points during training and can be ablated to reduce these capabilities <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>. Some researchers have proposed that even capabilities appearing to develop gradually can be decomposed into discrete, additive capabilities emerging through independent phase transitions <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>.\n\nBehavioral analysis supports this view, showing that various model capabilities\u2014such as predicting parts of speech and individual tokens\u2014are learned during independent phase transitions characterized by varying slopes and points of maximum curvature <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>. For example, in a synthetic factual recall task, models exhibit three distinct learning phases with a performance plateau that coincides with the formation of attention-based circuits supporting recall capabilities <Paper corpusId=\"277349236\" paperTitle=\"(Zucchet et al., 2025)\" isShortName></Paper>.\n\nToken-level analysis reveals that during training, significant loss reduction is limited to specific groups of tokens. Many tokens fall into categories of either \"easy tokens\" (already learned) or \"hard tokens\" that exhibit variable losses and resist convergence <Paper corpusId=\"269042762\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>. These different token behaviors contribute to the complex, non-linear training dynamics observed in language models.\n\nMemorization patterns also follow consistent developmental trends. Larger language models memorize training data faster across all settings and can memorize a larger portion of data before over-fitting compared to smaller models <Paper corpusId=\"248986465\" paperTitle=\"(Tirumala et al., 2022)\" isShortName></Paper> <Paper corpusId=\"271860164\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>. Interestingly, models memorize different parts of speech at different rates, with nouns and numbers being memorized first\u2014likely because they serve as unique identifiers for individual training examples <Paper corpusId=\"248986465\" paperTitle=\"(Tirumala et al., 2022)\" isShortName></Paper>.\n\nThe transition from memorization to generalization represents another critical phase change in training dynamics. Research has formalized the concept of a \"critical data size\" that marks a fundamental shift from quick memorization to slow generalization <Paper corpusId=\"267061159\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. This threshold creates distinct regimes in training: data insufficiency, data sufficiency, and data surplus, each with characteristic learning patterns. As model size increases, this critical point also becomes larger, indicating that larger models require proportionally more data to reach their generalization potential <Paper corpusId=\"267061159\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>.\n\nThe order in which models learn to extrapolate rules appears to be governed by their relative simplicity. Analysis of Transformer models learning formal languages shows that models first learn simpler rules before progressing to more complex ones, with simplicity quantified through Kolmogorov complexity <Paper corpusId=\"272826680\" paperTitle=\"(M'esz'aros et al., 2024)\" isShortName></Paper>. This \"simplicity bias\" means that models learn to generate sequences satisfying simpler constraints before mastering sequences that satisfy multiple, more complex constraints simultaneously.\n\nTraining dynamics also reveal interesting patterns in how attention mechanisms develop. Researchers analyzing shallow transformers have identified a \"scan and snap\" dynamic where self-attention initially scans for relevant tokens before snapping to a stable configuration <Paper corpusId=\"271860164\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258947127\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>. This process involves a gradual shift from uniform attention to attending more to distinctive key tokens for specific next-token predictions while paying less attention to common tokens appearing across different contexts.\n\nImportantly, not all emergent capabilities represent improvements. The acquisition of factual knowledge coincides with the emergence of hallucinations, and integrating new knowledge through fine-tuning often corrupts existing parametric memories <Paper corpusId=\"277349236\" paperTitle=\"(Zucchet et al., 2025)\" isShortName></Paper>. This suggests complex interactions between different capabilities during training that can lead to trade-offs rather than uniform improvements.", "citations": [{"id": "(Chen et al., 2023)", "paper": {"corpus_id": 261822542, "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Angelica Chen", "authorId": "13336152"}, {"name": "Ravid Schwartz-Ziv", "authorId": "2240524527"}, {"name": "Kyunghyun Cho", "authorId": "1979489"}, {"name": "Matthew L. Leavitt", "authorId": "2240527814"}, {"name": "Naomi Saphra", "authorId": "2362960"}], "n_citations": 74}, "snippets": ["While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2022), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens."], "score": 0.99169921875}, {"id": "(Caballero et al., 2022)", "paper": {"corpus_id": 253117181, "title": "Broken Neural Scaling Laws", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Ethan Caballero", "authorId": "24130593"}, {"name": "Kshitij Gupta", "authorId": "2066789106"}, {"name": "I. Rish", "authorId": "2109771"}, {"name": "David Krueger", "authorId": "145055042"}], "n_citations": 76}, "snippets": ["We present a smoothly broken power law functional form (that we refer to as a Broken Neural Scaling Law (BNSL)) that accurately models&extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as amount of compute used for training (or inference), number of model parameters, training dataset size, model input size, number of training steps, or upstream performance varies) for various architectures&for each of various tasks within a large&diverse set of upstream&downstream tasks, in zero-shot, prompted,&finetuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems,\"emergent phase transitions\", arithmetic, supervised learning, unsupervised/self-supervised learning,&reinforcement learning (single agent&multi-agent). When compared to other functional forms for neural scaling, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models&extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent&the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws"], "score": 0.0}, {"id": "(Quirke et al., 2023)", "paper": {"corpus_id": 264935245, "title": "Training Dynamics of Contextual N-Grams in Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Lucia Quirke", "authorId": "2243240285"}, {"name": "Lovis Heindrich", "authorId": "2047549746"}, {"name": "Wes Gurnee", "authorId": "2056771333"}, {"name": "Neel Nanda", "authorId": "2051128902"}], "n_citations": 5}, "snippets": ["Training dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020)Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model."], "score": 0.99755859375}, {"id": "(Chiang et al., 2020)", "paper": {"corpus_id": 222140842, "title": "Pretrained Language Model Embryology: The Birth of ALBERT", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Cheng-Han Chiang", "authorId": "1992777064"}, {"name": "Sung-Feng Huang", "authorId": "2210669195"}, {"name": "Hung-yi Lee", "authorId": "1706104"}], "n_citations": 42}, "snippets": ["While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."], "score": 0.0}, {"id": "(Zucchet et al., 2025)", "paper": {"corpus_id": 277349236, "title": "How do language models learn facts? Dynamics, curricula and hallucinations", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nicolas Zucchet", "authorId": "1729494470"}, {"name": "J\u00f6rg Bornschein", "authorId": "2320771936"}, {"name": "Stephanie Chan", "authorId": "2316336431"}, {"name": "Andrew K. Lampinen", "authorId": "2270676283"}, {"name": "Razvan Pascanu", "authorId": "1996134"}, {"name": "Soham De", "authorId": "2289159449"}], "n_citations": 7}, "snippets": ["This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories."], "score": 0.99755859375}, {"id": "(Lin et al., 2024)", "paper": {"corpus_id": 269042762, "title": "Rho-1: Not All Tokens Are What You Need", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zheng-Wen Lin", "authorId": "31113759"}, {"name": "Zhibin Gou", "authorId": "1797090"}, {"name": "Yeyun Gong", "authorId": "2254121650"}, {"name": "Xiao Liu", "authorId": "49544272"}, {"name": "Yelong Shen", "authorId": "2237948786"}, {"name": "Ruochen Xu", "authorId": "2266367743"}, {"name": "Chen Lin", "authorId": "2269773814"}, {"name": "Yujiu Yang", "authorId": "2284727148"}, {"name": "Jian Jiao", "authorId": "2143968416"}, {"name": "Nan Duan", "authorId": "2269471632"}, {"name": "Weizhu Chen", "authorId": "2249538838"}], "n_citations": 75}, "snippets": ["To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In \u00a72.1, we evaluated the model's token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are \"easy tokens\" that are already learned, and some are \"hard tokens\" that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates", ".Investigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations [Saphra and Lopez, 2018], the acquisition of linguistic knowledge [Choshen et al., 2021, Liu et al., 2021], and the phenomenon of grokking [Power et al., 2022]. The analysis by Xia et al. [2022] is the most related to ours, which examines token-level training trajectories in models of varying sizes. Our findings, however, diverge from those of Xia et al. [2022], who posit that tokens with little change in perplexity are \"already learned\". We identify a spectrum of token patterns, including \"easy tokens\" and \"hard tokens\" that resist convergence."], "score": 0.99169921875}, {"id": "(Tirumala et al., 2022)", "paper": {"corpus_id": 248986465, "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Kushal Tirumala", "authorId": "2551387"}, {"name": "Aram H. Markosyan", "authorId": "153608000"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Armen Aghajanyan", "authorId": "2201435"}], "n_citations": 197}, "snippets": ["We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples."], "score": 0.9990234375}, {"id": "(Sun et al., 2024)", "paper": {"corpus_id": 271860164, "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "year": 2024, "venue": "Workshop on Representation Learning for NLP", "authors": [{"name": "Kaiser Sun", "authorId": "2087314342"}, {"name": "Mark Dredze", "authorId": "1478928280"}], "n_citations": 2}, "snippets": ["Recent studies identify phase transition of model training (Olsson et al., 2022;Wei et al., 2022), where new capabilities or behaviors suddenly emerge when certain thresholds of model complexity are reached. The aspects of complexity often include model size, amount of training by FLOPs or tokens, and model architecture. Several prior works studied the training dynamics of language models by analyzing the internals of train-fromscratch models (Tirumala et al., 2022)Chen et al., 2023;(Tian et al., 2023)Chen et al., 2024;(Chang et al., 2023). The results of these works suggest that the behaviors that are often overlooked after training could be valuable signals for model analysis. In addition to train-from-scratch models, Ren and Sutherland (2024) studied the fine-tuning dynamics of language models."], "score": 0.99609375}, {"id": "(Zhu et al., 2024)", "paper": {"corpus_id": 267061159, "title": "Critical Data Size of Language Models from a Grokking Perspective", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xuekai Zhu", "authorId": "2145238612"}, {"name": "Yao Fu", "authorId": "2280103402"}, {"name": "Bowen Zhou", "authorId": "2218723159"}, {"name": "Zhouhan Lin", "authorId": "2280367391"}], "n_citations": 18}, "snippets": ["We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data."], "score": 0.99560546875}, {"id": "(M'esz'aros et al., 2024)", "paper": {"corpus_id": 272826680, "title": "Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Anna M'esz'aros", "authorId": "2214094238"}, {"name": "Szilvia Ujv'ary", "authorId": "2299943014"}, {"name": "Wieland Brendel", "authorId": "40634590"}, {"name": "Patrik Reizinger", "authorId": "1382657853"}, {"name": "Ferenc Husz'ar", "authorId": "2322443584"}], "n_citations": 0}, "snippets": ["We analize the dynamics of learning rule extrapolation. We report results on the Transformer (training dynamics of Mamba and the LSTM are in Appx. A), trained on the a n b n language-where high rule extrapolation ability is achieved. Fig. 3 shows that first, the model learns the sequences obeying (R2), then it learns the language (R1) \u2229 (R2) as its subset. We argue that the order in which rules are learnt is governed by the relative simplicity of the rules, quantified by Kolmogorov complexity. Given a formal language with rules (R1) and (R2), let p 1 , p 2 and p 1,2 be distributions defined by LMs that generate sequences that satisfy (R1), (R2) and (R1) \u2229 (R2), respectively. If, e.g., K(p 2 ) \u226a K(p 1,2 ), our normative algorithm will first learn (R2), and then learn the (R1) \u2229 (R2) as its subset. In the a n b n language, (R2) (a's before b's), is, on average, simpler to generate than (R1) (#a=#b) and (R1) \u2229 (R2). Therefore, we expect our normative algorithm to first learn (R2), and then learn (R1) \u2229 (R2) as its subset. Remarkably, our Transformer employs the same strategy ( Fig. 3), verifying the presence of simplicity bias. This result is matches past observations that Transformers are biased towards low Kolmogorov complexity [Goldblum et al., 2023]."], "score": 0.99365234375}, {"id": "(Tian et al., 2023)", "paper": {"corpus_id": 258947127, "title": "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yuandong Tian", "authorId": "1932187449"}, {"name": "Yiping Wang", "authorId": "2167496459"}, {"name": "Beidi Chen", "authorId": "4319427"}, {"name": "S. Du", "authorId": "145697585"}], "n_citations": 79}, "snippets": ["Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \\emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \\emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends more to distinct key tokens for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens. Among distinct tokens, it progressively drops attention weights, following the order of low to high co-occurrence between the key and the query token in the training set. Interestingly, this procedure does not lead to winner-takes-all, but decelerates due to a \\emph{phase transition} that is controllable by the learning rates of the two layers, leaving (almost) fixed token combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on synthetic and real-world data (WikiText)."], "score": 0.0}], "table": null}, {"title": "Applications and Implications", "tldr": "Understanding training dynamics enables practical applications that optimize language model development, including efficient training termination points, improved learning rate schedules, and more effective fine-tuning strategies. These insights have broad implications for reducing computational costs, enhancing model performance across domains, and developing theoretical frameworks that predict and explain language model behavior. (7 sources)", "text": "\nThe analysis of language model training dynamics has significant practical applications for optimizing training processes. One direct application is the identification of efficient termination points for training. Research has demonstrated that bifurcations in weight dynamics mark transitions to stationary states where further training yields diminishing returns. These transitions are observable across models of varying sizes trained on different datasets, suggesting they represent fundamental properties of the training process rather than artifacts of specific implementations <Paper corpusId=\"268379408\" paperTitle=\"(Nicolini et al., 2024)\" isShortName></Paper>. By detecting these transition points, practitioners can avoid unnecessary computation while maintaining model quality.\n\nLearning rate optimization represents another valuable application of training dynamics research. The discovery that cross-entropy loss curves follow a predictable scaling law with learning rate annealing has enabled more precise predictions of how loss evolves throughout training <Paper corpusId=\"271909320\" paperTitle=\"(Tissue et al., 2024)\" isShortName></Paper>. Building on this understanding, techniques like \"SkipLR,\" which involves switching learning rates at predetermined times during training, have been shown to cause loss curves to contract toward each other, providing new ways to accelerate convergence <Paper corpusId=\"274060474\" paperTitle=\"(Subramanian et al., 2024)\" isShortName></Paper>. These insights can inform the design of more efficient learning rate schedules that reduce training time without sacrificing performance.\n\nContinual pre-training (CPT) represents a third application area where training dynamics insights prove valuable. Recent work has shown that CPT loss curves can be modeled by decoupling the effects of distribution shift and learning rate annealing, enabling prediction of loss at any training step across different learning rate schedules <Paper corpusId=\"278534923\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. This predictive capability allows practitioners to better plan and manage the continual learning process as models are adapted to new domains or updated with fresh data.\n\nIn the fine-tuning context, understanding training dynamics has led to improved techniques. Analysis of Linear Probing followed by Fine-Tuning (LP-FT) for classification tasks has revealed that the linear head norm alongside initial prediction accuracy significantly influences fine-tuning outcomes <Paper corpusId=\"270062468\" paperTitle=\"(Tomihari et al., 2024)\" isShortName></Paper>. This insight can guide more effective fine-tuning strategies that balance feature preservation with adaptation to downstream tasks.\n\nThe broader implications of training dynamics research extend to theoretical frameworks for language model learning. Work on optimizing language model learning through maximizing data compression ratios has led to the development of the \"Learning Law,\" which characterizes properties of optimal learning processes <Paper corpusId=\"268041376\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>. Such theoretical advances provide foundational understanding that can guide future improvements in language model training.\n\nResearch on cross-domain knowledge transfer has revealed that enhancement of abilities across various domains tends to progress from basic to advanced levels, similar to curriculum learning in humans <Paper corpusId=\"268820276\" paperTitle=\"(Yang et al._1, 2024)\" isShortName></Paper>. This finding suggests that insights gained in one domain can potentially accelerate learning in others, pointing toward more efficient multi-task training strategies. Additionally, the observation that task dynamics within a domain can predict dynamics of unseen tasks indicates the potential for forecasting model performance on novel tasks based on existing benchmarks <Paper corpusId=\"268820276\" paperTitle=\"(Yang et al._1, 2024)\" isShortName></Paper>.\n\nThe analysis of training strategies across various 7B-scale models has highlighted that factors including dataset quality, learning rate adjustments, batch size, and regularization techniques significantly impact learning efficiency, particularly in early training stages <Paper corpusId=\"268820276\" paperTitle=\"(Yang et al._1, 2024)\" isShortName></Paper>. These insights can guide practitioners in making critical decisions about training configuration that maximize performance while minimizing computational resources.\n\nAs our understanding of training dynamics continues to deepen, it opens new possibilities for developing more efficient, effective, and predictable language models. By leveraging these insights, researchers and practitioners can better navigate the complex landscape of model development, ultimately producing more capable AI systems with fewer resources.", "citations": [{"id": "(Nicolini et al., 2024)", "paper": {"corpus_id": 268379408, "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Carlo Nicolini", "authorId": "2256990455"}, {"name": "Jacopo Staiano", "authorId": "2256994086"}, {"name": "Bruno Lepri", "authorId": "2291065942"}, {"name": "Raffaele Marino", "authorId": "2291066066"}], "n_citations": 1}, "snippets": ["In this study we have analyzed both the temporal and spatial dimensions of training a large language model.As discussed above, our work is the first one dealing with distribution of network weights as a whole, by means of computational methods borrowed from statistical mechanics.More specifically, this work shows that a bifurcation occurs in the dynamics of the weights during the training process.Such transitions are observed across various models of different sizes trained with distinct datasets.We have conducted a thorough and meticulous analysis of this aspect and concluded that this bifurcation marks a transition to a stationary state, indicating that further training is unlikely to significantly alter the weight values.Thus, training can be efficiently terminated upon reaching such a stationary state.Moreover, our study has offered a possible interpretation of the bifurcation phenomenon in terms of model perplexity."], "score": 0.9951171875}, {"id": "(Tissue et al., 2024)", "paper": {"corpus_id": 271909320, "title": "Scaling Law with Learning Rate Annealing", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Howe Tissue", "authorId": "2316486693"}, {"name": "Venus Wang", "authorId": "2316485233"}, {"name": "Lu Wang", "authorId": "2316501479"}], "n_citations": 9}, "snippets": ["We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are constant parameters. This formulation takes into account two factors: (1) power-law scaling over data size, and (2) the additional loss reduction during LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training."], "score": 0.98828125}, {"id": "(Subramanian et al., 2024)", "paper": {"corpus_id": 274060474, "title": "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Shreyas Vathul Subramanian", "authorId": "2237161748"}, {"name": "Vignesh Ganapathiraman", "authorId": "7194698"}, {"name": "Corey Barrett", "authorId": "2258574389"}], "n_citations": 0}, "snippets": ["We consider the effect of switching the learning rate at a predetermined time during training, which we refer to as \"SkipLR\". We model SGD as a stochastic gradient flow and show that when starting from the same initial parameters, switching the learning rate causes the loss curves to contract towards each other. We demonstrate this theoretically for some simple cases, and empirically on large language models. Our analysis provides insight into how learning rate schedules affect the training dynamics, and could inform the design of new schedules to accelerate convergence."], "score": 0.99658203125}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 278534923, "title": "Learning Dynamics in Continual Pre-Training for Large Language Models", "year": 2025, "venue": "", "authors": [{"name": "Xingjin Wang", "authorId": "2268303033"}, {"name": "Howe Tissue", "authorId": "2316486693"}, {"name": "Lu Wang", "authorId": "2316501479"}, {"name": "Linjing Li", "authorId": "2107923812"}, {"name": "D. Zeng", "authorId": "2263800857"}], "n_citations": 0}, "snippets": ["In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT."], "score": 0.99560546875}, {"id": "(Tomihari et al., 2024)", "paper": {"corpus_id": 270062468, "title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Akiyoshi Tomihari", "authorId": "2303396796"}, {"name": "Issei Sato", "authorId": "2303397982"}], "n_citations": 4}, "snippets": ["In this paper, we analyze the training dynamics of LP-FT for classification tasks on the basis of the neural tangent kernel (NTK) theory. Our analysis decomposes the NTK matrix into two components. This decomposition highlights the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage. We also observe a significant increase in the linear head norm during LP, which stems from training with the cross-entropy (CE) loss. This increase in the linear head norm effectively reduces changes in learned features."], "score": 0.99560546875}, {"id": "(Gu et al., 2024)", "paper": {"corpus_id": 268041376, "title": "Towards Optimal Learning of Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yuxian Gu", "authorId": "2116405624"}, {"name": "Li Dong", "authorId": "2286153844"}, {"name": "Y. Hao", "authorId": "34128716"}, {"name": "Qingxiu Dong", "authorId": "2287927238"}, {"name": "Minlie Huang", "authorId": "2285704485"}, {"name": "Furu Wei", "authorId": "2253471545"}], "n_citations": 7}, "snippets": ["This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an\"LM-training-as-lossless-compression\"view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task."], "score": 0.986328125}, {"id": "(Yang et al._1, 2024)", "paper": {"corpus_id": 268820276, "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chenghao Yang", "authorId": "2345264293"}, {"name": "Junzhuo Li", "authorId": "2294388803"}, {"name": "Xinyao Niu", "authorId": "2290184043"}, {"name": "Xinrun Du", "authorId": "2279346001"}, {"name": "Songyang Gao", "authorId": "2294382707"}, {"name": "Haoran Zhang", "authorId": "2281020035"}, {"name": "Zhaoliang Chen", "authorId": "2294810112"}, {"name": "Xingwei Qu", "authorId": "2239104064"}, {"name": "Ruibin Yuan", "authorId": "2032236274"}, {"name": "Yizhi Li", "authorId": "2129449392"}, {"name": "Jiaheng Liu", "authorId": "2294523552"}, {"name": "Stephen W. Huang", "authorId": "2283188391"}, {"name": "Shawn Yue", "authorId": "2282979169"}, {"name": "Wenhu Chen", "authorId": "2249847177"}, {"name": "Jie Fu", "authorId": "2265967208"}, {"name": "Ge Zhang", "authorId": "2143853895"}], "n_citations": 2}, "snippets": ["In this work, we first investigate the dynamics of several open-sourced large language models (i.e., Baichuan-7B (Yang et al., 2023), DeepSeek-7B (DeepSeek-AI et al., 2024), Amber-7B (Liu et al., 2023), OpenLLaMA-7B (Geng & Liu, 2023;Touvron et al., 2023;Computer, 2023), Yi-34B (AI et al., 2024) and DeepSeek-67B), and analyze their performance results across diverse tasks using the corresponding intermediate checkpoints based on the number of pre-trained tokens. Then, based on the theoretical framework of the Scaling Law (Kaplan et al., 2020), we analyze the performance patterns of different models across various downstream tasks, and provide findings to facilitate further research on the training dynamics of the LLMs, where the findings are shown as follows: \n\n\u2022 Findings on task dynamic prediction: Within the training process of LLMs, we observe that the dynamics of existing downstream tasks within a domain can predict the dynamics of unseen tasks. This suggests that a model's performance on known tasks can inform us about how it might perform on similar, yet unseen tasks within the same domain. (Section 4.1.1) \n\n\u2022 Findings on cross-domain promotion: Similar to the human cognitive process, the enhancement of different abilities across various domains progresses from basic to advanced levels following curriculum learning. The curriculum between crossdomain tasks can guide the training direction of models and the insights gained from one domain can potentially promote the learning process of other domains. (Section 4.1.2) \n\n\u2022 Findings on the effect of training strategies, model architecture, etc. : Based on the results of several 7b-scale models (Baichuan2-7b, DeepSeek-7b, OpenLLaMA-7b, and Amber-7b), we comprehensively analyze the effect of training strategies, dataset quality, and model architecture in training LLMs. For example, we observe that the training dataset, learning rate adjustments, batch size, and regularization techniques play a significant role in the learning efficiency in the early training stage (Section 4.2.1)."], "score": 0.99169921875}], "table": null}], "cost": 0.522171}}
