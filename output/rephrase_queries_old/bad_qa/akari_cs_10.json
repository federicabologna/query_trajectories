{"original_query": "What are some of the key advantages and challenges of building multilingual evaluation datasets by translating existing English datasets into other target languages? Or more specifically, how do these translated datasets impact the quality and reliability of cross-lingual evaluations, and what potential pitfalls should be considered when using translations as a primary method for creating non-English benchmarks? ", "input": "key advantages challenges of building multilingual evaluation datasets translating existing English datasets into other target languages how datasets the quality reliability of cross lingual evaluations what potential pitfalls when translations as primary method non English benchmarks", "output": {"sections": [{"title": "Background on Multilingual Evaluation Datasets", "tldr": "Multilingual evaluation datasets are essential for assessing language models across diverse languages, with most created through translation from English, independent collection, or compilation of similar tasks. While translation-based approaches offer parallel evaluation sets for measuring cross-lingual transfer capabilities, they introduce biases related to the source language and culture. (20 sources)", "text": "\nMultilingual evaluation datasets have become increasingly important for assessing the performance of language models across diverse languages. However, there remains a significant imbalance in resource availability, with most NLP resources being English-specific <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. Current evaluation benchmarks often lack good coverage of low-resource languages, are restricted to specific domains, or suffer from quality issues due to semi-automatic construction methods <Paper corpusId=\"235358129\" paperTitle=\"(Goyal et al., 2021)\" isShortName></Paper>. This disproportionate focus on English and a handful of high-resource languages limits our understanding of how language models perform in multilingual and lower-resource scenarios <Paper corpusId=\"277621694\" paperTitle=\"(Luo et al., 2025)\" isShortName></Paper>.\n\nResearchers have developed multilingual datasets primarily through three approaches <Paper corpusId=\"270285718\" paperTitle=\"(Sakai et al., 2024)\" isShortName></Paper>:\n\n1. **Translation from existing datasets**: Many datasets are created by translating English resources into other languages, such as XNLI <Paper corpusId=\"52271711\" paperTitle=\"(Conneau et al., 2018)\" isShortName></Paper>, XQuAD <Paper corpusId=\"204901567\" paperTitle=\"(Artetxe et al., 2019)\" isShortName></Paper>, and MLQA <Paper corpusId=\"204734128\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper>. These translations can be human-verified, as in AfriXNLI, or rely solely on machine translation <Paper corpusId=\"273229002\" paperTitle=\"(Kargaran et al., 2024)\" isShortName></Paper> <Paper corpusId=\"260334562\" paperTitle=\"(Lai et al., 2023)\" isShortName></Paper>.\n\n2. **Independent collection across languages**: Some datasets like TyDi QA <Paper corpusId=\"212657414\" paperTitle=\"(Clark et al., 2020)\" isShortName></Paper> and XLSum <Paper corpusId=\"235658519\" paperTitle=\"(Hasan et al., 2021)\" isShortName></Paper> are created by collecting and annotating data in multiple languages separately, resulting in non-parallel datasets where the size may vary between languages.\n\n3. **Compilation of similar tasks**: This approach involves gathering similar tasks across multiple languages to create comprehensive benchmarks like XTREME <Paper corpusId=\"214641214\" paperTitle=\"(Hu et al., 2020)\" isShortName></Paper> and XGLUE <Paper corpusId=\"214794966\" paperTitle=\"(Liang et al., 2020)\" isShortName></Paper>.\n\nTranslation-based approaches offer two significant advantages: they leverage existing professional translation services rather than requiring expertise in multiple languages, and they create parallel evaluation sets that provide meaningful measures of cross-lingual transfer gaps between models <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. However, this approach also introduces biases related to translations and cultural representation, potentially affecting experimental conclusions by reflecting the origin culture <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper>.\n\nRecent efforts have expanded the range of multilingual benchmarks beyond translation-based approaches. These include probing benchmarks for assessing linguistic properties like MELA <Paper corpusId=\"265213331\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> and multilingual knowledge probing datasets such as MLAMA <Paper corpusId=\"231740666\" paperTitle=\"(Kassner et al., 2021)\" isShortName></Paper> and BMIKE-53 <Paper corpusId=\"276647924\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>. Additionally, researchers have developed datasets for natural language generation tasks like summarization <Paper corpusId=\"216868259\" paperTitle=\"(Scialom et al., 2020)\" isShortName></Paper> <Paper corpusId=\"218470403\" paperTitle=\"(Mueller et al., 2020)\" isShortName></Paper>.\n\nSome researchers argue for avoiding benchmarks where English plays a special role, including datasets constructed in English and translated to other languages, as well as zero-shot transfer tasks where English is the sole source language <Paper corpusId=\"258187051\" paperTitle=\"(Chung et al., 2023)\" isShortName></Paper>. This perspective emphasizes the need for linguistically diverse evaluation tasks that are free from language bias and well-balanced across languages.", "citations": [{"id": "(Artetxe et al., 2020)", "paper": {"corpus_id": 215548041, "title": "Translation Artifacts in Cross-lingual Transfer Learning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Gorka Labaka", "authorId": "2064469779"}, {"name": "Eneko Agirre", "authorId": "1733049"}], "n_citations": 120}, "snippets": ["While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training", ".Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models."], "score": 0.85693359375}, {"id": "(Goyal et al., 2021)", "paper": {"corpus_id": 235358129, "title": "The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation", "year": 2021, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Cynthia Gao", "authorId": "2107063269"}, {"name": "Vishrav Chaudhary", "authorId": "113810201"}, {"name": "Peng-Jen Chen", "authorId": "2158170998"}, {"name": "Guillaume Wenzek", "authorId": "2293203"}, {"name": "Da Ju", "authorId": "3092435"}, {"name": "Sanjan Krishnan", "authorId": "1491706730"}, {"name": "Marc'Aurelio Ranzato", "authorId": "1706809"}, {"name": "Francisco Guzm\u00e1n", "authorId": "144204682"}, {"name": "Angela Fan", "authorId": "144270981"}], "n_citations": 587}, "snippets": ["Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures."], "score": 0.7666015625}, {"id": "(Luo et al., 2025)", "paper": {"corpus_id": 277621694, "title": "GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hengyu Luo", "authorId": "2325478503"}, {"name": "Zihao Li", "authorId": "2312330893"}, {"name": "Joseph Attieh", "authorId": "2308478088"}, {"name": "Sawal Devkota", "authorId": "2354172398"}, {"name": "Ona de Gibert", "authorId": "51436367"}, {"name": "Shaoxiong Ji", "authorId": "2312353271"}, {"name": "Peiqin Lin", "authorId": "2266791225"}, {"name": "Bhavani Sai Praneeth Varma Mantina", "authorId": "2354176011"}, {"name": "Ananda Sreenidhi", "authorId": "2329094725"}, {"name": "Ra'ul V'azquez", "authorId": "2257040447"}, {"name": "Mengjie Wang", "authorId": "2355403639"}, {"name": "Samea Yusofi", "authorId": "2354177102"}, {"name": "J\u00f6rg Tiedemann", "authorId": "2237430242"}], "n_citations": 0}, "snippets": ["Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios."], "score": 0.76171875}, {"id": "(Sakai et al., 2024)", "paper": {"corpus_id": 270285718, "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yusuke Sakai", "authorId": "2204051777"}, {"name": "Hidetaka Kamigaito", "authorId": "2300756"}, {"name": "Taro Watanabe", "authorId": "2266807418"}], "n_citations": 5}, "snippets": ["Multilingual datasets When focusing on the evaluation of multilingual performance of LMs,  the evaluation datasets are almost exclusively created through three methods, as shown in Table 2:\n\n(1) Translation from existing datasets in a major language, e.g., English (Lin et al., 2021;Ponti et al., 2020;Conneau et al., 2018;Artetxe et al., 2020;Yang et al., 2019); (2) Compilation of similar tasks across multiple languages (Zhang et al., 2023c;Hu et al., 2023;Adelani et al., 2022;Roy et al., 2020;Malmasi and Dras, 2015); (3) Creation from multilingual resources following the same dataset creation process (Keung et al., 2020;Huang et al., 2020;Buchholz and Marsi, 2006;Clark et al., 2020;Schwenk and Li, 2018;Kabra et al., 2023)."], "score": 0.7333984375}, {"id": "(Conneau et al., 2018)", "paper": {"corpus_id": 52271711, "title": "XNLI: Evaluating Cross-lingual Sentence Representations", "year": 2018, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Alexis Conneau", "authorId": "2480903"}, {"name": "Guillaume Lample", "authorId": "1830914"}, {"name": "Ruty Rinott", "authorId": "1905713"}, {"name": "Adina Williams", "authorId": "81840293"}, {"name": "Samuel R. Bowman", "authorId": "3644767"}, {"name": "Holger Schwenk", "authorId": "144518416"}, {"name": "Veselin Stoyanov", "authorId": "1759422"}], "n_citations": 1388}, "snippets": ["State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines."], "score": 0.0}, {"id": "(Artetxe et al., 2019)", "paper": {"corpus_id": 204901567, "title": "On the Cross-lingual Transferability of Monolingual Representations", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Sebastian Ruder", "authorId": "2884561"}, {"name": "Dani Yogatama", "authorId": "1755465"}], "n_citations": 799}, "snippets": ["State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators."], "score": 0.0}, {"id": "(Lewis et al., 2019)", "paper": {"corpus_id": 204734128, "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Ruty Rinott", "authorId": "1905713"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Holger Schwenk", "authorId": "144518416"}], "n_citations": 509}, "snippets": ["Parallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation."], "score": 0.90185546875}, {"id": "(Kargaran et al., 2024)", "paper": {"corpus_id": 273229002, "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Amir Hossein Kargaran", "authorId": "98623604"}, {"name": "Ali Modarressi", "authorId": "2054744"}, {"name": "Nafiseh Nikeghbal", "authorId": "2210982230"}, {"name": "Jana Diesner", "authorId": "2261085110"}, {"name": "Franccois Yvon", "authorId": "2302797923"}, {"name": "Hinrich Schutze", "authorId": "2130001188"}], "n_citations": 7}, "snippets": ["These datasets comprise a mix of translated datasets, some human-translated or verified by native speakers such as AfriXNLI (Adelani et al., 2024) and some relying only on machine translation (Lai et al., 2023). Additionally, there are datasets created independently for each language, such as XLSum (Hasan et al., 2021), where the data is not parallel and the size of the data varies between languages."], "score": 0.8271484375}, {"id": "(Lai et al., 2023)", "paper": {"corpus_id": 260334562, "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Viet Dac Lai", "authorId": "1405279380"}, {"name": "Chien Van Nguyen", "authorId": "2186540882"}, {"name": "Nghia Trung Ngo", "authorId": "1692755523"}, {"name": "Thuat Nguyen", "authorId": "2116085512"}, {"name": "Franck Dernoncourt", "authorId": "2462276"}, {"name": "Ryan A. Rossi", "authorId": "2066337266"}, {"name": "Thien Huu Nguyen", "authorId": "1811211"}], "n_citations": 150}, "snippets": ["A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi."], "score": 0.0}, {"id": "(Clark et al., 2020)", "paper": {"corpus_id": 212657414, "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages", "year": 2020, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "J. Clark", "authorId": "144797264"}, {"name": "Eunsol Choi", "authorId": "2890423"}, {"name": "Michael Collins", "authorId": "123052390"}, {"name": "Dan Garrette", "authorId": "2758616"}, {"name": "T. Kwiatkowski", "authorId": "15652489"}, {"name": "Vitaly Nikolaev", "authorId": "48942032"}, {"name": "J. Palomaki", "authorId": "52578817"}], "n_citations": 612}, "snippets": ["Abstract Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA\u2014a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology\u2014the set of linguistic features each language expresses\u2014such that we expect models performing well on this set to generalize across a large number of the world\u2019s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don\u2019t know the answer yet, and the data is collected directly in each language without the use of translation."], "score": 0.0}, {"id": "(Hasan et al., 2021)", "paper": {"corpus_id": 235658519, "title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages", "year": 2021, "venue": "Findings", "authors": [{"name": "Tahmid Hasan", "authorId": "1400373232"}, {"name": "Abhik Bhattacharjee", "authorId": "49785688"}, {"name": "Md Saiful Islam", "authorId": null}, {"name": "Kazi Samin Mubasshir", "authorId": "1955443619"}, {"name": "Yuan-Fang Li", "authorId": "4495301"}, {"name": "Yong-Bin Kang", "authorId": "145059666"}, {"name": "M. Rahman", "authorId": "2218125433"}, {"name": "Rifat Shahriyar", "authorId": "2046603"}], "n_citations": 372}, "snippets": ["Contemporary works on abstractive text summarization have focused primarily on high-resource languages like English, mostly due to the limited availability of datasets for low/mid-resource ones. In this work, we present XL-Sum, a comprehensive and diverse dataset comprising 1 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model, with XL-Sum and experiment on multilingual and low-resource summarization tasks. XL-Sum induces competitive results compared to the ones obtained using similar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10 languages we benchmark on, with some of them exceeding 15, as obtained by multilingual training. Additionally, training on low-resource languages individually also provides competitive performance. To the best of our knowledge, XL-Sum is the largest abstractive summarization dataset in terms of the number of samples collected from a single source and the number of languages covered. We are releasing our dataset and models to encourage future research on multilingual abstractive summarization. The resources can be found at \\url{https://github.com/csebuetnlp/xl-sum}."], "score": 0.0}, {"id": "(Hu et al., 2020)", "paper": {"corpus_id": 214641214, "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Junjie Hu", "authorId": "2149221827"}, {"name": "Sebastian Ruder", "authorId": "2884561"}, {"name": "Aditya Siddhant", "authorId": "9356387"}, {"name": "Graham Neubig", "authorId": "1700325"}, {"name": "Orhan Firat", "authorId": "2345617"}, {"name": "Melvin Johnson", "authorId": "145657834"}], "n_citations": 976}, "snippets": ["Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks."], "score": 0.0}, {"id": "(Liang et al., 2020)", "paper": {"corpus_id": 214794966, "title": "XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yaobo Liang", "authorId": "3887469"}, {"name": "Nan Duan", "authorId": "46429989"}, {"name": "Yeyun Gong", "authorId": "2171182"}, {"name": "Ning Wu", "authorId": "2068345080"}, {"name": "Fenfei Guo", "authorId": "2239234"}, {"name": "Weizhen Qi", "authorId": "15629561"}, {"name": "Ming Gong", "authorId": "50175330"}, {"name": "Linjun Shou", "authorId": "24962156"}, {"name": "Daxin Jiang", "authorId": "71790825"}, {"name": "Guihong Cao", "authorId": "3320836"}, {"name": "Xiaodong Fan", "authorId": "48025819"}, {"name": "Bruce Zhang", "authorId": "2116121771"}, {"name": "Rahul Agrawal", "authorId": "2054897723"}, {"name": "Edward Cui", "authorId": "144530394"}, {"name": "Sining Wei", "authorId": "2111326903"}, {"name": "Taroon Bharti", "authorId": "1490606819"}, {"name": "Ying Qiao", "authorId": "2056923187"}, {"name": "Jiun-Hung Chen", "authorId": "33926030"}, {"name": "Winnie Wu", "authorId": "2110027452"}, {"name": "Shuguang Liu", "authorId": "2108002297"}, {"name": "Fan Yang", "authorId": "145338263"}, {"name": "Daniel Fernando Campos", "authorId": "144081089"}, {"name": "Rangan Majumder", "authorId": "32431940"}, {"name": "Ming Zhou", "authorId": "92660691"}], "n_citations": 350}, "snippets": ["In this paper, we introduce XGLUE, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE(Wang et al., 2019), which is labeled in English for natural language understanding tasks only, XGLUE has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder(Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison."], "score": 0.0}, {"id": "(Etxaniz et al., 2024)", "paper": {"corpus_id": 270380088, "title": "BertaQA: How Much Do Language Models Know About Local Culture?", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Julen Etxaniz", "authorId": "2226458991"}, {"name": "Gorka Azkune", "authorId": "2481918"}, {"name": "A. Soroa", "authorId": "2260104163"}, {"name": "Oier L\u00f3pez de Lacalle", "authorId": "2251043402"}, {"name": "Mikel Artetxe", "authorId": "2347956"}], "n_citations": 11}, "snippets": ["Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture."], "score": 0.8037109375}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 265213331, "title": "MELA: Multilingual Evaluation of Linguistic Acceptability", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ziyin Zhang", "authorId": "2267343117"}, {"name": "Yikang Liu", "authorId": "2214607250"}, {"name": "Wei Huang", "authorId": "2000111554"}, {"name": "Junyu Mao", "authorId": "2266752587"}, {"name": "Rui Wang", "authorId": "2266813948"}, {"name": "Hai Hu", "authorId": "2118878378"}], "n_citations": 3}, "snippets": ["In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability -- MELA, with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark, and investigate cross-lingual transfer in acceptability judgements with XLM-R. In pursuit of multilingual interpretability, we conduct probing experiments with fine-tuned XLM-R to explore the process of syntax capability acquisition. Our results show that GPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R, while open-source multilingual models lag behind by a noticeable gap. Cross-lingual transfer experiments show that transfer in acceptability judgment is non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance in a completely unrelated language -- Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM-R on syntax-related tasks. Our data is available at https://github.com/sjtu-compling/MELA."], "score": 0.0}, {"id": "(Kassner et al., 2021)", "paper": {"corpus_id": 231740666, "title": "Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models", "year": 2021, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Nora Kassner", "authorId": "9529535"}, {"name": "Philipp Dufter", "authorId": "35501453"}, {"name": "Hinrich Sch\u00fctze", "authorId": "144418438"}], "n_citations": 141}, "snippets": ["Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as \"Paris is the capital of [MASK]\" are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT\u2019s performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin."], "score": 0.0}, {"id": "(He et al., 2025)", "paper": {"corpus_id": 276647924, "title": "XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Linyang He", "authorId": "2257338995"}, {"name": "Ercong Nie", "authorId": "2197254657"}, {"name": "Sukru Samet Dindar", "authorId": "2294424733"}, {"name": "Arsalan Firoozi", "authorId": "2321723374"}, {"name": "Adrian Florea", "authorId": "2311441706"}, {"name": "Van Nguyen", "authorId": "2349460444"}, {"name": "Corentin Puffay", "authorId": "2174811177"}, {"name": "Riki Shimizu", "authorId": "2347533359"}, {"name": "Haotian Ye", "authorId": "2278833873"}, {"name": "Jonathan Brennan", "authorId": "2293392156"}, {"name": "Helmut Schmid", "authorId": "2256998691"}, {"name": "Hinrich Schutze", "authorId": "2261745622"}, {"name": "N. Mesgarani", "authorId": "1686269"}], "n_citations": 1}, "snippets": ["Multilingual evaluation benchmarks have played a pivotal role in assessing the capabilities of language models across diverse languages. In the realm of multilingual probing, prior work has focused on evaluating linguistic properties and knowledge representation. For instance, (Zhang et al., 2023) introduced MELA to assess multilingual linguistic acceptability, while (Mueller et al., 2020) explored syntactic minimal pairs to evaluate cross-linguistic syntactic competence. On the knowledge probing front, benchmarks such as MLAMA (Kassner et al., 2021), BMLAMA (Qi et al., 2023), and BMIKE-53 (Nie et al., 2024) have been developed to investigate the factual knowledge encoded in multilingual models and their cross-lingual consistency. \n\nBeyond probing, multilingual natural language understanding (NLU) benchmarks like XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020)   have become standard for evaluating cross-lingual transferability. These benchmarks often rely on translating English datasets into other languages, as seen in tasks like XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), and MLQA (Lewis et al., 2019). On the generation side, multilingual natural language generation (NLG) benchmarks have emerged, covering tasks such as summarization (Scialom et al., 2020)(Ladhak et al., 2020)."], "score": 0.767578125}, {"id": "(Scialom et al., 2020)", "paper": {"corpus_id": 216868259, "title": "MLSUM: The Multilingual Summarization Corpus", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Thomas Scialom", "authorId": "90745780"}, {"name": "Paul-Alexis Dray", "authorId": "1502219404"}, {"name": "S. Lamprier", "authorId": "1782552"}, {"name": "Benjamin Piwowarski", "authorId": "1703777"}, {"name": "Jacopo Staiano", "authorId": "1767493"}], "n_citations": 177}, "snippets": ["We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset."], "score": 0.0}, {"id": "(Mueller et al., 2020)", "paper": {"corpus_id": 218470403, "title": "Cross-Linguistic Syntactic Evaluation of Word Prediction Models", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Aaron Mueller", "authorId": "49355602"}, {"name": "Garrett Nicolai", "authorId": "40156252"}, {"name": "Panayiota Petrou-Zeniou", "authorId": "1432608177"}, {"name": "N. Talmina", "authorId": "147714277"}, {"name": "Tal Linzen", "authorId": "2467508"}], "n_citations": 57}, "snippets": ["A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy. However, these studies are based primarily on monolingual evidence from English. To investigate how these models\u2019 ability to learn syntax varies by language, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models. CLAMS includes subject-verb agreement challenge sets for English, French, German, Hebrew and Russian, generated from grammars we develop. We use CLAMS to evaluate LSTM language models as well as monolingual and multilingual BERT. Across languages, monolingual LSTMs achieved high accuracy on dependencies without attractors, and generally poor accuracy on agreement across object relative clauses. On other constructions, agreement accuracy was generally higher in languages with richer morphology. Multilingual models generally underperformed monolingual models. Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages."], "score": 0.0}, {"id": "(Chung et al., 2023)", "paper": {"corpus_id": 258187051, "title": "UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Noah Constant", "authorId": "40832517"}, {"name": "Xavier Garc\u00eda", "authorId": "143936294"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Yi Tay", "authorId": "144447820"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Orhan Firat", "authorId": "2345617"}], "n_citations": 57}, "snippets": ["In selecting evaluation tasks, we aim to satisfy several key properties. First, tasks should be linguistically diverse, covering a range of languages from distinct families and regions, including both high-and low-resource languages. Second, tasks should be free of language bias. For example, the task training data and evaluation metrics should be well-balanced across languages. We also avoid benchmarks where English plays a special role, including datasets constructed in English and translated post-hoc to other languages, as well as zero-shot transfer tasks where English is the sole source language."], "score": 0.75341796875}], "table": null}, {"title": "Advantages of Translating English Datasets", "tldr": "Translating existing English datasets offers a cost-effective way to create multilingual evaluation resources with parallel data across languages, enabling direct cross-lingual performance comparisons and addressing the scarcity of resources in low-resource languages. (12 sources)", "text": "\nTranslating existing English-language datasets into other languages has emerged as a prevalent approach for creating multilingual evaluation resources due to several key advantages. First, translation leverages existing professional translation services rather than requiring expertise in multiple languages, making it a practical and accessible method for expanding language coverage <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. This approach is significantly more cost-effective and time-efficient compared to creating new datasets from scratch with human annotators across multiple languages <Paper corpusId=\"273323209\" paperTitle=\"(Thellmann et al., 2024)\" isShortName></Paper>.\n\nPerhaps most importantly, translation creates parallel evaluation sets that provide meaningful measures of cross-lingual transfer capabilities between models <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. This parallel structure allows researchers to attribute performance differences to language understanding versus task knowledge, isolating the factors that affect cross-lingual generalization <Paper corpusId=\"269362363\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>. For instance, machine-translated test sets enable direct evaluation of \"translate test\" cross-lingual transfer approaches <Paper corpusId=\"246294502\" paperTitle=\"(Bugliarello et al., 2022)\" isShortName></Paper>.\n\nTranslation-based approaches also help address the significant challenge of labeled data scarcity in non-English languages. In real-world applications, labeled data is often available primarily in high-resource languages like English but sparse or nonexistent in others <Paper corpusId=\"235097287\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. By translating existing high-quality English benchmarks, researchers can rapidly expand evaluation resources to multiple languages <Paper corpusId=\"276249513\" paperTitle=\"(Gomes et al., 2025)\" isShortName></Paper>.\n\nThe translation approach has proven particularly valuable for specific tasks like information retrieval, where a common problem with multilingual datasets is their low number of judgments per query. Translating densely annotated English benchmarks, such as TREC's Robust 2004, to multiple languages helps mitigate this issue, providing higher-quality evaluation resources <Paper corpusId=\"252568105\" paperTitle=\"(Jeronymo et al., 2022)\" isShortName></Paper>.\n\nAdditionally, translation-based datasets have demonstrated utility in improving multilingual model performance. Researchers have shown that using machine translation as a form of cross-lingual data augmentation can significantly enhance results on multilingual benchmarks <Paper corpusId=\"235097287\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. In retrieval tasks, studies have found a positive correlation between translation quality and retrieval effectiveness, suggesting that improvements in translation methods directly contribute to better multilingual information retrieval performance <Paper corpusId=\"237364084\" paperTitle=\"(Bonifacio et al., 2021)\" isShortName></Paper>.\n\nThe translation approach has been successfully applied across various benchmark types, from natural language inference tasks like XNLI <Paper corpusId=\"269362697\" paperTitle=\"(Moon et al., 2024)\" isShortName></Paper> <Paper corpusId=\"52271711\" paperTitle=\"(Conneau et al., 2018)\" isShortName></Paper> to open-domain question answering evaluations like MKQA, which comprises 10,000 question-answer pairs aligned across 26 typologically diverse languages <Paper corpusId=\"220871404\" paperTitle=\"(Longpre et al., 2020)\" isShortName></Paper>. In multimodal contexts, translated English captions have been widely used to build multilingual datasets for tasks like image retrieval <Paper corpusId=\"237532506\" paperTitle=\"(Aggarwal et al., 2021)\" isShortName></Paper>.", "citations": [{"id": "(Artetxe et al., 2020)", "paper": {"corpus_id": 215548041, "title": "Translation Artifacts in Cross-lingual Transfer Learning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Gorka Labaka", "authorId": "2064469779"}, {"name": "Eneko Agirre", "authorId": "1733049"}], "n_citations": 120}, "snippets": ["While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training", ".Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models."], "score": 0.85693359375}, {"id": "(Thellmann et al., 2024)", "paper": {"corpus_id": 273323209, "title": "Towards Multilingual LLM Evaluation for European Languages", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Klaudia Thellmann", "authorId": "2269539"}, {"name": "Bernhard Stadler", "authorId": "2325726089"}, {"name": "Michael Fromm", "authorId": "2258551638"}, {"name": "Jasper Schulze Buschhoff", "authorId": "2258550838"}, {"name": "Alex Jude", "authorId": "2325726070"}, {"name": "Fabio Barth", "authorId": "2325726142"}, {"name": "Johannes Leveling", "authorId": "2258552277"}, {"name": "Nicolas Flores-Herr", "authorId": "2347259072"}, {"name": "Joachim K\u00f6hler", "authorId": "2330411614"}, {"name": "Ren\u00e9 J\u00e4kel", "authorId": "2330412312"}, {"name": "Mehdi Ali", "authorId": "2258668067"}], "n_citations": 10}, "snippets": ["Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018)."], "score": 0.7265625}, {"id": "(Singh et al., 2024)", "paper": {"corpus_id": 269362363, "title": "IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Harman Singh", "authorId": "2119151340"}, {"name": "Nitish Gupta", "authorId": "2285178"}, {"name": "Shikhar Bharadwaj", "authorId": "2136381352"}, {"name": "Dinesh Tewari", "authorId": "2298270541"}, {"name": "Partha Talukdar", "authorId": "2298271272"}], "n_citations": 28}, "snippets": ["We chose to collect human translations of existing benchmarks as opposed to creating evaluation data from scratch due to various reasons: \n\n\u2022 Translation-based extension of existing benchmark results in multi-way parallel data, allowing researchers to attribute performance due to task knowledge vs. language understanding, and measure cross-lingual generalization"], "score": 0.7060546875}, {"id": "(Bugliarello et al., 2022)", "paper": {"corpus_id": 246294502, "title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Emanuele Bugliarello", "authorId": "83574123"}, {"name": "Fangyu Liu", "authorId": "144097210"}, {"name": "Jonas Pfeiffer", "authorId": "153733568"}, {"name": "Siva Reddy", "authorId": "145732771"}, {"name": "Desmond Elliott", "authorId": "50369944"}, {"name": "E. Ponti", "authorId": "3381663"}, {"name": "Ivan Vulic", "authorId": "1747849"}], "n_citations": 64}, "snippets": ["We also release machine-translated versions of the test sets to enable the evaluation of 'translate test' cross-lingual transfer."], "score": 0.77392578125}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 235097287, "title": "Practical Transformer-based Multilingual Text Classification", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Cindy Wang", "authorId": "2108725400"}, {"name": "Michele Banko", "authorId": "2339397"}], "n_citations": 28}, "snippets": ["In real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019). In single target language settings, we translate training data from other languages into the target language, yielding N times the number of training examples."], "score": 0.70166015625}, {"id": "(Gomes et al., 2025)", "paper": {"corpus_id": 276249513, "title": "Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?", "year": 2025, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Gonccalo Gomes", "authorId": "2282539942"}, {"name": "Chrysoula Zerva", "authorId": "36259430"}, {"name": "Bruno Martins", "authorId": "2344751390"}], "n_citations": 1}, "snippets": ["Using a machine translation scheme with quality filtering, we can cost-effectively extend well-established Englishcentric benchmarks to multiple languages, without compromising benchmark quality and validity, which can be very beneficial for the finetuning and evaluation of new, multilingual evaluation models."], "score": 0.91943359375}, {"id": "(Jeronymo et al., 2022)", "paper": {"corpus_id": 252568105, "title": "mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Vitor Jeronymo", "authorId": "2167031295"}, {"name": "Mauricio Nascimento", "authorId": "2186302660"}, {"name": "R. Lotufo", "authorId": "2066179820"}, {"name": "Rodrigo Nogueira", "authorId": "143744603"}], "n_citations": 3}, "snippets": ["A key ingredient in the development of information retrieval algorithms are reusable evaluation datasets [7,19,20]. For English, there are a number of such datasets. For other languages, there are initiatives such TREC CLIR [17], CLEF [13,[4][5][6], FIRE [12,11], NTCIR [16] and more recently HC4 [9]. A common problem with these multilingual IR datasets is their low number of judgments per query, that is, the number of documents marked as relevant or not relevant per query. For example, in the multilingual datasets mMARCO [3] and Mr.Tydi [21], there is only one or two documents manually marked as relevant per query. These \"sparse\" annotations, we argue, prevent correct evaluations of retrieval methods. For example, the RM3 query expansion method evaluated on the MS MARCO benchmark [1], which uses sparse annotations, shows no improvement over baselines such as BM25 [10]. However, the same method shows significant improvements over BM25 when evaluated on densely annotated benchmarks, such as TREC-DL.\n\nIn this work, to mitigate the issue with sparse annotations on multilingual IR datasets, we translate the TREC's Robust 2004 benchmark [18], an English dataset with a high number of judgments, to 8 languages using a high-quality automatic translator. We call this dataset mRobust04."], "score": 0.80810546875}, {"id": "(Bonifacio et al., 2021)", "paper": {"corpus_id": 237364084, "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset", "year": 2021, "venue": "", "authors": [{"name": "L. Bonifacio", "authorId": "2003019597"}, {"name": "Israel Campiotti", "authorId": "1472908948"}, {"name": "R. Lotufo", "authorId": "1809633"}, {"name": "Rodrigo Nogueira", "authorId": "143744603"}], "n_citations": 113}, "snippets": ["As most languages have none or a limited number of labeled datasets, using automatic translation is an attainable way to derive the same resources for a wider range of languages", ".researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets", ".we show a positive correlation between translation quality and retrieval effectiveness, providing evidence that improvements in translation methods might lead to improvements in multilingual information retrieval."], "score": 0.78759765625}, {"id": "(Moon et al., 2024)", "paper": {"corpus_id": 269362697, "title": "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Hyeonseok Moon", "authorId": "2101318920"}, {"name": "Seungyoon Lee", "authorId": "2275193495"}, {"name": "Seongtae Hong", "authorId": "2298449941"}, {"name": "Seungjun Lee", "authorId": "2144498836"}, {"name": "Chanjun Park", "authorId": "2115195904"}, {"name": "Heu-Jeoung Lim", "authorId": "83056580"}], "n_citations": 0}, "snippets": ["For validation, we select multilingual benchmark tasks in which the maintenance of the interrelation among data components plays a critical role. Specifically, we adopt the XNLI dataset (Conneau et al., 2018) and select two tasks in an XGLUE benchmark (Liang et al., 2020): Web Page Ranking (WPR) and Question Generation (QG). We construct training data for up to five languages (German, French, Chinese, Hindi, and Vietnamese) by translating the English dataset existing within each dataset. Subsequently, by evaluating the performance of the models trained on each translated data, we estimate the validity of each data translation strategy."], "score": 0.7421875}, {"id": "(Conneau et al., 2018)", "paper": {"corpus_id": 52271711, "title": "XNLI: Evaluating Cross-lingual Sentence Representations", "year": 2018, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Alexis Conneau", "authorId": "2480903"}, {"name": "Guillaume Lample", "authorId": "1830914"}, {"name": "Ruty Rinott", "authorId": "1905713"}, {"name": "Adina Williams", "authorId": "81840293"}, {"name": "Samuel R. Bowman", "authorId": "3644767"}, {"name": "Holger Schwenk", "authorId": "144518416"}, {"name": "Veselin Stoyanov", "authorId": "1759422"}], "n_citations": 1388}, "snippets": ["State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines."], "score": 0.0}, {"id": "(Longpre et al., 2020)", "paper": {"corpus_id": 220871404, "title": "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering", "year": 2020, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "S. Longpre", "authorId": "29909347"}, {"name": "Yi Lu", "authorId": "2143774328"}, {"name": "Joachim Daiber", "authorId": "1696803"}], "n_citations": 160}, "snippets": ["Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages."], "score": 0.8408203125}, {"id": "(Aggarwal et al., 2021)", "paper": {"corpus_id": 237532506, "title": "Towards Zero-shot Cross-lingual Image Retrieval and Tagging", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Pranav Aggarwal", "authorId": "30141435"}, {"name": "Ritiz Tambi", "authorId": "51251519"}, {"name": "Ajinkya Kale", "authorId": "37493415"}], "n_citations": 6}, "snippets": ["Multi-lingual dataset collection has always been a major hurdle when it comes to building models in a one-model-fits-all style that can provide good results for image retrieval across multiple languages. Most methods [22,28](Rotman et al., 2018) rely on direct translations of English captions while others (Gella et al., 2017)(Huang et al., 2019) have used independent image and language text pairs."], "score": 0.7958984375}], "table": null}, {"title": "Challenges in Building Translation-Based Datasets", "tldr": "Translation-based multilingual datasets face challenges including quality issues from machine translation errors, cultural biases that compromise authenticity, and difficulties transferring labels for specific NLP tasks, all of which can affect evaluation reliability. (12 sources)", "text": "\nDespite its advantages, creating multilingual evaluation datasets through translation introduces several significant challenges. One fundamental concern is the quality and naturalness of the translated content. Translation can introduce artifacts such as reduced fluency, unnatural phrasing, and \"translationese\" that may inadvertently affect model performance and evaluation reliability <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>. When machine translation is used, the quality heavily depends on the capabilities of the language model employed, with particular concerns for medium and low-resource languages where translation accuracy may be compromised <Paper corpusId=\"273323209\" paperTitle=\"(Thellmann et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259095896\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nThe translation process often fails to capture cultural nuances and language-specific expressions, potentially overlooking subtle linguistic or cultural differences <Paper corpusId=\"268357672\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. As Kim et al. demonstrate, language models often fail to capture biases in different languages due to cultural insensitivity <Paper corpusId=\"268357672\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. This limitation is particularly problematic as research has shown that integrating cultural knowledge can significantly improve model performance <Paper corpusId=\"268357672\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247762223\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>.\n\nTranslation-based approaches tend to bend the target language to match the structural and lexical properties of the source language, compromising the naturalness and idiomaticity of the resulting text <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. This issue can lead to evaluation benchmarks that fail to adequately measure system performance in the target language <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. Even when human translators are employed, achieving high-quality translations that maintain both cross-lingual alignment and linguistic authenticity requires careful selection of translators and allowing for necessary target-language adjustments <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>.\n\nAnother significant challenge lies in transferring labels or annotations across languages. While translation works well for some tasks, it can be problematic for others, particularly structure-dependent tasks like part-of-speech tagging and named entity recognition <Paper corpusId=\"248780386\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. Specific linguistic features of target languages, such as pronoun-dropping, compounding, and inflection, can degrade dataset quality and model performance <Paper corpusId=\"258557740\" paperTitle=\"(Hennig et al., 2023)\" isShortName></Paper>.\n\nFor certain tasks, finding naturally parallel documents is essential to avoid the unnatural quality that can result from manual translation at scale <Paper corpusId=\"204734128\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper>. However, such naturally occurring parallel content may not be available for all language pairs or domains.\n\nThe reliability of translation-based evaluation can be further compromised when models are trained on translated data, potentially leading to artificially inflated performance metrics <Paper corpusId=\"248780386\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. Xu et al. note that translated benchmarks may introduce additional biases due to translation errors and cultural differences <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"135465247\" paperTitle=\"(Lauscher et al., 2019)\" isShortName></Paper>.\n\nThese challenges highlight that naively translating existing evaluation benchmarks can introduce artifacts that affect judgment and overlook multilingual-specific nuances such as code-switching or unique linguistic patterns <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>. Recent research suggests that translation-based methods may generate less natural, less representative target language content, which can affect the validity of evaluations <Paper corpusId=\"273532366\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nDespite these challenges, studies have shown that machine translation can be a viable strategy for transferring certain tasks across languages, with native speakers judging more than 83% of machine-translated relation extraction instances to be linguistically and semantically acceptable <Paper corpusId=\"258557740\" paperTitle=\"(Hennig et al., 2023)\" isShortName></Paper>. However, achieving this level of quality requires careful quality control measures and an awareness of the specific linguistic features that may affect translation success.", "citations": [{"id": "(Son et al., 2024)", "paper": {"corpus_id": 273532388, "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Guijin Son", "authorId": "2321452295"}, {"name": "Dongkeun Yoon", "authorId": "29830817"}, {"name": "Juyoung Suk", "authorId": "2299329316"}, {"name": "Javier Aula-Blasco", "authorId": "2301578911"}, {"name": "Mano Aslan", "authorId": "2327215494"}, {"name": "Vu Trong Kim", "authorId": "2327216625"}, {"name": "Shayekh Bin Islam", "authorId": "2232783785"}, {"name": "Jaume Prats-Cristi\u00e0", "authorId": "2327215436"}, {"name": "Luc\u00eda Tormo-Ba\u00f1uelos", "authorId": "2327217057"}, {"name": "Seungone Kim", "authorId": "2184037220"}], "n_citations": 13}, "snippets": ["However, we find that naive translation of existing meta-evaluation benchmarks can (i) introduce translation artifacts (mistranslation, degraded fluency, \"translationese\", etc.) that inadvertently affect the judgement of evaluator LLMs, and (ii) overlook key multilingual-specific nuances such as code-switching, or linguistic patterns. Together, these issues underscore the importance of careful, multilingual-tailored design rather than naive machine translation. Accordingly, we propose the following design choices to ensure reliable and truly multilingual meta-evaluation: \n\n\u2022 Controlled Translation. Machine Translation of entire preference datasets should be avoided, as artifacts can introduce subtle errors and invalidate original labels. \n\n\u2022 Thorough Quality Checks. Each entry (prompt-chosen-rejected) triplet should undergo careful validation to ensure a clear while non-trivial distinction between chosen and rejected responses. Subsets that do not meet this standard should be discarded. \n\n\u2022 Objectively Correct or Human-Preferred. Whenever possible, the \"chosen\" response should be objectively (or verifiably) correct or reflect existing human judgments."], "score": 0.72900390625}, {"id": "(Thellmann et al., 2024)", "paper": {"corpus_id": 273323209, "title": "Towards Multilingual LLM Evaluation for European Languages", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Klaudia Thellmann", "authorId": "2269539"}, {"name": "Bernhard Stadler", "authorId": "2325726089"}, {"name": "Michael Fromm", "authorId": "2258551638"}, {"name": "Jasper Schulze Buschhoff", "authorId": "2258550838"}, {"name": "Alex Jude", "authorId": "2325726070"}, {"name": "Fabio Barth", "authorId": "2325726142"}, {"name": "Johannes Leveling", "authorId": "2258552277"}, {"name": "Nicolas Flores-Herr", "authorId": "2347259072"}, {"name": "Joachim K\u00f6hler", "authorId": "2330411614"}, {"name": "Ren\u00e9 J\u00e4kel", "authorId": "2330412312"}, {"name": "Mehdi Ali", "authorId": "2258668067"}], "n_citations": 10}, "snippets": ["Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018)."], "score": 0.7265625}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 259095896, "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Lei Li", "authorId": "49192881"}, {"name": "Yuwei Yin", "authorId": "2109472880"}, {"name": "Shicheng Li", "authorId": "2117947438"}, {"name": "Liang Chen", "authorId": "2146034504"}, {"name": "Peiyi Wang", "authorId": "144202874"}, {"name": "Shuhuai Ren", "authorId": "1906099"}, {"name": "Mukai Li", "authorId": "2027599235"}, {"name": "Yazheng Yang", "authorId": "14646700"}, {"name": "Jingjing Xu", "authorId": "47883405"}, {"name": "Xu Sun", "authorId": "2116530295"}, {"name": "Lingpeng Kong", "authorId": "47648549"}, {"name": "Qi Liu", "authorId": "2144831944"}], "n_citations": 120}, "snippets": ["To boost the language diversity and support the evaluation across different languages, we select a subset of datasets (OK-VQA, ImageNet, Winoground, VQAv2, VIST, MSRVTT and MSRVTT-QA) that covers different tasks and translate their evaluation data into 100 languages following FLORES-101 [13]. We translate 500 samples for each split of each task in our first version. More multilingual samples will be supported in the future. We adopt the distillation version NLLB-1.3B [6] for translation, one of the state-of-the-art open multilingual translation models. As there are no native speakers for different languages, we adopt an automatic filtering mechanism to ensure the translation quality, where languages with translation BLEU scores from English larger than 20 based on FLORES-101 results are kept. After this step, only 80 languages are kept (see Appendix for detailed language names)."], "score": 0.74072265625}, {"id": "(Kim et al., 2024)", "paper": {"corpus_id": 268357672, "title": "CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean", "year": 2024, "venue": "International Conference on Language Resources and Evaluation", "authors": [{"name": "Eunsu Kim", "authorId": "2283821804"}, {"name": "Juyoung Suk", "authorId": "2299329316"}, {"name": "Philhoon Oh", "authorId": "2262214736"}, {"name": "Haneul Yoo", "authorId": "47111345"}, {"name": "James Thorne", "authorId": "2262214700"}, {"name": "Alice Oh", "authorId": "2283768962"}], "n_citations": 23}, "snippets": ["Consequently, certain datasets incorporate or employ translated portions from English datasets (Seo et al., 2022), potentially overlooking subtle linguistic or cultural differences that may not be apparent to the audience (Tandon et al., 2018).Lee et al. 2023a demonstrated that language models fail to capture biases in different languages due to their cultural insensitivity, which can have societal impacts (Tamkin et al., 2021).Furthermore, (Ma et al., 2022) emphasized the importance of cultural background and showed that integrating cultural knowledge can improve models performance.These findings illustrate the need for cultural evaluation datasets.However, building a cultural evaluation dataset from scratch is challenging since it entails significant time and resources while relying on translated datasets fails to incorporate cultural knowledge in different languages."], "score": 0.7412109375}, {"id": "(Ma et al., 2022)", "paper": {"corpus_id": 247762223, "title": "EnCBP: A New Benchmark Dataset for Finer-Grained Cultural Background Prediction in English", "year": 2022, "venue": "Findings", "authors": [{"name": "Weicheng Ma", "authorId": "2227771"}, {"name": "Samiha Datta", "authorId": "2160540667"}, {"name": "Lili Wang", "authorId": "2117930921"}, {"name": "Soroush Vosoughi", "authorId": "1918441"}], "n_citations": 8}, "snippets": ["While cultural backgrounds have been shown to affect linguistic expressions, existing natural language processing (NLP) research on culture modeling is overly coarse-grained and does not examine cultural differences among speakers of the same language. To address this problem and augment NLP models with cultural background features, we collect, annotate, manually validate, and benchmark EnCBP, a finer-grained news-based cultural background prediction dataset in English. Through language modeling (LM) evaluations and manual analyses, we confirm that there are noticeable differences in linguistic expressions among five English-speaking countries and across four states in the US. Additionally, our evaluations on nine syntactic (CoNLL-2003), semantic (PAWS-Wiki, QNLI, STS-B, and RTE), and psycholinguistic tasks (SST-5, SST-2, Emotion, and Go-Emotions) show that, while introducing cultural background information does not benefit the Go-Emotions task due to text domain conflicts, it noticeably improves deep learning (DL) model performance on other tasks. Our findings strongly support the importance of cultural background modeling to a wide variety of NLP tasks and demonstrate the applicability of EnCBP in culture-related research."], "score": 0.0}, {"id": "(Ponti et al., 2020)", "paper": {"corpus_id": 218470125, "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "E. Ponti", "authorId": "3381663"}, {"name": "Goran Glavavs", "authorId": "1666177566"}, {"name": "Olga Majewska", "authorId": "46963731"}, {"name": "Qianchu Liu", "authorId": "50383712"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "A. Korhonen", "authorId": "145762466"}], "n_citations": 327}, "snippets": ["Design Objectives. The principal objectives in XCOPA creation were: 1) to create a genuinely typologically diverse multilingual dataset, aligned across target languages in order to make performance scores comparable, and 2) to ensure high quality, naturalness and idiomacity of each monolingual dataset. While the commonly used translation approach addresses the former objective, it is prone to compromise the latter goal, bending the target language to the structural and lexical properties of the source language: the resulting evaluation benchmarks thus fail to measure system performance adequately (Koppel and Ordan, 2011;Volansky et al., 2015;Artetxe et al., 2020a). \n\nIn order to avoid these pitfalls, we: (i) entrusted the translation task to a single (but carefully selected) translator for each target language, and (ii) offered enough leeway for necessary target-language adjustments (e.g., substitutions with culture-specific concepts and multi-word paraphrases, wherever the original text eluded direct translation)."], "score": 0.83740234375}, {"id": "(Ahuja et al., 2022)", "paper": {"corpus_id": 248780386, "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages", "year": 2022, "venue": "NLPPOWER", "authors": [{"name": "Kabir Ahuja", "authorId": "52154863"}, {"name": "Sandipan Dandapat", "authorId": "34725175"}, {"name": "Sunayana Sitaram", "authorId": "3010457"}, {"name": "M. Choudhury", "authorId": "143990839"}], "n_citations": 16}, "snippets": ["Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition."], "score": 0.765625}, {"id": "(Hennig et al., 2023)", "paper": {"corpus_id": 258557740, "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Leonhard Hennig", "authorId": "36943315"}, {"name": "Philippe E. Thomas", "authorId": "143680610"}, {"name": "Sebastian M\u00f6ller", "authorId": "145733288"}], "n_citations": 8}, "snippets": ["We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance."], "score": 0.7333984375}, {"id": "(Lewis et al., 2019)", "paper": {"corpus_id": 204734128, "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Ruty Rinott", "authorId": "1905713"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Holger Schwenk", "authorId": "144518416"}], "n_citations": 509}, "snippets": ["Parallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation."], "score": 0.90185546875}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 268819377, "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias", "year": 2024, "venue": "Frontiers Comput. Sci.", "authors": [{"name": "Yuemei Xu", "authorId": "2257136845"}, {"name": "Ling Hu", "authorId": "2258334185"}, {"name": "Jiayi Zhao", "authorId": "2294513520"}, {"name": "Zihan Qiu", "authorId": "2294361104"}, {"name": "Yuqi Ye", "authorId": "2294363807"}, {"name": "Hanwen Gu", "authorId": "2294933103"}], "n_citations": 43}, "snippets": ["The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English (Lauscher et al., 2019), [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences."], "score": 0.82080078125}, {"id": "(Lauscher et al., 2019)", "paper": {"corpus_id": 135465247, "title": "Are We Consistently Biased? Multidimensional Analysis of Biases in Distributional Word Vectors", "year": 2019, "venue": "International Workshop on Semantic Evaluation", "authors": [{"name": "Anne Lauscher", "authorId": "29891652"}, {"name": "Goran Glavas", "authorId": "2472657"}], "n_citations": 55}, "snippets": ["Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques."], "score": 0.0}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 273532366, "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Junhua Liu", "authorId": "2327246234"}, {"name": "Bin Fu", "authorId": "2325881142"}], "n_citations": 1}, "snippets": ["In general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation."], "score": 0.90771484375}], "table": null}, {"title": "Quality and Reliability Issues", "tldr": "Translation-based multilingual datasets face significant reliability challenges including translation artifacts that affect model evaluations, cultural bias that compromises authenticity, and overestimation of model performance when both training and evaluation use translated content. (13 sources)", "text": "\nThe reliability of translation-based multilingual evaluation datasets is compromised by several interrelated issues. A primary concern is the presence of translation artifacts\u2014including reduced fluency, unnatural phrasing, and \"translationese\"\u2014that inadvertently affect the judgment of evaluator models <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>. These artifacts can introduce subtle errors that may invalidate original labels and distort evaluation outcomes. Recent research demonstrates that naively translating existing meta-evaluation benchmarks can introduce translation artifacts that affect judgment and overlook key multilingual-specific nuances such as code-switching or linguistic patterns <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>.\n\nCultural bias presents another significant challenge for translation-based approaches. Many multilingual benchmarks are translated from English, introducing biases related to translations and cultural representation that affect experimental conclusions by reflecting the origin culture <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper>. This issue is particularly relevant as language models often fail to capture biases in different languages due to their cultural insensitivity <Paper corpusId=\"268357672\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265445838\" paperTitle=\"(Tao et al., 2023)\" isShortName></Paper>. Cultural biases in multilingual datasets present substantial obstacles regarding their scope, validity, and reliability as global benchmarks, and these biases are not mitigated if benchmarks are automatically translated from English into other languages <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.\n\nThe translation process often compromises the naturalness and idiomaticity of the target language text, bending it to match the structural and lexical properties of the source language <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. This results in evaluation benchmarks that fail to adequately measure system performance in the target language. Translation quality heavily depends on the capability of the language model used, with particular concerns for medium and low-resource languages where translation accuracy may be compromised <Paper corpusId=\"273323209\" paperTitle=\"(Thellmann et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246680398\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>.\n\nComparative studies reveal that simply translating English benchmarks proves insufficient for robust evaluation. Localized benchmarks show substantially higher correlation with human judgments (0.68) than translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>. Research has also found that using local versus English-translated data can result in differences of more than 20 percentage points for the best-performing models, drastically changing the estimated difficulty of some languages <Paper corpusId=\"277780370\" paperTitle=\"(Hupkes et al., 2025)\" isShortName></Paper> <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper>.\n\nThese reliability issues may also lead to misleading evaluation results. Translated datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions <Paper corpusId=\"258865558\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper> <Paper corpusId=\"216914383\" paperTitle=\"(Artetxe et al._1, 2020)\" isShortName></Paper>. Researchers highlight that when models are trained on translated data and then evaluated on translated benchmarks, their performance may be artificially inflated, giving a distorted picture of their actual multilingual capabilities.\n\nTo address these quality and reliability concerns, researchers recommend controlled translation approaches that avoid machine translation of entire preference datasets <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>, thorough quality checks by native speakers, and development of benchmarks based on regional resources rather than translating benchmarks with inherent cultural bias <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>. For instance, the Include benchmark was created based on local exam sources instead of translating benchmarks with inherent cultural bias <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Son et al., 2024)", "paper": {"corpus_id": 273532388, "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Guijin Son", "authorId": "2321452295"}, {"name": "Dongkeun Yoon", "authorId": "29830817"}, {"name": "Juyoung Suk", "authorId": "2299329316"}, {"name": "Javier Aula-Blasco", "authorId": "2301578911"}, {"name": "Mano Aslan", "authorId": "2327215494"}, {"name": "Vu Trong Kim", "authorId": "2327216625"}, {"name": "Shayekh Bin Islam", "authorId": "2232783785"}, {"name": "Jaume Prats-Cristi\u00e0", "authorId": "2327215436"}, {"name": "Luc\u00eda Tormo-Ba\u00f1uelos", "authorId": "2327217057"}, {"name": "Seungone Kim", "authorId": "2184037220"}], "n_citations": 13}, "snippets": ["However, we find that naive translation of existing meta-evaluation benchmarks can (i) introduce translation artifacts (mistranslation, degraded fluency, \"translationese\", etc.) that inadvertently affect the judgement of evaluator LLMs, and (ii) overlook key multilingual-specific nuances such as code-switching, or linguistic patterns. Together, these issues underscore the importance of careful, multilingual-tailored design rather than naive machine translation. Accordingly, we propose the following design choices to ensure reliable and truly multilingual meta-evaluation: \n\n\u2022 Controlled Translation. Machine Translation of entire preference datasets should be avoided, as artifacts can introduce subtle errors and invalidate original labels. \n\n\u2022 Thorough Quality Checks. Each entry (prompt-chosen-rejected) triplet should undergo careful validation to ensure a clear while non-trivial distinction between chosen and rejected responses. Subsets that do not meet this standard should be discarded. \n\n\u2022 Objectively Correct or Human-Preferred. Whenever possible, the \"chosen\" response should be objectively (or verifiably) correct or reflect existing human judgments."], "score": 0.72900390625}, {"id": "(Etxaniz et al., 2024)", "paper": {"corpus_id": 270380088, "title": "BertaQA: How Much Do Language Models Know About Local Culture?", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Julen Etxaniz", "authorId": "2226458991"}, {"name": "Gorka Azkune", "authorId": "2481918"}, {"name": "A. Soroa", "authorId": "2260104163"}, {"name": "Oier L\u00f3pez de Lacalle", "authorId": "2251043402"}, {"name": "Mikel Artetxe", "authorId": "2347956"}], "n_citations": 11}, "snippets": ["Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture."], "score": 0.8037109375}, {"id": "(Kim et al., 2024)", "paper": {"corpus_id": 268357672, "title": "CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean", "year": 2024, "venue": "International Conference on Language Resources and Evaluation", "authors": [{"name": "Eunsu Kim", "authorId": "2283821804"}, {"name": "Juyoung Suk", "authorId": "2299329316"}, {"name": "Philhoon Oh", "authorId": "2262214736"}, {"name": "Haneul Yoo", "authorId": "47111345"}, {"name": "James Thorne", "authorId": "2262214700"}, {"name": "Alice Oh", "authorId": "2283768962"}], "n_citations": 23}, "snippets": ["Consequently, certain datasets incorporate or employ translated portions from English datasets (Seo et al., 2022), potentially overlooking subtle linguistic or cultural differences that may not be apparent to the audience (Tandon et al., 2018).Lee et al. 2023a demonstrated that language models fail to capture biases in different languages due to their cultural insensitivity, which can have societal impacts (Tamkin et al., 2021).Furthermore, (Ma et al., 2022) emphasized the importance of cultural background and showed that integrating cultural knowledge can improve models performance.These findings illustrate the need for cultural evaluation datasets.However, building a cultural evaluation dataset from scratch is challenging since it entails significant time and resources while relying on translated datasets fails to incorporate cultural knowledge in different languages."], "score": 0.7412109375}, {"id": "(Tao et al., 2023)", "paper": {"corpus_id": 265445838, "title": "Cultural bias and cultural alignment of large language models", "year": 2023, "venue": "PNAS Nexus", "authors": [{"name": "Yan Tao", "authorId": "2105711243"}, {"name": "Olga Viberg", "authorId": "2269470454"}, {"name": "Ryan S. Baker", "authorId": "2255317189"}, {"name": "Ren\u00e9 F. Kizilcec", "authorId": "2246853606"}], "n_citations": 87}, "snippets": ["Abstract Culture fundamentally shapes people\u2019s reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people\u2019s authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI\u2019s GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models\u2019 responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models\u2019 output for 71\u201381% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI."], "score": 0.0}, {"id": "(Barth et al., 2025)", "paper": {"corpus_id": 276421738, "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Fabio Barth", "authorId": "2325726142"}, {"name": "Georg Rehm", "authorId": "2302558975"}], "n_citations": 0}, "snippets": ["Most benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2023). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations."], "score": 0.77392578125}, {"id": "(Ponti et al., 2020)", "paper": {"corpus_id": 218470125, "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "E. Ponti", "authorId": "3381663"}, {"name": "Goran Glavavs", "authorId": "1666177566"}, {"name": "Olga Majewska", "authorId": "46963731"}, {"name": "Qianchu Liu", "authorId": "50383712"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "A. Korhonen", "authorId": "145762466"}], "n_citations": 327}, "snippets": ["Design Objectives. The principal objectives in XCOPA creation were: 1) to create a genuinely typologically diverse multilingual dataset, aligned across target languages in order to make performance scores comparable, and 2) to ensure high quality, naturalness and idiomacity of each monolingual dataset. While the commonly used translation approach addresses the former objective, it is prone to compromise the latter goal, bending the target language to the structural and lexical properties of the source language: the resulting evaluation benchmarks thus fail to measure system performance adequately (Koppel and Ordan, 2011;Volansky et al., 2015;Artetxe et al., 2020a). \n\nIn order to avoid these pitfalls, we: (i) entrusted the translation task to a single (but carefully selected) translator for each target language, and (ii) offered enough leeway for necessary target-language adjustments (e.g., substitutions with culture-specific concepts and multi-word paraphrases, wherever the original text eluded direct translation)."], "score": 0.83740234375}, {"id": "(Thellmann et al., 2024)", "paper": {"corpus_id": 273323209, "title": "Towards Multilingual LLM Evaluation for European Languages", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Klaudia Thellmann", "authorId": "2269539"}, {"name": "Bernhard Stadler", "authorId": "2325726089"}, {"name": "Michael Fromm", "authorId": "2258551638"}, {"name": "Jasper Schulze Buschhoff", "authorId": "2258550838"}, {"name": "Alex Jude", "authorId": "2325726070"}, {"name": "Fabio Barth", "authorId": "2325726142"}, {"name": "Johannes Leveling", "authorId": "2258552277"}, {"name": "Nicolas Flores-Herr", "authorId": "2347259072"}, {"name": "Joachim K\u00f6hler", "authorId": "2330411614"}, {"name": "Ren\u00e9 J\u00e4kel", "authorId": "2330412312"}, {"name": "Mehdi Ali", "authorId": "2258668067"}], "n_citations": 10}, "snippets": ["Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018)."], "score": 0.7265625}, {"id": "(Meng et al., 2022)", "paper": {"corpus_id": 246680398, "title": "Generating Training Data with Language Models: Towards Zero-Shot Language Understanding", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yu Meng", "authorId": "145391513"}, {"name": "Jiaxin Huang", "authorId": "3488341"}, {"name": "Yu Zhang", "authorId": "49891156"}, {"name": "Jiawei Han", "authorId": "2111759643"}], "n_citations": 235}, "snippets": ["Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class."], "score": 0.0}, {"id": "(Wu et al., 2025)", "paper": {"corpus_id": 277993848, "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Minghao Wu", "authorId": "2327995656"}, {"name": "Weixuan Wang", "authorId": "2258789284"}, {"name": "Sinuo Liu", "authorId": "2349802912"}, {"name": "Huifeng Yin", "authorId": "2331767275"}, {"name": "Xintong Wang", "authorId": "2356794252"}, {"name": "Yu Zhao", "authorId": "2331686159"}, {"name": "Chenyang Lyu", "authorId": "2266387313"}, {"name": "Longyue Wang", "authorId": "2302342302"}, {"name": "Weihua Luo", "authorId": "2305289815"}, {"name": "Kaifu Zhang", "authorId": "2304530663"}], "n_citations": 5}, "snippets": ["Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)", "when comparing benchmark performance with human judgments, STEM-related tasks (ARC and MGSM) show consistently stronger correlations across languages (0.70-0.85), while other tasks like XQuAD correlate poorly (0.11-0.30). Importantly, simply translating English benchmarks proves insufficient for robust evaluation, localized benchmarks (like CMMLU for Chinese) show substantially higher correlation with human judgments (0.68) than translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources."], "score": 0.82080078125}, {"id": "(Hupkes et al., 2025)", "paper": {"corpus_id": 277780370, "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Dieuwke Hupkes", "authorId": "3449411"}, {"name": "Nikolay Bogoychev", "authorId": "3444222"}], "n_citations": 0}, "snippets": ["Translation as a proxy for multilinguality Another, more implicit method to assess multilinguality in LLMs is to evaluate their ability to translate from one language to another. This approach was famously used by (Brown et al., 2020), but has not been common since", "Lastly, we find that using local vs English-translated data can result in differences more than 20 points for the best performing models, drastically change the estimated difficulty of some languages. For using machines instead of human translations, we find a weaker effect on ordering of language difficulty, a larger difference in model rankings, and a substantial drop in estimated performance for all models."], "score": 0.72900390625}, {"id": "(Brown et al., 2020)", "paper": {"corpus_id": 218971783, "title": "Language Models are Few-Shot Learners", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tom B. Brown", "authorId": "31035595"}, {"name": "Benjamin Mann", "authorId": "2056658938"}, {"name": "Nick Ryder", "authorId": "39849748"}, {"name": "Melanie Subbiah", "authorId": "2065894334"}, {"name": "J. Kaplan", "authorId": "152724169"}, {"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Arvind Neelakantan", "authorId": "2072676"}, {"name": "Pranav Shyam", "authorId": "67311962"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Ariel Herbert-Voss", "authorId": "1404060687"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "T. Henighan", "authorId": "103143311"}, {"name": "R. Child", "authorId": "48422824"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Daniel M. Ziegler", "authorId": "2052152920"}, {"name": "Jeff Wu", "authorId": "49387725"}, {"name": "Clemens Winter", "authorId": "2059411355"}, {"name": "Christopher Hesse", "authorId": "144239765"}, {"name": "Mark Chen", "authorId": "2108828435"}, {"name": "Eric Sigler", "authorId": "2064673055"}, {"name": "Ma-teusz Litwin", "authorId": "1380985420"}, {"name": "Scott Gray", "authorId": "145565184"}, {"name": "Benjamin Chess", "authorId": "1490681878"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Christopher Berner", "authorId": "133740015"}, {"name": "Sam McCandlish", "authorId": "52238703"}, {"name": "Alec Radford", "authorId": "38909097"}, {"name": "I. Sutskever", "authorId": "1701686"}, {"name": "Dario Amodei", "authorId": "2698777"}], "n_citations": 42437}, "snippets": ["Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."], "score": 0.0}, {"id": "(Asai et al., 2023)", "paper": {"corpus_id": 258865558, "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Akari Asai", "authorId": "35584853"}, {"name": "Sneha Kudugunta", "authorId": "35871436"}, {"name": "Xinyan Velocity Yu", "authorId": "2118211280"}, {"name": "Terra Blevins", "authorId": "3443287"}, {"name": "Hila Gonen", "authorId": "1821892"}, {"name": "Machel Reid", "authorId": "1557386977"}, {"name": "Yulia Tsvetkov", "authorId": "2073587169"}, {"name": "Sebastian Ruder", "authorId": "2884561"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}], "n_citations": 62}, "snippets": ["Beyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;(Ponti et al., 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;(Artetxe et al., 2020). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation)."], "score": 0.70166015625}, {"id": "(Artetxe et al._1, 2020)", "paper": {"corpus_id": 216914383, "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Sebastian Ruder", "authorId": "2884561"}, {"name": "Dani Yogatama", "authorId": "1755465"}, {"name": "Gorka Labaka", "authorId": "3255091"}, {"name": "Eneko Agirre", "authorId": "1733049"}], "n_citations": 72}, "snippets": ["We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world\u2019s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models."], "score": 0.0}], "table": null}, {"title": "Alternative Approaches", "tldr": "To overcome translation limitations, researchers have developed multilingual datasets through native content collection, expert-created benchmarks tailored to specific cultures, and compilation of similar tasks across languages, providing more authentic evaluation resources. (10 sources)", "text": "\nRather than relying on translation-based approaches, researchers have developed several alternative methods for creating multilingual evaluation datasets:\n\n1. **Native language collection and annotation** - Datasets like TyDi QA collect and annotate data in multiple languages separately, resulting in non-parallel datasets that better represent linguistic characteristics native to each language <Paper corpusId=\"270285718\" paperTitle=\"(Sakai et al., 2024)\" isShortName></Paper>. This approach addresses the concerns about unnaturalness found in translated content by using documents that are authentically written in target languages <Paper corpusId=\"204734128\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper>.\n\n2. **Expert-created benchmarks for specific languages/cultures** - Recent efforts have focused on developing benchmarks based on regional resources rather than translating benchmarks with inherent cultural bias. The Include benchmark was created using local exam sources to maintain cultural authenticity <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>. Similarly, the E\u03c7\u03b1\u00b5s dataset was collected from multilingual high school examinations where questions were written by educational experts rather than non-expert annotators, making evaluation results comparable to a top-performing student <Paper corpusId=\"226262367\" paperTitle=\"(Hardalov et al., 2020)\" isShortName></Paper>.\n\n3. **Human-annotated multilingual datasets from scratch** - Though time and resource-intensive, creating multilingual datasets with human annotators of multiple languages can achieve higher quality and cultural relevance compared to translation approaches <Paper corpusId=\"273323209\" paperTitle=\"(Thellmann et al., 2024)\" isShortName></Paper>.\n\n4. **Task-specific collection across languages** - Researchers have collected similar tasks across multiple languages to create comprehensive benchmarks like XTREME and XGLUE <Paper corpusId=\"270285718\" paperTitle=\"(Sakai et al., 2024)\" isShortName></Paper>. This approach ensures task consistency while maintaining language authenticity.\n\n5. **Culturally-sensitive multilingual benchmarks** - Some benchmarks engage professional and community annotators to label samples as culturally-sensitive or culturally-agnostic, improving the quality of multilingual evaluation <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper> <Paper corpusId=\"265445838\" paperTitle=\"(Tao et al., 2023)\" isShortName></Paper>.\n\n6. **National exam questions in target languages** - Inspired by Hendrycks et al.'s approach with MMLU <Paper corpusId=\"221516475\" paperTitle=\"(Hendrycks et al., 2020)\" isShortName></Paper>, researchers have collected human exam questions from specific regions in target languages, such as IndoMMLU for Indonesian <Paper corpusId=\"263829211\" paperTitle=\"(Koto et al., 2023)\" isShortName></Paper> and TurkishMMLU with over 10,000 questions covering 9 different subjects from Turkish high-school curricula <Paper corpusId=\"271245129\" paperTitle=\"(Yuksel et al., 2024)\" isShortName></Paper>.\n\n7. **Parallel document discovery** - Rather than manually translating documents, some researchers find naturally-parallel documents across languages, providing high-quality content without requiring manual translation <Paper corpusId=\"204734128\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper>.\n\nResearch comparing these approaches with translation-based methods shows that localized benchmarks demonstrate substantially higher correlation with human judgments (0.68) compared to translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Sakai et al., 2024)", "paper": {"corpus_id": 270285718, "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yusuke Sakai", "authorId": "2204051777"}, {"name": "Hidetaka Kamigaito", "authorId": "2300756"}, {"name": "Taro Watanabe", "authorId": "2266807418"}], "n_citations": 5}, "snippets": ["Multilingual datasets When focusing on the evaluation of multilingual performance of LMs,  the evaluation datasets are almost exclusively created through three methods, as shown in Table 2:\n\n(1) Translation from existing datasets in a major language, e.g., English (Lin et al., 2021;Ponti et al., 2020;Conneau et al., 2018;Artetxe et al., 2020;Yang et al., 2019); (2) Compilation of similar tasks across multiple languages (Zhang et al., 2023c;Hu et al., 2023;Adelani et al., 2022;Roy et al., 2020;Malmasi and Dras, 2015); (3) Creation from multilingual resources following the same dataset creation process (Keung et al., 2020;Huang et al., 2020;Buchholz and Marsi, 2006;Clark et al., 2020;Schwenk and Li, 2018;Kabra et al., 2023)."], "score": 0.7333984375}, {"id": "(Lewis et al., 2019)", "paper": {"corpus_id": 204734128, "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Ruty Rinott", "authorId": "1905713"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Holger Schwenk", "authorId": "144518416"}], "n_citations": 509}, "snippets": ["Parallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation."], "score": 0.90185546875}, {"id": "(Barth et al., 2025)", "paper": {"corpus_id": 276421738, "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Fabio Barth", "authorId": "2325726142"}, {"name": "Georg Rehm", "authorId": "2302558975"}], "n_citations": 0}, "snippets": ["Most benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2023). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations."], "score": 0.77392578125}, {"id": "(Hardalov et al., 2020)", "paper": {"corpus_id": 226262367, "title": "EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Momchil Hardalov", "authorId": "3255454"}, {"name": "Todor Mihaylov", "authorId": "39980906"}, {"name": "Dimitrina Zlatkova", "authorId": "46190940"}, {"name": "Yoan Dinkov", "authorId": "1379925776"}, {"name": "Ivan Koychev", "authorId": "52553663"}, {"name": "Preslav Nakov", "authorId": "1683562"}], "n_citations": 54}, "snippets": ["We advance the task of science Question Answering (QA) with multilingual and crosslingual evaluations. \u2022 We collect a new challenging dataset E\u03c7\u03b1\u00b5s from multilingual high school examinations, which offers several advantages over existing datasets: (i) it covers various domains, (ii) it is nearly three times larger than pre-existing Science QA datasets, (iii) it extends multilingual QA tasks to more languages, (iv) the questions are written by experts, rather than translated or crowdsourced, (v) the questions are harder since they are from matriculation exams rather than 4-8th grade.\n\nRecently, some multilingual datasets, were released to the public. MLQA (Lewis et al., 2020), and XQuAD (Artetxe et al., 2020) use translations by professionals and extend the monolingual SQuAD (Rajpurkar et al., 2016) to 7 and 11 languages, respectively, thus forming cross-lingual evaluation benchmarks", "E\u03c7\u03b1\u00b5s differs from the aforementioned multilingual benchmarks in several aspects. First, we extend the multilingual QA efforts to a different, more challenging domain (Clark et al., 2018). Second, our datasets support more languages. Next, the questions in E\u03c7\u03b1\u00b5s are written by educational experts rather than non-expert annotators, making the evaluation results comparable to a top-performing student."], "score": 0.7666015625}, {"id": "(Thellmann et al., 2024)", "paper": {"corpus_id": 273323209, "title": "Towards Multilingual LLM Evaluation for European Languages", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Klaudia Thellmann", "authorId": "2269539"}, {"name": "Bernhard Stadler", "authorId": "2325726089"}, {"name": "Michael Fromm", "authorId": "2258551638"}, {"name": "Jasper Schulze Buschhoff", "authorId": "2258550838"}, {"name": "Alex Jude", "authorId": "2325726070"}, {"name": "Fabio Barth", "authorId": "2325726142"}, {"name": "Johannes Leveling", "authorId": "2258552277"}, {"name": "Nicolas Flores-Herr", "authorId": "2347259072"}, {"name": "Joachim K\u00f6hler", "authorId": "2330411614"}, {"name": "Ren\u00e9 J\u00e4kel", "authorId": "2330412312"}, {"name": "Mehdi Ali", "authorId": "2258668067"}], "n_citations": 10}, "snippets": ["Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018)."], "score": 0.7265625}, {"id": "(Tao et al., 2023)", "paper": {"corpus_id": 265445838, "title": "Cultural bias and cultural alignment of large language models", "year": 2023, "venue": "PNAS Nexus", "authors": [{"name": "Yan Tao", "authorId": "2105711243"}, {"name": "Olga Viberg", "authorId": "2269470454"}, {"name": "Ryan S. Baker", "authorId": "2255317189"}, {"name": "Ren\u00e9 F. Kizilcec", "authorId": "2246853606"}], "n_citations": 87}, "snippets": ["Abstract Culture fundamentally shapes people\u2019s reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people\u2019s authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI\u2019s GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models\u2019 responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models\u2019 output for 71\u201381% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI."], "score": 0.0}, {"id": "(Hendrycks et al., 2020)", "paper": {"corpus_id": 221516475, "title": "Measuring Massive Multitask Language Understanding", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "Dan Hendrycks", "authorId": "3422872"}, {"name": "Collin Burns", "authorId": "90909974"}, {"name": "Steven Basart", "authorId": "104444594"}, {"name": "Andy Zou", "authorId": "1380103052"}, {"name": "Mantas Mazeika", "authorId": "16787428"}, {"name": "D. Song", "authorId": "143711382"}, {"name": "J. Steinhardt", "authorId": "5164568"}], "n_citations": 4568}, "snippets": ["We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."], "score": 0.0}, {"id": "(Koto et al., 2023)", "paper": {"corpus_id": 263829211, "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Fajri Koto", "authorId": "2789148"}, {"name": "Nurul Aisyah", "authorId": "2256987672"}, {"name": "Haonan Li", "authorId": "49404498"}, {"name": "Timothy Baldwin", "authorId": "2256987316"}], "n_citations": 46}, "snippets": ["Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels."], "score": 0.0}, {"id": "(Yuksel et al., 2024)", "paper": {"corpus_id": 271245129, "title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Arda Yuksel", "authorId": "2311698426"}, {"name": "Abdullatif K\u00f6ksal", "authorId": "1999179692"}, {"name": "Lutfi Kerem cSenel", "authorId": "2126865294"}, {"name": "Anna Korhonen", "authorId": "2311700614"}, {"name": "Hinrich Schutze", "authorId": "2130001188"}], "n_citations": 14}, "snippets": ["Multiple choice question answering tasks evaluate the reasoning, comprehension, and mathematical abilities of Large Language Models (LLMs). While existing benchmarks employ automatic translation for multilingual evaluation, this approach is error-prone and potentially introduces culturally biased questions, especially in social sciences. We introduce the first multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs' understanding of the Turkish language. TurkishMMLU includes over 10,000 questions, covering 9 different subjects from Turkish high-school education curricula. These questions are written by curriculum experts, suitable for the high-school curricula in Turkey, covering subjects ranging from natural sciences and math questions to more culturally representative topics such as Turkish Literature and the history of the Turkish Republic. We evaluate over 20 LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5), closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol) models. We provide an extensive evaluation, including zero-shot and few-shot evaluation of LLMs, chain-of-thought reasoning, and question difficulty analysis along with model performance. We provide an in-depth analysis of the Turkish capabilities and limitations of current LLMs to provide insights for future LLMs for the Turkish language. We publicly release our code for the dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU."], "score": 0.0}, {"id": "(Wu et al., 2025)", "paper": {"corpus_id": 277993848, "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Minghao Wu", "authorId": "2327995656"}, {"name": "Weixuan Wang", "authorId": "2258789284"}, {"name": "Sinuo Liu", "authorId": "2349802912"}, {"name": "Huifeng Yin", "authorId": "2331767275"}, {"name": "Xintong Wang", "authorId": "2356794252"}, {"name": "Yu Zhao", "authorId": "2331686159"}, {"name": "Chenyang Lyu", "authorId": "2266387313"}, {"name": "Longyue Wang", "authorId": "2302342302"}, {"name": "Weihua Luo", "authorId": "2305289815"}, {"name": "Kaifu Zhang", "authorId": "2304530663"}], "n_citations": 5}, "snippets": ["Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)", "when comparing benchmark performance with human judgments, STEM-related tasks (ARC and MGSM) show consistently stronger correlations across languages (0.70-0.85), while other tasks like XQuAD correlate poorly (0.11-0.30). Importantly, simply translating English benchmarks proves insufficient for robust evaluation, localized benchmarks (like CMMLU for Chinese) show substantially higher correlation with human judgments (0.68) than translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources."], "score": 0.82080078125}], "table": null}, {"title": "Best Practices for Translation-Based Datasets", "tldr": "Despite inherent challenges, researchers have developed effective practices for creating reliable translation-based datasets, including using native translators, incorporating thorough quality verification processes, and allowing for cultural adaptations. (7 sources)", "text": "\nDespite the limitations of translation-based approaches, they remain widely used due to their efficiency and scalability. Researchers have identified several best practices to maximize the quality and reliability of translation-based multilingual evaluation datasets:\n\n1. **Use native speakers for translation and verification** - Employing carefully selected native-speaking translators for each target language helps ensure linguistic naturalness and idiomaticity <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. When machine translation is used, having multiple native-speaking annotators independently verify each translated sample significantly improves quality <Paper corpusId=\"276259236\" paperTitle=\"(Huang et al., 2025)\" isShortName></Paper>.\n\n2. **Allow for cultural and linguistic adaptations** - Rather than forcing direct translations, provide translators enough flexibility to make necessary target-language adjustments, including substitutions with culture-specific concepts and multi-word paraphrases where the original text cannot be directly translated <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>.\n\n3. **Implement controlled translation processes** - Machine translation of entire preference datasets should be avoided to prevent artifacts that can introduce subtle errors and invalidate original labels. Instead, use a more controlled approach with rigorous verification <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>.\n\n4. **Conduct thorough quality verification** - Each translated item should undergo careful validation to ensure clear distinctions between correct and incorrect responses. Subsets that don't meet quality standards should be discarded <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>. For instance, M-ABSA implemented efficient automatic data quality verification followed by manual inspection when necessary <Paper corpusId=\"276408022\" paperTitle=\"(Wu et al._1, 2025)\" isShortName></Paper>.\n\n5. **Select appropriate translation models** - When using machine translation, employ state-of-the-art multilingual translation models and apply automatic filtering mechanisms to ensure translation quality, such as keeping only languages with translation BLEU scores above certain thresholds <Paper corpusId=\"259095896\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\n6. **Consider linguistic features of target languages** - Be aware of specific linguistic features like pronoun-dropping, compounding, and inflection that can affect translation quality and model performance <Paper corpusId=\"258557740\" paperTitle=\"(Hennig et al., 2023)\" isShortName></Paper>.\n\n7. **Ensure cross-lingual consistency** - When building safety benchmarks or other sensitive datasets, verification by annotators should focus on maintaining cross-lingual consistency while preserving language-specific nuances <Paper corpusId=\"276408508\" paperTitle=\"(Banerjee et al., 2025)\" isShortName></Paper>.\n\n8. **Combine translation with native language data** - For optimal performance, consider using a combination of translated and native language data when training multilingual models, as this approach can outperform models trained solely on either source <Paper corpusId=\"258557740\" paperTitle=\"(Hennig et al., 2023)\" isShortName></Paper>.\n\nThese practices help mitigate many of the challenges associated with translation-based approaches while preserving their advantages in terms of cross-lingual alignment and efficiency.", "citations": [{"id": "(Ponti et al., 2020)", "paper": {"corpus_id": 218470125, "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "E. Ponti", "authorId": "3381663"}, {"name": "Goran Glavavs", "authorId": "1666177566"}, {"name": "Olga Majewska", "authorId": "46963731"}, {"name": "Qianchu Liu", "authorId": "50383712"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "A. Korhonen", "authorId": "145762466"}], "n_citations": 327}, "snippets": ["Design Objectives. The principal objectives in XCOPA creation were: 1) to create a genuinely typologically diverse multilingual dataset, aligned across target languages in order to make performance scores comparable, and 2) to ensure high quality, naturalness and idiomacity of each monolingual dataset. While the commonly used translation approach addresses the former objective, it is prone to compromise the latter goal, bending the target language to the structural and lexical properties of the source language: the resulting evaluation benchmarks thus fail to measure system performance adequately (Koppel and Ordan, 2011;Volansky et al., 2015;Artetxe et al., 2020a). \n\nIn order to avoid these pitfalls, we: (i) entrusted the translation task to a single (but carefully selected) translator for each target language, and (ii) offered enough leeway for necessary target-language adjustments (e.g., substitutions with culture-specific concepts and multi-word paraphrases, wherever the original text eluded direct translation)."], "score": 0.83740234375}, {"id": "(Huang et al., 2025)", "paper": {"corpus_id": 276259236, "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xu Huang", "authorId": "2345018196"}, {"name": "Wenhao Zhu", "authorId": "2279840735"}, {"name": "Hanxu Hu", "authorId": "2345123408"}, {"name": "Conghui He", "authorId": "2351236188"}, {"name": "Lei Li", "authorId": "2151532469"}, {"name": "Shujian Huang", "authorId": "2339084258"}, {"name": "Fei Yuan", "authorId": "2279544481"}], "n_citations": 4}, "snippets": ["To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction."], "score": 0.865234375}, {"id": "(Son et al., 2024)", "paper": {"corpus_id": 273532388, "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Guijin Son", "authorId": "2321452295"}, {"name": "Dongkeun Yoon", "authorId": "29830817"}, {"name": "Juyoung Suk", "authorId": "2299329316"}, {"name": "Javier Aula-Blasco", "authorId": "2301578911"}, {"name": "Mano Aslan", "authorId": "2327215494"}, {"name": "Vu Trong Kim", "authorId": "2327216625"}, {"name": "Shayekh Bin Islam", "authorId": "2232783785"}, {"name": "Jaume Prats-Cristi\u00e0", "authorId": "2327215436"}, {"name": "Luc\u00eda Tormo-Ba\u00f1uelos", "authorId": "2327217057"}, {"name": "Seungone Kim", "authorId": "2184037220"}], "n_citations": 13}, "snippets": ["However, we find that naive translation of existing meta-evaluation benchmarks can (i) introduce translation artifacts (mistranslation, degraded fluency, \"translationese\", etc.) that inadvertently affect the judgement of evaluator LLMs, and (ii) overlook key multilingual-specific nuances such as code-switching, or linguistic patterns. Together, these issues underscore the importance of careful, multilingual-tailored design rather than naive machine translation. Accordingly, we propose the following design choices to ensure reliable and truly multilingual meta-evaluation: \n\n\u2022 Controlled Translation. Machine Translation of entire preference datasets should be avoided, as artifacts can introduce subtle errors and invalidate original labels. \n\n\u2022 Thorough Quality Checks. Each entry (prompt-chosen-rejected) triplet should undergo careful validation to ensure a clear while non-trivial distinction between chosen and rejected responses. Subsets that do not meet this standard should be discarded. \n\n\u2022 Objectively Correct or Human-Preferred. Whenever possible, the \"chosen\" response should be objectively (or verifiably) correct or reflect existing human judgments."], "score": 0.72900390625}, {"id": "(Wu et al._1, 2025)", "paper": {"corpus_id": 276408022, "title": "M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Chengyan Wu", "authorId": "2335666192"}, {"name": "Bolei Ma", "authorId": "2335825370"}, {"name": "Yihong Liu", "authorId": "2107995084"}, {"name": "Zheyu Zhang", "authorId": "2144388883"}, {"name": "Ningyuan Deng", "authorId": "2266470510"}, {"name": "Yanshu Li", "authorId": "2345863610"}, {"name": "Baolan Chen", "authorId": "2346275292"}, {"name": "Yi Zhang", "authorId": "2345832186"}, {"name": "Barbara Plank", "authorId": "2284982105"}, {"name": "Yun Xue", "authorId": "2335829308"}], "n_citations": 1}, "snippets": ["Existing ABSA datasets are predominantly English-centric, limiting the scope for multilingual evaluation and research. To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7 domains and 21 languages, making it the most extensive multilingual parallel dataset for ABSA to date", "The dataset is constructed through an automatic translation process with human review to ensure quality", "Zhang et al. (2021a) construct a multilingual dataset by automatically translating the SemEval-2016 dataset (Pontiki et al., 2016), covering five languages for evaluation. However, there is no assessment of the translation quality, and most importantly -the number of languages in this dataset is limited, preventing researchers from conducting a strictly controlled evaluation of the effectiveness of cross-lingual transfer", "we use existing high-quality English datasets from multiple domains and construct a dataset (by manually annotating an English corpus) from another domain. These datasets are then automatically translated into 20 languages, followed by an efficient automatic data quality verification and manual inspection if necessary."], "score": 0.7392578125}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 259095896, "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Lei Li", "authorId": "49192881"}, {"name": "Yuwei Yin", "authorId": "2109472880"}, {"name": "Shicheng Li", "authorId": "2117947438"}, {"name": "Liang Chen", "authorId": "2146034504"}, {"name": "Peiyi Wang", "authorId": "144202874"}, {"name": "Shuhuai Ren", "authorId": "1906099"}, {"name": "Mukai Li", "authorId": "2027599235"}, {"name": "Yazheng Yang", "authorId": "14646700"}, {"name": "Jingjing Xu", "authorId": "47883405"}, {"name": "Xu Sun", "authorId": "2116530295"}, {"name": "Lingpeng Kong", "authorId": "47648549"}, {"name": "Qi Liu", "authorId": "2144831944"}], "n_citations": 120}, "snippets": ["To boost the language diversity and support the evaluation across different languages, we select a subset of datasets (OK-VQA, ImageNet, Winoground, VQAv2, VIST, MSRVTT and MSRVTT-QA) that covers different tasks and translate their evaluation data into 100 languages following FLORES-101 [13]. We translate 500 samples for each split of each task in our first version. More multilingual samples will be supported in the future. We adopt the distillation version NLLB-1.3B [6] for translation, one of the state-of-the-art open multilingual translation models. As there are no native speakers for different languages, we adopt an automatic filtering mechanism to ensure the translation quality, where languages with translation BLEU scores from English larger than 20 based on FLORES-101 results are kept. After this step, only 80 languages are kept (see Appendix for detailed language names)."], "score": 0.74072265625}, {"id": "(Hennig et al., 2023)", "paper": {"corpus_id": 258557740, "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Leonhard Hennig", "authorId": "36943315"}, {"name": "Philippe E. Thomas", "authorId": "143680610"}, {"name": "Sebastian M\u00f6ller", "authorId": "145733288"}], "n_citations": 8}, "snippets": ["We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance."], "score": 0.7333984375}, {"id": "(Banerjee et al., 2025)", "paper": {"corpus_id": 276408508, "title": "Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Somnath Banerjee", "authorId": "2246317582"}, {"name": "Sayan Layek", "authorId": "2273400615"}, {"name": "Pratyush Chatterjee", "authorId": "2345820876"}, {"name": "Animesh Mukherjee", "authorId": "2286311932"}, {"name": "Rima Hazra", "authorId": "2273549702"}], "n_citations": 1}, "snippets": ["XSafety: This is a multilingual safety benchmark designed to evaluate LLMs across multiple languages. It consists of 2,800 manually translated instances covering 14 safety categories in 10 widely spoken languages: English, Chinese, Spanish, French, Bengali, Arabic, Hindi, Russian, Japanese, and German. Built from existing monolingual safety datasets, XSafety was translated and verified by annotators, ensuring cross-lingual consistency. The benchmark reveals significant safety gaps in non-English responses, emphasizing the need for multilingual safety alignment. For our experiments, we use google translate8 to translate English queries into other languages when they are not present in the dataset."], "score": 0.74462890625}], "table": null}], "cost": 0.4935330000000001}}
