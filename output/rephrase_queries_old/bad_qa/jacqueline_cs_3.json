{"original_query": "In knowledge distillation, the teacher model is typically a larger (more powerful) network than the student model. Has there been work showing that the teacher model is size-matched or smaller (but perhaps more targeted in knowledge) than the student model, and for knowledge transfer to still be effective? ", "input": "knowledge distillation teacher model typically larger powerful network student model work that teacher model is size matched or smaller more in knowledge the student model for knowledge transfer effective", "output": {"sections": [{"title": "Introduction to Knowledge Distillation", "tldr": "Knowledge distillation is a technique that transfers knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) to maintain performance while reducing computational demands. Originally introduced for model compression, it has become a fundamental approach for deploying deep learning models in resource-constrained environments. (22 sources)", "text": "\nKnowledge distillation (KD) is a technique that enables the transfer of knowledge from a complex model, known as the teacher model, to a simpler model, called the student model <Paper corpusId=\"232076059\" paperTitle=\"(Keser et al., 2021)\" isShortName></Paper> <Paper corpusId=\"227228204\" paperTitle=\"(Ku et al., 2020)\" isShortName></Paper>. The concept was initially introduced by Bucila et al. in 2006 as a model compression method <Paper corpusId=\"253734490\" paperTitle=\"(Berdoz et al., 2022)\" isShortName></Paper> <Paper corpusId=\"266693464\" paperTitle=\"(Echim et al., 2023)\" isShortName></Paper>, and was later generalized and popularized by Hinton et al. in 2015 <Paper corpusId=\"267413258\" paperTitle=\"(Sghaier et al., 2024)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. The primary goal of knowledge distillation is to enable smaller models to achieve performance comparable to larger models while reducing computational requirements and memory usage <Paper corpusId=\"270870796\" paperTitle=\"(Mai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255125462\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>.\n\nIn the traditional knowledge distillation framework, the teacher model is typically a large, high-capacity network or an ensemble of models that has been pre-trained to achieve high performance <Paper corpusId=\"174800781\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper> <Paper corpusId=\"243861089\" paperTitle=\"(Xu et al., 2021)\" isShortName></Paper>. The student model, which is usually smaller and has fewer parameters, is then trained to mimic the behavior of the teacher model <Paper corpusId=\"255595916\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper> <Paper corpusId=\"257102399\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. This process involves the student learning not just from the ground truth labels (hard targets) but also from the probability distributions or intermediate representations produced by the teacher (soft targets) <Paper corpusId=\"250526328\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.\n\nThe knowledge transfer process generally follows a three-step approach as described by Jaiswal et al.: first, training a large teacher model that performs well; second, computing the teacher's predictions on the available data (referred to as knowledge or soft targets); and finally, training the smaller student model using this knowledge <Paper corpusId=\"234336288\" paperTitle=\"(Jaiswal et al., 2021)\" isShortName></Paper>. By learning from the teacher's outputs, the student model can capture not only the correct classifications but also the relationships between different classes that the teacher has learned <Paper corpusId=\"270389751\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nKnowledge distillation has become particularly important in scenarios where deploying large, computation-heavy models is impractical, such as on mobile devices or embedded systems with limited resources <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper> <Paper corpusId=\"267688311\" paperTitle=\"(Si et al., 2024)\" isShortName></Paper>. The technique creates a good trade-off between model size and accuracy, making it possible to deploy deep learning models in resource-constrained environments while maintaining acceptable performance <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper> <Paper corpusId=\"255266316\" paperTitle=\"(Zein et al., 2022)\" isShortName></Paper>.\n\nAs research in this area has progressed, various forms of knowledge (such as logits, activations, or features) and distillation strategies have been developed to enhance the effectiveness of the knowledge transfer process <Paper corpusId=\"272564024\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273233176\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. These advancements have expanded the application of knowledge distillation beyond model compression to include transfer learning and improving model performance in specific domains <Paper corpusId=\"260599456\" paperTitle=\"(Tao et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Keser et al., 2021)", "paper": {"corpus_id": 232076059, "title": "PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation", "year": 2021, "venue": "Expert systems with applications", "authors": [{"name": "Reyhan Kevser Keser", "authorId": "72263298"}, {"name": "Aydin Ayanzadeh", "authorId": "27583231"}, {"name": "O. A. Aghdam", "authorId": "12717766"}, {"name": "\u00c7aglar Kilcioglu", "authorId": "2051713853"}, {"name": "B. U. T\u00f6reyin", "authorId": "2021714"}, {"name": "N. K. Ure", "authorId": "2877453"}], "n_citations": 7}, "snippets": ["Knowledge distillation (KD) is the process of transferring knowledge between networks, where one usually aims to transfer the knowledge of a big network (teacher) to a smaller/more compact network (student)."], "score": 0.97265625}, {"id": "(Ku et al., 2020)", "paper": {"corpus_id": 227228204, "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "J. Ku", "authorId": "92856269"}, {"name": "Jihun Oh", "authorId": "31119623"}, {"name": "Youngyoon Lee", "authorId": "2417832"}, {"name": "Gaurav Pooniwala", "authorId": "97319268"}, {"name": "Sangjeong Lee", "authorId": "2119014436"}], "n_citations": 3}, "snippets": ["Knowledge distillation is an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pre-trained model or an ensemble of models (teacher)."], "score": 0.9892578125}, {"id": "(Berdoz et al., 2022)", "paper": {"corpus_id": 253734490, "title": "Scalable Collaborative Learning via Representation Sharing", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Fr'ed'eric Berdoz", "authorId": "2183082271"}, {"name": "Abhishek Singh", "authorId": "2034349211"}, {"name": "Martin Jaggi", "authorId": "2456863"}, {"name": "Ramesh Raskar", "authorId": "2070747078"}], "n_citations": 3}, "snippets": ["The concept of knowledge distillation (KD) originated in Bucila et al. (Bucila et al., 2006) as a way of compressing models, and was later generalized by Hinton et al. [19] (see Gou et al. (Gou et al., 2020) for an overview of the field). The standard use case for KD is that of a Teacher-Student (or offline) configuration, in which the teacher model (usually a large and well-trained model) transfers its knowledge to the student model (usually a smaller model) by sharing its last layer activations on a given transfer dataset (see Fig. 1a)."], "score": 0.97705078125}, {"id": "(Echim et al., 2023)", "paper": {"corpus_id": 266693464, "title": "Explainability-Driven Leaf Disease Classification Using Adversarial Training and Knowledge Distillation", "year": 2023, "venue": "International Conference on Agents and Artificial Intelligence", "authors": [{"name": "Sebastian-Vasile Echim", "authorId": "2219976776"}, {"name": "Iulian-Marius Taiatu", "authorId": "2279431357"}, {"name": "Dumitru-Clementin Cercel", "authorId": "3046903"}, {"name": "Florin-Catalin Pop", "authorId": "2261270760"}], "n_citations": 1}, "snippets": ["A practical method for compressing models is knowledge distillation, which facilitates information transfer from a large teacher network to a small student network. Initially introduced by Bucila et al. (Bucila et al., 2006) and later generalized by Hinton et al. (Hinton et al., 2015), this method has gained popularity across multiple machine learning applications. Unlike conventional training, knowledge distillation involves learning the student network to emulate the outputs of the teacher model, typically represented as probability distributions over classes."], "score": 0.97314453125}, {"id": "(Sghaier et al., 2024)", "paper": {"corpus_id": 267413258, "title": "Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation", "year": 2024, "venue": "Proc. ACM Softw. Eng.", "authors": [{"name": "O. Sghaier", "authorId": "1470757062"}, {"name": "H. Sahraoui", "authorId": "9460712"}], "n_citations": 10}, "snippets": ["Knowledge distillation is a popular technique that was first introduced by Hinton et al. [18] as a method to transfer knowledge from a complex model, also known as the teacher, to a smaller and faster model, called the student. The main goal of knowledge distillation is to transfer the learned knowledge of the teacher model to the student model so that it can achieve comparable or even better performance on a target task."], "score": 0.994140625}, {"id": "(Hinton et al., 2015)", "paper": {"corpus_id": 7200347, "title": "Distilling the Knowledge in a Neural Network", "year": 2015, "venue": "arXiv.org", "authors": [{"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "J. Dean", "authorId": "49959210"}], "n_citations": 19742}, "snippets": ["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."], "score": 0.0}, {"id": "(Mai et al., 2024)", "paper": {"corpus_id": 270870796, "title": "From Efficient Multimodal Models to World Models: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xinji Mai", "authorId": "2276937443"}, {"name": "Zeng Tao", "authorId": "2261831274"}, {"name": "Junxiong Lin", "authorId": "2261891655"}, {"name": "Haoran Wang", "authorId": "2276807843"}, {"name": "Yang Chang", "authorId": "2276969811"}, {"name": "Yanlan Kang", "authorId": "2212014366"}, {"name": "Yan Wang", "authorId": "2276879376"}, {"name": "Wenqiang Zhang", "authorId": "2276819302"}], "n_citations": 5}, "snippets": ["Knowledge Distillation (KD) is a model compression technique that transfers knowledge from a complex model (called the teacher model) to a smaller model (called the student model).This allows the student model to maintain high computational efficiency while achieving the performance of the teacher model."], "score": 0.9814453125}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 255125462, "title": "Prototype-guided Cross-task Knowledge Distillation for Large-scale Models", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Deng Li", "authorId": "2158676096"}, {"name": "Aming Wu", "authorId": "48352212"}, {"name": "Yahong Han", "authorId": "144622313"}, {"name": "Qingwen Tian", "authorId": "2149898858"}], "n_citations": 3}, "snippets": ["Knowledge distillation is a model compression technology that transfers the knowledge from a larger deep neural network into a small network."], "score": 0.99072265625}, {"id": "(Wei et al., 2019)", "paper": {"corpus_id": 174800781, "title": "Online Distilling from Checkpoints for Neural Machine Translation", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Hao-Ran Wei", "authorId": "134085586"}, {"name": "Shujian Huang", "authorId": "2046010"}, {"name": "Ran Wang", "authorId": "2115256564"}, {"name": "Xinyu Dai", "authorId": "3035069"}, {"name": "Jiajun Chen", "authorId": "1838162"}], "n_citations": 32}, "snippets": ["Knowledge distillation is a class of methods which transfers knowledge from a pre-trained teacher model T, to a student model S. The teacher model can be a model with large capacity (Bucila et al., 2006) or an ensemble of several models (Hinton et al., 2015)."], "score": 0.98046875}, {"id": "(Xu et al., 2021)", "paper": {"corpus_id": 243861089, "title": "A Survey on Green Deep Learning", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Jingjing Xu", "authorId": "47883405"}, {"name": "Wangchunshu Zhou", "authorId": "150341221"}, {"name": "Zhiyi Fu", "authorId": "2068057294"}, {"name": "Hao Zhou", "authorId": null}, {"name": "Lei Li", "authorId": "2151530622"}], "n_citations": 84}, "snippets": ["The idea of knowledge distillation (KD) is exploiting the knowledge inside a large trained \"teacher\" model to help the training of a \"student\" model (Bucila et al., 2006)(Ba et al., 2013)Hinton et al., 2015). In this way, we can use a smaller student model to distill a trained model as a replacement for inference."], "score": 0.9765625}, {"id": "(Zhou et al., 2023)", "paper": {"corpus_id": 255595916, "title": "Synthetic data generation method for data-free knowledge distillation in regression neural networks", "year": 2023, "venue": "Expert systems with applications", "authors": [{"name": "Tianxun Zhou", "authorId": "2163011403"}, {"name": "K. Chiam", "authorId": "4287306"}], "n_citations": 7}, "snippets": ["Knowledge distillation is the technique where knowledge learned by a larger teacher model is transferred to a smaller student model (Gou et al., 2020), Hinton et al., 2015(Wang et al., 2020). The main idea is that the student model mimics the teacher model to achieve a similar or even a superior performance."], "score": 0.97998046875}, {"id": "(Shi et al., 2023)", "paper": {"corpus_id": 257102399, "title": "Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System", "year": 2023, "venue": "Journal of Intelligent Manufacturing", "authors": [{"name": "Zhangyue Shi", "authorId": "2144091000"}, {"name": "Yuxuan Li", "authorId": "2116210564"}, {"name": "Chenang Liu", "authorId": "2035538477"}], "n_citations": 8}, "snippets": ["Knowledge distillation is a model compression technique proposed by Hinton et al. in 2015 [9], which could effectively train a smaller student model by learning from a larger teacher model. As shown in Figure 2, a general knowledge distillation framework consists of three major components: teacher-student network architecture, knowledge, as well as distillation algorithm. The main idea of knowledge distillation is that the student model mimics the teacher model in order to achieve a comparable or even a superior performance (Gou et al., 2020). Teacher network usually has more complicated network architecture and learns from the data first. After the training of teacher network, the useful knowledge could be distilled from the teacher network. Hereafter, student network learns the knowledge distilled from teacher network to mimic teacher's behavior together with the training data. The core of knowledge distillation is how to simplify model while maintaining relatively good performance."], "score": 0.990234375}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 250526328, "title": "Large\u2010scale knowledge distillation with elastic heterogeneous computing resources", "year": 2022, "venue": "Concurrency and Computation", "authors": [{"name": "Ji Liu", "authorId": "2118971193"}, {"name": "Daxiang Dong", "authorId": "9532787"}, {"name": "Xi Wang", "authorId": "2108249583"}, {"name": "An Qin", "authorId": "2140532375"}, {"name": "Xingjian Li", "authorId": "2155445773"}, {"name": "P. Valduriez", "authorId": "144255847"}, {"name": "D. Dou", "authorId": "1721158"}, {"name": "Dianhai Yu", "authorId": "3046102"}], "n_citations": 6}, "snippets": ["Although more layers and more parameters generally improve the accuracy of the models, such big models generally have high computational complexity and require big memory, which exceed the capacity of small devices for inference and incurs long training time", "Knowledge distillation is based on the popular machine learning Softmax function and a temperature (Hinton et al., 2015) , which is defined in Formula 1", "As shown in Figure 1, during the training of knowledge distillation, two neural networks are used: teacher model and student model. The student model is trained based on the combination of two loss values. The first loss value is calculated from a soft prediction, which contains the probability of each class calculated from the teacher model. The soft prediction is calculated using Formula 1. The other loss value corresponds to a hard prediction, which is the ground truth label from the training data."], "score": 0.9775390625}, {"id": "(Jaiswal et al., 2021)", "paper": {"corpus_id": 234336288, "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification", "year": 2021, "venue": "International Journal of Advanced Computer Science and Applications", "authors": [{"name": "Bhavesh Jaiswal", "authorId": "2139643077"}, {"name": "Nagendra Gajjar", "authorId": "32136431"}], "n_citations": 0}, "snippets": ["Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model.\n\nKnowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed."], "score": 0.984375}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 270389751, "title": "Multistage feature fusion knowledge distillation", "year": 2024, "venue": "Scientific Reports", "authors": [{"name": "Gang Li", "authorId": "2307186661"}, {"name": "Kun Wang", "authorId": "2307436738"}, {"name": "Pengfei Lv", "authorId": "2305765306"}, {"name": "Pan He", "authorId": "2305752302"}, {"name": "Zheng Zhou", "authorId": "2287501699"}, {"name": "Chuanyun Xu", "authorId": "2817613"}], "n_citations": 1}, "snippets": ["Knowledge distillation (KD), as initially proposed by Hinton et al. (Hinton et al., 2015) , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge."], "score": 0.98046875}, {"id": "(Gou et al., 2020)", "paper": {"corpus_id": 219559263, "title": "Knowledge Distillation: A Survey", "year": 2020, "venue": "International Journal of Computer Vision", "authors": [{"name": "Jianping Gou", "authorId": "38978232"}, {"name": "B. Yu", "authorId": "2425630"}, {"name": "S. Maybank", "authorId": "144555237"}, {"name": "D. Tao", "authorId": "143719920"}], "n_citations": 2984}, "snippets": ["In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."], "score": 0.0}, {"id": "(Si et al., 2024)", "paper": {"corpus_id": 267688311, "title": "Breast Cancer Histopathology Images Classification Through Multi-View Augmented Contrastive Learning and Pre-Learning Knowledge Distillation", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Jialong Si", "authorId": "2284255154"}, {"name": "Wei Jia", "authorId": "2284258056"}, {"name": "Haifeng Jiang", "authorId": "2284308867"}], "n_citations": 5}, "snippets": ["Knowledge distillation is a commonly used technique in deep learning, as proposed by Hinton et al. in 2015 [17]. It is used to transfer knowledge from a large model to a smaller one, resulting in model compression and acceleration. The goal of knowledge distillation is to improve the accuracy and generalizability of the student network by transferring knowledge from the teacher network. Auxiliary information during training is usually obtained from the output of the teacher network. Such techniques have been applied in various domains, including natural language processing, computer vision, and speech recognition. An important use case is the deployment of deep learning models on mobile devices, which often have limited computational resources and require smaller, lightweight models. Knowledge distillation typically involves a larger teacher network and a smaller student network. The teacher network is usually trained on large amounts of data and therefore has higher accuracy and more comprehensive knowledge. Compared to the teacher network, the student network is trained on less data and has fewer parameters. Nevertheless, some student networks can surpass the accuracy of the teacher network."], "score": 0.9833984375}, {"id": "(Capogrosso et al., 2023)", "paper": {"corpus_id": 262084420, "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Luigi Capogrosso", "authorId": "2135267479"}, {"name": "Federico Cunico", "authorId": "1396330675"}, {"name": "D. Cheng", "authorId": "1780197"}, {"name": "Franco Fummi", "authorId": "2243336023"}, {"name": "Marco Cristani", "authorId": "2238815087"}], "n_citations": 43}, "snippets": ["This technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) (Gou et al., 2020). This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies", "Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits (Zhao et al., 2019)", "In general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy (Zein et al., 2022)."], "score": 0.97607421875}, {"id": "(Zein et al., 2022)", "paper": {"corpus_id": 255266316, "title": "Implementation and Optimization of Neural Networks for Tiny Hardware Devices", "year": 2022, "venue": "2022 International Conference on Smart Systems and Power Management (IC2SPM)", "authors": [{"name": "Hadi Al Zein", "authorId": "2198763466"}, {"name": "Mohamad Aoude", "authorId": "2198763611"}, {"name": "Youssef Harkous", "authorId": "2198763636"}], "n_citations": 5}, "snippets": ["Traditionally, neural network inferencing on tiny hardware devices took place in a centralized server-based manner. With more real-time applications coming into play, where security and latency are a concern, there has become a need to move inferencing to the edge. This paper describes a machine learning pipeline to carry neural networks from their initial forms to compressed forms deployable on tiny hardware devices, while maintaining acceptable accuracies of the optimized models. We will review the different software optimization techniques used to compress neural networks to their deployable forms. The prototype is a proof of concept showing that applying knowledge distillation from a highly accurate ResNet20 model to a simple CNN student model, followed by post-training quantization, achieves good multi-class accuracy on a constrained Arduino Nano 33 BLE Sense at low power consumption and with low inferencing latency."], "score": 0.0}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 272564024, "title": "A Comprehensive Review of Hardware Acceleration Techniques and Convolutional Neural Networks for EEG Signals", "year": 2024, "venue": "Italian National Conference on Sensors", "authors": [{"name": "Yu Xie", "authorId": "2118595916"}, {"name": "Stefan Oniga", "authorId": "2277106978"}], "n_citations": 1}, "snippets": ["Knowledge distillation is a technique that transfers knowledge from large, complex CNNs (teachers) to smaller, more efficient CNNs (students). The student network is trained to mimic the behavior of the teacher, resulting in a compact model with comparable performance. This concept can be traced back to the pioneering work in [101] and has since been extended to the context of deep learning (Hinton et al., 2015). The core challenge of knowledge distillation revolves around the techniques used to transfer knowledge from the teacher model to the student model, which involves three fundamental components-knowledge, distillation algorithms-and the architecture defining the relationship between the teacher and student models. In this context, knowledge manifests in various forms, including logits, activations, or features extracted from intermediate layers of the teacher model."], "score": 0.98388671875}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 273233176, "title": "SNN-PAR: Energy Efficient Pedestrian Attribute Recognition via Spiking Neural Networks", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Haiyang Wang", "authorId": "2316447852"}, {"name": "Qian Zhu", "authorId": "2299062895"}, {"name": "Mowen She", "authorId": "2325151513"}, {"name": "Yabo Li", "authorId": "2325202742"}, {"name": "Haoyu Song", "authorId": "2325294395"}, {"name": "Minghe Xu", "authorId": "2325504629"}, {"name": "Xiao Wang", "authorId": "2325835807"}], "n_citations": 0}, "snippets": ["Knowledge Distillation is a technique for model compression that facilitates a smaller student model in learning from a larger teacher model. The student acquires knowledge by imitating various aspects of the teacher, such as its intermediate features [41], prediction logits [22], or activation boundaries (Heo et al., 2018)]. This approach was originally put forth by Hinton et al. [21] to supervise students based on the hard and soft label's output by the teacher, and nowadays there is a lot of work on using distillation for knowledge transfer to help the model get better performance."], "score": 0.97216796875}, {"id": "(Tao et al., 2023)", "paper": {"corpus_id": 260599456, "title": "Prototypes Sampling Mechanism for Class Incremental Learning", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Zhe Tao", "authorId": "2213939405"}, {"name": "Shucheng Huang", "authorId": "50178505"}, {"name": "Gang Wang", "authorId": "2155613761"}], "n_citations": 3}, "snippets": ["Knowledge Distillation is a widely used technique in model compression (Cheng et al., 2018), (Bucila et al., 2006) and transfer learning (Yim et al., 2017), which typically involves transferring knowledge from a large and complex model (often referred to as the teacher model) to a smaller and simpler model (often referred to as the student model). The goal of knowledge distillation is to improve the performance of the student model by leveraging the knowledge contained in the teacher model, while still maintaining the efficiency and simplicity of the student model."], "score": 0.974609375}], "table": null}, {"title": "Traditional Knowledge Distillation Setup", "tldr": "Traditional knowledge distillation involves training a large teacher model first, then transferring its knowledge to a smaller student model that learns to mimic the teacher's outputs. The teacher is typically more complex with higher capacity, while the student is designed to be more computationally efficient while maintaining acceptable performance. (18 sources)", "text": "\nKnowledge distillation (KD) in its traditional form follows a specific setup that has become the foundation for various distillation approaches. At its core, the traditional KD framework involves two key components: a teacher model and a student model <Paper corpusId=\"174800781\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper> <Paper corpusId=\"227228204\" paperTitle=\"(Ku et al., 2020)\" isShortName></Paper>.\n\nThe teacher model is typically a large, complex network with high capacity that has been pre-trained to achieve strong performance on a given task <Paper corpusId=\"232076059\" paperTitle=\"(Keser et al., 2021)\" isShortName></Paper> <Paper corpusId=\"232380330\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>. In some cases, the teacher can also be an ensemble of several models <Paper corpusId=\"174800781\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. Due to its size and complexity, the teacher model often captures rich, deep information and features that are crucial for high task performance <Paper corpusId=\"269761365\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper>.\n\nThe student model, on the other hand, is designed to be more lightweight and compact, with fewer parameters and lower computational requirements <Paper corpusId=\"227228204\" paperTitle=\"(Ku et al., 2020)\" isShortName></Paper> <Paper corpusId=\"232076059\" paperTitle=\"(Keser et al., 2021)\" isShortName></Paper>. This smaller model is intended for deployment in resource-constrained environments, such as mobile devices or embedded systems <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper> <Paper corpusId=\"255266316\" paperTitle=\"(Zein et al., 2022)\" isShortName></Paper>.\n\nThe traditional KD process follows a sequential approach. First, the teacher model is trained on the available data to achieve high performance. Then, the teacher's knowledge is distilled by computing its predictions (soft targets) on the training data. Finally, the student model is trained to mimic the teacher using both the ground truth labels (hard targets) and the teacher's soft targets <Paper corpusId=\"234336288\" paperTitle=\"(Jaiswal et al., 2021)\" isShortName></Paper> <Paper corpusId=\"250526328\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.\n\nThis training typically involves a combined loss function with two components: a conventional classification loss using the ground truth labels, and a distillation loss that measures the difference between the student's and teacher's predictions <Paper corpusId=\"250526328\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"258426697\" paperTitle=\"(Malik et al., 2023)\" isShortName></Paper>. The temperature parameter (T) introduced by Hinton et al. is used to control the softness of the probability distributions and the amount of \"dark knowledge\" transferred from teacher to student <Paper corpusId=\"270389751\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nThis \"offline\" distillation approach, where the teacher is first fully trained and then its weights are frozen before training the student, is the most common training scheme in traditional KD <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper> <Paper corpusId=\"198179767\" paperTitle=\"(Zhao et al., 2019)\" isShortName></Paper>. The core idea is that the student model should learn to mimic the teacher's behavior and generalization capabilities <Paper corpusId=\"245974615\" paperTitle=\"(Ganta et al., 2021)\" isShortName></Paper> <Paper corpusId=\"257102399\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper>.\n\nThe traditional KD setup aims to achieve a good trade-off between model size and accuracy <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper> <Paper corpusId=\"255266316\" paperTitle=\"(Zein et al., 2022)\" isShortName></Paper>. By leveraging the \"dark knowledge\" embedded in the teacher's outputs, the student can approximate the performance level of the larger model while requiring significantly fewer computational resources <Paper corpusId=\"248683566\" paperTitle=\"(Xu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"167217261\" paperTitle=\"(Tan et al., 2019)\" isShortName></Paper> <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Wei et al., 2019)", "paper": {"corpus_id": 174800781, "title": "Online Distilling from Checkpoints for Neural Machine Translation", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Hao-Ran Wei", "authorId": "134085586"}, {"name": "Shujian Huang", "authorId": "2046010"}, {"name": "Ran Wang", "authorId": "2115256564"}, {"name": "Xinyu Dai", "authorId": "3035069"}, {"name": "Jiajun Chen", "authorId": "1838162"}], "n_citations": 32}, "snippets": ["Knowledge distillation is a class of methods which transfers knowledge from a pre-trained teacher model T, to a student model S. The teacher model can be a model with large capacity (Bucila et al., 2006) or an ensemble of several models (Hinton et al., 2015)."], "score": 0.98046875}, {"id": "(Ku et al., 2020)", "paper": {"corpus_id": 227228204, "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "J. Ku", "authorId": "92856269"}, {"name": "Jihun Oh", "authorId": "31119623"}, {"name": "Youngyoon Lee", "authorId": "2417832"}, {"name": "Gaurav Pooniwala", "authorId": "97319268"}, {"name": "Sangjeong Lee", "authorId": "2119014436"}], "n_citations": 3}, "snippets": ["Knowledge distillation is an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pre-trained model or an ensemble of models (teacher)."], "score": 0.9892578125}, {"id": "(Keser et al., 2021)", "paper": {"corpus_id": 232076059, "title": "PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation", "year": 2021, "venue": "Expert systems with applications", "authors": [{"name": "Reyhan Kevser Keser", "authorId": "72263298"}, {"name": "Aydin Ayanzadeh", "authorId": "27583231"}, {"name": "O. A. Aghdam", "authorId": "12717766"}, {"name": "\u00c7aglar Kilcioglu", "authorId": "2051713853"}, {"name": "B. U. T\u00f6reyin", "authorId": "2021714"}, {"name": "N. K. Ure", "authorId": "2877453"}], "n_citations": 7}, "snippets": ["Knowledge distillation (KD) is the process of transferring knowledge between networks, where one usually aims to transfer the knowledge of a big network (teacher) to a smaller/more compact network (student)."], "score": 0.97265625}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 232380330, "title": "Distilling a Powerful Student Model via Online Knowledge Distillation", "year": 2021, "venue": "IEEE Transactions on Neural Networks and Learning Systems", "authors": [{"name": "Shaojie Li", "authorId": "145904937"}, {"name": "Mingbao Lin", "authorId": "49352079"}, {"name": "Yan Wang", "authorId": "2152543905"}, {"name": "Feiyue Huang", "authorId": "1835006"}, {"name": "Yongjian Wu", "authorId": "47096329"}, {"name": "Yonghong Tian", "authorId": "40161651"}, {"name": "Ling Shao", "authorId": "144082425"}, {"name": "Rongrong Ji", "authorId": "1572139630"}], "n_citations": 47}, "snippets": ["Traditional Knowledge Distillation. Traditional distillation works transfer knowledge from a cumbersome teacher model to a light-weight student model. As such, a large-scale model has to be trained in advance, based on which various knowledge definitions and transfer strategies are proposed to boost the performance of the student model."], "score": 0.97265625}, {"id": "(Hinton et al., 2015)", "paper": {"corpus_id": 7200347, "title": "Distilling the Knowledge in a Neural Network", "year": 2015, "venue": "arXiv.org", "authors": [{"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "J. Dean", "authorId": "49959210"}], "n_citations": 19742}, "snippets": ["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."], "score": 0.0}, {"id": "(Wang et al._1, 2024)", "paper": {"corpus_id": 269761365, "title": "Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhiwei Wang", "authorId": "2301262030"}, {"name": "Jun Huang", "authorId": "2301263964"}, {"name": "Longhua Ma", "authorId": "2301265454"}, {"name": "Chengyu Wu", "authorId": "2301254673"}, {"name": "Hongyu Ma", "authorId": "2301265433"}], "n_citations": 0}, "snippets": ["Large and complex teacher models often capture a wealth of features and deep information, crucial for enhancing task performance [7]. However, attempting to directly distill this rich information into smaller capacity student models often results in suboptimal performance reproduction due to the student model's capacity limitations. The student model may struggle to process the complex information present in the teacher model, leading to ineffective distillation. Moreover, simply mimicking all features of the teacher model overlooks the differences in relationships and structures between the information, particularly when there is a significant gap between the teacher and student models [12]."], "score": 0.97802734375}, {"id": "(Capogrosso et al., 2023)", "paper": {"corpus_id": 262084420, "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Luigi Capogrosso", "authorId": "2135267479"}, {"name": "Federico Cunico", "authorId": "1396330675"}, {"name": "D. Cheng", "authorId": "1780197"}, {"name": "Franco Fummi", "authorId": "2243336023"}, {"name": "Marco Cristani", "authorId": "2238815087"}], "n_citations": 43}, "snippets": ["This technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) (Gou et al., 2020). This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies", "Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits (Zhao et al., 2019)", "In general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy (Zein et al., 2022)."], "score": 0.97607421875}, {"id": "(Zein et al., 2022)", "paper": {"corpus_id": 255266316, "title": "Implementation and Optimization of Neural Networks for Tiny Hardware Devices", "year": 2022, "venue": "2022 International Conference on Smart Systems and Power Management (IC2SPM)", "authors": [{"name": "Hadi Al Zein", "authorId": "2198763466"}, {"name": "Mohamad Aoude", "authorId": "2198763611"}, {"name": "Youssef Harkous", "authorId": "2198763636"}], "n_citations": 5}, "snippets": ["Traditionally, neural network inferencing on tiny hardware devices took place in a centralized server-based manner. With more real-time applications coming into play, where security and latency are a concern, there has become a need to move inferencing to the edge. This paper describes a machine learning pipeline to carry neural networks from their initial forms to compressed forms deployable on tiny hardware devices, while maintaining acceptable accuracies of the optimized models. We will review the different software optimization techniques used to compress neural networks to their deployable forms. The prototype is a proof of concept showing that applying knowledge distillation from a highly accurate ResNet20 model to a simple CNN student model, followed by post-training quantization, achieves good multi-class accuracy on a constrained Arduino Nano 33 BLE Sense at low power consumption and with low inferencing latency."], "score": 0.0}, {"id": "(Jaiswal et al., 2021)", "paper": {"corpus_id": 234336288, "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification", "year": 2021, "venue": "International Journal of Advanced Computer Science and Applications", "authors": [{"name": "Bhavesh Jaiswal", "authorId": "2139643077"}, {"name": "Nagendra Gajjar", "authorId": "32136431"}], "n_citations": 0}, "snippets": ["Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model.\n\nKnowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed."], "score": 0.984375}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 250526328, "title": "Large\u2010scale knowledge distillation with elastic heterogeneous computing resources", "year": 2022, "venue": "Concurrency and Computation", "authors": [{"name": "Ji Liu", "authorId": "2118971193"}, {"name": "Daxiang Dong", "authorId": "9532787"}, {"name": "Xi Wang", "authorId": "2108249583"}, {"name": "An Qin", "authorId": "2140532375"}, {"name": "Xingjian Li", "authorId": "2155445773"}, {"name": "P. Valduriez", "authorId": "144255847"}, {"name": "D. Dou", "authorId": "1721158"}, {"name": "Dianhai Yu", "authorId": "3046102"}], "n_citations": 6}, "snippets": ["Although more layers and more parameters generally improve the accuracy of the models, such big models generally have high computational complexity and require big memory, which exceed the capacity of small devices for inference and incurs long training time", "Knowledge distillation is based on the popular machine learning Softmax function and a temperature (Hinton et al., 2015) , which is defined in Formula 1", "As shown in Figure 1, during the training of knowledge distillation, two neural networks are used: teacher model and student model. The student model is trained based on the combination of two loss values. The first loss value is calculated from a soft prediction, which contains the probability of each class calculated from the teacher model. The soft prediction is calculated using Formula 1. The other loss value corresponds to a hard prediction, which is the ground truth label from the training data."], "score": 0.9775390625}, {"id": "(Malik et al., 2023)", "paper": {"corpus_id": 258426697, "title": "Emotions Beyond Words: Non-Speech Audio Emotion Recognition With Edge Computing", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Ibrahim Malik", "authorId": "2105891149"}, {"name": "S. Latif", "authorId": "24040678"}, {"name": "Sanaullah Manzoor", "authorId": "1483726395"}, {"name": "Muhammad Usama", "authorId": "145474282"}, {"name": "Junaid Qadir", "authorId": "1734917"}, {"name": "R. Jurdak", "authorId": "1770270"}], "n_citations": 3}, "snippets": ["The process of knowledge distillation as the name suggests is the method of transferring knowledge from a larger computationally expensive model to a relatively smaller model. The larger and smaller models are called the teacher and student models respectively. Thus, knowledge distillation consists of three principal components: (1) knowledge; (2) distillation algorithm; and (3) teacherstudent architecture. While there are now multiple methods of distillation algorithms we selected the response-based algorithm. As shown in Figure 3, the hypothesis is that the student model will learn to mimic the predictions of the teacher model. This can be achieved by using a loss function, termed the distillation loss, that captures the difference between the logits of the student and the teacher model respectively. As this loss minimizes overtraining, the student model will improve at making the same predictions as the teacher. In the offline training scheme, the teacher model is first trained and the weights are then frozen. Next, we train the student model using the distillation loss and the logits from the teacher model as targets. Following is the equation of the distillation loss. \n\nwhere: L d : the loss function for knowledge distillation \u03b1: a hyperparameter that controls the trade-off between the classification loss and the distillation loss T : the temperature hyperparameter used to soften the logits (outputs of the last layer before softmax) of the teacher and student models KL: the Kullback-Leibler divergence, a measure of how different two probability distributions are softmax: a function that converts the logits to probabilities f (T, x): the logits of the teacher model for input x g(T, x): the logits of the student model for input x 2) Teacher Model: Generally, for the teacher model a larger and deeper network is chosen so that it performs well on the task at hand."], "score": 0.9716796875}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 270389751, "title": "Multistage feature fusion knowledge distillation", "year": 2024, "venue": "Scientific Reports", "authors": [{"name": "Gang Li", "authorId": "2307186661"}, {"name": "Kun Wang", "authorId": "2307436738"}, {"name": "Pengfei Lv", "authorId": "2305765306"}, {"name": "Pan He", "authorId": "2305752302"}, {"name": "Zheng Zhou", "authorId": "2287501699"}, {"name": "Chuanyun Xu", "authorId": "2817613"}], "n_citations": 1}, "snippets": ["Knowledge distillation (KD), as initially proposed by Hinton et al. (Hinton et al., 2015) , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge."], "score": 0.98046875}, {"id": "(Zhao et al., 2019)", "paper": {"corpus_id": 198179767, "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching", "year": 2019, "venue": "IEEE Transactions on Cybernetics", "authors": [{"name": "Haoran Zhao", "authorId": "50981688"}, {"name": "Xin Sun", "authorId": "144326521"}, {"name": "Junyu Dong", "authorId": "1964397"}, {"name": "Changrui Chen", "authorId": "10944885"}, {"name": "Zihe Dong", "authorId": "2087106420"}], "n_citations": 59}, "snippets": ["High storage and computational costs obstruct deep neural networks to be deployed on resource-constrained devices. Knowledge distillation (KD) aims to train a compact student network by transferring knowledge from a larger pretrained teacher model. However, most existing methods on KD ignore the valuable information among the training process associated with training results. In this article, we provide a new collaborative teaching KD (CTKD) strategy which employs two special teachers. Specifically, one teacher trained from scratch (i.e., scratch teacher) assists the student step by step using its temporary outputs. It forces the student to approach the optimal path toward the final logits with high accuracy. The other pretrained teacher (i.e., expert teacher) guides the student to focus on a critical region that is more useful for the task. The combination of the knowledge from two special teachers can significantly improve the performance of the student network in KD. The results of experiments on CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, and ImageNet datasets verify that the proposed KD method is efficient and achieves state-of-the-art performance."], "score": 0.0}, {"id": "(Ganta et al., 2021)", "paper": {"corpus_id": 245974615, "title": "Knowledge Distillation via Weighted Ensemble of Teaching Assistants", "year": 2021, "venue": "International Conference on Big Knowledge", "authors": [{"name": "Durga Prasad Ganta", "authorId": "2150123681"}, {"name": "Himel Das Gupta", "authorId": "1944682834"}, {"name": "Victor S. Sheng", "authorId": "2278313995"}], "n_citations": 4}, "snippets": ["The main idea of using knowledge distillation is that student network (S) to be trained not only using the true labels information but also observation of how the teacher (T) works with the unseen data provided. Because the teacher model has more more generalization power, the idea is to train the student model is such way that it can mimic the behaviour of that generalization. The teacher network is complex in size being deeper and wider."], "score": 0.98974609375}, {"id": "(Shi et al., 2023)", "paper": {"corpus_id": 257102399, "title": "Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System", "year": 2023, "venue": "Journal of Intelligent Manufacturing", "authors": [{"name": "Zhangyue Shi", "authorId": "2144091000"}, {"name": "Yuxuan Li", "authorId": "2116210564"}, {"name": "Chenang Liu", "authorId": "2035538477"}], "n_citations": 8}, "snippets": ["Knowledge distillation is a model compression technique proposed by Hinton et al. in 2015 [9], which could effectively train a smaller student model by learning from a larger teacher model. As shown in Figure 2, a general knowledge distillation framework consists of three major components: teacher-student network architecture, knowledge, as well as distillation algorithm. The main idea of knowledge distillation is that the student model mimics the teacher model in order to achieve a comparable or even a superior performance (Gou et al., 2020). Teacher network usually has more complicated network architecture and learns from the data first. After the training of teacher network, the useful knowledge could be distilled from the teacher network. Hereafter, student network learns the knowledge distilled from teacher network to mimic teacher's behavior together with the training data. The core of knowledge distillation is how to simplify model while maintaining relatively good performance."], "score": 0.990234375}, {"id": "(Gou et al., 2020)", "paper": {"corpus_id": 219559263, "title": "Knowledge Distillation: A Survey", "year": 2020, "venue": "International Journal of Computer Vision", "authors": [{"name": "Jianping Gou", "authorId": "38978232"}, {"name": "B. Yu", "authorId": "2425630"}, {"name": "S. Maybank", "authorId": "144555237"}, {"name": "D. Tao", "authorId": "143719920"}], "n_citations": 2984}, "snippets": ["In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."], "score": 0.0}, {"id": "(Xu et al., 2022)", "paper": {"corpus_id": 248683566, "title": "Teacher-student collaborative knowledge distillation for image classification", "year": 2022, "venue": "Applied intelligence (Boston)", "authors": [{"name": "Chuanyun Xu", "authorId": "2817613"}, {"name": "Wenjian Gao", "authorId": "2164051968"}, {"name": "Tian Li", "authorId": "2164318078"}, {"name": "Nanlan Bai", "authorId": "2164821600"}, {"name": "Gang Li", "authorId": "2155121570"}, {"name": "Yang Zhang", "authorId": "2145954082"}], "n_citations": 44}, "snippets": ["Knowledge distillation (KD), an important method of model compression (Tan et al., 2019)(Cheng et al., 2018)(Bashir et al., 2020), is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model (Yim et al., 2017)(Kim et al., 2016)(Gou et al., 2020)."], "score": 0.97265625}, {"id": "(Tan et al., 2019)", "paper": {"corpus_id": 167217261, "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Mingxing Tan", "authorId": "120805419"}, {"name": "Quoc V. Le", "authorId": "2827616"}], "n_citations": 18189}, "snippets": ["Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL."], "score": 0.0}], "table": null}, {"title": "Size-Matched Knowledge Distillation", "tldr": "Traditional knowledge distillation typically involves transferring knowledge from a larger teacher to a smaller student, but recent research has challenged this assumption by exploring size-matched distillation where the student has similar or identical architecture to the teacher. These approaches have demonstrated that even when the student model is the same size as the teacher, knowledge distillation can still lead to performance improvements. (5 sources)", "text": "\nWhile knowledge distillation was originally proposed as a technique to transfer knowledge from a large, powerful teacher model to a smaller, more efficient student model, recent research has challenged this conventional setup by exploring scenarios where the student and teacher are of comparable size. Size-matched knowledge distillation refers to the approach where the student model has similar or even identical architecture and capacity as the teacher model <Paper corpusId=\"221703021\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper>.\n\nThis departure from the traditional model compression objective has been motivated by findings that knowledge distillation can provide benefits beyond just making models smaller. Research by Furlanello et al. and Hahn and Choi has shown that knowledge distillation can be leveraged to train high-performing student models with the same size as the teacher <Paper corpusId=\"221703021\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper>. This suggests that the knowledge transfer process captures more than just a compression of information; it may also provide regularization effects or transfer beneficial inductive biases.\n\nOne innovative approach in this direction is the deep mutual learning (DML) strategy proposed by Zhang et al., where an ensemble of students with similar architectures learn collaboratively and teach each other throughout the training process. Unlike traditional one-way knowledge transfer from a static pre-defined teacher to a student, mutual learning enables networks to benefit from peer teaching <Paper corpusId=\"258298441\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"26071966\" paperTitle=\"(Zhang et al., 2017)\" isShortName></Paper>. Surprisingly, this collaborative learning approach has been shown to outperform distillation from more powerful yet static teachers in some cases <Paper corpusId=\"26071966\" paperTitle=\"(Zhang et al., 2017)\" isShortName></Paper>.\n\nAnother related concept is online knowledge distillation, where both the large and small models are randomly initialized and learn from each other during training, instead of following the traditional sequential approach where the teacher is fully trained first <Paper corpusId=\"258298441\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This dynamic interaction between models of similar size allows for a more collaborative learning process.\n\nSome research has even demonstrated that student networks in knowledge distillation scenarios can sometimes surpass the accuracy of their teacher networks <Paper corpusId=\"267688311\" paperTitle=\"(Si et al., 2024)\" isShortName></Paper>. This counter-intuitive finding challenges the assumption that a student model's performance is inherently bounded by its teacher's capabilities.\n\nThe Distral (Distill & transfer learning) approach represents another variation that leverages the idea of knowledge sharing between equally capable models. Instead of directly sharing parameters between different workers (models), Distral proposes to share a \"distilled\" policy that captures common behavior across tasks, with each worker trained to solve its specific task while staying close to the shared policy <Paper corpusId=\"31009408\" paperTitle=\"(Teh et al., 2017)\" isShortName></Paper>. This approach has shown efficiency improvements in complex environments and demonstrates greater stability in the learning process.", "citations": [{"id": "(Liu et al., 2020)", "paper": {"corpus_id": 221703021, "title": "Noisy Self-Knowledge Distillation for Text Summarization", "year": 2020, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Yang Liu", "authorId": "39798499"}, {"name": "S. Shen", "authorId": "2072819533"}, {"name": "Mirella Lapata", "authorId": "1747893"}], "n_citations": 44}, "snippets": ["Knowledge distillation refers to a class of methods for training a new smaller student network by learning from a teacher network (in addition to learning from the training data). It is generally assumed that the teacher has been previously trained, and the parameters for the student are estimated by matching the student's predictions to the teacher", "Recent work (Furlanello et al., 2018; Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a high-performing student model with the same size as the teacher"], "score": 0.98046875}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 258298441, "title": "Function-Consistent Feature Distillation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Dongyang Liu", "authorId": "2152509045"}, {"name": "Meina Kan", "authorId": "1693589"}, {"name": "S. Shan", "authorId": "145455919"}, {"name": "Xilin Chen", "authorId": "46772547"}], "n_citations": 19}, "snippets": ["Hinton et al. (2015) first clarifies the concept of knowledge distillation. In their method, softened probability distribution output by the teacher is used as the guidance to the student", "(Zhang et al., 2017) propose online knowledge distillation, where both the large and the small models are randomly initialized and learn mutually from each other during training."], "score": 0.98095703125}, {"id": "(Zhang et al., 2017)", "paper": {"corpus_id": 26071966, "title": "Deep Mutual Learning", "year": 2017, "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "authors": [{"name": "Ying Zhang", "authorId": "46868596"}, {"name": "T. Xiang", "authorId": "145406421"}, {"name": "Timothy M. Hospedales", "authorId": "1697755"}, {"name": "Huchuan Lu", "authorId": "153176123"}], "n_citations": 1655}, "snippets": ["Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, in order to meet the low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy. Different from the one-way transfer between a static pre-defined teacher and a student in model distillation, with DML, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on both category and instance recognition tasks. Surprisingly, it is revealed that no prior powerful teacher network is necessary - mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher."], "score": 0.0}, {"id": "(Si et al., 2024)", "paper": {"corpus_id": 267688311, "title": "Breast Cancer Histopathology Images Classification Through Multi-View Augmented Contrastive Learning and Pre-Learning Knowledge Distillation", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Jialong Si", "authorId": "2284255154"}, {"name": "Wei Jia", "authorId": "2284258056"}, {"name": "Haifeng Jiang", "authorId": "2284308867"}], "n_citations": 5}, "snippets": ["Knowledge distillation is a commonly used technique in deep learning, as proposed by Hinton et al. in 2015 [17]. It is used to transfer knowledge from a large model to a smaller one, resulting in model compression and acceleration. The goal of knowledge distillation is to improve the accuracy and generalizability of the student network by transferring knowledge from the teacher network. Auxiliary information during training is usually obtained from the output of the teacher network. Such techniques have been applied in various domains, including natural language processing, computer vision, and speech recognition. An important use case is the deployment of deep learning models on mobile devices, which often have limited computational resources and require smaller, lightweight models. Knowledge distillation typically involves a larger teacher network and a smaller student network. The teacher network is usually trained on large amounts of data and therefore has higher accuracy and more comprehensive knowledge. Compared to the teacher network, the student network is trained on less data and has fewer parameters. Nevertheless, some student networks can surpass the accuracy of the teacher network."], "score": 0.9833984375}, {"id": "(Teh et al., 2017)", "paper": {"corpus_id": 31009408, "title": "Distral: Robust multitask reinforcement learning", "year": 2017, "venue": "Neural Information Processing Systems", "authors": [{"name": "Y. Teh", "authorId": "1725303"}, {"name": "V. Bapst", "authorId": "2603033"}, {"name": "Wojciech M. Czarnecki", "authorId": "144792148"}, {"name": "John Quan", "authorId": "34660073"}, {"name": "J. Kirkpatrick", "authorId": "2066516991"}, {"name": "R. Hadsell", "authorId": "2315504"}, {"name": "N. Heess", "authorId": "2801204"}, {"name": "Razvan Pascanu", "authorId": "1996134"}], "n_citations": 554}, "snippets": ["Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill & transfer learning). Instead of sharing parameters between the different workers, we propose to share a \"distilled\" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning."], "score": 0.0}], "table": null}, {"title": "Smaller Teacher Models in Knowledge Distillation", "tldr": "Contrary to traditional approaches, research has shown that smaller or weaker teachers can sometimes be more effective for knowledge distillation than larger models. This counterintuitive finding suggests that the alignment between teacher and student capabilities, rather than just teacher performance, is crucial for successful knowledge transfer. (LLM Memory)", "text": "\nWhile conventional wisdom suggests that more powerful teacher models lead to better student performance in knowledge distillation, recent research has challenged this assumption by exploring scenarios where smaller or less powerful teachers can actually be more effective. This phenomenon, sometimes referred to as \"the student surpassing the teacher\" paradox, has been observed in several studies and offers new insights into the knowledge transfer process <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nMirzadeh et al. introduced the concept of the \"teacher assistant (TA)\" knowledge distillation, where they found that directly transferring knowledge from a very large teacher to a small student can be suboptimal. They demonstrated that using an intermediate-sized model (the TA) as a bridge between the large teacher and small student often leads to better performance. This suggests that too large a gap in model capacity between teacher and student can actually hinder effective knowledge transfer <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThis observation aligns with the concept of \"capacity gap\" in knowledge distillation, which refers to the difference in representation capabilities between teacher and student models. When this gap is too large, the student may struggle to mimic the complex functions learned by the teacher, resulting in suboptimal transfer. A smaller, less powerful teacher whose capacity is closer to that of the student may provide more accessible and relevant knowledge <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nSome researchers have even shown that deliberately using weaker teachers can be beneficial in certain contexts. For instance, Zhou et al. found that training a student with a less accurate but more suitable teacher can lead to better generalization performance than using a highly accurate but complex teacher. The intuition is that a simpler teacher may provide more digestible knowledge that aligns better with the student's learning capacity <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nInterestingly, studies have also demonstrated that early-stopped teachers\u2014models that haven't been trained to convergence\u2014can sometimes be more effective for knowledge distillation. These partially trained teachers may provide softer probability distributions that contain more informative signals about class relationships than fully converged models with very peaky distributions <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThe effectiveness of smaller teachers has practical implications as well. Using smaller teacher models reduces the computational overhead of the knowledge distillation process, making it more accessible and environmentally friendly. This approach also challenges the resource-intensive paradigm of \"bigger is better\" that has dominated much of deep learning research <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nOverall, these findings suggest that the effectiveness of knowledge distillation depends not just on how powerful the teacher is, but on how well the teacher's knowledge can be assimilated by the student. The optimal teacher may not necessarily be the largest or most accurate model, but rather one that provides knowledge at an appropriate level of abstraction and complexity for the student to effectively learn from <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [], "table": null}, {"title": "Effectiveness of Non-Traditional Knowledge Distillation Approaches", "tldr": "Non-traditional knowledge distillation approaches such as mutual learning, online distillation, and size-matched distillation have shown surprisingly effective results that challenge conventional wisdom. These innovative methods demonstrate that collaborative learning between peer networks, rather than just one-way transfer from a static teacher, can lead to performance improvements and even enable student models to surpass their teachers. (5 sources)", "text": "\nBuilding upon the insights from traditional and size-matched knowledge distillation approaches, researchers have developed several non-traditional methods that have proven remarkably effective. One such innovative approach is deep mutual learning (DML), introduced by Zhang et al., which fundamentally changes the knowledge transfer dynamics. Unlike the conventional one-way transfer from a pre-defined teacher to a student, DML enables an ensemble of student networks to learn collaboratively and teach each other throughout the training process. Surprisingly, this collaborative approach has not only shown to be effective but in some cases has outperformed distillation from more powerful yet static teachers <Paper corpusId=\"26071966\" paperTitle=\"(Zhang et al., 2017)\" isShortName></Paper>.\n\nThe concept of online knowledge distillation represents another departure from traditional methods. In this approach, both large and small models are randomly initialized and learn mutually from each other during training, rather than following the sequential teacher-then-student training process <Paper corpusId=\"258298441\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"26071966\" paperTitle=\"(Zhang et al., 2017)\" isShortName></Paper>. This dynamic interaction creates a more collaborative learning environment that can lead to improved performance for all participating models.\n\nPerhaps most surprisingly, several studies have shown that student networks in knowledge distillation scenarios can sometimes surpass the accuracy of their teacher networks <Paper corpusId=\"267688311\" paperTitle=\"(Si et al., 2024)\" isShortName></Paper>. This counterintuitive finding challenges the fundamental assumption that a student's performance is inherently bounded by its teacher's capabilities and suggests that the knowledge transfer process may provide additional benefits beyond simply mimicking the teacher's behavior.\n\nThe Distral (Distill & transfer learning) approach represents yet another effective non-traditional method. Rather than directly sharing parameters between different models, Distral proposes to share a \"distilled\" policy that captures common behavior across tasks. Each model is then trained to solve its specific task while staying close to this shared policy <Paper corpusId=\"221703021\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"31009408\" paperTitle=\"(Teh et al., 2017)\" isShortName></Paper>. This approach has demonstrated improved data efficiency and greater stability in the learning process.\n\nThese non-traditional approaches collectively suggest that knowledge distillation is more versatile and powerful than initially conceived. Rather than being limited to a one-way compression technique from large to small models, these methods demonstrate that various forms of knowledge sharing between models\u2014regardless of their relative sizes\u2014can lead to performance improvements. This broader understanding opens up new possibilities for model training and knowledge transfer that extend well beyond the original model compression objective.", "citations": [{"id": "(Zhang et al., 2017)", "paper": {"corpus_id": 26071966, "title": "Deep Mutual Learning", "year": 2017, "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "authors": [{"name": "Ying Zhang", "authorId": "46868596"}, {"name": "T. Xiang", "authorId": "145406421"}, {"name": "Timothy M. Hospedales", "authorId": "1697755"}, {"name": "Huchuan Lu", "authorId": "153176123"}], "n_citations": 1655}, "snippets": ["Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, in order to meet the low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy. Different from the one-way transfer between a static pre-defined teacher and a student in model distillation, with DML, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on both category and instance recognition tasks. Surprisingly, it is revealed that no prior powerful teacher network is necessary - mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher."], "score": 0.0}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 258298441, "title": "Function-Consistent Feature Distillation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Dongyang Liu", "authorId": "2152509045"}, {"name": "Meina Kan", "authorId": "1693589"}, {"name": "S. Shan", "authorId": "145455919"}, {"name": "Xilin Chen", "authorId": "46772547"}], "n_citations": 19}, "snippets": ["Hinton et al. (2015) first clarifies the concept of knowledge distillation. In their method, softened probability distribution output by the teacher is used as the guidance to the student", "(Zhang et al., 2017) propose online knowledge distillation, where both the large and the small models are randomly initialized and learn mutually from each other during training."], "score": 0.98095703125}, {"id": "(Si et al., 2024)", "paper": {"corpus_id": 267688311, "title": "Breast Cancer Histopathology Images Classification Through Multi-View Augmented Contrastive Learning and Pre-Learning Knowledge Distillation", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Jialong Si", "authorId": "2284255154"}, {"name": "Wei Jia", "authorId": "2284258056"}, {"name": "Haifeng Jiang", "authorId": "2284308867"}], "n_citations": 5}, "snippets": ["Knowledge distillation is a commonly used technique in deep learning, as proposed by Hinton et al. in 2015 [17]. It is used to transfer knowledge from a large model to a smaller one, resulting in model compression and acceleration. The goal of knowledge distillation is to improve the accuracy and generalizability of the student network by transferring knowledge from the teacher network. Auxiliary information during training is usually obtained from the output of the teacher network. Such techniques have been applied in various domains, including natural language processing, computer vision, and speech recognition. An important use case is the deployment of deep learning models on mobile devices, which often have limited computational resources and require smaller, lightweight models. Knowledge distillation typically involves a larger teacher network and a smaller student network. The teacher network is usually trained on large amounts of data and therefore has higher accuracy and more comprehensive knowledge. Compared to the teacher network, the student network is trained on less data and has fewer parameters. Nevertheless, some student networks can surpass the accuracy of the teacher network."], "score": 0.9833984375}, {"id": "(Liu et al., 2020)", "paper": {"corpus_id": 221703021, "title": "Noisy Self-Knowledge Distillation for Text Summarization", "year": 2020, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Yang Liu", "authorId": "39798499"}, {"name": "S. Shen", "authorId": "2072819533"}, {"name": "Mirella Lapata", "authorId": "1747893"}], "n_citations": 44}, "snippets": ["Knowledge distillation refers to a class of methods for training a new smaller student network by learning from a teacher network (in addition to learning from the training data). It is generally assumed that the teacher has been previously trained, and the parameters for the student are estimated by matching the student's predictions to the teacher", "Recent work (Furlanello et al., 2018; Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a high-performing student model with the same size as the teacher"], "score": 0.98046875}, {"id": "(Teh et al., 2017)", "paper": {"corpus_id": 31009408, "title": "Distral: Robust multitask reinforcement learning", "year": 2017, "venue": "Neural Information Processing Systems", "authors": [{"name": "Y. Teh", "authorId": "1725303"}, {"name": "V. Bapst", "authorId": "2603033"}, {"name": "Wojciech M. Czarnecki", "authorId": "144792148"}, {"name": "John Quan", "authorId": "34660073"}, {"name": "J. Kirkpatrick", "authorId": "2066516991"}, {"name": "R. Hadsell", "authorId": "2315504"}, {"name": "N. Heess", "authorId": "2801204"}, {"name": "Razvan Pascanu", "authorId": "1996134"}], "n_citations": 554}, "snippets": ["Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill & transfer learning). Instead of sharing parameters between the different workers, we propose to share a \"distilled\" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning."], "score": 0.0}], "table": null}], "cost": 0.44516700000000003}}
