{"original_query": "Could you please provide some references to work on multi-document summarization?", "input": "Could you references work on multi document summarization", "output": {"sections": [{"title": "Introduction and Background", "tldr": "Multi-document summarization (MDS) aims to generate concise summaries from multiple topic-related documents, requiring systems to handle redundancy, conflicting information, and cross-document relationships. This challenging task has evolved from traditional extractive methods to neural approaches, with recent advances leveraging graph-based techniques and pre-trained language models. (11 sources)", "text": "\nMulti-document summarization (MDS) is the task of generating a concise and informative summary from a set of topic-related documents <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"238221709\" paperTitle=\"(Jalil et al., 2021)\" isShortName></Paper>. Unlike single-document summarization, MDS presents unique challenges due to the need to synthesize information from diverse sources that may contain redundant, complementary, or contradictory information <Paper corpusId=\"272969413\" paperTitle=\"(Godbole et al., 2024)\" isShortName></Paper> <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>. A multi-document summary serves as a representation of the information contained in a cluster of documents, helping users understand the gist of those documents without having to read them all <Paper corpusId=\"238221709\" paperTitle=\"(Jalil et al., 2021)\" isShortName></Paper>.\n\nMDS methods can be broadly categorized into extractive and abstractive approaches <Paper corpusId=\"226283949\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper> <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Extractive approaches select and arrange key sentences from the original documents based on their importance <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Early extractive methods relied on techniques such as retrieval, centroids, graph centrality, and utility maximization <Paper corpusId=\"222140880\" paperTitle=\"(Bista et al., 2020)\" isShortName></Paper>. The LexRank algorithm, which uses graph-based centrality measures to identify important sentences, has been particularly influential in this area <Paper corpusId=\"226283949\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper> <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>.\n\nIn contrast, abstractive approaches can generate more flexible and coherent summaries by creating new sentences that capture the essence of the source documents <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. However, due to the historical lack of large-scale training data, most earlier MDS methods were extractive <Paper corpusId=\"226283949\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper>. This situation has been changing with the introduction of datasets like Multi-News, which has enabled more research into neural abstractive MDS <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper>.\n\nRecent advances in MDS have been driven by several key developments. First, the application of pre-trained language models, which have shown promising results in single-document summarization, to the multi-document setting <Paper corpusId=\"278000561\" paperTitle=\"(Tan et al., 2025)\" isShortName></Paper>. Second, the use of hierarchical models that can extract different-grained features and select important information across documents <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Third, the incorporation of graph-based methods to model relationships between textual units and enhance representation <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"278000561\" paperTitle=\"(Tan et al., 2025)\" isShortName></Paper>.\n\nDespite these advances, MDS still faces significant challenges. Simply adopting models that work well for single-document summarization may not lead to ideal results in the multi-document setting <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"52053741\" paperTitle=\"(Lebanoff et al., 2018)\" isShortName></Paper>. Cross-document relationships need to be effectively modeled <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"170079112\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>, and systems must deal with issues of redundancy, inconsistency, lack of context understanding, and difficulty handling diverse formats <Paper corpusId=\"272969413\" paperTitle=\"(Godbole et al., 2024)\" isShortName></Paper>.\n\nMDS has diverse applications across domains, including news aggregation, scientific research, and legal document analysis <Paper corpusId=\"278000561\" paperTitle=\"(Tan et al., 2025)\" isShortName></Paper>. Each domain presents its own unique challenges and requirements, driving the need for specialized approaches and domain-specific datasets.", "citations": [{"id": "(Liu et al., 2024)", "paper": {"corpus_id": 271903777, "title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization", "year": 2024, "venue": "European Conference on Artificial Intelligence", "authors": [{"name": "Ran Liu", "authorId": "2316447969"}, {"name": "Ming Liu", "authorId": "2287803134"}, {"name": "Min Yu", "authorId": "2316535336"}, {"name": "Jianguo Jiang", "authorId": "152634491"}, {"name": "Gang Li", "authorId": "1996030117"}, {"name": "Dan Zhang", "authorId": "2316503511"}, {"name": "Jingyuan Li", "authorId": "2316432820"}, {"name": "Xiang Meng", "authorId": "2282447587"}, {"name": "Weiqing Huang", "authorId": "2282485475"}], "n_citations": 0}, "snippets": ["Multi-document summarization (MDS) aims to produce a summary from a document set containing a series of related topics. The generated summary needs to cover all important information in the document set, while remaining fluent and concise.\n\nNon-neural approaches are primarily based on extracting key sentences (Erkan and Radev, 2004;Mihalcea and Tarau, 2004;Rossiello et al., 2017). These approaches assess sentence importance based on their relevance to each other or proximity to keywords, selecting sentences with high importance scores to form the summary.\n\nNeural approaches can generate more abstractive text and are recently widely used in multidocument summarization. Given the structural characteristics of the multi-document input, most approaches utilize attention mechanism to build hierarchical models (Fabbri et al., 2019;Mao et al., 2020;Jin et al., 2020), enabling the extraction of different-grained features and the selection of important information. Other methods employ graphs to model relationships and can leverage interaction features to enhance representation (Yasunaga et al., 2017;Yin et al., 2019;Li et al., 2020)."], "score": 0.90869140625}, {"id": "(Jalil et al., 2021)", "paper": {"corpus_id": 238221709, "title": "Extractive Multi-Document Summarization: A Review of Progress in the Last Decade", "year": 2021, "venue": "IEEE Access", "authors": [{"name": "Zakia Jalil", "authorId": "70642020"}, {"name": "Jamal Abdul Nasir", "authorId": "35560924"}, {"name": "Muhammad Nasir", "authorId": "2128893061"}], "n_citations": 14}, "snippets": ["For instance, the same contents may be covered from multiple sources, so at times, a number of documents may be available to gain an insight into the same event (Glavas et al., 2014). In this regard, a multi-document summary becomes a representation of the information contained in a cluster of documents which helps users understand the gist of those documents (Alguliyev et al., 2012), (Luo et al., 2013). A multi-document summary represents the information contained in the cluster of documents and helps users understand those documents (Alguliyev et al., 2013)."], "score": 0.904296875}, {"id": "(Godbole et al., 2024)", "paper": {"corpus_id": 272969413, "title": "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Aditi Godbole", "authorId": "2350511520"}, {"name": "Jabin Geevarghese George", "authorId": "2301049091"}, {"name": "Smita Shandilya", "authorId": "48781397"}], "n_citations": 5}, "snippets": ["Multi-document summarization presents unique challenges due to the need for synthesizing information from diverse sources, which may contain redundant, complementary, or contradictory information across documents [4]. Variations in writing style and level of detail add complexity to the task. Determining the relevance and importance of information from each source is crucial for creating a coherent and comprehensive summary [5].\n\nTraditional document summarization techniques often struggle with redundancy, inconsistency, lack of context understanding, scalability issues for multiple document summarization tasks, inability to capture cross-document relationships, difficulty handling diverse formats, and lack of domain adaptability [6,7]8]. These limitations highlight the need for more advanced approaches to multi-document summarization."], "score": 0.93115234375}, {"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 236478143, "title": "Entity-Aware Abstractive Multi-Document Summarization", "year": 2021, "venue": "Findings", "authors": [{"name": "Hao Zhou", "authorId": null}, {"name": "Weidong Ren", "authorId": "2053308860"}, {"name": "Gongshen Liu", "authorId": "150112803"}, {"name": "Bo Su", "authorId": "153253583"}, {"name": "Wei Lu", "authorId": "143844110"}], "n_citations": 28}, "snippets": ["Multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020).\n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018).\n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020)."], "score": 0.93408203125}, {"id": "(Jin et al., 2020)", "paper": {"corpus_id": 226283949, "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization", "year": 2020, "venue": "Findings", "authors": [{"name": "Hanqi Jin", "authorId": "1491212422"}, {"name": "Xiaojun Wan", "authorId": "145078589"}], "n_citations": 15}, "snippets": ["Multi-document summarization aims at producing a fluent, condensed summary for the given document set", "Compared with single-document summarization, multi-document summarization needs to handle multiple input documents. A simple method is to concatenate multiple documents into a long flat text and treat it as a long sequence-tosequence task.\n\nEmpowered by large parallel datasets automatically harvested from online news websites, sequence-to-sequence learning has shown promising results on abstractive single-document summarization (See et al., 2017; Paulus et al., 2018; Tan et al., 2017; \u00c7 elikyilmaz et al., 2018)", "Several works have explored adapting the neural encoder-decoder model trained for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural encoder-decoder model trained for singledocument summarization to address the information redundancy for multi-document summarization.\n\nThe methods for multi-document summarization can generally be categorized to extractive and abstractive", "Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004; Christensen et al., 2013; Yasunaga et al., 2017)."], "score": 0.958984375}, {"id": "(Bista et al., 2020)", "paper": {"corpus_id": 222140880, "title": "SupMMD: A Sentence Importance Model for Extractive Summarisation using Maximum Mean Discrepancy", "year": 2020, "venue": "Findings", "authors": [{"name": "Umanga Bista", "authorId": "2168767"}, {"name": "A. Mathews", "authorId": "3175685"}, {"name": "A. Menon", "authorId": "2844480"}, {"name": "Lexing Xie", "authorId": "33650938"}], "n_citations": 2}, "snippets": ["Most existing work on this topic has focused on the generic summarization task. However, update summarization is of equal practical interest.\n\nMulti-document extractive summarization methods can be unsupervised or supervised. Unsupervised methods typically define salience (or coverage) using a global model of sentence-sentence similarity. Methods based on retrieval (Goldstein et al., 1999), centroids (Radev et al., 2004), graph centrality (Erkan and Radev, 2004), or utility maximization (Lin andBilmes, 2010, 2011;Gillick and Favre, 2009) have been well explored. However, sentence salience also depends on surface features (e.g., position, length, presence of cue words); effectively capturing these requires supervised models specific to the dataset and task. A body of work has incorporated such information through supervised learning, for example based on point processes (Kulesza and Taskar, 2012), learning important words (Hong and Nenkova, 2014), graph neural networks (Yasunaga et al., 2017), and support vector regression (Varma et al., 2009)."], "score": 0.9072265625}, {"id": "(Erkan et al., 2004)", "paper": {"corpus_id": 506350, "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "year": 2004, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 3097}, "snippets": ["We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."], "score": 0.0}, {"id": "(Fabbri et al., 2019)", "paper": {"corpus_id": 174799390, "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Alexander R. Fabbri", "authorId": "46255971"}, {"name": "Irene Li", "authorId": "46331602"}, {"name": "Tianwei She", "authorId": "2106009217"}, {"name": "Suyi Li", "authorId": "50341789"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 590}, "snippets": ["Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples."], "score": 0.9287109375}, {"id": "(Tan et al., 2025)", "paper": {"corpus_id": 278000561, "title": "A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shiyin Tan", "authorId": "148149386"}, {"name": "Jaeeon Park", "authorId": "2357102667"}, {"name": "Dongyuan Li", "authorId": "2242195007"}, {"name": "Renhe Jiang", "authorId": "2299193401"}, {"name": "Manabu Okumura", "authorId": "2283854880"}], "n_citations": 0}, "snippets": ["Multi-document summarization (MDS) is a task that aims to generate concise and coherent summaries by synthesizing information from multiple documents on the same topic (Jin et al., 2020)(Li et al., 2020)(Ma, 2021)(Mao et al., 2020)(Pang et al., 2021). MDS can lead to diverse applications, such as news aggregation (Chen et al., 2024)(Fabbri et al., 2019)(Khatuya et al., 2024), scientific research (DeYoung et al., 2021)(Lu et al., 2020)(Wang et al., 2024), and legal document analysis [17,(Malik et al., 2024)(Shen et al., 2022). Current MDS approaches can be categorized into two classes: Graph-based models (Cui et al., 2021)(Li et al., 2023)(Pasunuru et al., 2021)(Qu, 2024)(Zhang et al., 2023) and pre-trained language models [2](Puduppully et al., 2022)(Xiao et al., 2021). Graph-based models rely on auxiliary information (e.g., discourse structures) as an input graph to capture the cross-document relationships, while pre-trained language models use the attention mechanisms to capture them."], "score": 0.92041015625}, {"id": "(Lebanoff et al., 2018)", "paper": {"corpus_id": 52053741, "title": "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization", "year": 2018, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Logan Lebanoff", "authorId": "50827114"}, {"name": "Kaiqiang Song", "authorId": "50982080"}, {"name": "Fei Liu", "authorId": "144544919"}], "n_citations": 157}, "snippets": ["Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors."], "score": 0.0}, {"id": "(Liu et al., 2019)", "paper": {"corpus_id": 170079112, "title": "Hierarchical Transformers for Multi-Document Summarization", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yang Liu", "authorId": "39798499"}, {"name": "Mirella Lapata", "authorId": "1747893"}], "n_citations": 298}, "snippets": ["In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines."], "score": 0.9189453125}], "table": null}, {"title": "Evolution of Multi-Document Summarization Approaches", "tldr": "Multi-document summarization has evolved from early graph-based extractive approaches in the late 1990s to sophisticated neural abstractive methods in recent years, with four major developmental phases: graph ranking methods, compression-based approaches, neural sequence-to-sequence models, and more recent hierarchical and graph-enhanced neural architectures. (13 sources)", "text": "\nThe development of multi-document summarization (MDS) began in the late 1990s, motivated largely by the growing popularity of the World Wide Web and the increasing need to synthesize information from multiple sources <Paper corpusId=\"6025826\" paperTitle=\"(Mani et al., 1997)\" isShortName></Paper> <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>. These early approaches were primarily graph-based, with Mani et al. developing methods to represent documents as graphs and perform graph matching to identify salient regions across documents <Paper corpusId=\"6025826\" paperTitle=\"(Mani et al., 1997)\" isShortName></Paper>. Similarly, Radev et al. explored mapping documents to template representations for summarization <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>.\n\nThe early 2000s marked a significant advancement with the introduction of the Document Understanding Conference (DUC) datasets, which included human-written summaries for multi-document clusters <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>. This period saw the development of influential algorithms like LexRank, which extracted salient sentences by constructing similarity graphs and applying PageRank-like algorithms <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. Other researchers extended single-document summarization approaches to the multi-document setting using various techniques, including template filling, named entity extraction, and co-reference chains <Paper corpusId=\"8294822\" paperTitle=\"(Goldstein et al., 2000)\" isShortName></Paper>.\n\nThe field has evolved through four primary approaches over time <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>. The first phase focused on graph ranking-based extractive methods like TextRank and LexRank <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. The second phase introduced syntax and structure-based compression methods to address redundancy and paraphrasing issues across documents, as demonstrated by Li et al. <Paper corpusId=\"10112929\" paperTitle=\"(Li et al., 2014)\" isShortName></Paper> <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>.\n\nThe third major shift came with neural sequence-to-sequence abstractive methods around 2017, building upon advances in single-document summarization <Paper corpusId=\"248496597\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. This transition was facilitated by the introduction of larger datasets like WikiSum and MultiNews, which enabled training of neural models for MDS <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper> <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper>.\n\nMost recently, approaches have become more sophisticated, with hierarchical and flat architectures addressing the complexities of multi-document input. Flat solutions treat MDS as a single-document task, leveraging pre-trained models but struggling with long inputs <Paper corpusId=\"248780330\" paperTitle=\"(Moro et al., 2022)\" isShortName></Paper>. Hierarchical solutions better preserve cross-document relations using graph-based techniques at word, sentence, and document levels <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"170079112\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>. These include attention-based models with maximal marginal relevance <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper> and models incorporating attention across different granularity representations <Paper corpusId=\"220045815\" paperTitle=\"(Jin et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"248780330\" paperTitle=\"(Moro et al., 2022)\" isShortName></Paper>.\n\nAnother noteworthy approach involves hierarchically combining single-document summaries to create multi-document summaries, which has achieved competitive results <Paper corpusId=\"9174081\" paperTitle=\"(Marujo et al., 2015)\" isShortName></Paper>. The latest research continues to integrate graph-based approaches with neural methods to better model relationships between textual units across documents <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Mani et al., 1997)", "paper": {"corpus_id": 6025826, "title": "Multi-Document Summarization by Graph Search and Matching", "year": 1997, "venue": "AAAI/IAAI", "authors": [{"name": "I. Mani", "authorId": "1729172"}, {"name": "E. Bloedorn", "authorId": "2740861"}], "n_citations": 252}, "snippets": ["We describe a new method for summarizing similarities and differences in a pair of related documents using a graph representation for text. Concepts denoted by words, phrases, and proper names in the document are represented positionally as nodes in the graph along with edges corresponding to semantic relations between items. Given a perspective in terms of which the pair of documents is to be summarized, the algorithm first uses a spreading activation technique to discover, in each document, nodes semantically related to the topic. The activated graphs of each document are then matched to yield a graph corresponding to similarities and differences between the pair, which is rendered in natural language. An evaluation of these techniques has been carried out."], "score": 0.0}, {"id": "(Pasunuru et al., 2021)", "paper": {"corpus_id": 235097309, "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ramakanth Pasunuru", "authorId": "10721120"}, {"name": "Mengwen Liu", "authorId": "2940333"}, {"name": "Mohit Bansal", "authorId": "143977268"}, {"name": "Sujith Ravi", "authorId": "120209444"}, {"name": "Markus Dreyer", "authorId": "40262269"}], "n_citations": 73}, "snippets": ["Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com"], "score": 0.91064453125}, {"id": "(Erkan et al., 2004)", "paper": {"corpus_id": 506350, "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "year": 2004, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 3097}, "snippets": ["We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."], "score": 0.0}, {"id": "(Goldstein et al., 2000)", "paper": {"corpus_id": 8294822, "title": "Multi-Document Summarization By Sentence Extraction", "year": 2000, "venue": "", "authors": [{"name": "J. Goldstein", "authorId": "40561307"}, {"name": "Vibhu Mittal", "authorId": "1751139"}, {"name": "J. Carbonell", "authorId": "143712374"}, {"name": "M. Kantrowitz", "authorId": "50404544"}], "n_citations": 447}, "snippets": ["Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani et al., 1997)(Carbonell et al., 1998)TIPSTER, 1998b;Radev and McKeown, 1998;Mani and Bloedorn, 1999;McKeown et al., .!999;(Stein, 1999). These include comparing templates filled in by extracting information -using specialized, domain specific knowledge sources -from the doc-\"ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), com--\u2022 paring named-entities -extracted using specialized lists between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building activation networks of related lexical items (identity mappings, synonyms, hypernyms, etc.) to extract text spans from the document set (Mani et al., 1997). Another system (Stein, 1999) creates a multi-document summary from multiple single document summaries, an approach that can be sub-optimal in some cases, due to the fact that the process of generating the final multi-document summary takes as input the individual summaries and not the complete documents. (Particularly if the single-document summaries can contain much overlapping information.) The Columbia University system (McKeown et al., 1999) creates a multi-document summary using machine learning and statistical techniques to identify similar sections and language generation to reformulate the summary."], "score": 0.9462890625}, {"id": "(Sankar et al., 2022)", "paper": {"corpus_id": 248571519, "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Aiswarya Sankar", "authorId": "2064325789"}, {"name": "Ankit R. Chadha", "authorId": "145934595"}], "n_citations": 0}, "snippets": ["Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."], "score": 0.9453125}, {"id": "(Li et al., 2014)", "paper": {"corpus_id": 10112929, "title": "Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees", "year": 2014, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Chen Li", "authorId": "40475614"}, {"name": "Yang Liu", "authorId": "2152797181"}, {"name": "Fei Liu", "authorId": "144544919"}, {"name": "Lin Zhao", "authorId": "46586837"}, {"name": "F. Weng", "authorId": "1807350"}], "n_citations": 46}, "snippets": ["In this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status \u2013 remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality of the compressed sentences. Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary. Compared with state-ofthe-art algorithms, our model has similar ROUGE-2 scores but better linguistic quality on TAC data."], "score": 0.0}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 248496597, "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Ning Wang", "authorId": "2152171283"}, {"name": "Han Liu", "authorId": "2140162523"}, {"name": "D. Klabjan", "authorId": "1753376"}], "n_citations": 1}, "snippets": ["Multi-document summarization (MDS) tries to solve the task of generating a summary for a collection of documents sharing the same topic. This field has been studied for a long time (Kathleen McKeown and Dragomir R Radev, 1995; Carbonell and Goldstein, 1998). Traditional approaches to MDS have been extractive, where part of the original text is selected and then organized to form a summary. After that many abstractive methods have been developed. In recent years, neural network-based methods have been predominant."], "score": 0.90380859375}, {"id": "(Fabbri et al., 2019)", "paper": {"corpus_id": 174799390, "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Alexander R. Fabbri", "authorId": "46255971"}, {"name": "Irene Li", "authorId": "46331602"}, {"name": "Tianwei She", "authorId": "2106009217"}, {"name": "Suyi Li", "authorId": "50341789"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 590}, "snippets": ["Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples."], "score": 0.9287109375}, {"id": "(Moro et al., 2022)", "paper": {"corpus_id": 248780330, "title": "Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "G. Moro", "authorId": "143853729"}, {"name": "Luca Ragazzi", "authorId": "134327204"}, {"name": "Lorenzo Valgimigli", "authorId": "2132084411"}, {"name": "Davide Freddi", "authorId": "2165227434"}], "n_citations": 32}, "snippets": ["Flat solutions. Flat concatenation is a simple yet powerful solution because the generation of the multi-document summary is treated as a singledocument summarization task, thus it can leverage state-of-the-art pre-trained summarization models. \n\nConsequently, processing all documents as a flat input requires models capable of handling long sequences. As previously experimented by (DeYoung et al., 2021), Xiao et al. (2021) proposed to leverage the Longformer-Encoder-Decoder model (Beltagy et al., 2020) pre-trained with a novel multi-document summarization specific task. They proved that a long-range Transformer that encodes all documents is a straightforward yet effective solution, and they achieved new state-of-the-art results in several multi-document summarization datasets. However, such models may struggle to handle a massive cluster of topic-related documents since they need to truncate them because of architectural limits. Further, processing all documents in a cluster could be noisy if some of them are not relevant or factual with respect to the summary. \n\nHierarchical solutions. To better preserve crossdocument relations and obtain semantic-rich representations, hierarchical concatenation solutions leverage graph-based techniques to work from word and sentence-level (Wan et al., 2006)(Liao et al., 2018)(Nayeem et al., 2018)Antognini and Faltings, 2019;(Li et al., 2020) to documentlevel (Amplayo and Lapata, 2021). Other hierarchical approaches include multi-head pooling and inter-paragraph attention architectures (Liu et al., 2019), attention models with maximal marginal relevance (Fabbri et al., 2019), and attention across different granularity representations (Jin et al., 2020). Such models are often datasetspecific because of the custom architecture, so they struggle to adapt to other datasets and effectively leverage pre-trained state-of-the-art Transformers."], "score": 0.9375}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 218718706, "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}, {"name": "Junping Du", "authorId": "2117218629"}], "n_citations": 136}, "snippets": ["Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries."], "score": 0.93505859375}, {"id": "(Liu et al., 2019)", "paper": {"corpus_id": 170079112, "title": "Hierarchical Transformers for Multi-Document Summarization", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yang Liu", "authorId": "39798499"}, {"name": "Mirella Lapata", "authorId": "1747893"}], "n_citations": 298}, "snippets": ["In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines."], "score": 0.9189453125}, {"id": "(Jin et al._1, 2020)", "paper": {"corpus_id": 220045815, "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Hanqi Jin", "authorId": "1491212422"}, {"name": "Tian-ming Wang", "authorId": "1751960"}, {"name": "Xiaojun Wan", "authorId": "145078589"}], "n_citations": 99}, "snippets": ["The methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Since the neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u0327elikyilmaz et al., 2018), some works trained abstractive summarization models on a single document dataset and adjusted the model to adapt the multi-document summarization task. Zhang et al. (2018) added a document set encoder into the single document summarization framework and tuned the pre-trained model on the multi-document summarization dataset. Lebanoff et al. (2018) combined an extractive summarization algorithm (MMR) for sentence extraction to reweight the original sentence importance distribution learned in the single document abstractive summarization model."], "score": 0.94775390625}, {"id": "(Marujo et al., 2015)", "paper": {"corpus_id": 9174081, "title": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach", "year": 2015, "venue": "International Workshop on Semantic Evaluation", "authors": [{"name": "Lu\u00eds Marujo", "authorId": "1728026"}, {"name": "Ricardo Ribeiro", "authorId": "145648625"}, {"name": "David Martins de Matos", "authorId": "4284219"}, {"name": "J. Neto", "authorId": "1723288"}, {"name": "A. Gershman", "authorId": "145001267"}, {"name": "J. Carbonell", "authorId": "143712374"}], "n_citations": 17}, "snippets": ["The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.\n\nIn general, multi-document summarization approaches have to address two different problems: passage selection and information ordering. Current multi-document systems adopt, for passage selection, approaches similar to the ones used in single-document summarization, and use the chronological order of the documents for information ordering (Christensen et al., 2013). The problem is that most approaches fail to generate summaries that cover generic topics which comprehend different, equally important, subtopics.\n\nWe propose to extend a state-of-the-art single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013), capable of focusing on diverse important topics while ignoring unimportant ones, to perform multi-document summarization."], "score": 0.93408203125}], "table": null}, {"title": "Extractive Approaches to Multi-Document Summarization", "tldr": "Extractive multi-document summarization selects important sentences from source documents using various computational approaches to identify the most salient information while addressing redundancy. The most influential methods include graph-based models like LexRank, centroid-based approaches, clustering techniques, and feature-based methods. (14 sources)", "text": "\nExtractive approaches to multi-document summarization aim to identify and select the most important sentences from input documents to produce a concise summary. These methods face a central challenge of identifying redundant information across related documents while determining which repeated content signals importance <Paper corpusId=\"152281\" paperTitle=\"(Branavan et al., 2008)\" isShortName></Paper> <Paper corpusId=\"7031344\" paperTitle=\"(Barzilay et al., 1999)\" isShortName></Paper>.\n\nGraph-based methods have been particularly influential in extractive MDS. LexRank, introduced by Erkan and Radev, uses eigenvector centrality in a graph representation where sentences are connected based on similarity, applying PageRank-like algorithms to identify salient sentences <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. This approach has proven remarkably effective, ranking first in DUC 2004 evaluations and outperforming both centroid-based methods and other competing systems. LexRank's effectiveness partly stems from its robustness to noise in document clustering <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>.\n\nOther notable graph-based approaches include the affinity graph method proposed by Wan et al., which incorporates a diffusion process to capture semantic relationships between sentences and differentiates between intra-document and inter-document links <Paper corpusId=\"5457260\" paperTitle=\"(Wan et al., 2006)\" isShortName></Paper>. Graph models effectively leverage the \"voting\" or \"recommendations\" between sentences to determine importance <Paper corpusId=\"17446655\" paperTitle=\"(Wan, 2008)\" isShortName></Paper>.\n\nCentroid-based approaches represent another important category of extractive methods. These techniques focus on identifying centroid words or embeddings that represent the commonality across all documents <Paper corpusId=\"257364970\" paperTitle=\"(Ma, 2023)\" isShortName></Paper>. Radev et al. introduced MEAD, a system that generates summaries using cluster centroids produced by topic detection and tracking <Paper corpusId=\"1320\" paperTitle=\"(Radev et al., 2000)\" isShortName></Paper>. More recently, Rossiello et al. enhanced centroid-based methods by exploiting word embeddings to capture semantic relationships between concepts, proving effective for multilingual datasets <Paper corpusId=\"2346086\" paperTitle=\"(Rossiello et al., 2017)\" isShortName></Paper>.\n\nClustering-based methods divide sentences into multiple groups and select representative sentences from each cluster <Paper corpusId=\"257364970\" paperTitle=\"(Ma, 2023)\" isShortName></Paper>. This approach helps address redundancy issues by grouping similar information together. Many algorithms first cluster sentences and then extract or generate representatives for those clusters <Paper corpusId=\"152281\" paperTitle=\"(Branavan et al., 2008)\" isShortName></Paper>.\n\nFeature-based methods utilize various sentence attributes to determine importance. Kumar et al. identified four primary approaches to multi-document summarization: feature-based, cluster-based, graph-based, and knowledge-based methods <Paper corpusId=\"15926944\" paperTitle=\"(Kumar et al., 2012)\" isShortName></Paper>. Supervised approaches incorporate surface features like sentence position, length, and presence of cue words through supervised learning algorithms, including support vector regression <Paper corpusId=\"222140880\" paperTitle=\"(Bista et al., 2020)\" isShortName></Paper>.\n\nOther significant extractive approaches include Maximum Marginal Relevance (MMR), which uses a greedy approach to select sentences while balancing relevance and redundancy <Paper corpusId=\"202889056\" paperTitle=\"(Antognini et al., 2019)\" isShortName></Paper>. Cross-document structure theory, proposed by Radev, provides a theoretical foundation for multi-document summarization by accounting for rhetorical structures across related documents <Paper corpusId=\"10103200\" paperTitle=\"(Radev, 2000)\" isShortName></Paper>.\n\nDespite the emergence of neural abstractive methods, extractive approaches remain important due to their effectiveness and relatively lower resource requirements <Paper corpusId=\"220045815\" paperTitle=\"(Jin et al._1, 2020)\" isShortName></Paper>. Traditional extractive methods have been the dominant approach historically due to the scarcity of large-scale training data needed for abstractive approaches <Paper corpusId=\"236150987\" paperTitle=\"(Perez-Beltrachini et al., 2021)\" isShortName></Paper>.", "citations": [{"id": "(Branavan et al., 2008)", "paper": {"corpus_id": 152281, "title": "Learning Document-Level Semantic Properties from Free-Text Annotations", "year": 2008, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "S. Branavan", "authorId": "1741598"}, {"name": "Harr Chen", "authorId": "3152698"}, {"name": "Jacob Eisenstein", "authorId": "144154709"}, {"name": "R. Barzilay", "authorId": "1741283"}], "n_citations": 122}, "snippets": ["This paper also relates to a large body of work in multi-document summarization. Researchers have long noted that a central challenge of multi-document summarization is identifying redundant information over input documents (Radev et al., 1998)(Carbonell et al., 1998)(Mani et al., 1997)Barzilay, McKeown, & Elhadad, 1999). This task is of crucial significance because multi-document summarizers operate over related documents that describe the same facts multiple times. In fact, it is common to assume that repetition of information among related sources is an indicator of its importance (Barzilay et al., 1999;(Radev et al., 2000)(Nenkova et al., 2006). Many of these algorithms first cluster sentences together, and then extract or generate sentence representatives for the clusters."], "score": 0.931640625}, {"id": "(Barzilay et al., 1999)", "paper": {"corpus_id": 7031344, "title": "Information Fusion in the Context of Multi-Document Summarization", "year": 1999, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "R. Barzilay", "authorId": "1741283"}, {"name": "K. McKeown", "authorId": "145590324"}, {"name": "Michael Elhadad", "authorId": "1754680"}], "n_citations": 484}, "snippets": ["Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin et al., 1997)(Marcu, 1997)(Salton et al., 1994). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while ex-tracting some similar sentences could produce a summary biased towards some sources."], "score": 0.90478515625}, {"id": "(Erkan et al., 2004)", "paper": {"corpus_id": 506350, "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "year": 2004, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 3097}, "snippets": ["We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."], "score": 0.0}, {"id": "(Wan et al., 2006)", "paper": {"corpus_id": 5457260, "title": "Improved Affinity Graph Based Multi-Document Summarization", "year": 2006, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Xiaojun Wan", "authorId": "145078589"}, {"name": "Jianwu Yang", "authorId": "1743923"}], "n_citations": 109}, "snippets": ["This paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems."], "score": 0.0}, {"id": "(Wan, 2008)", "paper": {"corpus_id": 17446655, "title": "An Exploration of Document Impact on Graph-Based Multi-Document Summarization", "year": 2008, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xiaojun Wan", "authorId": "145078589"}], "n_citations": 82}, "snippets": ["Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the \"voting\" or \"recommendations\" between sentences in the documents (Erkan et al., 2004)(Mihalcea et al., 2005)(Wan et al., 2006)."], "score": 0.9169921875}, {"id": "(Ma, 2023)", "paper": {"corpus_id": 257364970, "title": "Mining Both Commonality and Specificity From Multiple Documents for Multi-Document Summarization", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Bing Ma", "authorId": "2084599072"}], "n_citations": 3}, "snippets": ["Most extractive multi-document summarization approaches splice all the sentences contained in the original documents into a larger text, and then generate a summary by selecting sentences from the larger text (Lamsiyah et al., 2021)(Yang et al., 2014)(Erkan et al., 2004)", "The centroid-based summarization approaches focus on the commonality of all documents or all sentences and they select sentences based on the centroid words of all documents (Radev et al., 2004;(Rossiello et al., 2017) or the centroid embedding of all sentences (Lamsiyah et al., 2021). The clustering-based summarization approaches divide sentences into multiple groups and select sentences from each group (Yang et al., 2014)Sarkar, 2009)."], "score": 0.91455078125}, {"id": "(Radev et al., 2000)", "paper": {"corpus_id": 1320, "title": "Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies", "year": 2000, "venue": "arXiv.org", "authors": [{"name": "Dragomir R. Radev", "authorId": "9215251"}, {"name": "Hongyan Jing", "authorId": "40544823"}, {"name": "M. Budzikowska", "authorId": "3166871"}], "n_citations": 584}, "snippets": ["We also looked at a property of multi-document clusters, namely cross-sentence information subsumption (which is related to the MMR metric proposed in (Carbonell et al., 1998)) and showed how it can be used in evaluating multidocument summaries."], "score": 0.94189453125}, {"id": "(Rossiello et al., 2017)", "paper": {"corpus_id": 2346086, "title": "Centroid-based Text Summarization through Compositionality of Word Embeddings", "year": 2017, "venue": "MultiLing@EACL", "authors": [{"name": "Gaetano Rossiello", "authorId": "3415700"}, {"name": "Pierpaolo Basile", "authorId": "1731651"}, {"name": "G. Semeraro", "authorId": "145467353"}], "n_citations": 128}, "snippets": ["The textual similarity is a crucial aspect for many extractive text summarization methods. A bag-of-words representation does not allow to grasp the semantic relationships between concepts when comparing strongly related sentences with no words in common. To overcome this issue, in this paper we propose a centroid-based method for text summarization that exploits the compositional capabilities of word embeddings. The evaluations on multi-document and multilingual datasets prove the effectiveness of the continuous vector representation of words compared to the bag-of-words model. Despite its simplicity, our method achieves good performance even in comparison to more complex deep learning models. Our method is unsupervised and it can be adopted in other summarization tasks."], "score": 0.0}, {"id": "(Kumar et al., 2012)", "paper": {"corpus_id": 15926944, "title": "Automatic Multi Document Summarization Approaches", "year": 2012, "venue": "", "authors": [{"name": "Y. J. Kumar", "authorId": "1734844"}, {"name": "N. Salim", "authorId": "1680372"}], "n_citations": 72}, "snippets": ["In this study, some survey on multi document summarization approaches has been presented. We will direct our focus notably on four well known approaches to multi document summarization namely the feature based method, cluster based method, graph based method and knowledge based method. The general ideas behind these methods have been described.\n\nA number of research study have addressed multi document summarization in academia (Erkan and Radev, 2004a, Wan and Yang, 2008, Haribagiu and Lacatusu, 2010) and illustrated different types of approaches and available systems for multi document summarization."], "score": 0.9248046875}, {"id": "(Bista et al., 2020)", "paper": {"corpus_id": 222140880, "title": "SupMMD: A Sentence Importance Model for Extractive Summarisation using Maximum Mean Discrepancy", "year": 2020, "venue": "Findings", "authors": [{"name": "Umanga Bista", "authorId": "2168767"}, {"name": "A. Mathews", "authorId": "3175685"}, {"name": "A. Menon", "authorId": "2844480"}, {"name": "Lexing Xie", "authorId": "33650938"}], "n_citations": 2}, "snippets": ["Most existing work on this topic has focused on the generic summarization task. However, update summarization is of equal practical interest.\n\nMulti-document extractive summarization methods can be unsupervised or supervised. Unsupervised methods typically define salience (or coverage) using a global model of sentence-sentence similarity. Methods based on retrieval (Goldstein et al., 1999), centroids (Radev et al., 2004), graph centrality (Erkan and Radev, 2004), or utility maximization (Lin andBilmes, 2010, 2011;Gillick and Favre, 2009) have been well explored. However, sentence salience also depends on surface features (e.g., position, length, presence of cue words); effectively capturing these requires supervised models specific to the dataset and task. A body of work has incorporated such information through supervised learning, for example based on point processes (Kulesza and Taskar, 2012), learning important words (Hong and Nenkova, 2014), graph neural networks (Yasunaga et al., 2017), and support vector regression (Varma et al., 2009)."], "score": 0.9072265625}, {"id": "(Antognini et al., 2019)", "paper": {"corpus_id": 202889056, "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization", "year": 2019, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Diego Antognini", "authorId": "26399699"}, {"name": "B. Faltings", "authorId": "1735128"}], "n_citations": 22}, "snippets": ["Extractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. Radev (2000) introduced a cross-document structure theory, as a basis for multi-document summarization. Erkan and Radev (2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan and Yang, 2006;Antiqueira et al., 2009). Wan and Yang (2008) pairs graph-based methods (e.g. random walk) with clustering. Mei et al. (2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by Christensen et al. (2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann and Thompson, 1988).\n\nBesides graph-based methods, other viable approaches include Maximum Marginal Relevance (Carbonell and Goldstein, 1998), which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression (Li et al., 2007) ; conditional random field (Galley, 2006) ; or hidden markov model (Conroy et al., 2004). Yet other approaches rely on n-grams regression as in Li et (Christensen et al., 2013), based on hand-crafted features, where sentence nodes are normalized over all the incoming edges."], "score": 0.91748046875}, {"id": "(Radev, 2000)", "paper": {"corpus_id": 10103200, "title": "A Common Theory of Information Fusion from Multiple Text Sources Step One: Cross-Document Structure", "year": 2000, "venue": "SIGDIAL Workshop", "authors": [{"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 248}, "snippets": ["We introduce CST (cross-document structure theory), a paradigm for multi-document analysis. CST takes into account the rhetorical structure of clusters of related textual documents. We present a taxonomy of cross-document relationships. We argue that CST can be the basis for multi-document summarization guided by user preferences for summary length, information provenance, cross-source agreement, and chronological ordering of facts."], "score": 0.0}, {"id": "(Jin et al._1, 2020)", "paper": {"corpus_id": 220045815, "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Hanqi Jin", "authorId": "1491212422"}, {"name": "Tian-ming Wang", "authorId": "1751960"}, {"name": "Xiaojun Wan", "authorId": "145078589"}], "n_citations": 99}, "snippets": ["The methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Since the neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u0327elikyilmaz et al., 2018), some works trained abstractive summarization models on a single document dataset and adjusted the model to adapt the multi-document summarization task. Zhang et al. (2018) added a document set encoder into the single document summarization framework and tuned the pre-trained model on the multi-document summarization dataset. Lebanoff et al. (2018) combined an extractive summarization algorithm (MMR) for sentence extraction to reweight the original sentence importance distribution learned in the single document abstractive summarization model."], "score": 0.94775390625}, {"id": "(Perez-Beltrachini et al., 2021)", "paper": {"corpus_id": 236150987, "title": "Multi-Document Summarization with Determinantal Point Process Attention", "year": 2021, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "Laura Perez-Beltrachini", "authorId": "1400959575"}, {"name": "Mirella Lapata", "authorId": "1747893"}], "n_citations": 29}, "snippets": ["Multi-Document Summarization Most previous solutions to multi-document summarization adopt non-neural, extractive methods (Carbonell et al., 1998)(Radev et al., 2004)(Erkan et al., 2004)(Barzilay et al., 1999). \n\nMore recently, various encoder-decoder architectures (Liu & Lapata, 2019;Fabbri et al., 2019;Perez-Beltrachini et al., 2019;(Liu et al., 2018)(Zhang et al., 2018)Lebanoff, Song, & Liu, 2018) have been ported to this task thanks to the development of large-scale datasets for model training."], "score": 0.91552734375}], "table": null}, {"title": "Abstractive Approaches to Multi-Document Summarization", "tldr": "Abstractive multi-document summarization has evolved from early sentence fusion techniques to sophisticated neural approaches using pre-trained language models. Recent advances include hierarchical architectures, graph-based representations, and entity-aware models that effectively handle cross-document relations. (20 sources)", "text": "\nAbstractive approaches to multi-document summarization (MDS) aim to generate entirely new text that captures the essence of source documents, rather than simply extracting sentences. Unlike extractive methods, abstractive approaches can produce more flexible and coherent summaries by creating new sentences that synthesize information from multiple sources <Paper corpusId=\"220045815\" paperTitle=\"(Jin et al._1, 2020)\" isShortName></Paper>.\n\nEarly abstractive MDS methods focused on identifying and synthesizing similar elements across documents. Barzilay et al. introduced an approach that used language generation to reformulate the wording of the summary <Paper corpusId=\"7031344\" paperTitle=\"(Barzilay et al., 1999)\" isShortName></Paper> <Paper corpusId=\"236150987\" paperTitle=\"(Perez-Beltrachini et al., 2021)\" isShortName></Paper>. Banerjee et al. demonstrated that their abstractive summarizer outperformed extractive systems on DUC2004 and DUC2005 datasets, showing the potential of abstractive methods <Paper corpusId=\"15795297\" paperTitle=\"(Banerjee et al., 2015)\" isShortName></Paper>.\n\nThe development of neural abstractive MDS was initially constrained by the scarcity of large-scale training data, making extractive methods more prevalent <Paper corpusId=\"226283949\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper>. This situation changed with the introduction of large parallel datasets such as MultiNews, which enabled more research into neural abstractive MDS <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper> <Paper corpusId=\"266599825\" paperTitle=\"(Sakaji et al., 2023)\" isShortName></Paper>.\n\nRecent neural approaches to abstractive MDS can be categorized into several types:\n\n1. **Sequential models**: Some approaches treat MDS as a long sequence-to-sequence task by concatenating multiple documents into a flat text <Paper corpusId=\"226283949\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper>. This includes fine-tuning pre-trained models like BART <Paper corpusId=\"204960716\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper> and T5 <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper> for the summarization task <Paper corpusId=\"263610015\" paperTitle=\"(Mascarell et al., 2023)\" isShortName></Paper>.\n\n2. **Hierarchical models**: These architectures extract different-grained features (document, sentence, and word level) and select important information across documents <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Jin et al. proposed a multi-granularity interaction network that jointly learns semantic representations for words, sentences, and documents <Paper corpusId=\"220045815\" paperTitle=\"(Jin et al._1, 2020)\" isShortName></Paper>.\n\n3. **Adaptation from single-document models**: Several works have adapted neural encoder-decoder models trained on single-document summarization to the multi-document setting. Zhang et al. added a document set encoder to extend models trained on large-scale single-document summarization corpora <Paper corpusId=\"53223447\" paperTitle=\"(Zhang et al., 2018)\" isShortName></Paper>. Lebanoff et al. incorporated the maximal marginal relevance method into neural encoder-decoder models to address redundancy <Paper corpusId=\"52053741\" paperTitle=\"(Lebanoff et al., 2018)\" isShortName></Paper>.\n\n4. **Graph-based neural approaches**: These methods model relationships between textual units across documents to enhance representation. Li et al. developed a neural abstractive MDS model leveraging graph representations of documents to more effectively process multiple input documents <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. Cui et al. proposed representing multiple documents as a heterogeneous graph, considering semantic nodes of different granularities <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>.\n\n5. **Entity-aware models**: Zhou et al. presented EMSum, an entity-aware model augmenting Transformer-based encoders with knowledge graphs consisting of text units and entities as nodes, allowing it to capture cross-document information <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>.\n\n6. **Topic-based approaches**: Some models incorporate topic modeling to bridge different documents. Haghighi and Vanderwende utilized hierarchical LDA-style models to represent content specificity as a hierarchy of topic vocabulary distributions <Paper corpusId=\"678258\" paperTitle=\"(Haghighi et al., 2009)\" isShortName></Paper>. Cui et al. employed a neural topic model to discover latent topics that act as cross-document semantic units <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>.\n\n7. **Multi-stage approaches**: These methods first identify relevant information before feeding it into a summarization model. Pasunuru et al. presented BART-Long-Graph, which integrated a Longformer with both local and global attention mechanisms for encoding long texts, and leveraged a knowledge graph <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>.\n\nRecent advances in pre-trained language models have significantly improved abstractive MDS performance. Models like PRIMERA (Pre-trained Multi-document Representation for Abstractive summarization) reduce the need for dataset-specific architectures and large amounts of fine-tuning labeled data <Paper corpusId=\"247519084\" paperTitle=\"(Xiao et al., 2021)\" isShortName></Paper> <Paper corpusId=\"263610015\" paperTitle=\"(Mascarell et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Jin et al._1, 2020)", "paper": {"corpus_id": 220045815, "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Hanqi Jin", "authorId": "1491212422"}, {"name": "Tian-ming Wang", "authorId": "1751960"}, {"name": "Xiaojun Wan", "authorId": "145078589"}], "n_citations": 99}, "snippets": ["The methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Since the neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u0327elikyilmaz et al., 2018), some works trained abstractive summarization models on a single document dataset and adjusted the model to adapt the multi-document summarization task. Zhang et al. (2018) added a document set encoder into the single document summarization framework and tuned the pre-trained model on the multi-document summarization dataset. Lebanoff et al. (2018) combined an extractive summarization algorithm (MMR) for sentence extraction to reweight the original sentence importance distribution learned in the single document abstractive summarization model."], "score": 0.94775390625}, {"id": "(Barzilay et al., 1999)", "paper": {"corpus_id": 7031344, "title": "Information Fusion in the Context of Multi-Document Summarization", "year": 1999, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "R. Barzilay", "authorId": "1741283"}, {"name": "K. McKeown", "authorId": "145590324"}, {"name": "Michael Elhadad", "authorId": "1754680"}], "n_citations": 484}, "snippets": ["Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin et al., 1997)(Marcu, 1997)(Salton et al., 1994). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while ex-tracting some similar sentences could produce a summary biased towards some sources."], "score": 0.90478515625}, {"id": "(Perez-Beltrachini et al., 2021)", "paper": {"corpus_id": 236150987, "title": "Multi-Document Summarization with Determinantal Point Process Attention", "year": 2021, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "Laura Perez-Beltrachini", "authorId": "1400959575"}, {"name": "Mirella Lapata", "authorId": "1747893"}], "n_citations": 29}, "snippets": ["Multi-Document Summarization Most previous solutions to multi-document summarization adopt non-neural, extractive methods (Carbonell et al., 1998)(Radev et al., 2004)(Erkan et al., 2004)(Barzilay et al., 1999). \n\nMore recently, various encoder-decoder architectures (Liu & Lapata, 2019;Fabbri et al., 2019;Perez-Beltrachini et al., 2019;(Liu et al., 2018)(Zhang et al., 2018)Lebanoff, Song, & Liu, 2018) have been ported to this task thanks to the development of large-scale datasets for model training."], "score": 0.91552734375}, {"id": "(Banerjee et al., 2015)", "paper": {"corpus_id": 15795297, "title": "Multi-Document Abstractive Summarization Using ILP Based Multi-Sentence Compression", "year": 2015, "venue": "International Joint Conference on Artificial Intelligence", "authors": [{"name": "Siddhartha Banerjee", "authorId": "2169453878"}, {"name": "P. Mitra", "authorId": "143930195"}, {"name": "Kazunari Sugiyama", "authorId": "3060386"}], "n_citations": 136}, "snippets": ["On the DUC2004 and DUC2005 datasets, we demonstrate the effectiveness of our proposed method. Our proposed method outperforms not only some popular baselines but also the state-of-the-art extractive summarization systems. ROUGE scores (Oya et al., 2014) obtained by our system outperforms the best extractive summarizer on both the datasets. Our method also outperforms an abstractive summarizer based on multi-sentence compression (Filippova, 2010) when measured by ROUGE-2, ROUGE-L and ROUGE-SU4 scores."], "score": 0.9091796875}, {"id": "(Jin et al., 2020)", "paper": {"corpus_id": 226283949, "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization", "year": 2020, "venue": "Findings", "authors": [{"name": "Hanqi Jin", "authorId": "1491212422"}, {"name": "Xiaojun Wan", "authorId": "145078589"}], "n_citations": 15}, "snippets": ["Multi-document summarization aims at producing a fluent, condensed summary for the given document set", "Compared with single-document summarization, multi-document summarization needs to handle multiple input documents. A simple method is to concatenate multiple documents into a long flat text and treat it as a long sequence-tosequence task.\n\nEmpowered by large parallel datasets automatically harvested from online news websites, sequence-to-sequence learning has shown promising results on abstractive single-document summarization (See et al., 2017; Paulus et al., 2018; Tan et al., 2017; \u00c7 elikyilmaz et al., 2018)", "Several works have explored adapting the neural encoder-decoder model trained for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural encoder-decoder model trained for singledocument summarization to address the information redundancy for multi-document summarization.\n\nThe methods for multi-document summarization can generally be categorized to extractive and abstractive", "Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004; Christensen et al., 2013; Yasunaga et al., 2017)."], "score": 0.958984375}, {"id": "(Fabbri et al., 2019)", "paper": {"corpus_id": 174799390, "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Alexander R. Fabbri", "authorId": "46255971"}, {"name": "Irene Li", "authorId": "46331602"}, {"name": "Tianwei She", "authorId": "2106009217"}, {"name": "Suyi Li", "authorId": "50341789"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 590}, "snippets": ["Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples."], "score": 0.9287109375}, {"id": "(Sakaji et al., 2023)", "paper": {"corpus_id": 266599825, "title": "Summarization of Investment Reports Using Pre-trained Model", "year": 2023, "venue": "IIAI International Conference on Advanced Applied Informatics", "authors": [{"name": "Hiroki Sakaji", "authorId": "2879326"}, {"name": "Ryotaro Kobayashi", "authorId": "2276796103"}, {"name": "Kiyoshi Izumi", "authorId": "2276798525"}, {"name": "Hiroyuki Mitsugi", "authorId": "2276797477"}, {"name": "Wataru Kuramoto", "authorId": "2276798124"}], "n_citations": 0}, "snippets": ["Related research on multi-document summarization includes the following papers. Moro et al. proposed the probabilistic method based on the combination of three language models to tackle multi-document summarization in the medical domain (Moro et al., 2022). Liao et al. investigated the feasibility of utilizing Abstract Meaning Representation formalism for multidocument summarization (Liao et al., 2018). Fabbri et al. constructed Multi-News, the large-scale multi-document news summarization dataset (Fabbri et al., 2019). Xiao et al. introduced PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data [9]. Nayeem et al. designed an abstractive fusion generation model at the sentence level, which jointly performs sentence fusion and paraphrasing (Nayeem et al., 2018). They applied their sentence-level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. Liu et al. developed the neural summarization model, which can effectively process multiple input documents and distill abstractive summaries (Liu et al., 2019). Li et al. develop a neural abstractive multi-document summarization model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries (Li et al., 2020). Jin et al. proposed the multigranularity interaction network to encode semantic representations for documents, sentences, and words (Jin et al., 2020). Deyoung et al. released MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature (DeYoung et al., 2021)."], "score": 0.97119140625}, {"id": "(Lewis et al., 2019)", "paper": {"corpus_id": 204960716, "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "M. Lewis", "authorId": "35084211"}, {"name": "Yinhan Liu", "authorId": "11323179"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Marjan Ghazvininejad", "authorId": "2320509"}, {"name": "Abdel-rahman Mohamed", "authorId": "113947684"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "Veselin Stoyanov", "authorId": "1759422"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 10856}, "snippets": ["We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."], "score": 0.0}, {"id": "(Raffel et al., 2019)", "paper": {"corpus_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019, "venue": "Journal of machine learning research", "authors": [{"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Wei Li", "authorId": "2157338362"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 20336}, "snippets": ["Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."], "score": 0.0}, {"id": "(Mascarell et al., 2023)", "paper": {"corpus_id": 263610015, "title": "Entropy-based Sampling for Abstractive Multi-document Summarization in Low-resource Settings", "year": 2023, "venue": "International Conference on Natural Language Generation", "authors": [{"name": "Laura Mascarell", "authorId": "2121237"}, {"name": "Ribin Chalumattu", "authorId": "1879120021"}, {"name": "Julien Heitmann", "authorId": "2253607868"}], "n_citations": 1}, "snippets": ["Multi-document Summarization (MDS) aims at condensing the most important information from different documents. Despite the advances in single-document summarization (Zhang et al., 2019), summarizing multiple related documents remains a greater challenge due to its input length and the presence of redundant information (Fan et al., 2019)(Song et al., 2022). Therefore, some research focuses on implementing multi-stage approaches that first identify the relevant information to then feed it into a summarization model (Lebanoff et al., 2018)(Liu et al., 2019). More recent works utilize pre-trained language models (Lewis et al., 2019)(Raffel et al., 2019)(Xiao et al., 2021) finetuned for the summarization task and feed them with the source documents concatenated (Johner et al., 2021)(Xiao et al., 2021)."], "score": 0.9130859375}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 271903777, "title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization", "year": 2024, "venue": "European Conference on Artificial Intelligence", "authors": [{"name": "Ran Liu", "authorId": "2316447969"}, {"name": "Ming Liu", "authorId": "2287803134"}, {"name": "Min Yu", "authorId": "2316535336"}, {"name": "Jianguo Jiang", "authorId": "152634491"}, {"name": "Gang Li", "authorId": "1996030117"}, {"name": "Dan Zhang", "authorId": "2316503511"}, {"name": "Jingyuan Li", "authorId": "2316432820"}, {"name": "Xiang Meng", "authorId": "2282447587"}, {"name": "Weiqing Huang", "authorId": "2282485475"}], "n_citations": 0}, "snippets": ["Multi-document summarization (MDS) aims to produce a summary from a document set containing a series of related topics. The generated summary needs to cover all important information in the document set, while remaining fluent and concise.\n\nNon-neural approaches are primarily based on extracting key sentences (Erkan and Radev, 2004;Mihalcea and Tarau, 2004;Rossiello et al., 2017). These approaches assess sentence importance based on their relevance to each other or proximity to keywords, selecting sentences with high importance scores to form the summary.\n\nNeural approaches can generate more abstractive text and are recently widely used in multidocument summarization. Given the structural characteristics of the multi-document input, most approaches utilize attention mechanism to build hierarchical models (Fabbri et al., 2019;Mao et al., 2020;Jin et al., 2020), enabling the extraction of different-grained features and the selection of important information. Other methods employ graphs to model relationships and can leverage interaction features to enhance representation (Yasunaga et al., 2017;Yin et al., 2019;Li et al., 2020)."], "score": 0.90869140625}, {"id": "(Zhang et al., 2018)", "paper": {"corpus_id": 53223447, "title": "Adapting Neural Single-Document Summarization Model for Abstractive Multi-Document Summarization: A Pilot Study", "year": 2018, "venue": "International Conference on Natural Language Generation", "authors": [{"name": "Jianmin Zhang", "authorId": "2108172729"}, {"name": "Jiwei Tan", "authorId": "2796928"}, {"name": "Xiaojun Wan", "authorId": "145078589"}], "n_citations": 35}, "snippets": ["Till now, neural abstractive summarization methods have achieved great success for single document summarization (SDS). However, due to the lack of large scale multi-document summaries, such methods can be hardly applied to multi-document summarization (MDS). In this paper, we investigate neural abstractive methods for MDS by adapting a state-of-the-art neural abstractive summarization model for SDS. We propose an approach to extend the neural abstractive model trained on large scale SDS data to the MDS task. Our approach only makes use of a small number of multi-document summaries for fine tuning. Experimental results on two benchmark DUC datasets demonstrate that our approach can outperform a variety of baseline neural models."], "score": 0.0}, {"id": "(Lebanoff et al., 2018)", "paper": {"corpus_id": 52053741, "title": "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization", "year": 2018, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Logan Lebanoff", "authorId": "50827114"}, {"name": "Kaiqiang Song", "authorId": "50982080"}, {"name": "Fei Liu", "authorId": "144544919"}], "n_citations": 157}, "snippets": ["Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors."], "score": 0.0}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 218718706, "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}, {"name": "Junping Du", "authorId": "2117218629"}], "n_citations": 136}, "snippets": ["Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries."], "score": 0.93505859375}, {"id": "(Cui et al., 2021)", "paper": {"corpus_id": 239050558, "title": "Topic-Guided Abstractive Multi-Document Summarization", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Peng Cui", "authorId": "143738684"}, {"name": "Le Hu", "authorId": "2109312896"}], "n_citations": 41}, "snippets": ["A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics."], "score": 0.91259765625}, {"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 236478143, "title": "Entity-Aware Abstractive Multi-Document Summarization", "year": 2021, "venue": "Findings", "authors": [{"name": "Hao Zhou", "authorId": null}, {"name": "Weidong Ren", "authorId": "2053308860"}, {"name": "Gongshen Liu", "authorId": "150112803"}, {"name": "Bo Su", "authorId": "153253583"}, {"name": "Wei Lu", "authorId": "143844110"}], "n_citations": 28}, "snippets": ["Multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020).\n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018).\n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020)."], "score": 0.93408203125}, {"id": "(Qu, 2024)", "paper": {"corpus_id": 269762702, "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization", "year": 2024, "venue": "The Web Conference", "authors": [{"name": "Yutong Qu", "authorId": "2163451228"}], "n_citations": 0}, "snippets": ["Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."], "score": 0.92626953125}, {"id": "(Haghighi et al., 2009)", "paper": {"corpus_id": 678258, "title": "Exploring Content Models for Multi-Document Summarization", "year": 2009, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "A. Haghighi", "authorId": "1761880"}, {"name": "Lucy Vanderwende", "authorId": "1909300"}], "n_citations": 560}, "snippets": ["We present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our final model, HierSum, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions."], "score": 0.94482421875}, {"id": "(Pasunuru et al., 2021)", "paper": {"corpus_id": 235097309, "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ramakanth Pasunuru", "authorId": "10721120"}, {"name": "Mengwen Liu", "authorId": "2940333"}, {"name": "Mohit Bansal", "authorId": "143977268"}, {"name": "Sujith Ravi", "authorId": "120209444"}, {"name": "Markus Dreyer", "authorId": "40262269"}], "n_citations": 73}, "snippets": ["Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com"], "score": 0.91064453125}, {"id": "(Xiao et al., 2021)", "paper": {"corpus_id": 247519084, "title": "PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wen Xiao", "authorId": "49617120"}, {"name": "Iz Beltagy", "authorId": "46181066"}, {"name": "G. Carenini", "authorId": "1825424"}, {"name": "Arman Cohan", "authorId": "2527954"}], "n_citations": 119}, "snippets": ["We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins."], "score": 0.0}], "table": null}, {"title": "Graph-Based Methods for Multi-Document Summarization", "tldr": "Graph-based methods for multi-document summarization represent documents as graphs with textual units (words, sentences, documents) as nodes connected by various relationships. These approaches have evolved from classical algorithms like LexRank to modern neural graph-enhanced models that capture cross-document relationships and semantic dependencies. (14 sources)", "text": "\nGraph-based methods have been foundational in multi-document summarization since the late 1990s. Early researchers modeled document collections as graph structures, with Mani et al. representing documents as graphs and performing graph matching to identify salient regions across documents <Paper corpusId=\"6025826\" paperTitle=\"(Mani et al., 1997)\" isShortName></Paper>. Around the same time, Radev introduced Cross-document Structure Theory (CST) as a paradigm for multi-document analysis, providing a theoretical foundation for representing rhetorical structures across related documents <Paper corpusId=\"10103200\" paperTitle=\"(Radev, 2000)\" isShortName></Paper>.\n\nA breakthrough came with LexRank, proposed by Erkan and Radev, which constructs a graph representation where sentences are nodes connected based on similarity, and applies a PageRank-like algorithm to identify important sentences <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. LexRank ranked first in Document Understanding Conference (DUC) 2004 evaluations, demonstrating remarkable effectiveness partly due to its robustness to noise in document clustering <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>.\n\nThese graph-based models gained popularity by leveraging the \"voting\" or \"recommendations\" between sentences to determine importance <Paper corpusId=\"17446655\" paperTitle=\"(Wan, 2008)\" isShortName></Paper>. Wan et al. further enhanced this approach with an affinity graph method that incorporated a diffusion process to capture semantic relationships between sentences while differentiating between intra-document and inter-document links <Paper corpusId=\"5457260\" paperTitle=\"(Wan et al., 2006)\" isShortName></Paper>.\n\nOther notable graph-based approaches include:\n\n1. **Maximum Marginal Relevance (MMR)**: This technique uses a greedy approach to select sentences while balancing relevance and redundancy, and has been incorporated into the evaluation of multi-document summaries <Paper corpusId=\"1320\" paperTitle=\"(Radev et al., 2000)\" isShortName></Paper>.\n\n2. **Discourse graphs**: Christensen et al. developed a system that performs sentence selection while balancing coherence and salience by building a graph that approximates discourse relations across sentences <Paper corpusId=\"202889056\" paperTitle=\"(Antognini et al., 2019)\" isShortName></Paper>.\n\n3. **Reinforced random walk models**: These improved results by using reinforced random walks to rank sentences and remove redundancy <Paper corpusId=\"202889056\" paperTitle=\"(Antognini et al., 2019)\" isShortName></Paper>.\n\nAs neural models emerged, researchers began combining graph-based approaches with neural architectures. Li et al. developed a neural abstractive MDS model that leverages document graphs (similarity graphs and discourse graphs) to better capture cross-document relations and guide summary generation <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. This approach was shown to bring substantial improvements over several strong baselines <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>.\n\nRecent advancements include heterogeneous graph representations that consider semantic nodes of different granularities. Cui et al. proposed representing multiple documents as a heterogeneous graph with semantic nodes at different levels and applying a graph-to-sequence framework to generate summaries <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>. They also employed a neural topic model to discover latent topics that bridge different documents and guide summary generation <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>.\n\nEntity-aware models have further enhanced graph-based approaches. Zhou et al. presented EMSum, which augments Transformer-based encoders with knowledge graphs consisting of text units and entities as nodes while utilizing Graph Attention Networks <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper> <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>. This design allows the model to capture cross-document information and identify relative information among documents <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>.\n\nPasunuru et al. introduced BART-Long-Graph, an efficient graph-enhanced approach that integrates a Longformer with both local and global attention mechanisms for encoding long texts, while leveraging a knowledge graph <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>. This approach led to significant improvements on the Multi-News dataset and produced summaries that are more abstractive and factually consistent <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>.\n\nGraph-based models remain a vibrant area of research in multi-document summarization, with recent work focusing on creating more sophisticated graph structures that can better capture the complexities of cross-document relationships <Paper corpusId=\"278000561\" paperTitle=\"(Tan et al., 2025)\" isShortName></Paper>. These approaches continue to demonstrate competitive performance against pre-trained language models, particularly in their ability to represent explicit relationships between textual units.", "citations": [{"id": "(Mani et al., 1997)", "paper": {"corpus_id": 6025826, "title": "Multi-Document Summarization by Graph Search and Matching", "year": 1997, "venue": "AAAI/IAAI", "authors": [{"name": "I. Mani", "authorId": "1729172"}, {"name": "E. Bloedorn", "authorId": "2740861"}], "n_citations": 252}, "snippets": ["We describe a new method for summarizing similarities and differences in a pair of related documents using a graph representation for text. Concepts denoted by words, phrases, and proper names in the document are represented positionally as nodes in the graph along with edges corresponding to semantic relations between items. Given a perspective in terms of which the pair of documents is to be summarized, the algorithm first uses a spreading activation technique to discover, in each document, nodes semantically related to the topic. The activated graphs of each document are then matched to yield a graph corresponding to similarities and differences between the pair, which is rendered in natural language. An evaluation of these techniques has been carried out."], "score": 0.0}, {"id": "(Radev, 2000)", "paper": {"corpus_id": 10103200, "title": "A Common Theory of Information Fusion from Multiple Text Sources Step One: Cross-Document Structure", "year": 2000, "venue": "SIGDIAL Workshop", "authors": [{"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 248}, "snippets": ["We introduce CST (cross-document structure theory), a paradigm for multi-document analysis. CST takes into account the rhetorical structure of clusters of related textual documents. We present a taxonomy of cross-document relationships. We argue that CST can be the basis for multi-document summarization guided by user preferences for summary length, information provenance, cross-source agreement, and chronological ordering of facts."], "score": 0.0}, {"id": "(Erkan et al., 2004)", "paper": {"corpus_id": 506350, "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "year": 2004, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 3097}, "snippets": ["We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."], "score": 0.0}, {"id": "(Wan, 2008)", "paper": {"corpus_id": 17446655, "title": "An Exploration of Document Impact on Graph-Based Multi-Document Summarization", "year": 2008, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xiaojun Wan", "authorId": "145078589"}], "n_citations": 82}, "snippets": ["Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the \"voting\" or \"recommendations\" between sentences in the documents (Erkan et al., 2004)(Mihalcea et al., 2005)(Wan et al., 2006)."], "score": 0.9169921875}, {"id": "(Wan et al., 2006)", "paper": {"corpus_id": 5457260, "title": "Improved Affinity Graph Based Multi-Document Summarization", "year": 2006, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Xiaojun Wan", "authorId": "145078589"}, {"name": "Jianwu Yang", "authorId": "1743923"}], "n_citations": 109}, "snippets": ["This paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems."], "score": 0.0}, {"id": "(Radev et al., 2000)", "paper": {"corpus_id": 1320, "title": "Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies", "year": 2000, "venue": "arXiv.org", "authors": [{"name": "Dragomir R. Radev", "authorId": "9215251"}, {"name": "Hongyan Jing", "authorId": "40544823"}, {"name": "M. Budzikowska", "authorId": "3166871"}], "n_citations": 584}, "snippets": ["We also looked at a property of multi-document clusters, namely cross-sentence information subsumption (which is related to the MMR metric proposed in (Carbonell et al., 1998)) and showed how it can be used in evaluating multidocument summaries."], "score": 0.94189453125}, {"id": "(Antognini et al., 2019)", "paper": {"corpus_id": 202889056, "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization", "year": 2019, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Diego Antognini", "authorId": "26399699"}, {"name": "B. Faltings", "authorId": "1735128"}], "n_citations": 22}, "snippets": ["Extractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. Radev (2000) introduced a cross-document structure theory, as a basis for multi-document summarization. Erkan and Radev (2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan and Yang, 2006;Antiqueira et al., 2009). Wan and Yang (2008) pairs graph-based methods (e.g. random walk) with clustering. Mei et al. (2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by Christensen et al. (2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann and Thompson, 1988).\n\nBesides graph-based methods, other viable approaches include Maximum Marginal Relevance (Carbonell and Goldstein, 1998), which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression (Li et al., 2007) ; conditional random field (Galley, 2006) ; or hidden markov model (Conroy et al., 2004). Yet other approaches rely on n-grams regression as in Li et (Christensen et al., 2013), based on hand-crafted features, where sentence nodes are normalized over all the incoming edges."], "score": 0.91748046875}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 218718706, "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}, {"name": "Junping Du", "authorId": "2117218629"}], "n_citations": 136}, "snippets": ["Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries."], "score": 0.93505859375}, {"id": "(Sankar et al., 2022)", "paper": {"corpus_id": 248571519, "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Aiswarya Sankar", "authorId": "2064325789"}, {"name": "Ankit R. Chadha", "authorId": "145934595"}], "n_citations": 0}, "snippets": ["Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."], "score": 0.9453125}, {"id": "(Cui et al., 2021)", "paper": {"corpus_id": 239050558, "title": "Topic-Guided Abstractive Multi-Document Summarization", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Peng Cui", "authorId": "143738684"}, {"name": "Le Hu", "authorId": "2109312896"}], "n_citations": 41}, "snippets": ["A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics."], "score": 0.91259765625}, {"id": "(Qu, 2024)", "paper": {"corpus_id": 269762702, "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization", "year": 2024, "venue": "The Web Conference", "authors": [{"name": "Yutong Qu", "authorId": "2163451228"}], "n_citations": 0}, "snippets": ["Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."], "score": 0.92626953125}, {"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 236478143, "title": "Entity-Aware Abstractive Multi-Document Summarization", "year": 2021, "venue": "Findings", "authors": [{"name": "Hao Zhou", "authorId": null}, {"name": "Weidong Ren", "authorId": "2053308860"}, {"name": "Gongshen Liu", "authorId": "150112803"}, {"name": "Bo Su", "authorId": "153253583"}, {"name": "Wei Lu", "authorId": "143844110"}], "n_citations": 28}, "snippets": ["Multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020).\n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018).\n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020)."], "score": 0.93408203125}, {"id": "(Pasunuru et al., 2021)", "paper": {"corpus_id": 235097309, "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ramakanth Pasunuru", "authorId": "10721120"}, {"name": "Mengwen Liu", "authorId": "2940333"}, {"name": "Mohit Bansal", "authorId": "143977268"}, {"name": "Sujith Ravi", "authorId": "120209444"}, {"name": "Markus Dreyer", "authorId": "40262269"}], "n_citations": 73}, "snippets": ["Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com"], "score": 0.91064453125}, {"id": "(Tan et al., 2025)", "paper": {"corpus_id": 278000561, "title": "A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shiyin Tan", "authorId": "148149386"}, {"name": "Jaeeon Park", "authorId": "2357102667"}, {"name": "Dongyuan Li", "authorId": "2242195007"}, {"name": "Renhe Jiang", "authorId": "2299193401"}, {"name": "Manabu Okumura", "authorId": "2283854880"}], "n_citations": 0}, "snippets": ["Multi-document summarization (MDS) is a task that aims to generate concise and coherent summaries by synthesizing information from multiple documents on the same topic (Jin et al., 2020)(Li et al., 2020)(Ma, 2021)(Mao et al., 2020)(Pang et al., 2021). MDS can lead to diverse applications, such as news aggregation (Chen et al., 2024)(Fabbri et al., 2019)(Khatuya et al., 2024), scientific research (DeYoung et al., 2021)(Lu et al., 2020)(Wang et al., 2024), and legal document analysis [17,(Malik et al., 2024)(Shen et al., 2022). Current MDS approaches can be categorized into two classes: Graph-based models (Cui et al., 2021)(Li et al., 2023)(Pasunuru et al., 2021)(Qu, 2024)(Zhang et al., 2023) and pre-trained language models [2](Puduppully et al., 2022)(Xiao et al., 2021). Graph-based models rely on auxiliary information (e.g., discourse structures) as an input graph to capture the cross-document relationships, while pre-trained language models use the attention mechanisms to capture them."], "score": 0.92041015625}], "table": null}, {"title": "Datasets and Evaluation for Multi-Document Summarization", "tldr": "Multi-document summarization research has evolved from small-scale benchmarks like DUC to large-scale datasets such as WikiSum, Multi-News, and MS\u00b2 that enable neural approaches. Evaluation predominantly relies on ROUGE metrics, though newer frameworks also incorporate human assessment of coherence, factual consistency, and informativeness. (13 sources)", "text": "\nThe availability of appropriate datasets has been crucial to the development of multi-document summarization (MDS) approaches. Early MDS research was supported by the Document Understanding Conference (DUC) datasets from the early 2000s, which included human-written summaries for multi-document clusters and sparked increased research interest <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>. These datasets, though small in scale, provided valuable benchmarks for evaluating early extractive approaches.\n\nA significant limitation in MDS research has been the scarcity of large-scale training data, particularly for neural abstractive approaches <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper>. This situation changed with the introduction of several comprehensive datasets:\n\n1. **WikiSum**: Introduced by Liu et al., this dataset is based on Wikipedia articles and enabled researchers to train neural models for MDS <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper> <Paper corpusId=\"49210924\" paperTitle=\"(Liao et al., 2018)\" isShortName></Paper>.\n\n2. **Multi-News**: Developed by Fabbri et al., this is the first large-scale MDS news dataset, containing news articles and their corresponding human-written summaries <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper> <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>.\n\n3. **Multi-XScience**: This dataset focuses on generating \"related work\" sections by summarizing multiple scientific articles <Paper corpusId=\"272146191\" paperTitle=\"(Fernandes et al., 2024)\" isShortName></Paper>.\n\n4. **MS\u00b2** (Multi-Document Summarization of Medical Studies): Released by DeYoung et al., this dataset contains over 470,000 documents and 20,000 summaries derived from scientific literature, facilitating development of systems that can assess and aggregate contradictory evidence across multiple studies <Paper corpusId=\"266599825\" paperTitle=\"(Sakaji et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233231380\" paperTitle=\"(DeYoung et al., 2021)\" isShortName></Paper>.\n\n5. **BigSurvey**: Designed for creating structured summaries of academic articles, focusing on consolidating literature reviews <Paper corpusId=\"272146191\" paperTitle=\"(Fernandes et al., 2024)\" isShortName></Paper> <Paper corpusId=\"250636132\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.\n\n6. **HowSumm**: This dataset focuses on summarizing instructional content derived from WikiHow articles <Paper corpusId=\"272146191\" paperTitle=\"(Fernandes et al., 2024)\" isShortName></Paper>.\n\n7. **Multi-LexSum**: Tackles the summarization of legal cases, presenting civil rights litigation summaries with multiple granularities <Paper corpusId=\"272146191\" paperTitle=\"(Fernandes et al., 2024)\" isShortName></Paper>.\n\nFor evaluation, ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores remain the dominant metric in MDS research <Paper corpusId=\"15795297\" paperTitle=\"(Banerjee et al., 2015)\" isShortName></Paper>. Studies typically report ROUGE-1, ROUGE-2, and ROUGE-L scores, which measure unigram, bigram, and longest common subsequence overlaps between generated and reference summaries. For example, Banerjee et al. demonstrated that their abstractive summarizer outperformed extractive systems on DUC2004 and DUC2005 datasets when measured by ROUGE-2, ROUGE-L, and ROUGE-SU4 scores <Paper corpusId=\"15795297\" paperTitle=\"(Banerjee et al., 2015)\" isShortName></Paper>.\n\nBeyond automated metrics, human evaluation plays a crucial role in assessing summary quality. Recent evaluation frameworks consider multiple dimensions including:\n\n1. **Coherence**: How well-structured and logically connected the summary is.\n2. **Factual consistency**: Whether the summary is faithful to the source documents.\n3. **Informativeness**: How well the summary captures key information.\n4. **Redundancy**: Whether the summary contains unnecessary repetitive information.\n5. **Readability**: How clear and fluent the language is <Paper corpusId=\"256416214\" paperTitle=\"(DeYoung et al., 2023)\" isShortName></Paper> <Paper corpusId=\"15795297\" paperTitle=\"(Banerjee et al., 2015)\" isShortName></Paper>.\n\nThe development of these datasets and evaluation metrics has enabled more rigorous comparison of different MDS approaches and facilitated the transition from traditional extractive methods to neural abstractive techniques <Paper corpusId=\"263610015\" paperTitle=\"(Mascarell et al., 2023)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper> <Paper corpusId=\"209405420\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>.", "citations": [{"id": "(Pasunuru et al., 2021)", "paper": {"corpus_id": 235097309, "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ramakanth Pasunuru", "authorId": "10721120"}, {"name": "Mengwen Liu", "authorId": "2940333"}, {"name": "Mohit Bansal", "authorId": "143977268"}, {"name": "Sujith Ravi", "authorId": "120209444"}, {"name": "Markus Dreyer", "authorId": "40262269"}], "n_citations": 73}, "snippets": ["Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com"], "score": 0.91064453125}, {"id": "(Fabbri et al., 2019)", "paper": {"corpus_id": 174799390, "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Alexander R. Fabbri", "authorId": "46255971"}, {"name": "Irene Li", "authorId": "46331602"}, {"name": "Tianwei She", "authorId": "2106009217"}, {"name": "Suyi Li", "authorId": "50341789"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 590}, "snippets": ["Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples."], "score": 0.9287109375}, {"id": "(Liao et al., 2018)", "paper": {"corpus_id": 49210924, "title": "Abstract Meaning Representation for Multi-Document Summarization", "year": 2018, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Kexin Liao", "authorId": "49792730"}, {"name": "Logan Lebanoff", "authorId": "50827114"}, {"name": "Fei Liu", "authorId": "144544919"}], "n_citations": 105}, "snippets": ["Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research."], "score": 0.0}, {"id": "(Sankar et al., 2022)", "paper": {"corpus_id": 248571519, "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Aiswarya Sankar", "authorId": "2064325789"}, {"name": "Ankit R. Chadha", "authorId": "145934595"}], "n_citations": 0}, "snippets": ["Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."], "score": 0.9453125}, {"id": "(Fernandes et al., 2024)", "paper": {"corpus_id": 272146191, "title": "SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section", "year": 2024, "venue": "Brazilian Conference on Intelligent Systems", "authors": [{"name": "Leandro Car'isio Fernandes", "authorId": "2318273161"}, {"name": "Gustavo Bartz Guedes", "authorId": "2317979625"}, {"name": "T. Laitz", "authorId": "2188833716"}, {"name": "Thales Sales Almeida", "authorId": "2188833452"}, {"name": "R. Nogueira", "authorId": "2268315298"}, {"name": "R.A. Lotufo", "authorId": "2256889299"}, {"name": "Jayr Pereira", "authorId": "2257137831"}], "n_citations": 2}, "snippets": ["Multi-document summarization (MDS) addresses the problem of extracting information that is spread across multiple documents, making it more challenging than single-document summarization. It is still an evolving field, with no single approach to solve this problem. The development of this field relies on datasets that serve as benchmarks for evaluating and comparing different summarization methods. The scientific literature includes some examples of such datasets, each varying in domain, structure, size, and summarization objective. These datasets are important for advancing research and improving summarization models. \n\nOutside the scientific domain, Multi-News [4] presents a large volume of news articles and summaries, focusing on the abstractive summarization of multiple documents in the journalistic context. Also in this context, Ghalandari et al. [5] proposed a large-scale dataset for multi-document summarization that contains concise human-written summaries of news events. In the legal field, Multi-LexSum tackles the summarization of legal cases, presenting a set of civil rights litigation summaries with multiple granularities. The summarization of instructional content is the focus of the HowSumm dataset, derived from WikiHow articles [1]. The FINDSum dataset focuses on the challenge of summarizing long text and multiple tables [10]. \n\nIn the scientific domain, the need to manage the vast amount of literature has generated interest in datasets specific to the summarization of scientific articles. Multi-XScience [12] is a dataset focused on the generation of \"related work\" sections by summarizing multiple scientific articles. BigSurvey (Liu et al., 2022) was designed to create structured summaries of academic articles, focusing on consolidating literature reviews. SumPubMed (Gupta et al., 2021) and ScisummNet (Yasunaga et al., 2019) aim the summarization of single scientific articles. \n\nSeveral other datasets can be used for multi-document summarization. Koh et al. [8] work is an extensive survey on long document summarization, including ten other MDS datasets."], "score": 0.90966796875}, {"id": "(Sakaji et al., 2023)", "paper": {"corpus_id": 266599825, "title": "Summarization of Investment Reports Using Pre-trained Model", "year": 2023, "venue": "IIAI International Conference on Advanced Applied Informatics", "authors": [{"name": "Hiroki Sakaji", "authorId": "2879326"}, {"name": "Ryotaro Kobayashi", "authorId": "2276796103"}, {"name": "Kiyoshi Izumi", "authorId": "2276798525"}, {"name": "Hiroyuki Mitsugi", "authorId": "2276797477"}, {"name": "Wataru Kuramoto", "authorId": "2276798124"}], "n_citations": 0}, "snippets": ["Related research on multi-document summarization includes the following papers. Moro et al. proposed the probabilistic method based on the combination of three language models to tackle multi-document summarization in the medical domain (Moro et al., 2022). Liao et al. investigated the feasibility of utilizing Abstract Meaning Representation formalism for multidocument summarization (Liao et al., 2018). Fabbri et al. constructed Multi-News, the large-scale multi-document news summarization dataset (Fabbri et al., 2019). Xiao et al. introduced PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data [9]. Nayeem et al. designed an abstractive fusion generation model at the sentence level, which jointly performs sentence fusion and paraphrasing (Nayeem et al., 2018). They applied their sentence-level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. Liu et al. developed the neural summarization model, which can effectively process multiple input documents and distill abstractive summaries (Liu et al., 2019). Li et al. develop a neural abstractive multi-document summarization model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries (Li et al., 2020). Jin et al. proposed the multigranularity interaction network to encode semantic representations for documents, sentences, and words (Jin et al., 2020). Deyoung et al. released MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature (DeYoung et al., 2021)."], "score": 0.97119140625}, {"id": "(DeYoung et al., 2021)", "paper": {"corpus_id": 233231380, "title": "MS\u02c62: Multi-Document Summarization of Medical Studies", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jay DeYoung", "authorId": "48727916"}, {"name": "Iz Beltagy", "authorId": "46181066"}, {"name": "Madeleine van Zuylen", "authorId": "15292561"}, {"name": "Bailey Kuehl", "authorId": "2003338023"}, {"name": "Lucy Lu Wang", "authorId": "31860505"}], "n_citations": 113}, "snippets": ["To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system\u2019s generated summaries. Data and models are available at https://github.com/allenai/ms2."], "score": 0.0}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 250636132, "title": "Generating a Structured Summary of Numerous Academic Papers: Dataset and Method", "year": 2022, "venue": "International Joint Conference on Artificial Intelligence", "authors": [{"name": "Shuaiqi Liu", "authorId": "3344531"}, {"name": "Jiannong Cao", "authorId": "144115026"}, {"name": "Ruosong Yang", "authorId": "8092850"}, {"name": "Zhiyuan Wen", "authorId": "2000188918"}], "n_citations": 18}, "snippets": ["Writing a survey paper on one research topic usually needs to cover the salient content from numerous related papers, which can be modeled as a multi-document summarization (MDS) task. Existing MDS datasets usually focus on producing the structureless summary covering a few input documents. Meanwhile, previous structured summary generation works focus on summarizing a single document into a multi-section summary. These existing datasets and methods cannot meet the requirements of summarizing numerous academic papers into a structured summary. To deal with the scarcity of available data, we propose BigSurvey, the first large-scale dataset for generating comprehensive summaries of numerous academic papers on each topic. We collect target summaries from more than seven thousand survey papers and utilize their 430 thousand reference papers\u2019 abstracts as input documents. To organize the diverse content from dozens of input documents and ensure the efficiency of processing long text sequences, we propose a summarization method named category-based alignment and sparse transformer (CAST). The experimental results show that our CAST method outperforms various advanced summarization methods."], "score": 0.0}, {"id": "(Banerjee et al., 2015)", "paper": {"corpus_id": 15795297, "title": "Multi-Document Abstractive Summarization Using ILP Based Multi-Sentence Compression", "year": 2015, "venue": "International Joint Conference on Artificial Intelligence", "authors": [{"name": "Siddhartha Banerjee", "authorId": "2169453878"}, {"name": "P. Mitra", "authorId": "143930195"}, {"name": "Kazunari Sugiyama", "authorId": "3060386"}], "n_citations": 136}, "snippets": ["On the DUC2004 and DUC2005 datasets, we demonstrate the effectiveness of our proposed method. Our proposed method outperforms not only some popular baselines but also the state-of-the-art extractive summarization systems. ROUGE scores (Oya et al., 2014) obtained by our system outperforms the best extractive summarizer on both the datasets. Our method also outperforms an abstractive summarizer based on multi-sentence compression (Filippova, 2010) when measured by ROUGE-2, ROUGE-L and ROUGE-SU4 scores."], "score": 0.9091796875}, {"id": "(DeYoung et al., 2023)", "paper": {"corpus_id": 256416214, "title": "Do Multi-Document Summarization Models Synthesize?", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Jay DeYoung", "authorId": "48727916"}, {"name": "Stephanie C. Martinez", "authorId": "2203750000"}, {"name": "I. Marshall", "authorId": "1808775"}, {"name": "Byron C. Wallace", "authorId": "1912476"}], "n_citations": 8}, "snippets": ["Automatic (multi-document) summarization (Nenkova and McKeown, 2011;Maybury, 1999) has been an active subfield within NLP for decades. We have focused our analysis on modern, neural abstractive models for conditional text generation (Bahdanau et al., 2014). In light of their empirical success, we have specifically evaluated a set of Transformer-based (Vaswani et al., 2017) models which have recently been used for multidocument summarization (Beltagy et al., 2020;(Zhang et al., 2019)Xiao et al., 2022;(Raffel et al., 2019). There has been some work on highlighting conflicting evidence in health literature specifically (Shah et al., 2021b,a), though this focused primarily on highlighting conflicting evidence and explicitly aggregating extracted content."], "score": 0.91259765625}, {"id": "(Mascarell et al., 2023)", "paper": {"corpus_id": 263610015, "title": "Entropy-based Sampling for Abstractive Multi-document Summarization in Low-resource Settings", "year": 2023, "venue": "International Conference on Natural Language Generation", "authors": [{"name": "Laura Mascarell", "authorId": "2121237"}, {"name": "Ribin Chalumattu", "authorId": "1879120021"}, {"name": "Julien Heitmann", "authorId": "2253607868"}], "n_citations": 1}, "snippets": ["Multi-document Summarization (MDS) aims at condensing the most important information from different documents. Despite the advances in single-document summarization (Zhang et al., 2019), summarizing multiple related documents remains a greater challenge due to its input length and the presence of redundant information (Fan et al., 2019)(Song et al., 2022). Therefore, some research focuses on implementing multi-stage approaches that first identify the relevant information to then feed it into a summarization model (Lebanoff et al., 2018)(Liu et al., 2019). More recent works utilize pre-trained language models (Lewis et al., 2019)(Raffel et al., 2019)(Xiao et al., 2021) finetuned for the summarization task and feed them with the source documents concatenated (Johner et al., 2021)(Xiao et al., 2021)."], "score": 0.9130859375}, {"id": "(Raffel et al., 2019)", "paper": {"corpus_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019, "venue": "Journal of machine learning research", "authors": [{"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Wei Li", "authorId": "2157338362"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 20336}, "snippets": ["Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."], "score": 0.0}, {"id": "(Zhang et al., 2019)", "paper": {"corpus_id": 209405420, "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Jingqing Zhang", "authorId": "47540100"}, {"name": "Yao Zhao", "authorId": "2143397386"}, {"name": "Mohammad Saleh", "authorId": "144413479"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 2054}, "snippets": ["Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets."], "score": 0.0}], "table": null}, {"title": "Challenges in Multi-Document Summarization", "tldr": "Multi-document summarization faces unique challenges beyond single-document approaches, including handling redundancy, resolving contradictions, and modeling cross-document relationships. Systems must also address computational complexity, temporal relevance, factual consistency, and domain adaptation while maintaining coherence across diverse sources. (10 sources)", "text": "\nMulti-document summarization (MDS) presents several distinct challenges that distinguish it from single-document summarization, making it a particularly complex task in natural language processing. While single-document summarization models have made significant progress, simply adapting these approaches to multi-document settings often leads to suboptimal results <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"52053741\" paperTitle=\"(Lebanoff et al., 2018)\" isShortName></Paper>. These challenges can be categorized into several key areas:\n\nFirst, the volume of information that must be processed in MDS increases exponentially with the number of input documents, creating significant computational challenges and extending processing times <Paper corpusId=\"271525553\" paperTitle=\"(Shakil et al., 2024)\" isShortName></Paper>. This computational complexity often limits the practical application of sophisticated models to large document collections.\n\nInformation inconsistency represents another critical challenge, as multiple documents frequently contain contradictory information from different authors or viewpoints <Paper corpusId=\"271525553\" paperTitle=\"(Shakil et al., 2024)\" isShortName></Paper> <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. Systems must identify the most accurate or relevant information across sources, requiring sophisticated content selection mechanisms that can resolve conflicts while maintaining factual accuracy.\n\nThe temporal dimension of information adds another layer of complexity, particularly when summarizing news articles where recent information may be more relevant than older content <Paper corpusId=\"271525553\" paperTitle=\"(Shakil et al., 2024)\" isShortName></Paper>. This requires models to incorporate a sense of temporality in their content selection process.\n\nCross-document relationships must be effectively modeled to create coherent summaries, as shown by numerous research efforts <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"170079112\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>. Unlike single-document summarization, where relationships between sentences are often clear from the document structure, MDS requires identifying and modeling relationships across independent texts with potentially different styles, terminologies, and structures.\n\nRedundancy management is particularly challenging in MDS due to information overlap across documents. Traditional summarization techniques often struggle with identifying and eliminating redundant content while preserving important information <Paper corpusId=\"272969413\" paperTitle=\"(Godbole et al., 2024)\" isShortName></Paper>. This challenge is compounded by variations in how similar information may be expressed across different sources.\n\nMaintaining coherence while integrating information from diverse sources with varying writing styles and levels of detail represents another significant hurdle <Paper corpusId=\"272969413\" paperTitle=\"(Godbole et al., 2024)\" isShortName></Paper>. The summary must present a unified narrative despite drawing from potentially disparate sources.\n\nQuery-based multi-document summarization introduces additional complexities, as systems must not only synthesize information across documents but also ensure relevance to specific queries <Paper corpusId=\"221293184\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. This has led to specialized approaches incorporating query attention mechanisms <Paper corpusId=\"5673925\" paperTitle=\"(Nema et al., 2017)\" isShortName></Paper> and Bayesian models <Paper corpusId=\"6241932\" paperTitle=\"(Daume et al., 2006)\" isShortName></Paper>.\n\nDomain adaptability presents another challenge, as summarization systems often perform well on the domains they were trained on but struggle to generalize across different subject areas or document types <Paper corpusId=\"272969413\" paperTitle=\"(Godbole et al., 2024)\" isShortName></Paper>. This limitation is particularly relevant given the diverse applications of MDS across news, scientific research, legal documents, and other specialized domains.\n\nFinally, ensuring factual consistency is crucial for practical applications of MDS <Paper corpusId=\"233948337\" paperTitle=\"(Lamsiyah et al., 2021)\" isShortName></Paper>. Abstractive approaches in particular risk generating \"fake facts\" when fusing information from multiple sources, necessitating verification mechanisms to maintain faithfulness to the source documents.\n\nThese challenges highlight the need for more sophisticated approaches that can effectively process multiple documents, model their relationships, and generate coherent, non-redundant, and factually accurate summaries that represent the diverse information contained in the source documents.", "citations": [{"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 236478143, "title": "Entity-Aware Abstractive Multi-Document Summarization", "year": 2021, "venue": "Findings", "authors": [{"name": "Hao Zhou", "authorId": null}, {"name": "Weidong Ren", "authorId": "2053308860"}, {"name": "Gongshen Liu", "authorId": "150112803"}, {"name": "Bo Su", "authorId": "153253583"}, {"name": "Wei Lu", "authorId": "143844110"}], "n_citations": 28}, "snippets": ["Multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020).\n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018).\n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020)."], "score": 0.93408203125}, {"id": "(Lebanoff et al., 2018)", "paper": {"corpus_id": 52053741, "title": "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization", "year": 2018, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Logan Lebanoff", "authorId": "50827114"}, {"name": "Kaiqiang Song", "authorId": "50982080"}, {"name": "Fei Liu", "authorId": "144544919"}], "n_citations": 157}, "snippets": ["Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors."], "score": 0.0}, {"id": "(Shakil et al., 2024)", "paper": {"corpus_id": 271525553, "title": "Abstractive text summarization: State of the art, challenges, and improvements", "year": 2024, "venue": "Neurocomputing", "authors": [{"name": "Hassan Shakil", "authorId": "2300173312"}, {"name": "Ahmad Farooq", "authorId": "2313554804"}, {"name": "Jugal K. Kalita", "authorId": "2261083539"}], "n_citations": 24}, "snippets": ["Although much research focuses on single-document summarization, multi-document summarization presents unique challenges (Narayan et al., 2018)(Lamsiyah et al., 2021). Unlike its single-document counterpart, multi-document summarization involves synthesizing information from multiple sources, often necessitating the alignment of documents, identification, and resolution of redundancies, contradictions, and varying perspectives. These complexities introduce unique challenges such as ensuring coherence in the face of diverse inputs and maintaining a balanced representation of all source documents. The enormous amount of information that needs to be processed during multi-document summarization is one of the main challenges. The amount of data increases rapidly with many documents, causing computational difficulties and extending processing times [183]. The possibility of conflicting information across documents presents another challenge. Finding the most precise or relevant information can be troublesome, particularly if the source text comprises different authors or viewpoints (Erkan et al., 2004). Furthermore, the temporal part of the information can present difficulties. For instance, while summing up news articles, recent data may be more pertinent than older information, expecting models to have a sense of temporality (Wan et al., 2007)."], "score": 0.93359375}, {"id": "(Erkan et al., 2004)", "paper": {"corpus_id": 506350, "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "year": 2004, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 3097}, "snippets": ["We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."], "score": 0.0}, {"id": "(Liu et al., 2019)", "paper": {"corpus_id": 170079112, "title": "Hierarchical Transformers for Multi-Document Summarization", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yang Liu", "authorId": "39798499"}, {"name": "Mirella Lapata", "authorId": "1747893"}], "n_citations": 298}, "snippets": ["In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines."], "score": 0.9189453125}, {"id": "(Godbole et al., 2024)", "paper": {"corpus_id": 272969413, "title": "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Aditi Godbole", "authorId": "2350511520"}, {"name": "Jabin Geevarghese George", "authorId": "2301049091"}, {"name": "Smita Shandilya", "authorId": "48781397"}], "n_citations": 5}, "snippets": ["Multi-document summarization presents unique challenges due to the need for synthesizing information from diverse sources, which may contain redundant, complementary, or contradictory information across documents [4]. Variations in writing style and level of detail add complexity to the task. Determining the relevance and importance of information from each source is crucial for creating a coherent and comprehensive summary [5].\n\nTraditional document summarization techniques often struggle with redundancy, inconsistency, lack of context understanding, scalability issues for multiple document summarization tasks, inability to capture cross-document relationships, difficulty handling diverse formats, and lack of domain adaptability [6,7]8]. These limitations highlight the need for more advanced approaches to multi-document summarization."], "score": 0.93115234375}, {"id": "(Zhang et al., 2020)", "paper": {"corpus_id": 221293184, "title": "Query Understanding via Intent Description Generation", "year": 2020, "venue": "International Conference on Information and Knowledge Management", "authors": [{"name": "Ruqing Zhang", "authorId": "2109960367"}, {"name": "Jiafeng Guo", "authorId": "70414094"}, {"name": "Yixing Fan", "authorId": "7888704"}, {"name": "Yanyan Lan", "authorId": "37510256"}, {"name": "Xueqi Cheng", "authorId": "1717004"}], "n_citations": 17}, "snippets": ["Query-based Multi-document Summarization. Query-based multi-document summarization is the process of automatically generating natural summaries of text documents in the context of a given query. An early work for extractive query-based multidocument summarization is presented by (Goldstein-Stewart et al., 1999), which ranked sentences using a weighted combination of statistical and linguistic features. (Daum\u00e9 et al., 2006) presented to extract sentences based on the language model, Bayesian model, and graphical model. [40] introduced the graph information to look for relevant sentences. (Schilder et al., 2008) used the multi-modality manifold-ranking algorithm to extract topic-focused summary from multiple documents. Recently, some works employ the encoder-decoder framework to produce the query-based summaries. [24] trained a pointer-generator model, and [3] incorporated relevance into a neural seq2seq models for query-based abstractive summarization. (Nema et al., 2017) introduced a new diversity based attention mechanism to alleviate the problem of repeating phrases."], "score": 0.9423828125}, {"id": "(Nema et al., 2017)", "paper": {"corpus_id": 5673925, "title": "Diversity driven attention model for query-based abstractive summarization", "year": 2017, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Preksha Nema", "authorId": "9192775"}, {"name": "Mitesh M. Khapra", "authorId": "2361078"}, {"name": "Anirban Laha", "authorId": "2039596"}, {"name": "Balaraman Ravindran", "authorId": "1723632"}], "n_citations": 169}, "snippets": ["Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores."], "score": 0.0}, {"id": "(Daume et al., 2006)", "paper": {"corpus_id": 6241932, "title": "Bayesian Query-Focused Summarization", "year": 2006, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Hal Daum\u00e9", "authorId": "1722360"}, {"name": "D. Marcu", "authorId": "1695463"}], "n_citations": 285}, "snippets": ["We present BAYESUM (for \"Bayesian summarization\"), a model for sentence extraction in query-focused summarization. BAYESUM leverages the common case in which multiple documents are relevant to a single query. Using these documents as reinforcement for query terms, BAYESUM is not afflicted by the paucity of information in short queries. We show that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system. Furthermore, we show how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework."], "score": 0.0}, {"id": "(Lamsiyah et al., 2021)", "paper": {"corpus_id": 233948337, "title": "Unsupervised extractive multi-document summarization method based on transfer learning from BERT multi-task fine-tuning", "year": 2021, "venue": "Journal of information science", "authors": [{"name": "Salima Lamsiyah", "authorId": "81542199"}, {"name": "Abdelkader El Mahdaouy", "authorId": "3196929"}, {"name": "S. Ouatik", "authorId": "122945591"}, {"name": "B. Espinasse", "authorId": "2925748"}], "n_citations": 39}, "snippets": ["Text representation is a fundamental cornerstone that impacts the effectiveness of several text summarization methods. Transfer learning using pre-trained word embedding models has shown promising results. However, most of these representations do not consider the order and the semantic relationships between words in a sentence, and thus they do not carry the meaning of a full sentence. To overcome this issue, the current study proposes an unsupervised method for extractive multi-document summarization based on transfer learning from BERT sentence embedding model. Moreover, to improve sentence representation learning, we fine-tune BERT model on supervised intermediate tasks from GLUE benchmark datasets using single-task and multi-task fine-tuning methods. Experiments are performed on the standard DUC\u20192002\u20132004 datasets. The obtained results show that our method has significantly outperformed several baseline methods and achieves a comparable and sometimes better performance than the recent state-of-the-art deep learning\u2013based methods. Furthermore, the results show that fine-tuning BERT using multi-task learning has considerably improved the performance."], "score": 0.0}], "table": null}], "cost": 0.64575}}
