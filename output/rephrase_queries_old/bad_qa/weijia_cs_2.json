{"original_query": "What are papers that train multilmodal language models that can input and output both text and images?", "input": "are papers that multilmodal language models output text images", "output": {"sections": [{"title": "Introduction to Multimodal Language Models", "tldr": "Multimodal Large Language Models (MLLMs) extend traditional language models by incorporating multiple types of data like images, audio, and graphs alongside text. This evolution enables more human-like interaction with AI systems and opens up diverse applications ranging from visual question answering to image generation and editing. (11 sources)", "text": "\nMultimodal Large Language Models (MLLMs) represent a significant advancement in artificial intelligence by combining the processing of multiple data modalities\u2014primarily text and images\u2014within a single model architecture. At their core, MLLMs are designed to take both visual and textual inputs and generate text outputs <Paper corpusId=\"270285703\" paperTitle=\"(Basu et al., 2024)\" isShortName></Paper>. This multimodal capability marks an important evolution from text-only large language models (LLMs), significantly expanding their potential applications and use cases <Paper corpusId=\"271218251\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThe development of MLLMs is motivated by the human experience of interacting with the world through multiple sensory inputs simultaneously <Paper corpusId=\"271218251\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. While traditional approaches focused on single-modality models or unidirectional transformations (such as text-to-image or image-to-text), modern MLLMs are designed to process, understand, and manipulate visual and textual information in tandem <Paper corpusId=\"271218251\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper>. These models create a more nuanced integration of linguistic and visual information, enabling more sophisticated understanding and generation capabilities <Paper corpusId=\"276929408\" paperTitle=\"(Fang et al., 2025)\" isShortName></Paper>.\n\nSeveral pioneering architectures have shaped the MLLM landscape. Flamingo introduced key innovations to bridge pretrained vision and language models, handle interleaved visual and textual data, and process various visual inputs <Paper corpusId=\"277313749\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"248476411\" paperTitle=\"(Alayrac et al., 2022)\" isShortName></Paper>. BLIP-2 demonstrated an efficient pre-training strategy that bootstraps vision-language learning from frozen pre-trained components <Paper corpusId=\"277313749\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"256390509\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. LLaVA pioneered the use of GPT-4 to generate multimodal instruction-following data, creating an end-to-end trained model for general-purpose visual and language understanding <Paper corpusId=\"277313749\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"258179774\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nMore recent models like GPT-4o and InternVL have further advanced the field by demonstrating strong performance across multimodal tasks such as visual understanding, language generation, and visual question answering <Paper corpusId=\"275820726\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>. Beyond images and text, researchers have begun incorporating additional modalities into MLLMs, including audio, graph structures, and even structured charts <Paper corpusId=\"277857505\" paperTitle=\"(Xia et al., 2025)\" isShortName></Paper>.\n\nDespite these advances, the representation of multimodal information within MLLMs remains an active area of research <Paper corpusId=\"271245054\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>. These models typically leverage joint embedding spaces where images and their corresponding textual descriptions are closely aligned, allowing for more effective cross-modal understanding and generation <Paper corpusId=\"275820726\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>. This architectural approach enables MLLMs to not only understand multimodal inputs but also potentially generate outputs across different modalities.", "citations": [{"id": "(Basu et al., 2024)", "paper": {"corpus_id": 270285703, "title": "Understanding Information Storage and Transfer in Multi-modal Large Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Samyadeep Basu", "authorId": "2114710333"}, {"name": "Martin Grayson", "authorId": "2052886960"}, {"name": "C. Morrison", "authorId": "121927341"}, {"name": "Besmira Nushi", "authorId": "2571049"}, {"name": "S. Feizi", "authorId": "34389431"}, {"name": "Daniela Massiceti", "authorId": "2286878667"}], "n_citations": 12}, "snippets": ["We consider a MLLM to be a model that takes an image and text as input, and generates a text output [2]."], "score": 0.939453125}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 271218251, "title": "Visualization Literacy of Multimodal Large Language Models: A Comparative Study", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhimin Li", "authorId": "2116785555"}, {"name": "Haichao Miao", "authorId": "2253579462"}, {"name": "Valerio Pascucci", "authorId": "2251997858"}, {"name": "Shusen Liu", "authorId": "2289268771"}], "n_citations": 6}, "snippets": ["Since humans interact with their environment through multi-modal sensory input, the evolution from text-only LLM systems to multimodal LLMs significantly increases the potential usage cases for LLMs, prompting many applications [35]", "models designed for input in one modality and output in another, such as text-to-image (Rombach et al., 2021)[27] and image-to-text [37] models."], "score": 0.9140625}, {"id": "(Rombach et al., 2021)", "paper": {"corpus_id": 245335280, "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Robin Rombach", "authorId": "1660819540"}, {"name": "A. Blattmann", "authorId": "119843260"}, {"name": "Dominik Lorenz", "authorId": "2053482699"}, {"name": "Patrick Esser", "authorId": "35175531"}, {"name": "B. Ommer", "authorId": "1796707"}], "n_citations": 15768}, "snippets": ["By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."], "score": 0.0}, {"id": "(Fang et al., 2025)", "paper": {"corpus_id": 276929408, "title": "Recognition-Synergistic Scene Text Editing", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhengyao Fang", "authorId": "2313679002"}, {"name": "Pengyuan Lyu", "authorId": "10344582"}, {"name": "Jingjing Wu", "authorId": "2313920076"}, {"name": "Chengquan Zhang", "authorId": "2248958848"}, {"name": "Jun Yu", "authorId": "2349736964"}, {"name": "Guan-Lun Lu", "authorId": "2261251059"}, {"name": "Wenjie Pei", "authorId": "1678473"}], "n_citations": 1}, "snippets": ["In response to the notable progress of large language models in natural language processing [1,2,(Chowdhery et al., 2022), the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models (Ho et al., 2020), but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,(Sun et al., 2023)[40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs (Brooks et al., 2022)13,(Mokady et al., 2023) offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation."], "score": 0.93994140625}, {"id": "(Yu et al., 2025)", "paper": {"corpus_id": 277313749, "title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hao Yu", "authorId": "2352021201"}, {"name": "Zhuokai Zhao", "authorId": "2352021017"}, {"name": "Shen Yan", "authorId": "2334461697"}, {"name": "Lukasz Korycki", "authorId": "2352020894"}, {"name": "Jianyu Wang", "authorId": "2352039047"}, {"name": "Baosheng He", "authorId": "2334709723"}, {"name": "Jiayi Liu", "authorId": "2340394857"}, {"name": "Lizhu Zhang", "authorId": "2340399373"}, {"name": "Xiangjun Fan", "authorId": "2323788930"}, {"name": "Hanchao Yu", "authorId": "2352014487"}], "n_citations": 2}, "snippets": ["Recent advances in multimodal large language models (MLLMs) have led to the development of models that can receive, reason, and output with multimodal information. Notable examples include BLIP-2 (Li et al., 2023), Flamingo (Alayrac et al., 2022), and LLaVA (Liu et al., 2023), which integrate additional encoders into textual LLMs to process inputs from other modalities."], "score": 0.943359375}, {"id": "(Alayrac et al., 2022)", "paper": {"corpus_id": 248476411, "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jean-Baptiste Alayrac", "authorId": "2285263"}, {"name": "Jeff Donahue", "authorId": "7408951"}, {"name": "Pauline Luc", "authorId": "152831141"}, {"name": "Antoine Miech", "authorId": "19200186"}, {"name": "Iain Barr", "authorId": "2159207795"}, {"name": "Yana Hasson", "authorId": "66535271"}, {"name": "Karel Lenc", "authorId": "3257286"}, {"name": "A. Mensch", "authorId": "1697879"}, {"name": "Katie Millican", "authorId": "2143434227"}, {"name": "Malcolm Reynolds", "authorId": "47447264"}, {"name": "Roman Ring", "authorId": "81387328"}, {"name": "Eliza Rutherford", "authorId": "2143538252"}, {"name": "Serkan Cabi", "authorId": "12159303"}, {"name": "Tengda Han", "authorId": "22237490"}, {"name": "Zhitao Gong", "authorId": "48398849"}, {"name": "Sina Samangooei", "authorId": "2412073"}, {"name": "Marianne Monteiro", "authorId": "49601928"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "Andy Brock", "authorId": "2065040422"}, {"name": "Aida Nematzadeh", "authorId": "3208081"}, {"name": "Sahand Sharifzadeh", "authorId": "7782886"}, {"name": "Mikolaj Binkowski", "authorId": "9961753"}, {"name": "Ricardo Barreira", "authorId": "2026369796"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "Andrew Zisserman", "authorId": "1688869"}, {"name": "K. Simonyan", "authorId": "34838386"}], "n_citations": 3607}, "snippets": ["Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data."], "score": 0.0}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 256390509, "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Junnan Li", "authorId": "49299019"}, {"name": "Dongxu Li", "authorId": "2981509"}, {"name": "S. Savarese", "authorId": "1702137"}, {"name": "Steven C. H. Hoi", "authorId": "2184854289"}], "n_citations": 4653}, "snippets": ["The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."], "score": 0.0}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 258179774, "title": "Visual Instruction Tuning", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Haotian Liu", "authorId": "2143856368"}, {"name": "Chunyuan Li", "authorId": "2109737569"}, {"name": "Qingyang Wu", "authorId": "31060482"}, {"name": "Yong Jae Lee", "authorId": "144756076"}], "n_citations": 4921}, "snippets": ["Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."], "score": 0.0}, {"id": "(Lee et al., 2025)", "paper": {"corpus_id": 275820726, "title": "Multi-aspect Knowledge Distillation with Large Language Model", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Taegyeong Lee", "authorId": "2238019356"}, {"name": "Jinsik Bang", "authorId": "2341715815"}, {"name": "Soyeong Kwon", "authorId": "2294517321"}, {"name": "Taehwan Kim", "authorId": "2174161125"}], "n_citations": 0}, "snippets": ["Multimodal Large Language Models (MLLMs) [1,(Alayrac et al., 2022)(Liu et al., 2023)40,41] have shown significant performance improvements in multi-modal problems such as visual question answering and image captioning by leveraging large-scale datasets to learn a joint embedding space where images and their corresponding textual descriptions are closely aligned. GPT-4o [1] has the ability to get the context and has a humanlike text generation ability, showing strong performance not only in the natural language processing area but also in multi-modal tasks. InternVL (Chen et al., 2023) can address both text and image data and shows better performances in various multimodal tasks (such as visual understanding, language generation, and visual QA) while using fewer computing resources compared to other MLLMs."], "score": 0.97265625}, {"id": "(Xia et al., 2025)", "paper": {"corpus_id": 277857505, "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yutong Xia", "authorId": "2279954226"}, {"name": "Ao Qu", "authorId": "2283843071"}, {"name": "Yunhan Zheng", "authorId": "97527544"}, {"name": "Yihong Tang", "authorId": "2243088943"}, {"name": "Dingyi Zhuang", "authorId": "1580217088"}, {"name": "Yuxuan Liang", "authorId": "2261965063"}, {"name": "Cathy Wu", "authorId": "2334035234"}, {"name": "Roger Zimmermann", "authorId": "2249532794"}, {"name": "Jinhua Zhao", "authorId": "2283883570"}], "n_citations": 2}, "snippets": ["Multimodal Large Language Models (MLLMs) extend this paradigm by incorporating additional modalities, e.g., imagery (Liu et al., 2023)(Yan et al., 2023), audio (Chu et al., 2023), graph (Tang et al., 2023), and structured charts (Han et al., 2023), alongside text."], "score": 0.9033203125}, {"id": "(Jiang et al., 2024)", "paper": {"corpus_id": 271245054, "title": "E5-V: Universal Embeddings with Multimodal Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ting Jiang", "authorId": "2259012212"}, {"name": "Minghui Song", "authorId": "2260342581"}, {"name": "Zihan Zhang", "authorId": "2260851231"}, {"name": "Haizhen Huang", "authorId": "2146285313"}, {"name": "Weiwei Deng", "authorId": "2066621592"}, {"name": "Feng Sun", "authorId": "2247156451"}, {"name": "Qi Zhang", "authorId": "2256972722"}, {"name": "Deqing Wang", "authorId": "2145351849"}, {"name": "Fuzhen Zhuang", "authorId": "2162961864"}], "n_citations": 34}, "snippets": ["Multimodal large language models (MLLMs) have shown promising advancements in general visual and language understanding. However, the representation of multimodal information using MLLMs remains largely unexplored."], "score": 0.89306640625}], "table": null}, {"title": "MLLMs with Text Output Capabilities", "tldr": "Multimodal Large Language Models primarily generate text outputs from various input modalities including images, videos, and audio. These models align visual representations with language model input spaces to enable coherent textual responses to multimodal inputs. (7 sources)", "text": "\nMultimodal Large Language Models (MLLMs) predominantly function as \"Any-to-Text\" systems, where they process various input modalities but produce textual outputs <Paper corpusId=\"274023126\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258179774\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This architecture represents the most common approach to multimodality in current systems, where visual, audio, or other data types are encoded and projected into the word embedding space of the language model for generating coherent text responses <Paper corpusId=\"274023126\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nA key technical challenge in these systems involves effectively aligning visual representations with the input space of the language model. LLaVA exemplifies this approach by using the LLM's native self-attention mechanism to process visual information alongside textual input <Paper corpusId=\"269354784\" paperTitle=\"(Bazi et al., 2024)\" isShortName></Paper>. This alignment enables the model to generate descriptive text about visual content, answer questions about images, and provide detailed explanations based on visual inputs <Paper corpusId=\"258179774\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nMany MLLMs achieve this multimodal integration by pre-processing input modalities into textual descriptions that can be understood by the language model. For example, several approaches convert images into captions and object detection results into text descriptions to create multimodal training data <Paper corpusId=\"267637150\" paperTitle=\"(Tsai et al., 2024)\" isShortName></Paper>. This technique is particularly evident in specialized domains: OphGLM extracts information from fundus images to create diagnostic reports, while ChatCAD transforms X-ray outputs from CAD models into natural language <Paper corpusId=\"267637150\" paperTitle=\"(Tsai et al., 2024)\" isShortName></Paper>.\n\nThe architectural approach to text generation in MLLMs typically involves decoupling language modules into a text decoder for generation and a text encoder for embedding <Paper corpusId=\"270094821\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. This separation allows models to efficiently process multiple modalities while maintaining strong text generation capabilities. Similar principles extend beyond visual inputs to other modalities, as demonstrated by Audio Flamingo, which enables LLMs to understand and respond to non-speech sounds and non-verbal speech <Paper corpusId=\"267412182\" paperTitle=\"(Kong et al., 2024)\" isShortName></Paper>.\n\nRecent developments have expanded beyond text-only outputs to include speech-based interactions, enhancing user experience compared to traditional text-based interfaces. Models like GPT-4o now enable voice interaction with LLMs, representing an important advance in making multimodal systems more accessible and natural to use <Paper corpusId=\"274981694\" paperTitle=\"(Pham et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Liu et al., 2024)", "paper": {"corpus_id": 274023126, "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xuannan Liu", "authorId": "2263703053"}, {"name": "Xing Cui", "authorId": "2212020693"}, {"name": "Peipei Li", "authorId": "2253583840"}, {"name": "Zekun Li", "authorId": "2265220595"}, {"name": "Huaibo Huang", "authorId": "2268583100"}, {"name": "Shuhan Xia", "authorId": "2306060428"}, {"name": "Miaoxuan Zhang", "authorId": "2330734159"}, {"name": "Yueying Zou", "authorId": "2305465534"}, {"name": "Ran He", "authorId": "2261958972"}], "n_citations": 11}, "snippets": ["The first category includes Any-to-Text (Any Modality to Text) Models, which integrate inputs from multiple modalities, encode them, and project into the word embedding space of the LLM for generating textual output (Liu et al., 2023)- (Kong et al., 2024), (Zhu et al., 2023)- [30]."], "score": 0.91357421875}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 258179774, "title": "Visual Instruction Tuning", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Haotian Liu", "authorId": "2143856368"}, {"name": "Chunyuan Li", "authorId": "2109737569"}, {"name": "Qingyang Wu", "authorId": "31060482"}, {"name": "Yong Jae Lee", "authorId": "144756076"}], "n_citations": 4921}, "snippets": ["Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."], "score": 0.0}, {"id": "(Bazi et al., 2024)", "paper": {"corpus_id": 269354784, "title": "RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery", "year": 2024, "venue": "Remote Sensing", "authors": [{"name": "Y. Bazi", "authorId": "1795469"}, {"name": "Laila Bashmal", "authorId": "25206458"}, {"name": "Mohamad Mahmoud Al Rahhal", "authorId": "2267672536"}, {"name": "Riccardo Ricci", "authorId": "2212706249"}, {"name": "F. Melgani", "authorId": "1774633"}], "n_citations": 44}, "snippets": ["The open-source project LLaVA [15] aims to replicate this performance by aligning visual representations with the input space of the LLM.By leveraging the original self-attention mechanism within the LLM, LLaVA enables effective processing of visual information alongside textual input."], "score": 0.90234375}, {"id": "(Tsai et al., 2024)", "paper": {"corpus_id": 267637150, "title": "Text-centric Alignment for Multi-Modality Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yun-Da Tsai", "authorId": "3328096"}, {"name": "Ting-Yu Yen", "authorId": "2283934604"}, {"name": "Pei-Fu Guo", "authorId": "2256994069"}, {"name": "Zhe-Yan Li", "authorId": "2283982736"}, {"name": "Shou-De Lin", "authorId": "2237949904"}], "n_citations": 9}, "snippets": ["Several works involve processing images with foundation models and combining the results into text for LLM training, linking visual information with text. For example, LLaVA [26] utilizes GPT4 to transfer images into captions and object detection results into text descriptions as their multimodal training data. VideoChat-Text [25] encodes video into textual descriptions. In medicine, OphGLM [15] uses classification and segmentation models to extract information from fundus images and create diagnostic reports for LLMs. Similarly, ChatCAD [43] transforms X-ray outputs from CAD models into natural language for LLM input."], "score": 0.95166015625}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 270094821, "title": "Multi-Modal Generative Embedding Model", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Feipeng Ma", "authorId": "2218044713"}, {"name": "Hongwei Xue", "authorId": "1878753365"}, {"name": "Guangting Wang", "authorId": "2303832848"}, {"name": "Yizhou Zhou", "authorId": "2141747950"}, {"name": "Fengyun Rao", "authorId": "2276428924"}, {"name": "Shilin Yan", "authorId": "2303910455"}, {"name": "Yueyi Zhang", "authorId": "2240587447"}, {"name": "Siying Wu", "authorId": "2303712729"}, {"name": "Mike Zheng Shou", "authorId": "2303652188"}, {"name": "Xiaoyan Sun", "authorId": "2303715824"}], "n_citations": 4}, "snippets": ["Most multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding."], "score": 0.91943359375}, {"id": "(Kong et al., 2024)", "paper": {"corpus_id": 267412182, "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Zhifeng Kong", "authorId": "2229373549"}, {"name": "Arushi Goel", "authorId": "2282529324"}, {"name": "Rohan Badlani", "authorId": "3433816"}, {"name": "Wei Ping", "authorId": "2253664013"}, {"name": "Rafael Valle", "authorId": "2281035249"}, {"name": "Bryan Catanzaro", "authorId": "2264406909"}], "n_citations": 94}, "snippets": ["Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo."], "score": 0.0}, {"id": "(Pham et al., 2024)", "paper": {"corpus_id": 274981694, "title": "SilVar: Speech Driven Multimodal Model for Reasoning Visual Question Answering and Object Localization", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tan-Hanh Pham", "authorId": "2312271163"}, {"name": "Hoang-Nam Le", "authorId": "2337102747"}, {"name": "Phu-Vinh Nguyen", "authorId": "2337398686"}, {"name": "Chris Ngo", "authorId": "2336865725"}, {"name": "Truong-Son Hy", "authorId": "2308040088"}], "n_citations": 1}, "snippets": ["Despite the significant advancements in VLMs, most of them currently support only text-based interactions, limiting their application to scenarios where text input is inconvenient or unavailable. In addition, while reasoning and prompting techniques for LLMs have been explored, these techniques for speech-based instruction remain largely underexplored. Recently, models like GPT-4o [42] have enabled interaction with LLMs through speech, significantly enhancing the user experience compared to traditional text-based interactions."], "score": 0.9013671875}], "table": null}, {"title": "MLLMs with Image Output Capabilities", "tldr": "While many MLLMs primarily generate text, newer architectures have expanded to enable image generation capabilities through various approaches including text-to-layout generation, visual token production, and integration with diffusion models. (4 sources)", "text": "\nMultimodal Large Language Models have evolved beyond text-only outputs to incorporate image generation capabilities through several innovative approaches. One significant development has been the integration of text-to-image generation with layout planning. VPGEN exemplifies this approach by leveraging language models to handle layout generation, producing textual descriptions of objects, their counts, and positions, which enables more flexible adaptation of pretrained LMs to understand diverse region descriptions <Paper corpusId=\"258865230\" paperTitle=\"(Cho et al., 2023)\" isShortName></Paper>. This approach offers advantages over previous methods that were limited to predefined object classes and required training new layout predictor modules from scratch.\n\nThe development of image generation capabilities in MLLMs has been significantly influenced by pioneering models like DALL-E, which introduced a Transformer-based approach for converting text to images by treating images as sequences of discrete tokens <Paper corpusId=\"267412060\" paperTitle=\"(Jing et al., 2024)\" isShortName></Paper>. Building upon this foundation, researchers have achieved subsequent improvements through model scaling, pre-training, and enhanced image quantization techniques. Models like CM3 have demonstrated the ability to generate both images and text by training on large HTML corpora with randomly masked content <Paper corpusId=\"267412060\" paperTitle=\"(Jing et al., 2024)\" isShortName></Paper>.\n\nRecent research has expanded MLLMs' visual generation capabilities beyond the previously well-explored areas of visual understanding and reasoning <Paper corpusId=\"273403986\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. These newer approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. For image generation specifically, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders <Paper corpusId=\"273403986\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.\n\nSome MLLMs achieve image generation through integration with external models rather than generating images directly. For instance, HuggingGPT generates textual image descriptions that are then passed to external diffusion models to synthesize the actual images <Paper corpusId=\"272911245\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. This cooperative approach leverages the strengths of specialized models while maintaining the multimodal capabilities of MLLMs.", "citations": [{"id": "(Cho et al., 2023)", "paper": {"corpus_id": 258865230, "title": "Visual Programming for Text-to-Image Generation and Evaluation", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jaemin Cho", "authorId": "2706729"}, {"name": "Abhaysinh Zala", "authorId": "2008198436"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 51}, "snippets": ["Bridging text-to-image generation with layouts. One line of research decomposes the T2I generation task into two stages: text-to-layout generation and layout-to-image generation [25; 26; 27; 28]. However, the previous approaches focus on a set of predefined object classes by training a new layout predictor module from scratch and therefore cannot place new objects unseen during training. In contrast, our VPGEN uses an LM to handle layout generation by generating objects/counts/positions in text, allowing flexible adaptation of pretrained LMs that can understand diverse region descriptions."], "score": 0.8916015625}, {"id": "(Jing et al., 2024)", "paper": {"corpus_id": 267412060, "title": "When Large Language Models Meet Vector Databases: A Survey", "year": 2024, "venue": "2025 Conference on Artificial Intelligence x Multimedia (AIxMM)", "authors": [{"name": "Zhi Jing", "authorId": "15217571"}, {"name": "Yongye Su", "authorId": "2278407298"}, {"name": "Yikun Han", "authorId": "2260277757"}, {"name": "Bo Yuan", "authorId": "2282533661"}, {"name": "Haiyun Xu", "authorId": "2260396712"}, {"name": "Chunjiang Liu", "authorId": "2260828894"}, {"name": "Kehai Chen", "authorId": "2266796043"}, {"name": "Min Zhang", "authorId": "2267153486"}], "n_citations": 38}, "snippets": ["The impressive achievements of LLMs have inspired significant advancements in vision-language research. DALL-E from OpenAI introduced a Transformer-based approach for converting text to images, treating images as sequences of discrete tokens. Subsequent improvements in the text-to-image area [38] have been achieved through methods like model scaling, pre-training, and enhanced image quantization models", "CM3 [41] trained a randomly masked model on a large HTML corpus and showed that the model is capable of generating images and text."], "score": 0.9384765625}, {"id": "(Fang et al., 2024)", "paper": {"corpus_id": 273403986, "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Rongyao Fang", "authorId": "144484147"}, {"name": "Chengqi Duan", "authorId": "2326302007"}, {"name": "Kun Wang", "authorId": "2267252383"}, {"name": "Hao Li", "authorId": "2274232642"}, {"name": "Hao Tian", "authorId": "2274118690"}, {"name": "Xingyu Zeng", "authorId": "2257433221"}, {"name": "Rui Zhao", "authorId": "2319817758"}, {"name": "Jifeng Dai", "authorId": "2292283383"}, {"name": "Hongsheng Li", "authorId": "2268799275"}, {"name": "Xihui Liu", "authorId": "2286520347"}], "n_citations": 15}, "snippets": ["Recent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders."], "score": 0.94775390625}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 272911245, "title": "MIO: A Foundation Model on Multimodal Tokens", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Z. Wang", "authorId": "2252413754"}, {"name": "King Zhu", "authorId": "2323111394"}, {"name": "Chunpu Xu", "authorId": "2280285922"}, {"name": "Wangchunshu Zhou", "authorId": "2284803168"}, {"name": "Jiaheng Liu", "authorId": "2294523552"}, {"name": "Yibo Zhang", "authorId": "2277149577"}, {"name": "Jiashuo Wang", "authorId": "2310651349"}, {"name": "Ning Shi", "authorId": "2322957423"}, {"name": "Siyu Li", "authorId": "2323142735"}, {"name": "Yizhi Li", "authorId": "2129449392"}, {"name": "Haoran Que", "authorId": "2249762724"}, {"name": "Zhaoxiang Zhang", "authorId": "2322607797"}, {"name": "Yuanxing Zhang", "authorId": "2279778502"}, {"name": "Ge Zhang", "authorId": "2307558672"}, {"name": "Ke Xu", "authorId": "2249845515"}, {"name": "Jie Fu", "authorId": "2253966125"}, {"name": "Wenhao Huang", "authorId": "2239245627"}], "n_citations": 12}, "snippets": ["For example, HuggingGPT (Shen et al., 2023) generates textual image descriptions for external diffusion models to synthesize images."], "score": 0.89697265625}], "table": null}, {"title": "MLLMs with Combined Text and Image Output", "tldr": "Recent advances in multimodal language models have enabled systems that can generate both text and images simultaneously. These models use various approaches including interleaved generation, joint autoregressive frameworks, and visual token integration to produce coherent multimodal outputs. (14 sources)", "text": "\nMultimodal Large Language Models (MLLMs) have evolved beyond single-modality outputs to systems capable of generating both text and images in a unified manner. Early steps toward this integration began with alignment of latent representations between modalities. For instance, Contrastive Language-Image Pretraining (CLIP) aligned image and text encoders to enable caption generation from images <Paper corpusId=\"233168627\" paperTitle=\"(Franceschelli et al., 2021)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>.\n\nSignificant advancements have been made in creating MLLMs that can process and generate interleaved image-text content. Models like Flamingo demonstrated the ability to \"process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images\" <Paper corpusId=\"256416164\" paperTitle=\"(Koh et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258947258\" paperTitle=\"(Koh et al._1, 2023)\" isShortName></Paper>. This approach maintains the language model's core functionality while adding cross-modality interactions through fine-tuned input and output layers.\n\nBuilding on this foundation, GILL (Generating Images with Large Language Models) expanded multimodal capabilities to include novel image generation alongside text, establishing a more comprehensive multimodal system <Paper corpusId=\"258959284\" paperTitle=\"(Koh et al._2, 2023)\" isShortName></Paper>. Similarly, FROMAGe implemented image-text inputs and outputs for multimodal dialogue while keeping the pre-trained language model frozen <Paper corpusId=\"259164940\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\nFor coherent multimodal content generation, some approaches use specialized representations that bridge visual and textual elements. The concept of \"generative vokens\" enables interleaved vision-and-language generation with coherent image-text outputs <Paper corpusId=\"263608981\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>. Other systems like MAnTiS use modality-specific encoders that project to a common textual token space to form a conditionality prefix guiding the generation process <Paper corpusId=\"237417089\" paperTitle=\"(Sollami et al., 2021)\" isShortName></Paper>.\n\nMore recent research has focused on creating unified frameworks specifically designed for mixed-modal generation. The Joint Autoregressive Mixture (JAM) framework systematically fuses existing text and image generation models with a specialized instruction-tuning strategy for mixed-modal tasks <Paper corpusId=\"262898422\" paperTitle=\"(Aiello et al., 2023)\" isShortName></Paper>. This approach has demonstrated high-quality multimodal outputs and represents one of the first models explicitly designed for this purpose.\n\nThe trend toward unified multimodal language models continues with systems capable of both comprehension and generation <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>. Some approaches like Chameleon, Show-o, and Emu3 directly adopt VQ tokenizers to encode images for multimodal understanding and generation, though aligning visual tokens with language tokens remains challenging <Paper corpusId=\"274305717\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>. Other models use continuous representations interleaved with text tokens for autoregressive modeling to generate images <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258822817\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>.\n\nThese advances represent significant progress toward the goal of creating MLLMs that can seamlessly process multimodal inputs and generate coherent multimodal outputs, treating text and images as interchangeable elements within a unified representation space <Paper corpusId=\"265498814\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>.", "citations": [{"id": "(Franceschelli et al., 2021)", "paper": {"corpus_id": 233168627, "title": "Creativity and Machine Learning: A Survey", "year": 2021, "venue": "ACM Computing Surveys", "authors": [{"name": "Giorgio Franceschelli", "authorId": "2067291198"}, {"name": "Mirco Musolesi", "authorId": "1806767"}], "n_citations": 42}, "snippets": ["The first examples of these systems consider text and images together as the output of the Transformer architecture. By aligning their latent representations, images and texts can be generated by Transformer-based decoders given a multimodal representation. For instance, Contrastive Language-Image Pretraining (CLIP) (Radford et al., 2021) has an image encoder pre-trained together with a text encoder to generate a caption for an image."], "score": 0.97021484375}, {"id": "(Radford et al., 2021)", "paper": {"corpus_id": 231591445, "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alec Radford", "authorId": "38909097"}, {"name": "Jong Wook Kim", "authorId": "2110935237"}, {"name": "Chris Hallacy", "authorId": "2004021329"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Gabriel Goh", "authorId": "40087786"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "I. Sutskever", "authorId": "1701686"}], "n_citations": 29867}, "snippets": ["State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."], "score": 0.0}, {"id": "(Koh et al., 2023)", "paper": {"corpus_id": 256416164, "title": "Grounding Language Models to Images for Multimodal Generation", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}, {"name": "Daniel Fried", "authorId": "47070750"}], "n_citations": 93}, "snippets": ["This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images."], "score": 0.95556640625}, {"id": "(Koh et al._1, 2023)", "paper": {"corpus_id": 258947258, "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}, {"name": "Daniel Fried", "authorId": "47070750"}], "n_citations": 122}, "snippets": ["and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images."], "score": 0.966796875}, {"id": "(Koh et al._2, 2023)", "paper": {"corpus_id": 258959284, "title": "Generating Images with Multimodal Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "Daniel Fried", "authorId": "47070750"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}], "n_citations": 259}, "snippets": ["In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1)."], "score": 0.984375}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 259164940, "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation", "year": 2023, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Zhiwei Zhang", "authorId": "2187404140"}, {"name": "Yuliang Liu", "authorId": "2108353180"}], "n_citations": 0}, "snippets": ["Another relevant work, FROMAGe (Jing Yu Koh, 2023), also involves image-text inputs and outputs for multimodal dialogue", "FROMAGe (Jing Yu Koh, 2023) also involves image-text inputs and outputs for multimodal dialogue, with a few linear layers fine-tuned while keeping the pre-trained language model frozen."], "score": 0.927734375}, {"id": "(Zheng et al., 2023)", "paper": {"corpus_id": 263608981, "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Kaizhi Zheng", "authorId": "2253444275"}, {"name": "Xuehai He", "authorId": "2149253467"}, {"name": "Xin Eric Wang", "authorId": "2256599634"}], "n_citations": 98}, "snippets": ["However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs."], "score": 0.91943359375}, {"id": "(Sollami et al., 2021)", "paper": {"corpus_id": 237417089, "title": "Multimodal Conditionality for Natural Language Generation", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Michael Sollami", "authorId": "3423000"}, {"name": "Aashish Jain", "authorId": "10818273"}], "n_citations": 10}, "snippets": ["We propose MAnTiS, Multimodal Adaptation for Text Synthesis, a general approach for multimodal conditionality in transformer-based NLG models. In this method, we pass inputs from each modality through modality-specific encoders, project to textual token space, and finally join to form a conditionality prefix. We fine-tune the pretrained language model and encoders with the conditionality prefix guiding the generation. We apply MAnTiS to the task of product description generation, conditioning a network on both product images and titles to generate descriptive text."], "score": 0.95361328125}, {"id": "(Aiello et al., 2023)", "paper": {"corpus_id": 262898422, "title": "Jointly Training Large Autoregressive Multimodal Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Emanuele Aiello", "authorId": "2185505465"}, {"name": "L. Yu", "authorId": "49297123"}, {"name": "Yixin Nie", "authorId": "2247227174"}, {"name": "Armen Aghajanyan", "authorId": "2201435"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}], "n_citations": 31}, "snippets": ["To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose."], "score": 0.95068359375}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 271924334, "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Jinheng Xie", "authorId": "2306603347"}, {"name": "Weijia Mao", "authorId": "2258958963"}, {"name": "Zechen Bai", "authorId": "2237427303"}, {"name": "David Junhao Zhang", "authorId": "2248388634"}, {"name": "Weihao Wang", "authorId": "2316668014"}, {"name": "Kevin Qinghong Lin", "authorId": "2298569956"}, {"name": "Yuchao Gu", "authorId": "2248678348"}, {"name": "Zhijie Chen", "authorId": "2316662510"}, {"name": "Zhenheng Yang", "authorId": "2309246065"}, {"name": "Mike Zheng Shou", "authorId": "2244623976"}], "n_citations": 229}, "snippets": ["In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;(Tang et al., 2023)Ye et al., 2024a;(Dong et al., 2023)Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images."], "score": 0.962890625}, {"id": "(Xie et al._1, 2024)", "paper": {"corpus_id": 274305717, "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Rongchang Xie", "authorId": "2332574955"}, {"name": "Chen Du", "authorId": "2332709350"}, {"name": "Ping Song", "authorId": "2332538297"}, {"name": "Chang Liu", "authorId": "2333419477"}], "n_citations": 10}, "snippets": ["Other works like Chameleon [52], Show-o [62] and Emu3 [59] have tried to directly adopt the VQ tokenizer to encode images for both multimodal understanding and generation. However, since these visual tokenizers do not contain semantic information, aligning visual tokens with language tokens becomes difficult, and these models usually yield suboptimal performance in multimodal understanding tasks."], "score": 0.923828125}, {"id": "(Tang et al., 2023)", "paper": {"corpus_id": 258822817, "title": "Any-to-Any Generation via Composable Diffusion", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zineng Tang", "authorId": "151270642"}, {"name": "Ziyi Yang", "authorId": "2155459391"}, {"name": "Chenguang Zhu", "authorId": "8652308"}, {"name": "Michael Zeng", "authorId": "48262024"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 188}, "snippets": ["We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io"], "score": 0.0}, {"id": "(Dong et al., 2023)", "paper": {"corpus_id": 261975252, "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Runpei Dong", "authorId": "2056965063"}, {"name": "Chunrui Han", "authorId": "2118643247"}, {"name": "Yuang Peng", "authorId": "2211415443"}, {"name": "Zekun Qi", "authorId": "3424017"}, {"name": "Zheng Ge", "authorId": "2242581956"}, {"name": "Jinrong Yang", "authorId": "2161319190"}, {"name": "Liang Zhao", "authorId": "48096671"}, {"name": "Jian\u2010Yuan Sun", "authorId": "26913717"}, {"name": "Hongyu Zhou", "authorId": "2157473950"}, {"name": "Hao-Ran Wei", "authorId": "134085586"}, {"name": "Xiangwen Kong", "authorId": "2069029697"}, {"name": "Xiangyu Zhang", "authorId": "2185865433"}, {"name": "Kaisheng Ma", "authorId": "2244131405"}, {"name": "Li Yi", "authorId": "2242612318"}], "n_citations": 201}, "snippets": ["This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."], "score": 0.0}, {"id": "(Li et al._1, 2023)", "paper": {"corpus_id": 265498814, "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Bohao Li", "authorId": "1491798621"}, {"name": "Yuying Ge", "authorId": "51123495"}, {"name": "Yixiao Ge", "authorId": "152988335"}, {"name": "Guangzhi Wang", "authorId": "2243452799"}, {"name": "Rui Wang", "authorId": "2151036422"}, {"name": "Ruimao Zhang", "authorId": "2268726753"}, {"name": "Ying Shan", "authorId": "2265579883"}], "n_citations": 69}, "snippets": ["Recent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer."], "score": 0.95654296875}], "table": null}, {"title": "Applications and Examples of Multimodal Outputs", "tldr": "Multimodal language models with combined text and image output capabilities enable diverse real-world applications spanning creative content generation, visual editing, education, and healthcare. These systems demonstrate practical utility through text-guided image generation, interactive visual content creation, multimodal document production, and specialized domain applications. (8 sources)", "text": "\nMultimodal Large Language Models (MLLMs) with output capabilities across both text and visual modalities have enabled numerous practical applications:\n\n## Content Creation and Editing\n- **Text-to-Image Generation**: Systems built on diffusion models can create highly detailed images from textual descriptions, allowing users to generate artwork, illustrations, and visual content without specialized skills <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper> <Paper corpusId=\"219955663\" paperTitle=\"(Ho et al., 2020)\" isShortName></Paper>.\n- **Image Editing Through Language**: Advanced MLLMs enable users to manipulate existing images through natural language instructions rather than pixel-level editing, such as changing attributes or inserting new elements <Paper corpusId=\"253581213\" paperTitle=\"(Brooks et al., 2022)\" isShortName></Paper>.\n- **Subject-Driven Generation**: State-of-the-art MLLMs can perform targeted visual creation where specific subjects or themes are maintained across multiple generated images <Paper corpusId=\"266374640\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.\n\n## Multimodal Communication\n- **Visual Storytelling**: Models can generate coherent narratives combining text and supporting imagery, enhancing engagement and comprehension <Paper corpusId=\"271218251\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n- **Interactive Assistants**: MLLMs power conversational agents that can both explain concepts textually and visualize them on demand <Paper corpusId=\"276929408\" paperTitle=\"(Fang et al., 2025)\" isShortName></Paper>.\n- **Document Creation**: Systems can produce multimodal documents including reports, presentations, and educational materials with integrated text and visual elements <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\n## Specialized Domain Applications\n- **Educational Tools**: MLLMs create interactive learning materials that combine explanatory text with supporting visualizations, enhancing understanding of complex concepts <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n- **Healthcare Visualization**: Medical applications where textual diagnoses are paired with visual representations of conditions or anatomical structures <Paper corpusId=\"270094821\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>.\n- **Design Assistance**: Architecture and design applications where textual specifications can be transformed into visual mock-ups and iteratively refined through language <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n- **Accessibility Solutions**: Systems that can transform visual information into descriptive text and vice versa, improving information access for users with different needs <Paper corpusId=\"271218251\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\n## Emerging Applications\n- **Visual Reasoning**: MLLMs demonstrate capabilities to solve problems requiring reasoning about visual information and expressing the solution process in text with supporting visuals <Paper corpusId=\"247951931\" paperTitle=\"(Chowdhery et al., 2022)\" isShortName></Paper>.\n- **Multimodal Brainstorming**: Creative applications where text prompts generate diverse visual variations to inspire designers and creators <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n- **Visual Prompting**: Advanced models can understand visual inputs and generate appropriate responses with both textual and visual components <Paper corpusId=\"266374640\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Rombach et al., 2021)", "paper": {"corpus_id": 245335280, "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Robin Rombach", "authorId": "1660819540"}, {"name": "A. Blattmann", "authorId": "119843260"}, {"name": "Dominik Lorenz", "authorId": "2053482699"}, {"name": "Patrick Esser", "authorId": "35175531"}, {"name": "B. Ommer", "authorId": "1796707"}], "n_citations": 15768}, "snippets": ["By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."], "score": 0.0}, {"id": "(Ho et al., 2020)", "paper": {"corpus_id": 219955663, "title": "Denoising Diffusion Probabilistic Models", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jonathan Ho", "authorId": "2126278"}, {"name": "Ajay Jain", "authorId": "1623995772"}, {"name": "P. Abbeel", "authorId": "1689992"}], "n_citations": 18352}, "snippets": ["We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL"], "score": 0.0}, {"id": "(Brooks et al., 2022)", "paper": {"corpus_id": 253581213, "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Tim Brooks", "authorId": "2679394"}, {"name": "Aleksander Holynski", "authorId": "2248172435"}, {"name": "Alexei A. Efros", "authorId": "1763086"}], "n_citations": 1833}, "snippets": ["We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions."], "score": 0.0}, {"id": "(Sun et al., 2023)", "paper": {"corpus_id": 266374640, "title": "Generative Multimodal Models are In-Context Learners", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Quan Sun", "authorId": "2264040642"}, {"name": "Yufeng Cui", "authorId": "2263702215"}, {"name": "Xiaosong Zhang", "authorId": "2264589602"}, {"name": "Fan Zhang", "authorId": "2264274388"}, {"name": "Qiying Yu", "authorId": "23716915"}, {"name": "Zhengxiong Luo", "authorId": "2275808712"}, {"name": "Yueze Wang", "authorId": "2217456303"}, {"name": "Yongming Rao", "authorId": "2275170369"}, {"name": "Jingjing Liu", "authorId": "2222717281"}, {"name": "Tiejun Huang", "authorId": "2257137554"}, {"name": "Xinlong Wang", "authorId": "2263959521"}], "n_citations": 290}, "snippets": ["The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research."], "score": 0.0}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 271218251, "title": "Visualization Literacy of Multimodal Large Language Models: A Comparative Study", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhimin Li", "authorId": "2116785555"}, {"name": "Haichao Miao", "authorId": "2253579462"}, {"name": "Valerio Pascucci", "authorId": "2251997858"}, {"name": "Shusen Liu", "authorId": "2289268771"}], "n_citations": 6}, "snippets": ["Since humans interact with their environment through multi-modal sensory input, the evolution from text-only LLM systems to multimodal LLMs significantly increases the potential usage cases for LLMs, prompting many applications [35]", "models designed for input in one modality and output in another, such as text-to-image (Rombach et al., 2021)[27] and image-to-text [37] models."], "score": 0.9140625}, {"id": "(Fang et al., 2025)", "paper": {"corpus_id": 276929408, "title": "Recognition-Synergistic Scene Text Editing", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhengyao Fang", "authorId": "2313679002"}, {"name": "Pengyuan Lyu", "authorId": "10344582"}, {"name": "Jingjing Wu", "authorId": "2313920076"}, {"name": "Chengquan Zhang", "authorId": "2248958848"}, {"name": "Jun Yu", "authorId": "2349736964"}, {"name": "Guan-Lun Lu", "authorId": "2261251059"}, {"name": "Wenjie Pei", "authorId": "1678473"}], "n_citations": 1}, "snippets": ["In response to the notable progress of large language models in natural language processing [1,2,(Chowdhery et al., 2022), the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models (Ho et al., 2020), but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,(Sun et al., 2023)[40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs (Brooks et al., 2022)13,(Mokady et al., 2023) offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation."], "score": 0.93994140625}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 270094821, "title": "Multi-Modal Generative Embedding Model", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Feipeng Ma", "authorId": "2218044713"}, {"name": "Hongwei Xue", "authorId": "1878753365"}, {"name": "Guangting Wang", "authorId": "2303832848"}, {"name": "Yizhou Zhou", "authorId": "2141747950"}, {"name": "Fengyun Rao", "authorId": "2276428924"}, {"name": "Shilin Yan", "authorId": "2303910455"}, {"name": "Yueyi Zhang", "authorId": "2240587447"}, {"name": "Siying Wu", "authorId": "2303712729"}, {"name": "Mike Zheng Shou", "authorId": "2303652188"}, {"name": "Xiaoyan Sun", "authorId": "2303715824"}], "n_citations": 4}, "snippets": ["Most multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding."], "score": 0.91943359375}, {"id": "(Chowdhery et al., 2022)", "paper": {"corpus_id": 247951931, "title": "PaLM: Scaling Language Modeling with Pathways", "year": 2022, "venue": "Journal of machine learning research", "authors": [{"name": "Aakanksha Chowdhery", "authorId": "2841893"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Gaurav Mishra", "authorId": "2159632445"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "P. Barham", "authorId": "152399055"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Charles Sutton", "authorId": "152549864"}, {"name": "Sebastian Gehrmann", "authorId": "3159346"}, {"name": "Parker Schuh", "authorId": "2620528"}, {"name": "Kensen Shi", "authorId": "2362367"}, {"name": "Sasha Tsvyashchenko", "authorId": "2160888237"}, {"name": "Joshua Maynez", "authorId": "2124977868"}, {"name": "Abhishek Rao", "authorId": "1484043592"}, {"name": "Parker Barnes", "authorId": "80940648"}, {"name": "Yi Tay", "authorId": "144447820"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Vinodkumar Prabhakaran", "authorId": "3331141"}, {"name": "Emily Reif", "authorId": "49849144"}, {"name": "Nan Du", "authorId": "2140321952"}, {"name": "Ben Hutchinson", "authorId": "2044655623"}, {"name": "Reiner Pope", "authorId": "2161431901"}, {"name": "James Bradbury", "authorId": "2065251344"}, {"name": "Jacob Austin", "authorId": "2058365883"}, {"name": "M. Isard", "authorId": "2090818"}, {"name": "Guy Gur-Ari", "authorId": "2284681044"}, {"name": "Pengcheng Yin", "authorId": "38253388"}, {"name": "Toju Duke", "authorId": "2145151992"}, {"name": "Anselm Levskaya", "authorId": "6639036"}, {"name": "S. Ghemawat", "authorId": "1780892"}, {"name": "Sunipa Dev", "authorId": "50991767"}, {"name": "H. Michalewski", "authorId": "47407464"}, {"name": "Xavier Garc\u00eda", "authorId": "143936294"}, {"name": "Vedant Misra", "authorId": "40055795"}, {"name": "Kevin Robinson", "authorId": "2148473059"}, {"name": "L. Fedus", "authorId": "2096916416"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "Daphne Ippolito", "authorId": "7975935"}, {"name": "D. Luan", "authorId": "150970919"}, {"name": "Hyeontaek Lim", "authorId": "8939217"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "A. Spiridonov", "authorId": "1572884723"}, {"name": "Ryan Sepassi", "authorId": "35474601"}, {"name": "David Dohan", "authorId": "35363891"}, {"name": "Shivani Agrawal", "authorId": "3504647"}, {"name": "Mark Omernick", "authorId": "3175815"}, {"name": "Andrew M. Dai", "authorId": "2555924"}, {"name": "Thanumalayan Sankaranarayana Pillai", "authorId": "2598683"}, {"name": "Marie Pellat", "authorId": "97905921"}, {"name": "Aitor Lewkowycz", "authorId": "102549875"}, {"name": "Erica Moreira", "authorId": "2057453483"}, {"name": "R. Child", "authorId": "48422824"}, {"name": "Oleksandr Polozov", "authorId": "2636739"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Zongwei Zhou", "authorId": "2198519"}, {"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Brennan Saeta", "authorId": "4125424"}, {"name": "Mark D\u00edaz", "authorId": "2152965375"}, {"name": "Orhan Firat", "authorId": "2345617"}, {"name": "Michele Catasta", "authorId": "1754926"}, {"name": "Jason Wei", "authorId": "119640649"}, {"name": "K. Meier-Hellstern", "authorId": "1398655031"}, {"name": "D. Eck", "authorId": "2396681"}, {"name": "J. Dean", "authorId": "48448318"}, {"name": "Slav Petrov", "authorId": "1754497"}, {"name": "Noah Fiedel", "authorId": "22640071"}], "n_citations": 6293}, "snippets": ["Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."], "score": 0.0}], "table": null}], "cost": 0.333096}}
