{"original_query": "Pruning is a means of model compression that can improve model efficiency in NLP. What are some recent general, task-agnostic pruning methods for large language methods that have worked well?", "input": "Pruning means model compression improve model efficiency in NLP What recent general task agnostic pruning methods for large language methods that", "output": {"sections": [{"title": "Introduction to Pruning for LLM Efficiency", "tldr": "Pruning is a model compression technique that removes unnecessary weights or components from neural networks to improve efficiency without significantly degrading performance. This approach has become increasingly important for making large language models more deployable on resource-constrained devices. (13 sources)", "text": "\nPruning is a well-established compression technique that identifies and eliminates redundant or less important parameters in neural networks, significantly reducing model size and computational requirements while maintaining performance <Paper corpusId=\"278033481\" paperTitle=\"(Garg et al., 2025)\" isShortName></Paper> <Paper corpusId=\"2238772\" paperTitle=\"(Han et al., 2015)\" isShortName></Paper>. As large language models (LLMs) continue to grow in size and complexity, they demand substantial computational resources for both training and inference, making them challenging to deploy on resource-constrained devices. Pruning addresses this challenge by removing unnecessary components, thereby improving efficiency in terms of storage capacity, memory usage, and inference speed <Paper corpusId=\"275342899\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"264146875\" paperTitle=\"(Ko et al., 2023)\" isShortName></Paper>.\n\nThere are two primary categories of pruning approaches for language models. Unstructured pruning targets individual weights by applying a mask or setting their values to zero based on certain criteria, while structured pruning removes entire blocks of weights or components such as attention heads or encoder layers <Paper corpusId=\"247741658\" paperTitle=\"(Movva et al., 2022)\" isShortName></Paper> <Paper corpusId=\"258833347\" paperTitle=\"(Tan, 2023)\" isShortName></Paper> <Paper corpusId=\"162183964\" paperTitle=\"(Voita et al., 2019)\" isShortName></Paper> <Paper corpusId=\"202750230\" paperTitle=\"(Fan et al., 2019)\" isShortName></Paper>. Recent research has shown that models can be pruned during pre-training with minimal performance loss, making it possible to remove 70% or more of model weights in some cases <Paper corpusId=\"257901132\" paperTitle=\"(Campos et al., 2023)\" isShortName></Paper>. When implemented with sparsity-aware inference engines, pruned models can achieve 3-5x speedups in inference throughput <Paper corpusId=\"257901132\" paperTitle=\"(Campos et al., 2023)\" isShortName></Paper>.\n\nPruning works on the principle that neural networks contain substantial redundancy, with many weights that can be eliminated without significantly affecting the model's capabilities <Paper corpusId=\"263835309\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>. This approach differs from other compression techniques such as quantization (which reduces bit precision) and knowledge distillation (which transfers knowledge from larger to smaller models), though these methods can be used in combination for greater efficiency gains <Paper corpusId=\"267897588\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nThe growing interest in pruning for LLMs stems from the critical need to deploy these powerful models in scenarios with limited computational resources, memory, and storage <Paper corpusId=\"259263947\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"231740691\" paperTitle=\"(Hoefler et al., 2021)\" isShortName></Paper>. As LLMs continue to grow in size and capabilities, pruning offers a promising path to make them more accessible and practical for a wider range of applications.", "citations": [{"id": "(Garg et al., 2025)", "paper": {"corpus_id": 278033481, "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Muskan Garg", "authorId": "2258141722"}, {"name": "Shaina Raza", "authorId": "2278330619"}, {"name": "Shebuti Rayana", "authorId": "3023076"}, {"name": "Xingyi Liu", "authorId": "2278394763"}, {"name": "Sunghwan Sohn", "authorId": "2267490593"}], "n_citations": 2}, "snippets": ["Pruning is a long-established technique for model compression that reduces the size of a model by removing unnecessary or less important parameters (Han et al., 2015)(LeCun et al., 1989). In language models, many weights can be zeroed out or eliminated with little to no effect on overall performance (Bergsma et al., 2025). By pruning these redundant parameters, we can significantly shrink model size and accelerate inference, with only a minor drop in accuracy if done carefully. Pruning thus makes models more storage-friendly, memoryefficient, and computation-efficient. Based on the granularity of removal, pruning is categorized as unstructured pruning and structured pruning", "Recent research has focused on one-shot pruning methods and improved criteria to minimize the need for costly retraining [211], validating its performance for healthcare tasks."], "score": 0.966796875}, {"id": "(Han et al., 2015)", "paper": {"corpus_id": 2238772, "title": "Learning both Weights and Connections for Efficient Neural Network", "year": 2015, "venue": "Neural Information Processing Systems", "authors": [{"name": "Song Han", "authorId": "143840275"}, {"name": "Jeff Pool", "authorId": "47325862"}, {"name": "J. Tran", "authorId": "2066786849"}, {"name": "W. Dally", "authorId": "80724002"}], "n_citations": 6708}, "snippets": ["Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy."], "score": 0.0}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 275342899, "title": "Optimizing Edge AI: A Comprehensive Survey on Data, Model, and System Strategies", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xubin Wang", "authorId": "2321412685"}, {"name": "Weijia Jia", "authorId": "2321432219"}], "n_citations": 2}, "snippets": ["Model compression is a group of methods (such as pruning, parameter sharing, quantization, knowledge distillation and low-rank factorization) for shrinking the size of deep learning models without significantly affecting their accuracy or performance by eliminating extraneous components, such as duplicated parameters or layers. Due to deep learning models' high computational and storage needs, model compression has become more and more crucial. These methods are created to make it possible to deploy complicated models on devices with limited resources or in large-scale distributed systems with constrained processing, memory, and storage. To enhance the efficacy and efficiency of deep learning models in various applications, it is possible to employ these techniques either in isolation or in conjunction."], "score": 0.93212890625}, {"id": "(Ko et al., 2023)", "paper": {"corpus_id": 264146875, "title": "NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jongwoo Ko", "authorId": "2051385328"}, {"name": "Seungjoon Park", "authorId": "1424318100"}, {"name": "Yujin Kim", "authorId": "2258986738"}, {"name": "Sumyeong Ahn", "authorId": "40917250"}, {"name": "Du-Seong Chang", "authorId": "2258714847"}, {"name": "Euijai Ahn", "authorId": "2258721819"}, {"name": "SeYoung Yun", "authorId": "2256998999"}], "n_citations": 6}, "snippets": ["In recent years, pre-trained language models (LMs) have demonstrated their effectiveness in various downstream tasks, such as natural language understanding (NLU) and natural language generation (NLG). Especially, there have been three main types of research, e.g.,encoder-only LMs (Devlin et al., 2019;He et al., 2023), decoder-only LMs (Touvron et al., 2023;OpenAI, 2023), and encoder-decoder LMs (Lewis et al., 2020;Raffel et al., 2020;Chung et al., 2022b;Tay et al., 2023), which aim for their specific expertise. On the other perspective of LM researches rather than performances, efficiency of LMs (e.g.,computational and memory cost) have been intensively studied because of their huge computational requirements. This research direction is called model compression. Among the various model compression techniques (Jiao et al., 2020;Yao et al., 2022), pruning (Frankle and Carbin, 2018;Sanh et al., 2020;Wang et al., 2020c;Xia et al., 2022) is a promising method that aims to remove redundant weights from networks, resulting in improved efficiency by saving storage capacity and enhancing inference speed."], "score": 0.96142578125}, {"id": "(Movva et al., 2022)", "paper": {"corpus_id": 247741658, "title": "Combining Compressions for Multiplicative Size Scaling on Natural Language Tasks", "year": 2022, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Rajiv Movva", "authorId": "1405369173"}, {"name": "Jinhao Lei", "authorId": "33019343"}, {"name": "S. Longpre", "authorId": "29909347"}, {"name": "Ajay Gupta", "authorId": "2124739758"}, {"name": "Chris DuBois", "authorId": "2126499571"}], "n_citations": 5}, "snippets": ["Pruning identifies weights which can be omitted at test time without significantly degrading performance. Some pruning methods remove individual weights according to magnitudes or other heuristics (Gordon et al., 2020; Chen et al., 2020; Sanh et al., 2020), while others remove structured blocks of weights or entire attention heads (Wang et al., 2020; Hou et al., 2020; Voita et al., 2019; Michel et al., 2019)."], "score": 0.9404296875}, {"id": "(Tan, 2023)", "paper": {"corpus_id": 258833347, "title": "Infor-Coef: Information Bottleneck-based Dynamic Token Downsampling for Compact and Efficient language model", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Wenxin Tan", "authorId": "2070761774"}], "n_citations": 1}, "snippets": ["Pruning has emerged as a promising approach to compress and accelerate DNN models, significantly reducing storage and computational costs. Structured pruning method delivers a static compact model by removing structured blocks of weights, e.g. heads (Voita et al., 2019), Michel et al., 2019) and encoder layers (Fan et al., 2019). However, removing a large proportion of parameters may result in noticeable accuracy loss. To address this, the distillation paradigm is commonly adopted for recovery training, where the pruned model learns the knowledge delivered from the unpruned model. (Sanh et al., 2020) While these pruning methods achieve compelling results, they are static and have a fixed computation route for all inputs, regardless of the differing information redundancy of various sequences."], "score": 0.95849609375}, {"id": "(Voita et al., 2019)", "paper": {"corpus_id": 162183964, "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Elena Voita", "authorId": "46235299"}, {"name": "David Talbot", "authorId": "144251066"}, {"name": "F. Moiseev", "authorId": "2157158"}, {"name": "Rico Sennrich", "authorId": "2082372"}, {"name": "Ivan Titov", "authorId": "144889265"}], "n_citations": 1147}, "snippets": ["Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU."], "score": 0.0}, {"id": "(Fan et al., 2019)", "paper": {"corpus_id": 202750230, "title": "Reducing Transformer Depth on Demand with Structured Dropout", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Angela Fan", "authorId": "144270981"}, {"name": "Edouard Grave", "authorId": "3024698"}, {"name": "Armand Joulin", "authorId": "2319608"}], "n_citations": 596}, "snippets": ["Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation."], "score": 0.0}, {"id": "(Campos et al., 2023)", "paper": {"corpus_id": 257901132, "title": "oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes", "year": 2023, "venue": "SUSTAINLP", "authors": [{"name": "Daniel Fernando Campos", "authorId": "144081089"}, {"name": "Alexandre Marques", "authorId": "2166312585"}, {"name": "Mark Kurtz", "authorId": "2070446213"}, {"name": "Chengxiang Zhai", "authorId": "143869012"}], "n_citations": 2}, "snippets": ["Unstructured Pruning is a compression approach that removes individual weights or groups of weights in a model by applying a mask or setting the weight values to 0. This compression approach has been broadly studied in computer vision (Han et al., 2015), and many methods can remove 70% or more of model weights with little to no loss in accuracy. Models pruned can be 20x smaller in terms of pure model size and, when paired with a sparsity-aware inference engine such as DeepSparse (Magic, 2023), provide 3-5x speedups in inference throughput. Focused on language models, recent work has shown that it is possible to prune models during pre-training with little to no loss in accuracy (Sanh et al., 2020) (Kurti\u0107 et al., 2022"], "score": 0.978515625}, {"id": "(Yang et al., 2023)", "paper": {"corpus_id": 263835309, "title": "SparseCoder: Advancing Source Code Analysis with Sparse Attention and Learned Token Pruning", "year": 2023, "venue": "Empirical Software Engineering", "authors": [{"name": "Xueqi Yang", "authorId": "47008250"}, {"name": "Mariusz Jakubowski", "authorId": "2257039528"}, {"name": "Kelly Kang", "authorId": "2258109380"}, {"name": "Haojie Yu", "authorId": "2257209428"}, {"name": "Tim Menzies", "authorId": "2279833589"}], "n_citations": 2}, "snippets": ["Pruning proposed by Lecun et al. (LeCun et al., 1989) is one of the popular approaches in the compression of Transformer models. Generally, by getting rid of trial or unimportant weights in neural networks, pruning can reduce inference time and memory requirement with limited performance loss by avoiding unnecessary computation with limited performance loss [47]."], "score": 0.9462890625}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 267897588, "title": "CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jiayi Liu", "authorId": "2286338976"}, {"name": "Tinghan Yang", "authorId": "2286427471"}, {"name": "Jennifer Neville", "authorId": "2286321906"}], "n_citations": 11}, "snippets": ["Within the realm of model compression, three prominent techniques have garnered significant attention: quantization, distillation, and pruning. Quantization is a widely employed approach in the compression of post-training large language models [9,40,42]. By reducing the bit precision, quantization methods necessitate lower memory utilization, thus facilitating faster computations and, consequently, higher efficiency. Distillation methods distill the knowledge from larger models and embed it into smaller models to approach model compression [5,15,17]. Another noteworthy method for model compression is pruning [21]26]. Pruning achieves reduced complexity by eliminating relatively less impactful model components, effectively reducing the model's size."], "score": 0.96533203125}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 259263947, "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zhenyu (Allen) Zhang", "authorId": "2109338656"}, {"name": "Ying Sheng", "authorId": "2209360681"}, {"name": "Tianyi Zhou", "authorId": "2190694474"}, {"name": "Tianlong Chen", "authorId": "2034263179"}, {"name": "Lianmin Zheng", "authorId": "2149970173"}, {"name": "Ruisi Cai", "authorId": "2209882676"}, {"name": "Zhao Song", "authorId": "2214956470"}, {"name": "Yuandong Tian", "authorId": "1932187449"}, {"name": "Christopher R\u00e9", "authorId": "1803218"}, {"name": "Clark W. Barrett", "authorId": "2052981589"}, {"name": "Zhangyang Wang", "authorId": "2108404505"}, {"name": "Beidi Chen", "authorId": "4319427"}], "n_citations": 313}, "snippets": ["(2) pruning or sparsity [59,60](He et al., 2018)(Hoefler et al., 2021), which aims to eliminate unnecessary neurons or weights within the models"], "score": 0.9423828125}, {"id": "(Hoefler et al., 2021)", "paper": {"corpus_id": 231740691, "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks", "year": 2021, "venue": "Journal of machine learning research", "authors": [{"name": "T. Hoefler", "authorId": "1713648"}, {"name": "Dan Alistarh", "authorId": "3311387"}, {"name": "Tal Ben-Nun", "authorId": "1402921119"}, {"name": "Nikoli Dryden", "authorId": "2134146"}, {"name": "Alexandra Peste", "authorId": "3341722"}], "n_citations": 725}, "snippets": ["The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field."], "score": 0.0}], "table": null}, {"title": "Categories of Pruning Methods for Language Models", "tldr": "Pruning methods for language models fall into two main categories: unstructured pruning, which targets individual weights, and structured pruning, which removes entire model components. Each approach offers different trade-offs between compression ratio, performance preservation, and hardware compatibility. (13 sources)", "text": "\nPruning techniques for language models can be broadly categorized into two main approaches: unstructured and structured pruning. Unstructured pruning targets individual weights by zeroing them out or applying masks based on certain criteria, such as magnitude or other heuristics <Paper corpusId=\"247741658\" paperTitle=\"(Movva et al., 2022)\" isShortName></Paper> <Paper corpusId=\"259287257\" paperTitle=\"(Shen et al., 2023)\" isShortName></Paper>. While this approach can achieve high sparsity without significant accuracy degradation, it often requires specialized hardware support to realize actual speed benefits due to the irregular sparse patterns it creates <Paper corpusId=\"259251699\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271909421\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nIn contrast, structured pruning removes entire coherent blocks of weights or model components, such as attention heads, encoder layers, or neurons <Paper corpusId=\"247741658\" paperTitle=\"(Movva et al., 2022)\" isShortName></Paper> <Paper corpusId=\"258833347\" paperTitle=\"(Tan, 2023)\" isShortName></Paper>. This approach is more hardware-friendly as it directly reduces model dimensions and computational requirements without requiring specialized sparse computation support <Paper corpusId=\"259251699\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. Structured pruning can target various levels of model architecture:\n\n1. **Head-level pruning**: Removes entire attention heads that are deemed less important <Paper corpusId=\"162183964\" paperTitle=\"(Voita et al., 2019)\" isShortName></Paper> <Paper corpusId=\"258833347\" paperTitle=\"(Tan, 2023)\" isShortName></Paper>.\n\n2. **Layer-level pruning**: Eliminates complete encoder or decoder layers, significantly reducing model depth <Paper corpusId=\"202750230\" paperTitle=\"(Fan et al., 2019)\" isShortName></Paper> <Paper corpusId=\"258833347\" paperTitle=\"(Tan, 2023)\" isShortName></Paper>.\n\n3. **Block-level pruning**: Removes blocks of weight matrices rather than individual weights <Paper corpusId=\"259287257\" paperTitle=\"(Shen et al., 2023)\" isShortName></Paper>.\n\n4. **Neuron-level pruning**: Prunes entire neurons or hidden units <Paper corpusId=\"264146174\" paperTitle=\"(Shao et al., 2023)\" isShortName></Paper>.\n\n5. **Width-reduction pruning**: Reduces model width by pruning coupled structures across dimensions <Paper corpusId=\"277275922\" paperTitle=\"(Belhaouari et al., 2025)\" isShortName></Paper>.\n\nRecent approaches have also introduced semi-structured pruning patterns, such as 2:4 or 4:8 sparsity, which strike a balance between the flexibility of unstructured pruning and the hardware efficiency of structured pruning <Paper corpusId=\"259287257\" paperTitle=\"(Shen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"276482745\" paperTitle=\"(Qin et al., 2025)\" isShortName></Paper>. These patterns can be accelerated by emerging hardware technologies that support specific sparse formats <Paper corpusId=\"271909421\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nTo recover performance after pruning, various techniques are employed. The distillation paradigm is commonly used, where the pruned model learns knowledge from the unpruned model <Paper corpusId=\"258833347\" paperTitle=\"(Tan, 2023)\" isShortName></Paper> <Paper corpusId=\"218665313\" paperTitle=\"(Sanh et al., 2020)\" isShortName></Paper>. Some methods also incorporate layerwise distillation strategies to transfer knowledge between the original and pruned models <Paper corpusId=\"247922354\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper>.\n\nFinally, recent research has explored more comprehensive approaches that combine different levels of pruning. For example, CoFi jointly prunes coarse-grained (layers) and fine-grained (heads and hidden units) modules to achieve greater speedups while maintaining performance <Paper corpusId=\"247922354\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper>. Similarly, Sheared-LLaMA reduces both network width and depth by removing entire layers for more efficient models <Paper corpusId=\"263830786\" paperTitle=\"(Xia et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277275922\" paperTitle=\"(Belhaouari et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Movva et al., 2022)", "paper": {"corpus_id": 247741658, "title": "Combining Compressions for Multiplicative Size Scaling on Natural Language Tasks", "year": 2022, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Rajiv Movva", "authorId": "1405369173"}, {"name": "Jinhao Lei", "authorId": "33019343"}, {"name": "S. Longpre", "authorId": "29909347"}, {"name": "Ajay Gupta", "authorId": "2124739758"}, {"name": "Chris DuBois", "authorId": "2126499571"}], "n_citations": 5}, "snippets": ["Pruning identifies weights which can be omitted at test time without significantly degrading performance. Some pruning methods remove individual weights according to magnitudes or other heuristics (Gordon et al., 2020; Chen et al., 2020; Sanh et al., 2020), while others remove structured blocks of weights or entire attention heads (Wang et al., 2020; Hou et al., 2020; Voita et al., 2019; Michel et al., 2019)."], "score": 0.9404296875}, {"id": "(Shen et al., 2023)", "paper": {"corpus_id": 259287257, "title": "An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Haihao Shen", "authorId": "1921920"}, {"name": "Hengyu Meng", "authorId": "2190820495"}, {"name": "Bo Dong", "authorId": "2057588093"}, {"name": "Zhe Wang", "authorId": "2108195736"}, {"name": "Ofir Zafrir", "authorId": "1387202086"}, {"name": "Yi Ding", "authorId": "2111239073"}, {"name": "Yu Luo", "authorId": "2118198689"}, {"name": "Hanwen Chang", "authorId": "2190931047"}, {"name": "Qun Gao", "authorId": "2220820474"}, {"name": "Zi. Wang", "authorId": "2145041576"}, {"name": "Guy Boudoukh", "authorId": "3150063"}, {"name": "Moshe Wasserblat", "authorId": "2134755"}], "n_citations": 4}, "snippets": ["Pruning has been proven to be an effective way of reducing model size while maintaining the similar model quality (Le-Cun et al., 1989) (Sanh et al., 2020) (Wang, 2021). Structured pruning is gaining popularity to prune the weights with a pre-defined sparsity pattern such as block-wise pruning (Lagunas et al., 2021) and fine-grained 2:4 (Pool et al., 2021) or N:M structured sparsity (Zhou et al., 2021). Recent works (Zafrir et al., 2021;Kurtic et al., 2022) proposed pruning Transformer models at pre-training to create sparse pre-trained LMs and fine-tuning on downstream tasks."], "score": 0.947265625}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 259251699, "title": "Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference", "year": 2023, "venue": "Knowledge Discovery and Data Mining", "authors": [{"name": "Junyan Li", "authorId": "2214284233"}, {"name": "L. Zhang", "authorId": "48571328"}, {"name": "Jiahang Xu", "authorId": "2257094139"}, {"name": "Yujing Wang", "authorId": "46394401"}, {"name": "Shaoguang Yan", "authorId": "2181972735"}, {"name": "Yunqing Xia", "authorId": "33420715"}, {"name": "Yuqing Yang", "authorId": "2108623481"}, {"name": "Ting Cao", "authorId": "2069445596"}, {"name": "Hao Sun", "authorId": "2118180377"}, {"name": "Weiwei Deng", "authorId": "2066621592"}, {"name": "Qi Zhang", "authorId": "2145908588"}, {"name": "Mao Yang", "authorId": "2168609907"}], "n_citations": 10}, "snippets": ["To reduce the inference cost of pre-trained transformer models, a variety of compression techniques have been proposed, including weight pruning [8,(Lagunas et al., 2021)[30], quantization [4,17,(Shen et al., 2019) and distillation [15,29]. Token-level pruning has been shown to complement knowledge distillation and quantization (Kim et al., 2021)", "Weight pruning is categorized into 1) unstructured and 2) structured pruning. Unstructured methods [8]30] achieve high sparsity without accuracy drop but offer minimal latency benefits due to irregular sparse patterns. In contrast, structured pruning removes coherent weight groups, reducing latency without special hardware support. CoFi (Xia et al., 2022) achieves 10\u00d7 speedup with a small accuracy drop by jointly pruning layers, attention heads, FFN, and hidden units. SwiftPruner (Zhang et al., 2022) is a latency-aware pruning method that finds optimal layer-wise pruning policies under a given latency requirement through AutoML."], "score": 0.958984375}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 271909421, "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Guanchen Li", "authorId": "2301331844"}, {"name": "Xiandong Zhao", "authorId": "2270847262"}, {"name": "Lian Liu", "authorId": "2316517251"}, {"name": "Zeping Li", "authorId": "2307589652"}, {"name": "Dong Li", "authorId": "2279335698"}, {"name": "Lu Tian", "authorId": "2279539118"}, {"name": "Jie He", "authorId": "2316522396"}, {"name": "Ashish Sirasao", "authorId": "2316484957"}, {"name": "E. Barsoum", "authorId": "2271751612"}], "n_citations": 1}, "snippets": ["Pruning for Language Model Compression. The surging complexity of Transformer-based language models, which now feature hundreds of billions of parameters, has accentuated the urgent need for effective and efficient model pruning methods (Han et al., 2015)(Han et al., 2015)(Hassibi et al., 1993). These pruning methods can be broadly classified into structured and unstructured approaches. Structured pruning is more hardware-friendly, as it directly prunes entire segments of weights, thereby removing consecutive computations [17,(Liu et al., 2023). Unstructured pruning (sparsification) is also receiving interest, particularly as hardware advancements increasingly support the acceleration of sparse patterns such as 2:4 or 4:8 sparse [20]. Techniques such as SparseGPT (Frantar et al., 2023) extend the OBS (Hassibi et al., 1993) methodology to column-wise prune weights, allowing the modification of values in the unpruned columns to compensate for the pruning errors. Aaquib et al. (Syed et al., 2023) enhance SparseGPT by incorporating minimal iterative task fine-tuning during the pruning process, demonstrating performance improvements at high sparsity levels. Wanda [27] introduces a simple yet effective no-retraining-needed pruning strategy that prunes weights based on their magnitudes and corresponding activations. OWL (Yin et al., 2023) considers the inhomogeneous distribution of interlayer outliers and further extends Wanda to a non-uniform sparsity distribution, achieving better performance. DS\u2205T (Zhang et al., 2023) and SPP (Lu et al., 2024), as fine-tuning methods designed for sparse models, can improve the performance of pruned PLMs within limited complexity."], "score": 0.958984375}, {"id": "(Tan, 2023)", "paper": {"corpus_id": 258833347, "title": "Infor-Coef: Information Bottleneck-based Dynamic Token Downsampling for Compact and Efficient language model", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Wenxin Tan", "authorId": "2070761774"}], "n_citations": 1}, "snippets": ["Pruning has emerged as a promising approach to compress and accelerate DNN models, significantly reducing storage and computational costs. Structured pruning method delivers a static compact model by removing structured blocks of weights, e.g. heads (Voita et al., 2019), Michel et al., 2019) and encoder layers (Fan et al., 2019). However, removing a large proportion of parameters may result in noticeable accuracy loss. To address this, the distillation paradigm is commonly adopted for recovery training, where the pruned model learns the knowledge delivered from the unpruned model. (Sanh et al., 2020) While these pruning methods achieve compelling results, they are static and have a fixed computation route for all inputs, regardless of the differing information redundancy of various sequences."], "score": 0.95849609375}, {"id": "(Voita et al., 2019)", "paper": {"corpus_id": 162183964, "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Elena Voita", "authorId": "46235299"}, {"name": "David Talbot", "authorId": "144251066"}, {"name": "F. Moiseev", "authorId": "2157158"}, {"name": "Rico Sennrich", "authorId": "2082372"}, {"name": "Ivan Titov", "authorId": "144889265"}], "n_citations": 1147}, "snippets": ["Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU."], "score": 0.0}, {"id": "(Fan et al., 2019)", "paper": {"corpus_id": 202750230, "title": "Reducing Transformer Depth on Demand with Structured Dropout", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Angela Fan", "authorId": "144270981"}, {"name": "Edouard Grave", "authorId": "3024698"}, {"name": "Armand Joulin", "authorId": "2319608"}], "n_citations": 596}, "snippets": ["Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation."], "score": 0.0}, {"id": "(Shao et al., 2023)", "paper": {"corpus_id": 264146174, "title": "One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models", "year": 2023, "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "authors": [{"name": "Hang Shao", "authorId": "2216418068"}, {"name": "Bei Liu", "authorId": "2168549481"}, {"name": "Yanmin Qian", "authorId": "2259050251"}], "n_citations": 21}, "snippets": ["In addition, Model sparsity pruning method mainly involves removing network elements by individual weights (unstructured pruning) or by entire rows and columns of weight matrices (structured pruning). Pruning can also be applied to various parts of the model, including entire layers [17], heads, intermediate dimensions [18], and blocks of weight matrices [19]. The study of model pruning emerged since the 1980s' [20][21][22], and it has demonstrated favorable outcomes across Computer Vision and NLP tasks."], "score": 0.94580078125}, {"id": "(Belhaouari et al., 2025)", "paper": {"corpus_id": 277275922, "title": "Efficient self-attention with smart pruning for sustainable large language models", "year": 2025, "venue": "Scientific Reports", "authors": [{"name": "S. Belhaouari", "authorId": "102804035"}, {"name": "Insaf Kraidia", "authorId": "2292003273"}], "n_citations": 1}, "snippets": ["The compression of language models has gained significant attention, leading to the development of various methods, including network pruning 12 , knowledge distillation, and quantization 13 . LLM-Pruner 14 and FLAP 15 aim to reduce network width by pruning coupled structures. Sheared-LLaMA 16 takes a more comprehensive approach, reducing network width and depth by removing entire layers. While methods that address both width and depth aspects exist 17,18 , there is still a need for in-depth analysis comparing the impact of these factors on LLM inference efficiency. Applying traditional pruning techniques to LLMs presents unique challenges due to their vast number of parameters and substantial computational requirements 19 . Pruning methods for LLMs can be broadly categorized into unstructured and structured approaches.\n\nStructured pruning methods 20 focus on removing entire groups of parameters, maintaining dense weight matrices, and improving hardware efficiency. Techniques such as LLM-Pruner 14 and LoRAPrune 11 emphasize efficient deployment and inference acceleration. Sheared-LLaMA 16 aims to prune models to a target architecture and train them dynamically. The work of Tao introduces a novel compression technique called QuantGPT. This method focused on quantization and module adaptation, allowing for systematic compression while maintaining the integrity of the model's architecture and performance 25 . LightPAFF focuses on transferring knowledge from a more prominent (teacher) model to a smaller (student) model. This process inherently involves structuring the model to retain important features learned by the teacher rather than simply removing individual weights, as seen in unstructured pruning 21 . Unstructured pruning methods 22 target individual weights, maintaining performance. Notable examples include SparseGPT 23 , which employs sophisticated weight updates and pruning without retraining."], "score": 0.93505859375}, {"id": "(Qin et al., 2025)", "paper": {"corpus_id": 276482745, "title": "MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jiayu Qin", "authorId": "2290611525"}, {"name": "Jianchao Tan", "authorId": "2326256572"}, {"name": "Kefeng Zhang", "authorId": "2326248013"}, {"name": "Xunliang Cai", "authorId": "2326248599"}, {"name": "Wei Wang", "authorId": "2338695871"}], "n_citations": 0}, "snippets": ["The pruning technique reduces the size and computational complexity of the models by eliminating redundant parameters, which can generally be categorized into unstructured pruning (Frantar et al., 2022)Sun et al., 2023;(Dong et al., 2024), semi-structured pruning (Mishra et al., 2021), and structured pruning (Ma et al., 2023;Xia et al., 2023;An et al., 2023)."], "score": 0.953125}, {"id": "(Sanh et al., 2020)", "paper": {"corpus_id": 218665313, "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Victor Sanh", "authorId": "51918868"}, {"name": "Thomas Wolf", "authorId": "50335211"}, {"name": "Alexander M. Rush", "authorId": "2531268"}], "n_citations": 487}, "snippets": ["Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters."], "score": 0.0}, {"id": "(Xia et al., 2022)", "paper": {"corpus_id": 247922354, "title": "Structured Pruning Learns Compact and Accurate Models", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mengzhou Xia", "authorId": "67284811"}, {"name": "Zexuan Zhong", "authorId": "49164966"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 187}, "snippets": ["The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches."], "score": 0.0}, {"id": "(Xia et al., 2023)", "paper": {"corpus_id": 263830786, "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Mengzhou Xia", "authorId": "67284811"}, {"name": "Tianyu Gao", "authorId": "4800645"}, {"name": "Zhiyuan Zeng", "authorId": "2150468823"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 310}, "snippets": ["The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and the concurrent TinyLlama models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building competitive small-scale LLMs"], "score": 0.0}], "table": null}, {"title": "Recent Task-Agnostic Pruning Methods for LLMs", "tldr": "Recent task-agnostic pruning methods for LLMs include structured approaches like LLM-Pruner and FLAP that remove coupled structures, as well as unstructured approaches like SparseGPT, Wanda, and OWL that create sparse weight patterns. These methods aim to compress models with minimal performance degradation while avoiding costly retraining. (14 sources)", "text": "\nRecent advances in pruning large language models have produced several notable task-agnostic methods that can be applied across different domains and tasks:\n\n1. **SparseGPT**: This one-shot pruning method enables large-scale GPT models to be pruned to at least 50% sparsity without retraining, with minimal accuracy loss. It is designed to work efficiently on massive models like OPT-175B and BLOOM-176B, and can prune more than 100 billion weights while maintaining model performance <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>.\n\n2. **Wanda (Weights and Activations)**: A straightforward yet effective pruning method that targets weights with the smallest magnitudes multiplied by corresponding input activations. Wanda requires no retraining or weight updates, making it practical for billion-parameter models <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.\n\n3. **LLM-Pruner**: A structured pruning approach that maintains multi-task solving and language generation abilities while minimizing reliance on extensive training data. It removes non-critical coupled structures based on gradient information and can efficiently recover performance through minimal tuning <Paper corpusId=\"270411995\" paperTitle=\"(Touheed et al., 2024)\" isShortName></Paper>.\n\n4. **OWL (Outlier Weighted Layerwise sparsity)**: This method incorporates non-uniform layerwise sparsity ratios proportional to the outlier ratio within each layer. OWL significantly outperforms Wanda and SparseGPT at high sparsity levels (70%) while delivering substantial inference speed improvements <Paper corpusId=\"263829692\" paperTitle=\"(Yin et al., 2023)\" isShortName></Paper>.\n\n5. **SparseLLM**: A framework that redefines global pruning into manageable, coordinated subproblems for resource-efficient optimization. This approach addresses the scalability issues of traditional global pruning while overcoming the suboptimal solutions of local pruning <Paper corpusId=\"268041812\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper>.\n\n6. **FLAP (FLuctuation-based Adaptive Structured Pruning)**: A retraining-free structured pruning framework that is hardware-friendly by effectively reducing storage and enhancing inference speed. It determines weight importance based on how easily output feature maps can be recovered when columns are removed <Paper corpusId=\"266362404\" paperTitle=\"(An et al., 2023)\" isShortName></Paper>.\n\n7. **TextPruner**: An open-source toolkit designed for pre-trained language models that offers structured post-training pruning methods, including vocabulary pruning and transformer pruning. It can be applied without labeled data through self-supervised pruning <Paper corpusId=\"247794014\" paperTitle=\"(Yang et al., 2022)\" isShortName></Paper>.\n\n8. **SliceGPT**: A post-training sparsification approach that replaces weight matrices with smaller dense matrices, reducing embedding dimensions. This method can remove up to 25% of model parameters while maintaining 90-99% of zero-shot task performance across various models <Paper corpusId=\"267301573\" paperTitle=\"(Ashkboos et al., 2024)\" isShortName></Paper>.\n\n9. **Dynamic Sparse No Training (DSnoT)**: A training-free fine-tuning approach that updates sparse LLMs without backpropagation or weight updates. It performs iterative weight pruning-and-growing to minimize reconstruction error between dense and sparse models <Paper corpusId=\"264128029\" paperTitle=\"(Zhang et al._1, 2023)\" isShortName></Paper>.\n\n10. **SV-NUP (Shapley Value-based Non-Uniform Pruning)**: This method quantifies the contribution of each transformer layer to overall model performance, allowing for tailored pruning budgets across different layers to retain critical parameters <Paper corpusId=\"278327238\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.\n\n11. **Thanos**: A block-wise pruning algorithm with adaptive masks that dynamically adjust to weight importance, enabling flexible sparsity patterns and structured formats optimized for hardware acceleration <Paper corpusId=\"277626866\" paperTitle=\"(Ilin et al., 2025)\" isShortName></Paper>.\n\n12. **D-Pruner**: A dual-pruning methodology that identifies weights important for both general capabilities and domain-specific knowledge, preserving both generality and specificity in the pruned model <Paper corpusId=\"269741380\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nMost of these approaches aim to balance compression ratio with performance preservation, with special attention to hardware compatibility for actual deployment benefits. Recent methods increasingly focus on avoiding expensive retraining procedures, which is particularly important for LLMs given their massive size and computational requirements <Paper corpusId=\"271909582\" paperTitle=\"(Su et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273345395\" paperTitle=\"(Thangarasa et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Frantar et al., 2023)", "paper": {"corpus_id": 255372747, "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 734}, "snippets": ["We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."], "score": 0.0}, {"id": "(Sun et al., 2023)", "paper": {"corpus_id": 259203115, "title": "A Simple and Effective Pruning Approach for Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Mingjie Sun", "authorId": "2984183"}, {"name": "Zhuang Liu", "authorId": "2109168016"}, {"name": "Anna Bair", "authorId": "25901845"}, {"name": "J. Z. Kolter", "authorId": "145116464"}], "n_citations": 439}, "snippets": ["As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda."], "score": 0.0}, {"id": "(Touheed et al., 2024)", "paper": {"corpus_id": 270411995, "title": "Applications of Pruning Methods in Natural Language Processing", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Marva Touheed", "authorId": "2305959319"}, {"name": "Urooj Zubair", "authorId": "2305868456"}, {"name": "Dilshad Sabir", "authorId": "17492832"}, {"name": "Ali Hassan", "authorId": "2293111925"}, {"name": "Muhammad Fasih Uddin Butt", "authorId": "2305969817"}, {"name": "Farhan Riaz", "authorId": "1713703"}, {"name": "Wadood Abdul", "authorId": "2305963536"}, {"name": "R. Ayub", "authorId": "119778535"}], "n_citations": 1}, "snippets": ["Ma et al. [29] proposes Large Language Model-Pruner (LLM-Pruner), a task-agnostic compression method, aiming to maintain the multi-task solving and language generation abilities of the original LLM while minimizing reliance on the extensive training dataset. LLM-Pruner adopts structural pruning, removing non-critical coupled structures based on gradient information to preserve most of the LLM's functionality. The pruned models can be efficiently recovered through tuning techniques in a short time and with minimal data. Experimental results on three LLMs, Large Language Model Meta AI (LLaMA), Vicuna, and General Language Model (ChatGLM), demonstrate that the compressed models perform well in zero-shot classification and generation tasks."], "score": 0.98291015625}, {"id": "(Yin et al., 2023)", "paper": {"corpus_id": 263829692, "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Lu Yin", "authorId": "2254142682"}, {"name": "You Wu", "authorId": "2325206905"}, {"name": "Zhenyu (Allen) Zhang", "authorId": "2109338656"}, {"name": "Cheng-Yu Hsieh", "authorId": "2256992922"}, {"name": "Yaqing Wang", "authorId": "2257105674"}, {"name": "Yiling Jia", "authorId": "2257230381"}, {"name": "Mykola Pechenizkiy", "authorId": "1691997"}, {"name": "Yi Liang", "authorId": "2260290217"}, {"name": "Zhangyang Wang", "authorId": "2254949434"}, {"name": "Shiwei Liu", "authorId": "2255081092"}], "n_citations": 102}, "snippets": ["Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, while delivering 2.6x end-to-end inference speed-up in the DeepSparse inference engine. Codes are available at https://github.com/luuyin/OWL."], "score": 0.0}, {"id": "(Bai et al., 2024)", "paper": {"corpus_id": 268041812, "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Guangji Bai", "authorId": "7583867"}, {"name": "Yijiang Li", "authorId": "2288037157"}, {"name": "Chen Ling", "authorId": "2284591355"}, {"name": "Kibaek Kim", "authorId": "2288023827"}, {"name": "Liang Zhao", "authorId": "2284637383"}], "n_citations": 11}, "snippets": ["The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality."], "score": 0.955078125}, {"id": "(An et al., 2023)", "paper": {"corpus_id": 266362404, "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Yongqi An", "authorId": "2167834971"}, {"name": "Xu Zhao", "authorId": "2118489444"}, {"name": "Tao Yu", "authorId": "40418746"}, {"name": "Ming Tang", "authorId": "2113727378"}, {"name": "Jinqiao Wang", "authorId": "2241943585"}], "n_citations": 61}, "snippets": ["Network Pruning is a promising way to address the huge computing resource demands of the deployment and inference of Large Language Models (LLMs). Retraining-free is important for LLMs' pruning methods. However, almost all of the existing retraining-free pruning approaches for LLMs focus on unstructured pruning, which requires specific hardware support for acceleration. In this paper, we propose a novel retraining-free structured pruning framework for LLMs, named FLAP (FLuctuation-based Adaptive\nStructured Pruning). It is hardware-friendly by effectively reducing storage and enhancing inference speed. For effective structured pruning of LLMs, we highlight three critical elements that demand the utmost attention: formulating structured importance metrics, adaptively searching the global compressed model, and implementing compensation mechanisms to mitigate performance loss. First, FLAP determines whether the output feature map is easily recoverable when a column of weight is removed, based on the fluctuation pruning metric. Then it standardizes the importance scores to adaptively determine the global compressed model structure. At last, FLAP adds additional bias terms to recover the output feature maps using the baseline values. We thoroughly evaluate our approach on a variety of language benchmarks. Without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. The code is released at https://github.com/CASIA-IVA-Lab/FLAP."], "score": 0.0}, {"id": "(Yang et al., 2022)", "paper": {"corpus_id": 247794014, "title": "TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ziqing Yang", "authorId": "48599077"}, {"name": "Yiming Cui", "authorId": "3043830"}, {"name": "Zhigang Chen", "authorId": "2156610145"}], "n_citations": 12}, "snippets": ["Pre-trained language models have been prevailed in natural language processing and become the backbones of many NLP tasks, but the demands for computational resources have limited their applications. In this paper, we introduce TextPruner, an open-source model pruning toolkit designed for pre-trained language models, targeting fast and easy model compression. TextPruner offers structured post-training pruning methods, including vocabulary pruning and transformer pruning, and can be applied to various models and tasks. We also propose a self-supervised pruning method that can be applied without the labeled data."], "score": 0.9609375}, {"id": "(Ashkboos et al., 2024)", "paper": {"corpus_id": 267301573, "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Saleh Ashkboos", "authorId": "9543395"}, {"name": "Maximilian L. Croci", "authorId": "2008063761"}, {"name": "Marcelo Gennari do Nascimento", "authorId": "2281641743"}, {"name": "Torsten Hoefler", "authorId": "2258547286"}, {"name": "James Hensman", "authorId": "2266803418"}], "n_citations": 184}, "snippets": ["Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression"], "score": 0.0}, {"id": "(Zhang et al._1, 2023)", "paper": {"corpus_id": 264128029, "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yu-xin Zhang", "authorId": "2108078624"}, {"name": "Lirui Zhao", "authorId": "2258678648"}, {"name": "Mingbao Lin", "authorId": "49352079"}, {"name": "Yunyun Sun", "authorId": "2258670567"}, {"name": "Yiwu Yao", "authorId": "2258671504"}, {"name": "Xingjia Han", "authorId": "2258598205"}, {"name": "Jared Tanner", "authorId": "2258549938"}, {"name": "Shiwei Liu", "authorId": "2258718674"}, {"name": "Rongrong Ji", "authorId": "2258551942"}], "n_citations": 43}, "snippets": ["The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT."], "score": 0.0}, {"id": "(Sun et al., 2025)", "paper": {"corpus_id": 278327238, "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Chuan Sun", "authorId": "2359207803"}, {"name": "Han Yu", "authorId": "2148706587"}, {"name": "Li-zhen Cui", "authorId": "2313694394"}, {"name": "Xiaoxiao Li", "authorId": "2283747425"}], "n_citations": 3}, "snippets": ["Pruning large language models (LLMs) is a promising solution for reducing model sizes and computational complexity while preserving performance. Traditional layer-wise pruning methods often adopt a uniform sparsity approach across all layers, which leads to suboptimal performance due to the varying significance of individual transformer layers within the model not being accounted for. To this end, we propose the Shapley Value-based Non-Uniform Pruning (SV-NUP) method for LLMs. This approach quantifies the contribution of each transformer layer to the overall model performance, enabling the assignment of tailored pruning budgets to different layers to retain critical parameters. To further improve efficiency, we design the Sliding Window-based Shapley Value approximation method."], "score": 0.95751953125}, {"id": "(Ilin et al., 2025)", "paper": {"corpus_id": 277626866, "title": "Thanos: A Block-wise Pruning Algorithm for Efficient Large Language Model Compression", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ivan Ilin", "authorId": "2268766339"}, {"name": "Peter Richt\u00e1rik", "authorId": "2268766087"}], "n_citations": 0}, "snippets": ["This paper presents Thanos, a novel weight-pruning algorithm designed to reduce the memory footprint and enhance the computational efficiency of large language models (LLMs) by removing redundant weights while maintaining accuracy. Thanos introduces a block-wise pruning strategy with adaptive masks that dynamically adjust to weight importance, enabling flexible sparsity patterns and structured formats, such as $n:m$ sparsity, optimized for hardware acceleration."], "score": 0.9375}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 269741380, "title": "Pruning as a Domain-specific LLM Extractor", "year": 2024, "venue": "NAACL-HLT", "authors": [{"name": "Nan Zhang", "authorId": "2266469940"}, {"name": "Yanchi Liu", "authorId": "2238385975"}, {"name": "Xujiang Zhao", "authorId": "2255325982"}, {"name": "Wei Cheng", "authorId": "2249879747"}, {"name": "Runxue Bao", "authorId": "1491239652"}, {"name": "Rui Zhang", "authorId": "144142360"}, {"name": "Prasenjit Mitra", "authorId": "2161482238"}, {"name": "Haifeng Chen", "authorId": "2204622281"}], "n_citations": 12}, "snippets": ["Large Language Models (LLMs) have exhibited remarkable proficiency across a wide array of NLP tasks. However, the escalation in model size also engenders substantial deployment costs. While few efforts have explored model pruning techniques to reduce the size of LLMs, they mainly center on general or task-specific weights. This leads to suboptimal performance due to lacking specificity on the target domain or generality on different tasks when applied to domain-specific challenges. This work introduces an innovative unstructured dual-pruning methodology, D-Pruner, for domain-specific compression on LLM. It extracts a compressed, domain-specific, and task-agnostic LLM by identifying LLM weights that are pivotal for general capabilities, like linguistic capability and multi-task solving, and domain-specific knowledge. More specifically, we first assess general weight importance by quantifying the error incurred upon their removal with the help of an open-domain calibration dataset. Then, we utilize this general weight importance to refine the training loss, so that it preserves generality when fitting into a specific domain. Moreover, by efficiently approximating weight importance with the refined training loss on a domain-specific calibration dataset, we obtain a pruned model emphasizing generality and specificity. Our comprehensive experiments across various tasks in healthcare and legal domains show the effectiveness of D-Pruner in domain-specific compression. Our code is available at https://github.com/psunlpgroup/D-Pruner."], "score": 0.0}, {"id": "(Su et al., 2024)", "paper": {"corpus_id": 271909582, "title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yupeng Su", "authorId": "2286850679"}, {"name": "Ziyi Guan", "authorId": "2120170158"}, {"name": "Xiaoqun Liu", "authorId": "2316519699"}, {"name": "Tianlai Jin", "authorId": "2316487762"}, {"name": "Dongkuan Wu", "authorId": "2316516436"}, {"name": "G. Chesi", "authorId": "1698669"}, {"name": "Ngai Wong", "authorId": "2287187433"}, {"name": "Hao Yu", "authorId": "2316516782"}], "n_citations": 2}, "snippets": ["Model compression techniques commonly employ quantization and pruning to enhance model efficiency. Quantization reduces the precision of model parameters, while pruning removes less critical parameters. Traditional pruning methods, such as magnitude-based pruning (Han et al. 2015), directly trim weights based on their absolute values. While effective for smaller models, these methods often struggle with large-scale LLMs, resulting in suboptimal sparsity due to their inability to capture the complex interactions and importance of weights in such massive scalability. To address these limitations, recent post-training pruning techniques such as SparseGPT and Wanda have emerged."], "score": 0.970703125}, {"id": "(Thangarasa et al., 2024)", "paper": {"corpus_id": 273345395, "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Vithursan Thangarasa", "authorId": "51153332"}, {"name": "Ganesh Venkatesh", "authorId": "2325876819"}, {"name": "Nish Sinnadurai", "authorId": "2325902410"}, {"name": "Sean Lie", "authorId": "2212029838"}], "n_citations": 2}, "snippets": ["Large language models have driven significant progress in natural language processing, but their deployment requires substantial compute and memory resources. As models scale, compression techniques become essential for balancing model quality with computational efficiency. Structured pruning, which removes less critical components of the model, is a promising strategy for reducing complexity. However, one-shot pruning often results in significant quality degradation, particularly in tasks requiring multi-step reasoning."], "score": 0.96728515625}], "table": null}, {"title": "Implementation Considerations and Integration with Other Techniques", "tldr": "Implementing pruning for LLMs requires careful integration with other compression techniques like quantization and knowledge distillation to maximize efficiency gains. Combining these approaches in the right sequence and proportion can deliver multiplicative benefits while maintaining model performance. (15 sources)", "text": "\nSuccessful implementation of pruning techniques for large language models requires strategic consideration of how pruning integrates with other compression methods. While pruning alone can significantly reduce model size, combining it with complementary techniques can yield multiplicative efficiency gains <Paper corpusId=\"259251699\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"278501529\" paperTitle=\"(Laborde et al., 2025)\" isShortName></Paper>. Three key integration approaches have emerged:\n\n## Pruning Combined with Quantization\n\nQuantization, which reduces the bit precision of model weights and activations, works particularly well alongside pruning. Recent work demonstrates that post-training quantization can be effectively combined with pruning methods to substantially reduce model size and inference latency <Paper corpusId=\"273507514\" paperTitle=\"(Williams et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258999941\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>. For instance, QPruner employs structured pruning followed by layer-wise mixed-precision quantization, using Bayesian optimization to determine optimal precision allocation for different layers based on their importance <Paper corpusId=\"274776787\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>. This combined approach addresses both computational and memory constraints simultaneously.\n\n## Pruning with Knowledge Distillation\n\nKnowledge distillation, where knowledge is transferred from larger to smaller models, provides an effective mechanism to recover performance after pruning. Layerwise distillation strategies can efficiently transfer knowledge between original and pruned models <Paper corpusId=\"263677297\" paperTitle=\"(Malihi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258035138\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. Some approaches use pre-pruning and post-pruning networks as teacher-student pairs, enabling the pruned model to learn intermediate and output representations from the original model <Paper corpusId=\"258035138\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. CoFi, for example, combines coarse-grained and fine-grained pruning with layerwise distillation to achieve significant speedups with minimal accuracy loss <Paper corpusId=\"247922354\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper>.\n\n## Sequence and Proportion of Compression Techniques\n\nThe order and proportion in which pruning and other techniques are applied significantly impacts the final model's performance. Some research indicates that applying pruning before quantization and then distillation yields optimal results <Paper corpusId=\"235719025\" paperTitle=\"(Aghli et al., 2021)\" isShortName></Paper>. For example, PQK combines pruning, quantization, and knowledge distillation in a structured progression specifically designed for edge device deployment <Paper corpusId=\"263677297\" paperTitle=\"(Malihi et al., 2023)\" isShortName></Paper>.\n\n## Hardware Considerations\n\nThe choice of pruning method should align with hardware capabilities. While unstructured pruning achieves higher sparsity rates, it requires specialized hardware support to realize actual speedups due to irregular sparse patterns <Paper corpusId=\"259251699\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. Structured pruning methods are generally more hardware-friendly as they directly reduce model dimensions without requiring specialized sparse computation support <Paper corpusId=\"259251699\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. \n\n## Adaptive Compression Frameworks\n\nEmerging frameworks adopt dynamic approaches that adapt compression techniques based on specific usage scenarios. These frameworks integrate multiple techniques\u2014pruning, quantization, and knowledge distillation\u2014adjusting model size according to deployment requirements <Paper corpusId=\"275869391\" paperTitle=\"(Chander, 2025)\" isShortName></Paper>. For instance, layer pruning techniques for Sentence-BERT models have demonstrated the ability to maintain 98% of original performance even after removing 40% of model layers <Paper corpusId=\"272828010\" paperTitle=\"(Shelke et al., 2024)\" isShortName></Paper>.\n\n## Resource-Constrained Applications\n\nFor applications with specific latency requirements, methods like SwiftPruner leverage evolution-based search to automatically find the best-performing layer-wise sparse model under desired latency constraints <Paper corpusId=\"251979775\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper>. Similarly, for mixture-of-experts (MoE) models, specialized approaches like MC-SMoE merge experts based on routing statistics before applying additional compression techniques, achieving up to 80% memory reduction and 20% FLOPs reduction with negligible performance loss <Paper corpusId=\"263605809\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>.\n\nThe integration of pruning with other compression techniques represents a promising direction for making large language models more accessible across a wider range of applications and deployment scenarios, particularly those with limited computational resources <Paper corpusId=\"273811289\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Li et al., 2023)", "paper": {"corpus_id": 259251699, "title": "Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference", "year": 2023, "venue": "Knowledge Discovery and Data Mining", "authors": [{"name": "Junyan Li", "authorId": "2214284233"}, {"name": "L. Zhang", "authorId": "48571328"}, {"name": "Jiahang Xu", "authorId": "2257094139"}, {"name": "Yujing Wang", "authorId": "46394401"}, {"name": "Shaoguang Yan", "authorId": "2181972735"}, {"name": "Yunqing Xia", "authorId": "33420715"}, {"name": "Yuqing Yang", "authorId": "2108623481"}, {"name": "Ting Cao", "authorId": "2069445596"}, {"name": "Hao Sun", "authorId": "2118180377"}, {"name": "Weiwei Deng", "authorId": "2066621592"}, {"name": "Qi Zhang", "authorId": "2145908588"}, {"name": "Mao Yang", "authorId": "2168609907"}], "n_citations": 10}, "snippets": ["To reduce the inference cost of pre-trained transformer models, a variety of compression techniques have been proposed, including weight pruning [8,(Lagunas et al., 2021)[30], quantization [4,17,(Shen et al., 2019) and distillation [15,29]. Token-level pruning has been shown to complement knowledge distillation and quantization (Kim et al., 2021)", "Weight pruning is categorized into 1) unstructured and 2) structured pruning. Unstructured methods [8]30] achieve high sparsity without accuracy drop but offer minimal latency benefits due to irregular sparse patterns. In contrast, structured pruning removes coherent weight groups, reducing latency without special hardware support. CoFi (Xia et al., 2022) achieves 10\u00d7 speedup with a small accuracy drop by jointly pruning layers, attention heads, FFN, and hidden units. SwiftPruner (Zhang et al., 2022) is a latency-aware pruning method that finds optimal layer-wise pruning policies under a given latency requirement through AutoML."], "score": 0.958984375}, {"id": "(Laborde et al., 2025)", "paper": {"corpus_id": 278501529, "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?", "year": 2025, "venue": "", "authors": [{"name": "Stanislas Laborde", "authorId": "2360373404"}, {"name": "Martin Cousseau", "authorId": "2360359994"}, {"name": "Antoun Yaacoub", "authorId": "40605834"}, {"name": "Lionel Prevost", "authorId": "2266474578"}], "n_citations": 0}, "snippets": ["Recent years have seen the development of various compression approaches, which are usually either training-aware or post-training. These approaches include low-rank matrix factorization, which reduces parameter count by decomposing weight matrices (Hu et al., 2021); knowledge distillation, which transfers knowledge from larger to smaller models (Sanh et al., 2019); pruning, which removes less important connections (Sun et al., 2023); and quantization, which reduces the precision of model weights (Frantar et al., 2022)."], "score": 0.9375}, {"id": "(Williams et al., 2024)", "paper": {"corpus_id": 273507514, "title": "Self-calibration for Language Model Quantization and Pruning", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Miles Williams", "authorId": "2244005949"}, {"name": "G. Chrysostomou", "authorId": "51015453"}, {"name": "Nikolaos Aletras", "authorId": "3238627"}], "n_citations": 0}, "snippets": ["Pruning removes less important weights from the model, while quantization represents the weights (and possibly activations) using fewer bits. Both quantization and pruning can be effectively applied in a post-training setting, retaining comparable performance across a range of downstream tasks (Frantar et al., 2023;Frantar and Alistarh, 2023;(Sun et al., 2023)(Lin et al., 2023)."], "score": 0.931640625}, {"id": "(Lin et al., 2023)", "paper": {"corpus_id": 258999941, "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration", "year": 2023, "venue": "Conference on Machine Learning and Systems", "authors": [{"name": "Ji Lin", "authorId": "46698300"}, {"name": "Jiaming Tang", "authorId": "2214687479"}, {"name": "Haotian Tang", "authorId": "150127950"}, {"name": "Shang Yang", "authorId": "2202210853"}, {"name": "Xingyu Dang", "authorId": "2219266839"}, {"name": "Song Han", "authorId": "2115659426"}], "n_citations": 577}, "snippets": ["Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce cloud computing costs and protect users' privacy. However, the astronomical model size and the limited hardware resources pose significant deployment challenges. To solve these issues, we propose Activation-aware Weight Quantization (AWQ) and TinyChat, an algorithm-system full-stack solution for efficient on-device LLM deployment. AWQ is a novel quantization method that identifies and protects salient weights based on activation distribution, significantly reducing model size while preserving performance. TinyChat, an optimized inference framework, translates AWQ's theoretical memory savings into practical speedups through techniques such as on-the-fly dequantization, SIMD-aware weight packing, and kernel fusion. Together, they enable 4x model size reduction and 3-4x acceleration across various edge platforms, from high-end desktop GPUs to resource-constrained IoT devices. This solution democratizes on-device LLM deployment, offering privacy-preserving, low-latency AI capabilities across a wide range of applications."], "score": 0.0}, {"id": "(Sun et al., 2023)", "paper": {"corpus_id": 259203115, "title": "A Simple and Effective Pruning Approach for Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Mingjie Sun", "authorId": "2984183"}, {"name": "Zhuang Liu", "authorId": "2109168016"}, {"name": "Anna Bair", "authorId": "25901845"}, {"name": "J. Z. Kolter", "authorId": "145116464"}], "n_citations": 439}, "snippets": ["As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda."], "score": 0.0}, {"id": "(Zhou et al., 2024)", "paper": {"corpus_id": 274776787, "title": "QPruner: Probabilistic Decision Quantization for Structured Pruning in Large Language Models", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Changhai Zhou", "authorId": "2308067782"}, {"name": "Yuhua Zhou", "authorId": "2331676258"}, {"name": "Shijie Han", "authorId": "2308186632"}, {"name": "Qian Qiao", "authorId": "2335563748"}, {"name": "Hongguang Li", "authorId": "2335617494"}], "n_citations": 0}, "snippets": ["To address these challenges, we introduce quantization into the structured pruning framework to reduce memory consumption during both fine-tuning and inference. However, the combined errors from pruning and quantization increase the difficulty of fine-tuning, requiring a more refined quantization scheme. To this end, we propose QPruner, a novel framework that employs structured pruning to reduce model size, followed by a layer-wise mixed-precision quantization scheme. Quantization precisions are assigned to each layer based on their importance to the target task, and Bayesian optimization is employed to refine precision allocation strategies, ensuring a balance between model accuracy and memory efficiency."], "score": 0.93603515625}, {"id": "(Malihi et al., 2023)", "paper": {"corpus_id": 263677297, "title": "Efficient and Controllable Model Compression through Sequential Knowledge Distillation and Pruning", "year": 2023, "venue": "Big Data and Cognitive Computing", "authors": [{"name": "Leila Malihi", "authorId": "9106542"}, {"name": "Gunther Heidemann", "authorId": "2254394013"}], "n_citations": 5}, "snippets": ["Aghli and Ribeiro's work [16] pioneered the integration of weighted pruning and knowledge distillation, coordinating selective pruning on ResNet layers and subsequent distillation for enhanced model compression without loss of accuracy. Xie et al.'s study [17], ventures into the realm of person re-identification. Employing a sequence of pruning followed by knowledge distillation, they strike a balance between effective parameter reduction and accurate performance. Cui and Li, the architects of [18], unveil a complex model compression approach that combines structural pruning with dense knowledge distillation for large language models. Kim et al. [19] address the needs of edge devices with PQK, an innovative combination of pruning, quantization, and knowledge distillation. A structured progression of pruning, quantization, and distillation provides a comprehensive strategy for efficient edge-based model deployment.\n\nFinally, Wang et al. [20] introduce an innovative approach that combines structured pruning with multilevel distillation. By using pre-and post-pruning networks as teacherstudent pairs, they reduce the loss of accuracy through distillation and highlight the synergy between the two techniques."], "score": 0.94384765625}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 258035138, "title": "Progressive multi-level distillation learning for pruning network", "year": 2023, "venue": "Complex & Intelligent Systems", "authors": [{"name": "Ruiqing Wang", "authorId": "2115688298"}, {"name": "Shengmin Wan", "authorId": "2148393891"}, {"name": "Wu Zhang", "authorId": "2108317894"}, {"name": "Chenlu Zhang", "authorId": "2209148679"}, {"name": "Yu Li", "authorId": "2213834216"}, {"name": "Shaoxiang Xu", "authorId": "2213858051"}, {"name": "Lifu Zhang", "authorId": "2209205186"}, {"name": "Xiuliang Jin", "authorId": "2148654963"}, {"name": "Zhaohui Jiang", "authorId": "2108871581"}, {"name": "Yuan Rao", "authorId": "2177392273"}], "n_citations": 6}, "snippets": ["Although the classification method based on the deep neural network has achieved excellent results in classification tasks, it is difficult to apply to real-time scenarios because of high memory footprints and prohibitive inference times. Compared to unstructured pruning, structured pruning techniques can reduce the computation cost of the model runtime more effectively, but inevitably reduces the precision of the model. Traditional methods use fine tuning to restore model damage performance. However, there is still a large gap between the pruned model and the original one. In this paper, we use progressive multi-level distillation learning to compensate for the loss caused by pruning. Pre-pruning and post-pruning networks serve as the teacher and student networks. The proposed approach utilizes the complementary properties of structured pruning and knowledge distillation, which allows the pruned network to learn the intermediate and output representations of the teacher network, thus reducing the influence of the model subject to pruning. Experiments demonstrate that our approach performs better on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets with different pruning rates. For instance, GoogLeNet can achieve near lossless pruning on the CIFAR-10 dataset with 60% pruning. Moreover, this paper also proves that using the proposed distillation learning method during the pruning process achieves more significant performance gains than after completing the pruning."], "score": 0.0}, {"id": "(Xia et al., 2022)", "paper": {"corpus_id": 247922354, "title": "Structured Pruning Learns Compact and Accurate Models", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mengzhou Xia", "authorId": "67284811"}, {"name": "Zexuan Zhong", "authorId": "49164966"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 187}, "snippets": ["The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches."], "score": 0.0}, {"id": "(Aghli et al., 2021)", "paper": {"corpus_id": 235719025, "title": "Combining Weight Pruning and Knowledge Distillation For CNN Compression", "year": 2021, "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "authors": [{"name": "Nima Aghli", "authorId": "46191994"}, {"name": "Eraldo Ribeiro", "authorId": "145559941"}], "n_citations": 60}, "snippets": ["Complex deep convolutional neural networks such as ResNet require expensive hardware such as powerful GPUs to achieve real-time performance. This problem is critical for applications that run on low-end embedded GPU or CPU systems with limited resources. As a result, model compression for deep neural networks becomes an important research topic. Popular compression methods such as weight pruning remove redundant neurons from the CNN without affecting the network\u2019s output accuracy. While these pruning methods work well on simple networks such as VGG or AlexNet, they are not suitable for compressing current state-of-the-art networks such as ResNets because of these networks\u2019 complex architectures with dimensionality dependencies. This dependency results in filter pruning breaking the structure of ResNets leading to an untrainable network. In this paper, we first use the weight pruning method only on a selective number of layers in the ResNet architecture to avoid breaking the network structure. Second, we introduce a knowledge distillation architecture and a loss function to compress the untouched layers during the pruning. We test our method on both image-based regression and classification networks for head-pose estimation and image classification. Our compression method reduces the models\u2019 size significantly while maintaining the accuracy very close to the baseline model."], "score": 0.0}, {"id": "(Chander, 2025)", "paper": {"corpus_id": 275869391, "title": "Optimizing Memory Efficiency in Large Language Models: Adaptive Compression Techniques", "year": 2025, "venue": "International Journal for Research in Applied Science and Engineering Technology", "authors": [{"name": "Dr. T. Prem Chander", "authorId": "2342013216"}], "n_citations": 0}, "snippets": ["The proposed framework integrates multiple techniques, including quantization, pruning, and knowledge distillation, dynamically adjusting the model size based on specific usage scenarios."], "score": 0.93505859375}, {"id": "(Shelke et al., 2024)", "paper": {"corpus_id": 272828010, "title": "Towards Building Efficient Sentence BERT Models using Layer Pruning", "year": 2024, "venue": "Pacific Asia Conference on Language, Information and Computation", "authors": [{"name": "Anushka Shelke", "authorId": "2305898307"}, {"name": "Riya Savant", "authorId": "2305843017"}, {"name": "Raviraj Joshi", "authorId": "2253467830"}], "n_citations": 0}, "snippets": ["In our research, we delve into recent developments in adapting Sentence-BERT (SBERT) models for low-resource languages, focusing particularly on Marathi and Hindi. The L3Cube-MahaSBERT and HindSBERT (Joshi et al., 2022) models were established as benchmarks for generating high-quality sentence embeddings in Marathi and Hindi, respectively. These specialized models are highlighted for their effectiveness in processing these low-resource languages. These models have been rigorously trained and evaluated across various NLP tasks, including text classification and semantic similarity.\n\nOur research aims to extend these foundational models by applying layer-pruning techniques to enhance their efficiency without compromising the quality of the embeddings. By integrating layer pruning, we seek to reduce the computational demand and improve the operational feasibility of deploying SBERT models in real-world applications, making advanced NLP tools more accessible for languages that traditionally have fewer technological resources.\n\n\u2022 A research (Sajjad et al., 2022) has showcased a range of layer pruning strategies, under-scoring their effectiveness. These techniques maintain an impressive 98% of the original performance even after removing 40% of the layers from BERT, RoBERTa, and XLNet models."], "score": 0.95654296875}, {"id": "(Zhang et al., 2022)", "paper": {"corpus_id": 251979775, "title": "SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance", "year": 2022, "venue": "International Conference on Information and Knowledge Management", "authors": [{"name": "L. Zhang", "authorId": "48571328"}, {"name": "Youkow Homma", "authorId": "4133298"}, {"name": "Yujing Wang", "authorId": "46394401"}, {"name": "Min Wu", "authorId": "1390606776"}, {"name": "Mao Yang", "authorId": "2168609907"}, {"name": "Ruofei Zhang", "authorId": "2124601065"}, {"name": "Ting Cao", "authorId": "2137096570"}, {"name": "Wei Shen", "authorId": null}], "n_citations": 5}, "snippets": ["Ad relevance modeling plays a critical role in online advertising systems including Microsoft Bing. To leverage powerful transformers like BERT in this low-latency setting, many existing approaches perform ad-side computations offline. While efficient, these approaches are unable to serve cold start ads, resulting in poor relevance predictions for such ads. This work aims to design a new, low-latency BERT via structured pruning to empower real-time online inference for cold start ads relevance on a CPU platform. Our challenge is that previous methods typically prune all layers of the transformer to a high, uniform sparsity, thereby producing models which cannot achieve satisfactory inference speed with an acceptable accuracy. In this paper, we propose SwiftPruner - an efficient framework that leverages evolution-based search to automatically find the best-performing layer-wise sparse BERT model under the desired latency constraint. Different from existing evolution algorithms that conduct random mutations, we propose a reinforced mutator with a latency-aware multi-objective reward to conduct better mutations for efficiently searching the large space of layer-wise sparse models. Extensive experiments demonstrate that our method consistently achieves higher ROC AUC and lower latency than the uniform sparse baseline and state-of-the-art search methods. Remarkably, under our latency requirement of 1900us on CPU, SwiftPruner achieves a 0.86% higher AUC than the state-of-the-art uniform sparse baseline for BERT-Mini on a large scale real-world dataset. Online A/B testing shows that our model also achieves a significant 11.7% cut in the ratio of defective cold start ads with satisfactory real-time serving latency."], "score": 0.0}, {"id": "(Li et al._1, 2023)", "paper": {"corpus_id": 263605809, "title": "Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Pingzhi Li", "authorId": "2253560631"}, {"name": "Zhenyu (Allen) Zhang", "authorId": "2109338656"}, {"name": "Prateek Yadav", "authorId": "46841632"}, {"name": "Yi-Lin Sung", "authorId": "31238770"}, {"name": "Yu Cheng", "authorId": "2255343392"}, {"name": "Mohit Bansal", "authorId": "2253396640"}, {"name": "Tianlong Chen", "authorId": "2034263179"}], "n_citations": 39}, "snippets": ["Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like (a) High Memory Usage, due to duplication of the network layers into multiple copies as experts; and (b) Redundancy in Experts, as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: (1) redundant information overshadows critical experts; (2) appropriate neuron permutation for each expert is missing to bring all of them in alignment. To address this, we propose M-SMoE, which leverages routing statistics to guide expert merging. Specifically, it starts with neuron permutation alignment for experts; then, dominant experts and their\"group members\"are formed; lastly, every expert group is merged into a single expert by utilizing each expert's activation frequency as their weight for merging, thus diminishing the impact of insignificant experts. Moreover, we observed that our proposed merging promotes a low dimensionality in the merged expert's weight space, naturally paving the way for additional compression. Hence, our final method, MC-SMoE (i.e., Merge, then Compress SMoE), further decomposes the merged experts into low-rank and structural sparse alternatives. Extensive experiments across 8 benchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE achieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in performance."], "score": 0.0}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 273811289, "title": "MoE-I\u00b2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Cheng Yang", "authorId": "2329224758"}, {"name": "Yang Sui", "authorId": "2117517225"}, {"name": "Jinqi Xiao", "authorId": "2196307128"}, {"name": "Lingyi Huang", "authorId": "2152279863"}, {"name": "Yu Gong", "authorId": "2168502148"}, {"name": "Yuanlin Duan", "authorId": "2329727093"}, {"name": "Wenqi Jia", "authorId": "2297818320"}, {"name": "Miao Yin", "authorId": "1471722186"}, {"name": "Yu Cheng", "authorId": "2329746797"}, {"name": "Bo Yuan", "authorId": "2241581494"}], "n_citations": 7}, "snippets": ["Recent advancements in large language models have underscored the need to reduce parameter sizes and latency (Ma et al., 2023). Compression techniques for language models include network pruning (Xu et al., 2021), knowledge distillation (Sun et al., 2019(Sun et al., , 2020)), quantization (Yao et al., 2022), decomposition (Hsu et al., 2022;Yuan et al., 2023;Wang et al., 2024), and methods like early exit (Xin et al., 2020). Building on these techniques, pruning, and sparsity is crucial for MoE models, which often have up to 95% of parameters dedicated to experts. Pruning MoE models involves removing less important experts or neurons to reduce the number of active parameters during inference. For example, (Kim et al., 2021) retains the most activated experts to enhance machine translation MoE models, while (Koishekenov et al., 2022) introduces gate statistics-based pruning during decoding. Although effective, these methods are mostly confined to linguistic models in machine translation. (Chen et al., 2022) dropping-while-training approach progressively removes non-essential experts for specific tasks, tested on Switch Transformers (Fedus et al., 2021). The merge-compression (Li et al., 2023) method and EPP (Lu et al., 2024) approach, which is similar to ours, consider pruning and skipping in MoE models but face challenges in reducing computational costs."], "score": 0.93701171875}], "table": null}, {"title": "Performance and Efficiency Benefits", "tldr": "Pruning LLMs can achieve significant efficiency gains, with recent methods demonstrating up to 70% parameter reduction while maintaining performance. When combined with hardware-aware implementation, pruned models can deliver 3-5x throughput improvements and up to 10x speedups for real-world deployment. (13 sources)", "text": "\nPruning large language models yields substantial performance benefits across multiple dimensions, making these powerful models more accessible for resource-constrained environments. Recent pruning methods have demonstrated remarkable compression capabilities while preserving model functionality. Unstructured pruning approaches can remove 70% or more of model weights with minimal performance degradation, resulting in models that are up to 20x smaller in terms of pure model size <Paper corpusId=\"257901132\" paperTitle=\"(Campos et al., 2023)\" isShortName></Paper>. When implemented with specialized sparsity-aware inference engines such as DeepSparse, these pruned models can achieve 3-5x improvements in inference throughput <Paper corpusId=\"257901132\" paperTitle=\"(Campos et al., 2023)\" isShortName></Paper>.\n\nThe efficiency gains extend beyond storage requirements to computational performance. Structured pruning approaches like CoFi (which jointly prunes layers, attention heads, and hidden units) have demonstrated up to 10x speedups with only small accuracy drops <Paper corpusId=\"259251699\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247922354\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper>. These performance benefits are particularly important for deployment scenarios with strict latency requirements. For instance, SwiftPruner, which uses evolution-based search to find optimal layer-wise pruning policies, can automatically identify the best-performing sparse model configuration under specific latency constraints <Paper corpusId=\"259251699\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"251979775\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper>.\n\nThe efficiency gains from pruning are especially valuable as models continue to grow in size. By removing redundant parameters, pruning enhances not only storage efficiency but also memory utilization and computational speed <Paper corpusId=\"267897588\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. SparseLLM, for example, redefines global pruning into manageable, coordinated subproblems for resource-efficient optimization, addressing both scalability issues and optimization quality <Paper corpusId=\"268041812\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper>.\n\nWhile traditional pruning methods like magnitude-based pruning have been effective for smaller models, they often struggle with large-scale LLMs due to their inability to capture complex parameter interactions <Paper corpusId=\"271909582\" paperTitle=\"(Su et al., 2024)\" isShortName></Paper>. More sophisticated approaches like SparseGPT and Wanda have emerged to address these limitations, offering better sparsity-performance tradeoffs for billion-parameter models <Paper corpusId=\"271909582\" paperTitle=\"(Su et al., 2024)\" isShortName></Paper>.\n\nThe recovery process after pruning is critical for maintaining performance. Many approaches adopt a distillation paradigm, where the pruned model learns from the unpruned model <Paper corpusId=\"258833347\" paperTitle=\"(Tan, 2023)\" isShortName></Paper>. This knowledge transfer helps preserve accuracy even with significant parameter reduction <Paper corpusId=\"258833347\" paperTitle=\"(Tan, 2023)\" isShortName></Paper>. Additionally, layerwise distillation strategies have proven effective for transferring knowledge between original and pruned models <Paper corpusId=\"273345395\" paperTitle=\"(Thangarasa et al., 2024)\" isShortName></Paper>.\n\nFor practical deployment, both quantization and pruning can be effectively applied in post-training settings while retaining comparable performance across downstream tasks <Paper corpusId=\"273507514\" paperTitle=\"(Williams et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258999941\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>. This is particularly valuable since it avoids the costly retraining typically associated with compression techniques <Paper corpusId=\"278033481\" paperTitle=\"(Garg et al., 2025)\" isShortName></Paper>.\n\nThe combined efficiency benefits of pruning\u2014reduced storage requirements, lower memory usage, faster inference, and minimal accuracy loss\u2014make it an essential technique for deploying LLMs in environments with limited computational resources. By carefully applying pruning methods, developers can significantly improve model efficiency while maintaining the powerful capabilities that make LLMs so valuable for a wide range of applications.", "citations": [{"id": "(Campos et al., 2023)", "paper": {"corpus_id": 257901132, "title": "oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes", "year": 2023, "venue": "SUSTAINLP", "authors": [{"name": "Daniel Fernando Campos", "authorId": "144081089"}, {"name": "Alexandre Marques", "authorId": "2166312585"}, {"name": "Mark Kurtz", "authorId": "2070446213"}, {"name": "Chengxiang Zhai", "authorId": "143869012"}], "n_citations": 2}, "snippets": ["Unstructured Pruning is a compression approach that removes individual weights or groups of weights in a model by applying a mask or setting the weight values to 0. This compression approach has been broadly studied in computer vision (Han et al., 2015), and many methods can remove 70% or more of model weights with little to no loss in accuracy. Models pruned can be 20x smaller in terms of pure model size and, when paired with a sparsity-aware inference engine such as DeepSparse (Magic, 2023), provide 3-5x speedups in inference throughput. Focused on language models, recent work has shown that it is possible to prune models during pre-training with little to no loss in accuracy (Sanh et al., 2020) (Kurti\u0107 et al., 2022"], "score": 0.978515625}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 259251699, "title": "Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference", "year": 2023, "venue": "Knowledge Discovery and Data Mining", "authors": [{"name": "Junyan Li", "authorId": "2214284233"}, {"name": "L. Zhang", "authorId": "48571328"}, {"name": "Jiahang Xu", "authorId": "2257094139"}, {"name": "Yujing Wang", "authorId": "46394401"}, {"name": "Shaoguang Yan", "authorId": "2181972735"}, {"name": "Yunqing Xia", "authorId": "33420715"}, {"name": "Yuqing Yang", "authorId": "2108623481"}, {"name": "Ting Cao", "authorId": "2069445596"}, {"name": "Hao Sun", "authorId": "2118180377"}, {"name": "Weiwei Deng", "authorId": "2066621592"}, {"name": "Qi Zhang", "authorId": "2145908588"}, {"name": "Mao Yang", "authorId": "2168609907"}], "n_citations": 10}, "snippets": ["To reduce the inference cost of pre-trained transformer models, a variety of compression techniques have been proposed, including weight pruning [8,(Lagunas et al., 2021)[30], quantization [4,17,(Shen et al., 2019) and distillation [15,29]. Token-level pruning has been shown to complement knowledge distillation and quantization (Kim et al., 2021)", "Weight pruning is categorized into 1) unstructured and 2) structured pruning. Unstructured methods [8]30] achieve high sparsity without accuracy drop but offer minimal latency benefits due to irregular sparse patterns. In contrast, structured pruning removes coherent weight groups, reducing latency without special hardware support. CoFi (Xia et al., 2022) achieves 10\u00d7 speedup with a small accuracy drop by jointly pruning layers, attention heads, FFN, and hidden units. SwiftPruner (Zhang et al., 2022) is a latency-aware pruning method that finds optimal layer-wise pruning policies under a given latency requirement through AutoML."], "score": 0.958984375}, {"id": "(Xia et al., 2022)", "paper": {"corpus_id": 247922354, "title": "Structured Pruning Learns Compact and Accurate Models", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mengzhou Xia", "authorId": "67284811"}, {"name": "Zexuan Zhong", "authorId": "49164966"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 187}, "snippets": ["The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches."], "score": 0.0}, {"id": "(Zhang et al., 2022)", "paper": {"corpus_id": 251979775, "title": "SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance", "year": 2022, "venue": "International Conference on Information and Knowledge Management", "authors": [{"name": "L. Zhang", "authorId": "48571328"}, {"name": "Youkow Homma", "authorId": "4133298"}, {"name": "Yujing Wang", "authorId": "46394401"}, {"name": "Min Wu", "authorId": "1390606776"}, {"name": "Mao Yang", "authorId": "2168609907"}, {"name": "Ruofei Zhang", "authorId": "2124601065"}, {"name": "Ting Cao", "authorId": "2137096570"}, {"name": "Wei Shen", "authorId": null}], "n_citations": 5}, "snippets": ["Ad relevance modeling plays a critical role in online advertising systems including Microsoft Bing. To leverage powerful transformers like BERT in this low-latency setting, many existing approaches perform ad-side computations offline. While efficient, these approaches are unable to serve cold start ads, resulting in poor relevance predictions for such ads. This work aims to design a new, low-latency BERT via structured pruning to empower real-time online inference for cold start ads relevance on a CPU platform. Our challenge is that previous methods typically prune all layers of the transformer to a high, uniform sparsity, thereby producing models which cannot achieve satisfactory inference speed with an acceptable accuracy. In this paper, we propose SwiftPruner - an efficient framework that leverages evolution-based search to automatically find the best-performing layer-wise sparse BERT model under the desired latency constraint. Different from existing evolution algorithms that conduct random mutations, we propose a reinforced mutator with a latency-aware multi-objective reward to conduct better mutations for efficiently searching the large space of layer-wise sparse models. Extensive experiments demonstrate that our method consistently achieves higher ROC AUC and lower latency than the uniform sparse baseline and state-of-the-art search methods. Remarkably, under our latency requirement of 1900us on CPU, SwiftPruner achieves a 0.86% higher AUC than the state-of-the-art uniform sparse baseline for BERT-Mini on a large scale real-world dataset. Online A/B testing shows that our model also achieves a significant 11.7% cut in the ratio of defective cold start ads with satisfactory real-time serving latency."], "score": 0.0}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 267897588, "title": "CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jiayi Liu", "authorId": "2286338976"}, {"name": "Tinghan Yang", "authorId": "2286427471"}, {"name": "Jennifer Neville", "authorId": "2286321906"}], "n_citations": 11}, "snippets": ["Within the realm of model compression, three prominent techniques have garnered significant attention: quantization, distillation, and pruning. Quantization is a widely employed approach in the compression of post-training large language models [9,40,42]. By reducing the bit precision, quantization methods necessitate lower memory utilization, thus facilitating faster computations and, consequently, higher efficiency. Distillation methods distill the knowledge from larger models and embed it into smaller models to approach model compression [5,15,17]. Another noteworthy method for model compression is pruning [21]26]. Pruning achieves reduced complexity by eliminating relatively less impactful model components, effectively reducing the model's size."], "score": 0.96533203125}, {"id": "(Bai et al., 2024)", "paper": {"corpus_id": 268041812, "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Guangji Bai", "authorId": "7583867"}, {"name": "Yijiang Li", "authorId": "2288037157"}, {"name": "Chen Ling", "authorId": "2284591355"}, {"name": "Kibaek Kim", "authorId": "2288023827"}, {"name": "Liang Zhao", "authorId": "2284637383"}], "n_citations": 11}, "snippets": ["The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality."], "score": 0.955078125}, {"id": "(Su et al., 2024)", "paper": {"corpus_id": 271909582, "title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yupeng Su", "authorId": "2286850679"}, {"name": "Ziyi Guan", "authorId": "2120170158"}, {"name": "Xiaoqun Liu", "authorId": "2316519699"}, {"name": "Tianlai Jin", "authorId": "2316487762"}, {"name": "Dongkuan Wu", "authorId": "2316516436"}, {"name": "G. Chesi", "authorId": "1698669"}, {"name": "Ngai Wong", "authorId": "2287187433"}, {"name": "Hao Yu", "authorId": "2316516782"}], "n_citations": 2}, "snippets": ["Model compression techniques commonly employ quantization and pruning to enhance model efficiency. Quantization reduces the precision of model parameters, while pruning removes less critical parameters. Traditional pruning methods, such as magnitude-based pruning (Han et al. 2015), directly trim weights based on their absolute values. While effective for smaller models, these methods often struggle with large-scale LLMs, resulting in suboptimal sparsity due to their inability to capture the complex interactions and importance of weights in such massive scalability. To address these limitations, recent post-training pruning techniques such as SparseGPT and Wanda have emerged."], "score": 0.970703125}, {"id": "(Tan, 2023)", "paper": {"corpus_id": 258833347, "title": "Infor-Coef: Information Bottleneck-based Dynamic Token Downsampling for Compact and Efficient language model", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Wenxin Tan", "authorId": "2070761774"}], "n_citations": 1}, "snippets": ["Pruning has emerged as a promising approach to compress and accelerate DNN models, significantly reducing storage and computational costs. Structured pruning method delivers a static compact model by removing structured blocks of weights, e.g. heads (Voita et al., 2019), Michel et al., 2019) and encoder layers (Fan et al., 2019). However, removing a large proportion of parameters may result in noticeable accuracy loss. To address this, the distillation paradigm is commonly adopted for recovery training, where the pruned model learns the knowledge delivered from the unpruned model. (Sanh et al., 2020) While these pruning methods achieve compelling results, they are static and have a fixed computation route for all inputs, regardless of the differing information redundancy of various sequences."], "score": 0.95849609375}, {"id": "(Thangarasa et al., 2024)", "paper": {"corpus_id": 273345395, "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Vithursan Thangarasa", "authorId": "51153332"}, {"name": "Ganesh Venkatesh", "authorId": "2325876819"}, {"name": "Nish Sinnadurai", "authorId": "2325902410"}, {"name": "Sean Lie", "authorId": "2212029838"}], "n_citations": 2}, "snippets": ["Large language models have driven significant progress in natural language processing, but their deployment requires substantial compute and memory resources. As models scale, compression techniques become essential for balancing model quality with computational efficiency. Structured pruning, which removes less critical components of the model, is a promising strategy for reducing complexity. However, one-shot pruning often results in significant quality degradation, particularly in tasks requiring multi-step reasoning."], "score": 0.96728515625}, {"id": "(Williams et al., 2024)", "paper": {"corpus_id": 273507514, "title": "Self-calibration for Language Model Quantization and Pruning", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Miles Williams", "authorId": "2244005949"}, {"name": "G. Chrysostomou", "authorId": "51015453"}, {"name": "Nikolaos Aletras", "authorId": "3238627"}], "n_citations": 0}, "snippets": ["Pruning removes less important weights from the model, while quantization represents the weights (and possibly activations) using fewer bits. Both quantization and pruning can be effectively applied in a post-training setting, retaining comparable performance across a range of downstream tasks (Frantar et al., 2023;Frantar and Alistarh, 2023;(Sun et al., 2023)(Lin et al., 2023)."], "score": 0.931640625}, {"id": "(Lin et al., 2023)", "paper": {"corpus_id": 258999941, "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration", "year": 2023, "venue": "Conference on Machine Learning and Systems", "authors": [{"name": "Ji Lin", "authorId": "46698300"}, {"name": "Jiaming Tang", "authorId": "2214687479"}, {"name": "Haotian Tang", "authorId": "150127950"}, {"name": "Shang Yang", "authorId": "2202210853"}, {"name": "Xingyu Dang", "authorId": "2219266839"}, {"name": "Song Han", "authorId": "2115659426"}], "n_citations": 577}, "snippets": ["Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce cloud computing costs and protect users' privacy. However, the astronomical model size and the limited hardware resources pose significant deployment challenges. To solve these issues, we propose Activation-aware Weight Quantization (AWQ) and TinyChat, an algorithm-system full-stack solution for efficient on-device LLM deployment. AWQ is a novel quantization method that identifies and protects salient weights based on activation distribution, significantly reducing model size while preserving performance. TinyChat, an optimized inference framework, translates AWQ's theoretical memory savings into practical speedups through techniques such as on-the-fly dequantization, SIMD-aware weight packing, and kernel fusion. Together, they enable 4x model size reduction and 3-4x acceleration across various edge platforms, from high-end desktop GPUs to resource-constrained IoT devices. This solution democratizes on-device LLM deployment, offering privacy-preserving, low-latency AI capabilities across a wide range of applications."], "score": 0.0}, {"id": "(Sun et al., 2023)", "paper": {"corpus_id": 259203115, "title": "A Simple and Effective Pruning Approach for Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Mingjie Sun", "authorId": "2984183"}, {"name": "Zhuang Liu", "authorId": "2109168016"}, {"name": "Anna Bair", "authorId": "25901845"}, {"name": "J. Z. Kolter", "authorId": "145116464"}], "n_citations": 439}, "snippets": ["As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda."], "score": 0.0}, {"id": "(Garg et al., 2025)", "paper": {"corpus_id": 278033481, "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Muskan Garg", "authorId": "2258141722"}, {"name": "Shaina Raza", "authorId": "2278330619"}, {"name": "Shebuti Rayana", "authorId": "3023076"}, {"name": "Xingyi Liu", "authorId": "2278394763"}, {"name": "Sunghwan Sohn", "authorId": "2267490593"}], "n_citations": 2}, "snippets": ["Pruning is a long-established technique for model compression that reduces the size of a model by removing unnecessary or less important parameters (Han et al., 2015)(LeCun et al., 1989). In language models, many weights can be zeroed out or eliminated with little to no effect on overall performance (Bergsma et al., 2025). By pruning these redundant parameters, we can significantly shrink model size and accelerate inference, with only a minor drop in accuracy if done carefully. Pruning thus makes models more storage-friendly, memoryefficient, and computation-efficient. Based on the granularity of removal, pruning is categorized as unstructured pruning and structured pruning", "Recent research has focused on one-shot pruning methods and improved criteria to minimize the need for costly retraining [211], validating its performance for healthcare tasks."], "score": 0.966796875}], "table": null}], "cost": 0.50724}}
