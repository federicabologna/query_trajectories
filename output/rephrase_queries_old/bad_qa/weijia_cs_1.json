{"original_query": "If we have a lot of specialized language models, how can we combine them to build a general system that combine the abilities of different models? Can this unified model be as good as the single large LM? ", "input": "lot specialized language models to build a general system combine abilities different models this unified model be good the single large LM", "output": {"sections": [{"title": "Background on Specialized vs. General Language Models", "tldr": "Specialized language models are designed to excel at specific tasks or domains while general language models aim to perform well across a wide range of applications. Combining specialized models into a unified system can potentially leverage the strengths of each component to create a more capable overall system than a single large general model. (LLM Memory)", "text": "\nIn the landscape of language models, we can distinguish between two primary approaches: specialized and general models. Specialized language models are trained or fine-tuned for specific domains or tasks, such as medical diagnosis, legal document analysis, code generation, or mathematical reasoning. These models develop deep expertise in their targeted areas but may underperform when faced with tasks outside their specialization. Their focused nature often allows them to achieve superior performance with fewer parameters compared to general models when operating within their intended domain. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nGeneral language models, in contrast, are designed to handle a broad spectrum of tasks across multiple domains. Models like GPT-4, PaLM, and Claude aim to be versatile by training on diverse datasets spanning numerous subjects and applications. While these models demonstrate impressive breadth, they may not match the depth of specialized models in particular domains. This limitation stems from the inherent tradeoff between breadth and depth - as a single model attempts to cover more ground, it may sacrifice some performance in specific areas. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe tension between specialized and general approaches has led researchers to explore whether combining multiple specialized models might create systems that benefit from both the depth of specialized expertise and the breadth of general capabilities. Rather than viewing this as an either/or choice, the field is increasingly investigating how to integrate specialized components into unified systems that can seamlessly access the right capabilities for each task. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Approaches to Combining Specialized Models", "tldr": "Researchers have developed several strategies for combining specialized language models, including parameter-space merging, mixture-of-experts architectures, and dynamic gating mechanisms. These approaches aim to preserve the specialized knowledge of individual models while creating unified systems that can effectively handle diverse tasks. (7 sources)", "text": "\nSeveral promising approaches have emerged for integrating specialized language models into unified systems. One key strategy is parameter-space merging, where models trained on different datasets are combined at the parameter level rather than simply ensembling their outputs. Jin et al. propose a \"dataless knowledge fusion\" method that merges models by minimizing prediction differences between the merged model and individual specialized models, outperforming techniques like Fisher-weighted averaging and traditional ensembling <Paper corpusId=\"254877510\" paperTitle=\"(Jin et al., 2022)\" isShortName></Paper>.\n\nMixture-of-Experts (MoE) architectures represent another prominent approach. Si et al. developed the Mixture-of-Reasoning-Experts (MORE) framework, which maintains a pool of specialized models for different reasoning types and employs a classifier to select the most appropriate expert for each question <Paper corpusId=\"258865893\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>. This method leverages the complementary strengths of specialized models while allowing the system to abstain from answering when confidence is low.\n\nDynamic routing mechanisms further enhance the flexibility of combined model systems. Ding et al. introduced ULTRAFUSER, which implements a dynamic gating mechanism that sits atop specialized models for text, code, and math, adaptively controlling each specialist's contribution to the final output at the token level <Paper corpusId=\"268379027\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>. This approach enables both specialization of individual components and generalization of the fused model.\n\nMore sophisticated merging approaches recognize the importance of distinguishing between shared and task-specific knowledge. Lu et al. propose Twin-Merging, which first modularizes knowledge into shared and exclusive components, then dynamically merges them based on input characteristics <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This method addresses interference issues that can arise when directly combining models with exclusive knowledge domains.\n\nThe integration of knowledge distillation (KD) with MoE architectures represents another promising direction. Al-Maamari et al. explore various strategies for combining these techniques to create specialized, efficient, and modular language models <Paper corpusId=\"271533631\" paperTitle=\"(Al-Maamari et al., 2024)\" isShortName></Paper>. This approach is particularly relevant for domains like programming languages, where specialized models have demonstrated superior performance in tasks like code completion and bug detection compared to general-purpose alternatives <Paper corpusId=\"271533631\" paperTitle=\"(Al-Maamari et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256808267\" paperTitle=\"(Jiang et al., 2023)\" isShortName></Paper>.\n\nA final approach focuses on merging multiple fine-tuned versions of a single base model. Yadav et al. propose a flexible, modular method for post-training large language models by combining expert models trained on distinct datasets covering different tasks and domains <Paper corpusId=\"273162841\" paperTitle=\"(Yadav et al., 2024)\" isShortName></Paper>. This strategy aims to preserve the capabilities of individual experts on their specialized tasks while improving zero-shot generalization to unseen tasks.", "citations": [{"id": "(Jin et al., 2022)", "paper": {"corpus_id": 254877510, "title": "Dataless Knowledge Fusion by Merging Weights of Language Models", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xisen Jin", "authorId": "2148654757"}, {"name": "Xiang Ren", "authorId": "1384550891"}, {"name": "Daniel Preotiuc-Pietro", "authorId": "1398830377"}, {"name": "Pengxiang Cheng", "authorId": "2904366"}], "n_citations": 250}, "snippets": ["In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data."], "score": 0.57373046875}, {"id": "(Si et al., 2023)", "paper": {"corpus_id": 258865893, "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Chenglei Si", "authorId": "152358188"}, {"name": "Weijia Shi", "authorId": "3040379"}, {"name": "Chen Zhao", "authorId": "145756130"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Jordan L. Boyd-Graber", "authorId": "1389036863"}], "n_citations": 29}, "snippets": ["Therefore, we go against this trend of building a single generalist language model, but rather design a more interpretable system that consists of a pool of specialized models and each question is answered by one of them. Crucially, to best use complementary strengths of multiple QA models, we implement a pool of diverse and capable specialized models (e.g., by equipping LLMs with corresponding prompting strategies) for each specific reasoning type; then we train a classifier to select the best candidate answer from the specialized models for each question or to abstain from answering (Figure 1). This framework, Mixture-of-Reasoning-Experts (MORE), aims to both generalize and answer selectively", "Experiments validate that by ensembling the specialized experts this way, MORE significantly outperforms any single specialized model across all four diverse reasoning types."], "score": 0.75390625}, {"id": "(Ding et al., 2024)", "paper": {"corpus_id": 268379027, "title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ning Ding", "authorId": "46649145"}, {"name": "Yulin Chen", "authorId": "2135835258"}, {"name": "Ganqu Cui", "authorId": "52297757"}, {"name": "Xingtai Lv", "authorId": "2221271501"}, {"name": "Ruobing Xie", "authorId": "2257007994"}, {"name": "Bowen Zhou", "authorId": "2218723159"}, {"name": "Zhiyuan Liu", "authorId": "2273470196"}, {"name": "Maosong Sun", "authorId": "2273551430"}], "n_citations": 7}, "snippets": ["In this paper, we propose to leverage separate models that are already highly specialized via a fusing structure. In this fusing framework, namely ULTRAFUSER, we use three well-trained LLMs as initial specialist models in text, code, and math. To ensure that the fused model benefits from the specialized knowledge of each specialist model, a dynamic gating mechanism is implemented, which sits on top of the three specialists and adaptively controls the contribution of each specialist to the final output logits based on the input data. Such a mechanism is adopted at the token level, which allows both the specialization of individual specialists and the generalization of the fused model."], "score": 0.61328125}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 270702345, "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zhenyi Lu", "authorId": "2262512474"}, {"name": "Chenghao Fan", "authorId": "2277238906"}, {"name": "Wei Wei", "authorId": "2284721764"}, {"name": "Xiaoye Qu", "authorId": "2262446609"}, {"name": "Dangyang Chen", "authorId": "2182623368"}, {"name": "Yu Cheng", "authorId": "2284687448"}], "n_citations": 63}, "snippets": ["In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input."], "score": 0.623046875}, {"id": "(Al-Maamari et al., 2024)", "paper": {"corpus_id": 271533631, "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mohammed Al-Maamari", "authorId": "2313634509"}, {"name": "Mehdi Ben Amor", "authorId": "2074055134"}, {"name": "Michael Granitzer", "authorId": "2259357506"}], "n_citations": 2}, "snippets": ["Specialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in specific tasks like code completion and bug detection over their general-purpose counterparts (Jiang et al., 2023). Introducing modularity into neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network segments without necessitating a complete retraining. This research primarily focuses on exploring various integration strategies of KD and MoE to create specialized, efficient, and modular language models. While we employed straightforward knowledge distillation techniques, reaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the feasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to mimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures, on the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied domains and languages [10]."], "score": 0.630859375}, {"id": "(Jiang et al., 2023)", "paper": {"corpus_id": 256808267, "title": "Impact of Code Language Models on Automated Program Repair", "year": 2023, "venue": "International Conference on Software Engineering", "authors": [{"name": "Nan Jiang", "authorId": "2057958465"}, {"name": "Kevin Liu", "authorId": "2152352414"}, {"name": "Thibaud Lutellier", "authorId": "2492099"}, {"name": "Lin Tan", "authorId": "2106349652"}], "n_citations": 188}, "snippets": ["Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task. Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31%-1,267% improvement to CLMs and enables them to fix 46%-164 % more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs. This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem."], "score": 0.0}, {"id": "(Yadav et al., 2024)", "paper": {"corpus_id": 273162841, "title": "What Matters for Model Merging at Scale?", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Prateek Yadav", "authorId": "46841632"}, {"name": "Tu Vu", "authorId": "144244743"}, {"name": "Jonathan Lai", "authorId": "2325489176"}, {"name": "Alexandra Chronopoulou", "authorId": "2324583448"}, {"name": "Manaal Faruqui", "authorId": "1779225"}, {"name": "Mohit Bansal", "authorId": "2253396640"}, {"name": "Tsendsuren Munkhdalai", "authorId": "2227827"}], "n_citations": 22}, "snippets": ["This work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models."], "score": 0.640625}], "table": null}, {"title": "Benefits of Combining Specialized Models", "tldr": "Combining specialized language models offers numerous advantages including enhanced performance across diverse tasks, more efficient resource utilization, and a path to decentralized AI development. This approach allows systems to leverage the complementary strengths of specialized experts while avoiding the task interference that can occur in monolithic general models. (10 sources)", "text": "\nThe integration of specialized language models into unified systems provides several significant benefits over relying on a single general-purpose model. Perhaps the most compelling advantage is the ability to achieve superior performance across a broader range of tasks by leveraging the complementary strengths of different experts. Si et al. demonstrated that their Mixture-of-Reasoning-Experts (MORE) framework \"significantly outperforms any single specialized model across all four diverse reasoning types\" <Paper corpusId=\"258865893\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>. This performance improvement stems from the system's ability to route each task to the most appropriate specialized model, rather than forcing a single model to handle all tasks with varying degrees of competence.\n\nAnother key benefit is the mitigation of task interference that can plague general models. Zhu et al. identified \"interference among different tasks and modalities\" as the primary factor causing performance degradation in generalist models compared to task-specialized alternatives <Paper corpusId=\"249538647\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>. By maintaining separation between specialized components while intelligently combining their outputs, approaches like ULTRAFUSER's dynamic gating mechanism enable \"both the specialization of individual specialists and the generalization of the fused model\" <Paper corpusId=\"268379027\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>. This architecture preserves the depth of expertise in each domain while creating a unified interface for users.\n\nThe combined model approach also offers a more resource-efficient path to building comprehensive AI systems. Sengupta et al. note that their ensemble methodology for combining domain-adapted specialized language models creates \"a more versatile final model\" while \"maintaining a smaller computational footprint compared to traditional large-scale models\" <Paper corpusId=\"278310893\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. This efficiency advantage is particularly important as model sizes continue to grow, potentially making the development of massive general models increasingly concentrated among organizations with substantial computing resources.\n\nPerhaps most importantly, combining specialized models enables a more decentralized and collaborative approach to advancing language model capabilities. Muqeeth et al. highlight that such an approach \"would provide a path to decentralized development of generalist language models, which otherwise require a huge amount of centralized compute\" and \"provide a way to recycle the widespread effort and compute already being expended to create specialized models\" <Paper corpusId=\"267547973\" paperTitle=\"(Muqeeth et al., 2024)\" isShortName></Paper>. This aligns with extensive evidence that multitask training improves zero-shot generalization <Paper corpusId=\"267547973\" paperTitle=\"(Muqeeth et al., 2024)\" isShortName></Paper> <Paper corpusId=\"237416585\" paperTitle=\"(Wei et al., 2021)\" isShortName></Paper> <Paper corpusId=\"237421373\" paperTitle=\"(Mishra et al., 2021)\" isShortName></Paper>.\n\nThe modular nature of combined approaches also facilitates ongoing enhancement of system capabilities. Yadav et al. describe their merging approach as providing \"a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models\" <Paper corpusId=\"273162841\" paperTitle=\"(Yadav et al., 2024)\" isShortName></Paper>. This allows for incremental improvement as new specialized models are developed, rather than requiring complete retraining of massive general models to incorporate new capabilities.\n\nAdvanced ensemble techniques further enhance the collaborative potential of specialized model integration. Methods developed by He et al. and Yadav et al. enable effective aggregation of domain expertise from component models in \"a more collaborative manner, ensuring better combined performance\" <Paper corpusId=\"278310893\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper> <Paper corpusId=\"220496384\" paperTitle=\"(He et al., 2020)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. This collaborative approach stands in contrast to traditional mixture-of-experts architectures that primarily encourage sparsity within experts, instead focusing on truly leveraging the complementary strengths of each specialized component.", "citations": [{"id": "(Si et al., 2023)", "paper": {"corpus_id": 258865893, "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Chenglei Si", "authorId": "152358188"}, {"name": "Weijia Shi", "authorId": "3040379"}, {"name": "Chen Zhao", "authorId": "145756130"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Jordan L. Boyd-Graber", "authorId": "1389036863"}], "n_citations": 29}, "snippets": ["Therefore, we go against this trend of building a single generalist language model, but rather design a more interpretable system that consists of a pool of specialized models and each question is answered by one of them. Crucially, to best use complementary strengths of multiple QA models, we implement a pool of diverse and capable specialized models (e.g., by equipping LLMs with corresponding prompting strategies) for each specific reasoning type; then we train a classifier to select the best candidate answer from the specialized models for each question or to abstain from answering (Figure 1). This framework, Mixture-of-Reasoning-Experts (MORE), aims to both generalize and answer selectively", "Experiments validate that by ensembling the specialized experts this way, MORE significantly outperforms any single specialized model across all four diverse reasoning types."], "score": 0.75390625}, {"id": "(Zhu et al., 2022)", "paper": {"corpus_id": 249538647, "title": "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jinguo Zhu", "authorId": "150167730"}, {"name": "Xizhou Zhu", "authorId": "2578924"}, {"name": "Wenhai Wang", "authorId": "71074736"}, {"name": "Xiaohua Wang", "authorId": "2155455424"}, {"name": "Hongsheng Li", "authorId": "49404547"}, {"name": "Xiaogang Wang", "authorId": "31843833"}, {"name": "Jifeng Dai", "authorId": "3304536"}], "n_citations": 69}, "snippets": ["To build an artificial neural network like the biological intelligence system, recent works have unified numerous tasks into a generalist model, which can process various tasks with shared parameters and do not have any task-specific modules. While generalist models achieve promising results on various benchmarks, they have performance degradation on some tasks compared with task-specialized models. In this work, we find that interference among different tasks and modalities is the main factor to this phenomenon."], "score": 0.5703125}, {"id": "(Ding et al., 2024)", "paper": {"corpus_id": 268379027, "title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ning Ding", "authorId": "46649145"}, {"name": "Yulin Chen", "authorId": "2135835258"}, {"name": "Ganqu Cui", "authorId": "52297757"}, {"name": "Xingtai Lv", "authorId": "2221271501"}, {"name": "Ruobing Xie", "authorId": "2257007994"}, {"name": "Bowen Zhou", "authorId": "2218723159"}, {"name": "Zhiyuan Liu", "authorId": "2273470196"}, {"name": "Maosong Sun", "authorId": "2273551430"}], "n_citations": 7}, "snippets": ["In this paper, we propose to leverage separate models that are already highly specialized via a fusing structure. In this fusing framework, namely ULTRAFUSER, we use three well-trained LLMs as initial specialist models in text, code, and math. To ensure that the fused model benefits from the specialized knowledge of each specialist model, a dynamic gating mechanism is implemented, which sits on top of the three specialists and adaptively controls the contribution of each specialist to the final output logits based on the input data. Such a mechanism is adopted at the token level, which allows both the specialization of individual specialists and the generalization of the fused model."], "score": 0.61328125}, {"id": "(Sengupta et al., 2025)", "paper": {"corpus_id": 278310893, "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ayan Sengupta", "authorId": "34920835"}, {"name": "Yash Goel", "authorId": "2345922770"}, {"name": "Tanmoy Chakraborty", "authorId": "2249914540"}], "n_citations": 0}, "snippets": ["The final stage of our pipeline implements a model ensemble methodology to combine these domain-adapted SLMs into a unified system. This ensemble approach preserves the specialized capabilities of individual SLMs while creating a more versatile final model. By leveraging advanced ensemble techniques (He et al., 2020)(Yadav et al., 2023), we can effectively aggregate the domain expertise of each component model while maintaining a smaller computational footprint compared to traditional large-scale models. This approach is fundamentally different from mixture of experts, where the primary motivation is usually to encour-age sparsity within different experts. Rather, we focus on combining multiple smaller experts together in a more collaborative manner, ensuring better combined performance. As shown earlier in Proposition 4.1, this ensemble would give a better performance than the original model at the same computational cost."], "score": 0.6923828125}, {"id": "(Muqeeth et al., 2024)", "paper": {"corpus_id": 267547973, "title": "Learning to Route Among Specialized Experts for Zero-Shot Generalization", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Mohammed Muqeeth", "authorId": "1582888954"}, {"name": "Haokun Liu", "authorId": "48447436"}, {"name": "Yufan Liu", "authorId": "2283449435"}, {"name": "Colin Raffel", "authorId": "2269733851"}], "n_citations": 38}, "snippets": ["The allure of general-purpose language models and the proliferation of specialized PEFT-based models raises a natural question: Can we leverage a large collection of specialized modules to improve zero-shot generalization of a base language model? Such an approach is attractive for various reasons: First, it would provide a path to decentralized development of generalist language models, which otherwise require a huge amount of centralized compute (Kaplan et al., 2020;Hoffmann et al., 2022). In addition, it would provide a way to recycle the widespread effort and compute already being expended to create specialized models. We might hope such an approach might be successful given the extensive evidence that multitask training improves zero-shot generalization (Sanh et al., 2021;(Wei et al., 2021)(Mishra et al., 2021), and combining specialized models could be seen as a form of multitask learning that does not require simultaneous data access."], "score": 0.73095703125}, {"id": "(Wei et al., 2021)", "paper": {"corpus_id": 237416585, "title": "Finetuned Language Models Are Zero-Shot Learners", "year": 2021, "venue": "International Conference on Learning Representations", "authors": [{"name": "Jason Wei", "authorId": "144026731"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Vincent Zhao", "authorId": "2664737"}, {"name": "Kelvin Guu", "authorId": "2091768"}, {"name": "Adams Wei Yu", "authorId": "40625240"}, {"name": "Brian Lester", "authorId": "144104130"}, {"name": "Nan Du", "authorId": "2140321952"}, {"name": "Andrew M. Dai", "authorId": "2555924"}, {"name": "Quoc V. Le", "authorId": "2827616"}], "n_citations": 3788}, "snippets": ["This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning."], "score": 0.0}, {"id": "(Mishra et al., 2021)", "paper": {"corpus_id": 237421373, "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Swaroop Mishra", "authorId": "1817207"}, {"name": "Daniel Khashabi", "authorId": "1783281"}, {"name": "Chitta Baral", "authorId": "2064619864"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}], "n_citations": 752}, "snippets": ["Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction."], "score": 0.0}, {"id": "(Yadav et al., 2024)", "paper": {"corpus_id": 273162841, "title": "What Matters for Model Merging at Scale?", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Prateek Yadav", "authorId": "46841632"}, {"name": "Tu Vu", "authorId": "144244743"}, {"name": "Jonathan Lai", "authorId": "2325489176"}, {"name": "Alexandra Chronopoulou", "authorId": "2324583448"}, {"name": "Manaal Faruqui", "authorId": "1779225"}, {"name": "Mohit Bansal", "authorId": "2253396640"}, {"name": "Tsendsuren Munkhdalai", "authorId": "2227827"}], "n_citations": 22}, "snippets": ["This work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models."], "score": 0.640625}, {"id": "(He et al., 2020)", "paper": {"corpus_id": 220496384, "title": "Bayesian Deep Ensembles via the Neural Tangent Kernel", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Bobby He", "authorId": "1810714948"}, {"name": "Balaji Lakshminarayanan", "authorId": "40627523"}, {"name": "Y. Teh", "authorId": "1725303"}], "n_citations": 121}, "snippets": ["We explore the link between deep ensembles and Gaussian processes (GPs) through the lens of the Neural Tangent Kernel (NTK): a recent development in understanding the training dynamics of wide neural networks (NNs). Previous work has shown that even in the infinite width limit, when NNs become GPs, there is no GP posterior interpretation to a deep ensemble trained with squared error loss. We introduce a simple modification to standard deep ensembles training, through addition of a computationally-tractable, randomised and untrainable function to each ensemble member, that enables a posterior interpretation in the infinite width limit. When ensembled together, our trained NNs give an approximation to a posterior predictive distribution, and we prove that our Bayesian deep ensembles make more conservative predictions than standard deep ensembles in the infinite width limit. Finally, using finite width NNs we demonstrate that our Bayesian deep ensembles faithfully emulate the analytic posterior predictive when available, and can outperform standard deep ensembles in various out-of-distribution settings, for both regression and classification tasks."], "score": 0.0}, {"id": "(Yadav et al., 2023)", "paper": {"corpus_id": 259064039, "title": "TIES-Merging: Resolving Interference When Merging Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prateek Yadav", "authorId": "46841632"}, {"name": "Derek Tam", "authorId": "1390031652"}, {"name": "Leshem Choshen", "authorId": "41019330"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 317}, "snippets": ["Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"], "score": 0.0}], "table": null}, {"title": "Challenges and Solutions in Model Combination", "tldr": "Combining specialized language models faces significant challenges including task interference, knowledge conflicts, and the limited availability of specialized training data. Researchers have developed solutions such as knowledge modularization, dynamic merging based on input characteristics, and creating specialists from generalist data when domain-specific corpora are scarce. (3 sources)", "text": "\nDespite the promising benefits of combining specialized language models, several significant challenges must be addressed to create effective unified systems. Task interference emerges as one of the primary obstacles in model combination. Zhu et al. identified that \"interference among different tasks and modalities is the main factor\" causing performance degradation in generalist models compared to task-specialized alternatives <Paper corpusId=\"249538647\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>. This interference occurs when knowledge required for different tasks conflicts or competes for the same model capacity, potentially degrading performance across multiple domains.\n\nKnowledge conflicts present another substantial challenge, particularly when merging models with exclusive domain-specific knowledge. Lu et al. found that \"directly merging exclusive knowledge hinders overall performance\" even though both shared and exclusive task-specific knowledge are crucial for effective model merging <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. Traditional merging approaches often produce significant performance gaps compared to individually fine-tuned models due to these knowledge conflicts and their inability to adapt to heterogeneous test data.\n\nTo address these challenges, researchers have developed increasingly sophisticated merging techniques. Lu et al. proposed Twin-Merging, which implements a two-stage approach: first \"modularizing knowledge into shared and exclusive components\" with compression to reduce redundancy, and then \"dynamically merging shared and task-specific knowledge based on the input\" <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This method explicitly addresses the tension between shared and exclusive knowledge, allowing the system to adaptively emphasize different aspects of its knowledge depending on the specific input characteristics.\n\nAnother practical challenge is the limited availability of domain-specific training data for creating specialized models in many fields. Grangier et al. tackled this constraint by developing an approach to \"build specialist models from large generalist training sets\" when specialist data is available only in limited amounts <Paper corpusId=\"273185866\" paperTitle=\"(Grangier et al., 2024)\" isShortName></Paper>. This technique enables the creation of effective specialized models even for domains where collecting substantial amounts of targeted training data would be prohibitively expensive or impractical.\n\nThese solutions represent important progress in addressing the core challenges of combining specialized models. By explicitly managing knowledge conflicts, dynamically adjusting model contributions based on input characteristics, and developing methods to create specialists even with limited domain data, researchers are steadily improving the feasibility and effectiveness of unified multi-specialist systems <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Zhu et al., 2022)", "paper": {"corpus_id": 249538647, "title": "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jinguo Zhu", "authorId": "150167730"}, {"name": "Xizhou Zhu", "authorId": "2578924"}, {"name": "Wenhai Wang", "authorId": "71074736"}, {"name": "Xiaohua Wang", "authorId": "2155455424"}, {"name": "Hongsheng Li", "authorId": "49404547"}, {"name": "Xiaogang Wang", "authorId": "31843833"}, {"name": "Jifeng Dai", "authorId": "3304536"}], "n_citations": 69}, "snippets": ["To build an artificial neural network like the biological intelligence system, recent works have unified numerous tasks into a generalist model, which can process various tasks with shared parameters and do not have any task-specific modules. While generalist models achieve promising results on various benchmarks, they have performance degradation on some tasks compared with task-specialized models. In this work, we find that interference among different tasks and modalities is the main factor to this phenomenon."], "score": 0.5703125}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 270702345, "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zhenyi Lu", "authorId": "2262512474"}, {"name": "Chenghao Fan", "authorId": "2277238906"}, {"name": "Wei Wei", "authorId": "2284721764"}, {"name": "Xiaoye Qu", "authorId": "2262446609"}, {"name": "Dangyang Chen", "authorId": "2182623368"}, {"name": "Yu Cheng", "authorId": "2284687448"}], "n_citations": 63}, "snippets": ["In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input."], "score": 0.623046875}, {"id": "(Grangier et al., 2024)", "paper": {"corpus_id": 273185866, "title": "Task-Adaptive Pretrained Language Models via Clustered-Importance Sampling", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "David Grangier", "authorId": "2529182"}, {"name": "Simin Fan", "authorId": "2324421747"}, {"name": "Skyler Seto", "authorId": "2324783252"}, {"name": "Pierre Ablin", "authorId": "1763708"}], "n_citations": 5}, "snippets": ["Specialist language models (LMs) focus on a specific task or domain on which they often outperform generalist LMs of the same size. However, the specialist data needed to pretrain these models is only available in limited amount for most tasks. In this work, we build specialist models from large generalist training sets instead."], "score": 0.587890625}], "table": null}, {"title": "Performance Comparisons", "tldr": "Studies comparing specialized model combinations with general models consistently show that well-designed multi-specialist systems can outperform single general models across diverse tasks. These performance advantages are achieved while maintaining smaller computational footprints than traditional large-scale models. (4 sources)", "text": "\nEmpirical evaluations of systems that combine specialized language models reveal significant performance advantages over both individual specialized models and general-purpose alternatives. Si et al. demonstrated that their Mixture-of-Reasoning-Experts (MORE) framework \"significantly outperforms any single specialized model across all four diverse reasoning types\" evaluated in their study <Paper corpusId=\"258865893\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>. This performance improvement stems from MORE's ability to leverage the complementary strengths of different specialized models by routing each question to the most appropriate expert or abstaining when confidence is low.\n\nThe computational efficiency of combined specialist approaches provides another quantifiable advantage. Sengupta et al. found that their ensemble methodology for combining domain-adapted specialized language models creates \"a more versatile final model\" while \"maintaining a smaller computational footprint compared to traditional large-scale models\" <Paper corpusId=\"278310893\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. This efficiency advantage allows combined specialist systems to achieve better performance than original general models at equivalent computational costs, as demonstrated in their research.\n\nAdvanced ensemble techniques further enhance performance outcomes when combining specialized models. By leveraging methods developed by He et al. and Yadav et al., researchers can \"effectively aggregate the domain expertise of each component model\" in \"a more collaborative manner, ensuring better combined performance\" <Paper corpusId=\"278310893\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper> <Paper corpusId=\"220496384\" paperTitle=\"(He et al., 2020)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. These approaches differ fundamentally from traditional mixture-of-experts architectures by focusing on true collaboration between specialists rather than merely encouraging sparsity within different experts.\n\nUnlike simply increasing the size of a single general model, which often yields diminishing returns in performance improvements relative to computational costs, combining specialized models offers a more efficient scaling path. The performance benefits of multi-specialist systems can be particularly pronounced in domains requiring deep expertise, where even the largest general models may struggle to match the capabilities of targeted specialists working in concert.", "citations": [{"id": "(Si et al., 2023)", "paper": {"corpus_id": 258865893, "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Chenglei Si", "authorId": "152358188"}, {"name": "Weijia Shi", "authorId": "3040379"}, {"name": "Chen Zhao", "authorId": "145756130"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Jordan L. Boyd-Graber", "authorId": "1389036863"}], "n_citations": 29}, "snippets": ["Therefore, we go against this trend of building a single generalist language model, but rather design a more interpretable system that consists of a pool of specialized models and each question is answered by one of them. Crucially, to best use complementary strengths of multiple QA models, we implement a pool of diverse and capable specialized models (e.g., by equipping LLMs with corresponding prompting strategies) for each specific reasoning type; then we train a classifier to select the best candidate answer from the specialized models for each question or to abstain from answering (Figure 1). This framework, Mixture-of-Reasoning-Experts (MORE), aims to both generalize and answer selectively", "Experiments validate that by ensembling the specialized experts this way, MORE significantly outperforms any single specialized model across all four diverse reasoning types."], "score": 0.75390625}, {"id": "(Sengupta et al., 2025)", "paper": {"corpus_id": 278310893, "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ayan Sengupta", "authorId": "34920835"}, {"name": "Yash Goel", "authorId": "2345922770"}, {"name": "Tanmoy Chakraborty", "authorId": "2249914540"}], "n_citations": 0}, "snippets": ["The final stage of our pipeline implements a model ensemble methodology to combine these domain-adapted SLMs into a unified system. This ensemble approach preserves the specialized capabilities of individual SLMs while creating a more versatile final model. By leveraging advanced ensemble techniques (He et al., 2020)(Yadav et al., 2023), we can effectively aggregate the domain expertise of each component model while maintaining a smaller computational footprint compared to traditional large-scale models. This approach is fundamentally different from mixture of experts, where the primary motivation is usually to encour-age sparsity within different experts. Rather, we focus on combining multiple smaller experts together in a more collaborative manner, ensuring better combined performance. As shown earlier in Proposition 4.1, this ensemble would give a better performance than the original model at the same computational cost."], "score": 0.6923828125}, {"id": "(He et al., 2020)", "paper": {"corpus_id": 220496384, "title": "Bayesian Deep Ensembles via the Neural Tangent Kernel", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Bobby He", "authorId": "1810714948"}, {"name": "Balaji Lakshminarayanan", "authorId": "40627523"}, {"name": "Y. Teh", "authorId": "1725303"}], "n_citations": 121}, "snippets": ["We explore the link between deep ensembles and Gaussian processes (GPs) through the lens of the Neural Tangent Kernel (NTK): a recent development in understanding the training dynamics of wide neural networks (NNs). Previous work has shown that even in the infinite width limit, when NNs become GPs, there is no GP posterior interpretation to a deep ensemble trained with squared error loss. We introduce a simple modification to standard deep ensembles training, through addition of a computationally-tractable, randomised and untrainable function to each ensemble member, that enables a posterior interpretation in the infinite width limit. When ensembled together, our trained NNs give an approximation to a posterior predictive distribution, and we prove that our Bayesian deep ensembles make more conservative predictions than standard deep ensembles in the infinite width limit. Finally, using finite width NNs we demonstrate that our Bayesian deep ensembles faithfully emulate the analytic posterior predictive when available, and can outperform standard deep ensembles in various out-of-distribution settings, for both regression and classification tasks."], "score": 0.0}, {"id": "(Yadav et al., 2023)", "paper": {"corpus_id": 259064039, "title": "TIES-Merging: Resolving Interference When Merging Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prateek Yadav", "authorId": "46841632"}, {"name": "Derek Tam", "authorId": "1390031652"}, {"name": "Leshem Choshen", "authorId": "41019330"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 317}, "snippets": ["Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"], "score": 0.0}], "table": null}], "cost": 0.187077}}
