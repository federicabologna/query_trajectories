{"original_query": "What is membership inference attack? ", "input": "is membership inference attack", "output": {"sections": [{"title": "Introduction/Background - What is a Membership Inference Attack", "tldr": "Membership inference attacks determine whether a specific data sample was used to train a machine learning model. These attacks represent a fundamental privacy risk, as they can reveal sensitive information about individuals whose data was included in training datasets. (15 sources)", "text": "\nMembership inference attacks (MIAs) are a significant privacy concern in machine learning, focusing on determining whether a particular data sample was part of a model's training dataset <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper> <Paper corpusId=\"220831381\" paperTitle=\"(Choquette-Choo et al., 2020)\" isShortName></Paper>. These attacks exploit the fact that machine learning models can inadvertently leak information about their training data through their predictions and behavior <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper> <Paper corpusId=\"214623088\" paperTitle=\"(Song et al., 2020)\" isShortName></Paper>. The goal of a membership inference attack is straightforward: given a data record and access to a model, determine if that record was used to train the model <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper> <Paper corpusId=\"244920593\" paperTitle=\"(Carlini et al., 2021)\" isShortName></Paper>.\n\nFormally, membership inference attacks can be defined as a function that takes a target data sample, the target model, and potentially auxiliary knowledge to produce a binary output indicating whether the sample was in the training dataset (member) or not (non-member) <Paper corpusId=\"221586480\" paperTitle=\"(Zou et al., 2020)\" isShortName></Paper> <Paper corpusId=\"231846491\" paperTitle=\"(He et al., 2021)\" isShortName></Paper>. This can be represented as A_MemInf: x, T \u2192 {member, non-member}, where x is the data sample and T is the target model <Paper corpusId=\"231846491\" paperTitle=\"(He et al., 2021)\" isShortName></Paper>.\n\nThe privacy implications of these attacks are significant. If an attacker can determine that an individual's data was used to train a specific model, it may reveal sensitive personal information <Paper corpusId=\"221586480\" paperTitle=\"(Zou et al., 2020)\" isShortName></Paper>. For example, if a patient's clinical records were used to train a model associated with a particular disease, this could reveal that the patient has that disease <Paper corpusId=\"221586480\" paperTitle=\"(Zou et al., 2020)\" isShortName></Paper> <Paper corpusId=\"269605305\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Similarly, membership in datasets related to sensitive attributes like health status, financial information, or personal preferences can leak private information about individuals <Paper corpusId=\"231802143\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>.\n\nMembership inference attacks have become a standard method for evaluating the privacy risks of machine learning models <Paper corpusId=\"251719178\" paperTitle=\"(He et al., 2022)\" isShortName></Paper> <Paper corpusId=\"265220927\" paperTitle=\"(Cilloni et al., 2023)\" isShortName></Paper>. As the simplest and most widely deployed attack for auditing training data privacy <Paper corpusId=\"244920593\" paperTitle=\"(Carlini et al., 2021)\" isShortName></Paper>, they serve as a fundamental benchmark for assessing how much information models leak about their training data. These attacks operate at the local level, focusing on individual data points rather than attempting to recover the entire training dataset <Paper corpusId=\"50778569\" paperTitle=\"(Truex et al., 2018)\" isShortName></Paper>.\n\nUnderstanding membership inference attacks is crucial for developing privacy-preserving machine learning models, as these attacks represent a fundamental privacy risk that must be addressed to protect the confidentiality of training data <Paper corpusId=\"3619789\" paperTitle=\"(Long et al., 2018)\" isShortName></Paper> <Paper corpusId=\"237563320\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"276928094\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Shokri et al., 2016)", "paper": {"corpus_id": 10488675, "title": "Membership Inference Attacks Against Machine Learning Models", "year": 2016, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "R. Shokri", "authorId": "2520493"}, {"name": "M. Stronati", "authorId": "34828439"}, {"name": "Congzheng Song", "authorId": "3469125"}, {"name": "Vitaly Shmatikov", "authorId": "1723945"}], "n_citations": 4160}, "snippets": ["We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset."], "score": 0.998046875}, {"id": "(Choquette-Choo et al., 2020)", "paper": {"corpus_id": 220831381, "title": "Label-Only Membership Inference Attacks", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Christopher A. Choquette-Choo", "authorId": "1415982317"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}, {"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Nicolas Papernot", "authorId": "1967156"}], "n_citations": 516}, "snippets": ["Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model."], "score": 0.998046875}, {"id": "(Leino et al., 2019)", "paper": {"corpus_id": 195699554, "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference", "year": 2019, "venue": "USENIX Security Symposium", "authors": [{"name": "Klas Leino", "authorId": "35802340"}, {"name": "Matt Fredrikson", "authorId": "2623167"}], "n_citations": 272}, "snippets": ["Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model."], "score": 0.99658203125}, {"id": "(Song et al., 2020)", "paper": {"corpus_id": 214623088, "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models", "year": 2020, "venue": "USENIX Security Symposium", "authors": [{"name": "Liwei Song", "authorId": "144173853"}, {"name": "Prateek Mittal", "authorId": "143615345"}], "n_citations": 375}, "snippets": ["Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model."], "score": 0.99755859375}, {"id": "(Carlini et al., 2021)", "paper": {"corpus_id": 244920593, "title": "Membership Inference Attacks From First Principles", "year": 2021, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Steve Chien", "authorId": "2059189068"}, {"name": "Milad Nasr", "authorId": "3490923"}, {"name": "Shuang Song", "authorId": "144206374"}, {"name": "A. Terzis", "authorId": "1763579"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}], "n_citations": 708}, "snippets": ["The objective of a membership inference attack (MIA) [60] is to predict if a specific training example was, or was not, used as training data in a particular model. This makes MIAs the simplest and most widely deployed attack for auditing training data privacy."], "score": 0.9990234375}, {"id": "(Zou et al., 2020)", "paper": {"corpus_id": 221586480, "title": "Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Yang Zou", "authorId": "2113958751"}, {"name": "Zhikun Zhang", "authorId": "48806102"}, {"name": "M. Backes", "authorId": "144588806"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 32}, "snippets": ["In machine learning, the objective of membership inference is to determine whether a data sample was used to train the machine learning models. Knowing the membership status of individual user's data may cause severe information leakage. For example, knowing that a certain patient's clinical records were used to train a model associated with a disease (e.g., to determine the appropriate drug dosage or to discover the genetic basis of the disease) can reveal that the patient carries the associated disease. Formally, membership inference attack can be defined as the following function: \n\nHere, x target is a target data sample, M is the target model, and K is the auxiliary knowledge of adversaries. The output value equals 1 means that x target is a member of M 's training dataset D Train and 0 otherwise. The attack model A is essentially a binary classifier."], "score": 0.99658203125}, {"id": "(He et al., 2021)", "paper": {"corpus_id": 231846491, "title": "Quantifying and Mitigating Privacy Risks of Contrastive Learning", "year": 2021, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Xinlei He", "authorId": "2116553732"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 52}, "snippets": ["Membership inference attack is one of the most popular privacy attacks against ML models (Chen et al., 2019)8,10,19,(Jia et al., 2019)(Leino et al., 2019)(Li et al., 2020)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2020). The goal of membership inference is to determine whether a data sample x is part of the training dataset of a target model T . We formally define a membership inference attack model A MemInf : x, T \u2192 {member, non-member}."], "score": 0.99853515625}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 269605305, "title": "The Federation Strikes Back: A Survey of Federated Learning Privacy Attacks, Defenses, Applications, and Policy Landscape", "year": 2024, "venue": "ACM Computing Surveys", "authors": [{"name": "Joshua C. Zhao", "authorId": "2133363467"}, {"name": "Saurabh Bagchi", "authorId": "2268859358"}, {"name": "S. Avestimehr", "authorId": "121011351"}, {"name": "Kevin S. Chan", "authorId": "2300143852"}, {"name": "S. Chaterji", "authorId": "2228303"}, {"name": "Dimitris Dimitriadis", "authorId": "2300092534"}, {"name": "Jiacheng Li", "authorId": "2300130544"}, {"name": "Ninghui Li", "authorId": "2265278308"}, {"name": "Arash Nourian", "authorId": "2300093540"}, {"name": "Holger Roth", "authorId": "2302008236"}], "n_citations": 3}, "snippets": ["The goal of membership inference attacks is to infer whether any particular data instance has been used in the training of a specific model. If a particular data instance has been used in the training, this instance is called a member, otherwise it is a non-member. Knowing the membership of one particular data point could result in revealing private information, for example, if someone's data is known to be in a cancer dataset (used to train a cancer prediction model), then it is highly likely that this particular person has cancer."], "score": 0.9970703125}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 231802143, "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models", "year": 2021, "venue": "USENIX Security Symposium", "authors": [{"name": "Yugeng Liu", "authorId": "2108101945"}, {"name": "Rui Wen", "authorId": "2054749404"}, {"name": "Xinlei He", "authorId": "2116553732"}, {"name": "A. Salem", "authorId": "66697271"}, {"name": "Zhikun Zhang", "authorId": "48806102"}, {"name": "M. Backes", "authorId": "144588806"}, {"name": "Emiliano De Cristofaro", "authorId": "1728207"}, {"name": "Mario Fritz", "authorId": "1739548"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 132}, "snippets": ["Membership Inference (MemInf) (Shokri et al., 2016) against ML models involves an adversary aiming to determine whether or not a target data sample was used to train a target ML model. More formally, given a target data sample x target , (the access to) a target model M , and an auxiliary dataset D aux , a membership inference attack can be defined as: \n\nwhere M \u2208 {M B , M W } and D aux \u2208 {D P aux , D S aux }. Membership inference has been extensively studied in literature (Chen et al., 2019)(Chen et al., 2020)(Jia et al., 2019)(Leino et al., 2019)(Li et al., 2020)(Nasr et al., 2018)(Sablayrolles et al., 2019)(Salem et al., 2018)(Shokri et al., 2016). Inferring membership of a target sample prompts severe privacy threats; for instance, if an ML model for drug dose prediction is trained using data from patients with a certain disease, then inclusion in the training set inherently leaks the individuals' health status. Overall, MemInf is also often a signal that a target model is \"leaky\" and can be a gateway to additional attacks [10]."], "score": 0.99853515625}, {"id": "(He et al., 2022)", "paper": {"corpus_id": 251719178, "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Xinlei He", "authorId": "2116553732"}, {"name": "Zheng Li", "authorId": "2146247989"}, {"name": "Weilin Xu", "authorId": "2110631311"}, {"name": "Cory Cornelius", "authorId": "35372584"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 25}, "snippets": ["In membership inference, the adversary's goal is to infer whether a given data sample is used to train a target model. Currently, membership inference is one of the major methods to evaluate the privacy risks of machine learning models (Hayes et al., 2017)19,(Leino et al., 2019)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017)."], "score": 0.9990234375}, {"id": "(Cilloni et al., 2023)", "paper": {"corpus_id": 265220927, "title": "Privacy Threats in Stable Diffusion Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Thomas Cilloni", "authorId": "1900852898"}, {"name": "Charles Fleming", "authorId": "2061097936"}, {"name": "Charles Walter", "authorId": "2057153656"}], "n_citations": 3}, "snippets": ["The goal of a membership inference attack (MIA) is to recognize what data samples were used in training a model, ignoring those that were not. MI attacks are the most fundamental attack on data privacy and are widely used for measuring the privacy of a training dataset."], "score": 0.99755859375}, {"id": "(Truex et al., 2018)", "paper": {"corpus_id": 50778569, "title": "Towards Demystifying Membership Inference Attacks", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "Stacey Truex", "authorId": "25121568"}, {"name": "Ling Liu", "authorId": "46458150"}, {"name": "M. E. Gursoy", "authorId": "2327300"}, {"name": "Lei Yu", "authorId": "2112532900"}, {"name": "Wenqi Wei", "authorId": "47747953"}], "n_citations": 112}, "snippets": ["In this section, we formalize membership inference attacks against machine learning models as follows: Given an instance x and blackbox access to a classification model F t trained on a dataset D, can an adversary infer with high confidence that the instance x was contained in D at the train time of F t ? This definition states that membership inference focuses on the question of the membership of x in D and not about the contents of x. This divergence separates membership inference from existing areas of privacy research, such as differential privacy (Blum et al., 2005), (Vaidya et al., 2014), (Dwork, 2008) or secure multiparty computation (Wu et al., 2016), (Cock et al., 2019), [9]. Also notable is that membership inference attacks are at the local level: an adversary wishes to know if a particular x is in D and not D in its entirety."], "score": 0.99951171875}, {"id": "(Long et al., 2018)", "paper": {"corpus_id": 3619789, "title": "Understanding Membership Inferences on Well-Generalized Learning Models", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "Yunhui Long", "authorId": "3147214"}, {"name": "Vincent Bindschaedler", "authorId": "3094927"}, {"name": "Lei Wang", "authorId": "2152507640"}, {"name": "Diyue Bu", "authorId": "3203018"}, {"name": "Xiaofeng Wang", "authorId": "50141047"}, {"name": "Haixu Tang", "authorId": "2112389071"}, {"name": "Carl A. Gunter", "authorId": "1785347"}, {"name": "Kai Chen", "authorId": "145126969"}], "n_citations": 224}, "snippets": ["Membership Inference Attack (MIA) determines the presence of a record in a machine learning model's training data by querying the model. Prior work has shown that the attack is feasible when the model is overfitted to its training data or when the adversary controls the training algorithm. However, when the model is not overfitted and the adversary does not control the training algorithm, the threat is not well understood.\n\nIn a membership inference attack, the adversary's goal is to infer the membership status of a target individuals data in the input dataset to some computation. For a survey, the adversary wishes to ascertain, from aggregate survey responses, whether the individual participated in the survey. For machine learning, the adversary wishes to ascertain whether the target's record was part of the dataset used to train a specific model."], "score": 0.99755859375}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 237563320, "title": "Membership Leakage in Label-Only Exposures", "year": 2020, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Zheng Li", "authorId": "2146247989"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 246}, "snippets": ["Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision."], "score": 0.998046875}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 276928094, "title": "Efficient Membership Inference Attacks by Bayesian Neural Network", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhenlong Liu", "authorId": "2283439172"}, {"name": "Wenyu Jiang", "authorId": "2283510083"}, {"name": "Feng Zhou", "authorId": "2350314174"}, {"name": "Hongxin Wei", "authorId": "2283524907"}], "n_citations": 1}, "snippets": ["Membership Inference Attacks (MIAs) aim to estimate whether a specific data point was used in the training of a given model."], "score": 0.9970703125}], "table": null}, {"title": "Attack Mechanism and Methodologies", "tldr": "Membership inference attacks employ various methodologies including shadow modeling, metric-based comparisons, and machine learning classifiers to detect differences in model behavior between training and non-training data. These attacks range from black-box approaches that only access model outputs to white-box approaches that leverage internal model parameters. (24 sources)", "text": "\nMembership inference attacks operate on a fundamental principle: machine learning models tend to behave differently on data they were trained on versus data they haven't seen before. This difference forms the basis for detecting whether a specific data sample was part of the training dataset <Paper corpusId=\"165163934\" paperTitle=\"(Song et al., 2019)\" isShortName></Paper> <Paper corpusId=\"210942951\" paperTitle=\"(Farokhi et al., 2020)\" isShortName></Paper>. Formally, a membership inference attack can be defined as a function that takes a data point, access to a target model, and potentially some background knowledge to determine whether the data point was used to train the model <Paper corpusId=\"195346528\" paperTitle=\"(Yeom et al., 2017)\" isShortName></Paper> <Paper corpusId=\"251066729\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper>.\n\nOne of the most influential attack methodologies was introduced by Shokri et al., who designed an approach based on \"shadow training\" <Paper corpusId=\"165163934\" paperTitle=\"(Song et al., 2019)\" isShortName></Paper> <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>. This technique involves three main steps: (1) training multiple shadow models that simulate the behavior of the target model, (2) creating a labeled dataset of member versus non-member predictions based on these shadow models, and (3) training an inference model (typically a neural network) to distinguish between members and non-members based on the target model's prediction vector <Paper corpusId=\"227152059\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper>. This approach essentially treats membership inference as a binary classification problem <Paper corpusId=\"258236265\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nSubsequent research has refined and simplified this approach. Salem et al. optimized the attack by reducing the number of required shadow models from multiple to just one, making the attack more practical <Paper corpusId=\"246706163\" paperTitle=\"(Zhou et al., 2022)\" isShortName></Paper> <Paper corpusId=\"46933970\" paperTitle=\"(Salem et al., 2018)\" isShortName></Paper>. Yeom et al. proposed a simpler inference method that compares the prediction confidence value of a target example with a threshold (which can be learned through shadow training), where higher confidence suggests membership <Paper corpusId=\"165163934\" paperTitle=\"(Song et al., 2019)\" isShortName></Paper> <Paper corpusId=\"2656445\" paperTitle=\"(Yeom et al._1, 2017)\" isShortName></Paper>.\n\nThe attack settings can be categorized into black-box and white-box approaches:\n\n1. Black-box attacks: These rely solely on the outputs (predictions or confidence scores) of the target model <Paper corpusId=\"250048543\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. The attacker queries the model with the target data sample and analyzes the response patterns to infer membership <Paper corpusId=\"218674569\" paperTitle=\"(Huang et al., 2020)\" isShortName></Paper>. Recent advances in black-box attacks include label-only membership inference attacks that can operate even when only the predicted labels (without confidence scores) are available <Paper corpusId=\"250048543\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"220831381\" paperTitle=\"(Choquette-Choo et al., 2020)\" isShortName></Paper>.\n\n2. White-box attacks: These leverage access to the model's internal parameters and architecture <Paper corpusId=\"259375769\" paperTitle=\"(Bertran et al., 2023)\" isShortName></Paper>. Nasr et al. designed white-box membership inference attacks that exploit the privacy vulnerabilities of the stochastic gradient descent algorithm used to train deep neural networks <Paper corpusId=\"246706163\" paperTitle=\"(Zhou et al., 2022)\" isShortName></Paper> <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>. Leino et al. utilized the model's idiosyncratic use of features to develop white-box attacks that outperform black-box methods <Paper corpusId=\"246706163\" paperTitle=\"(Zhou et al., 2022)\" isShortName></Paper> <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>.\n\nThe success of these attacks is often attributed to model overfitting, where the model memorizes training data rather than generalizing from it <Paper corpusId=\"221725546\" paperTitle=\"(Tanuwidjaja et al., 2020)\" isShortName></Paper> <Paper corpusId=\"210942951\" paperTitle=\"(Farokhi et al., 2020)\" isShortName></Paper>. However, research has shown that overfitting, while sufficient, is not necessary for successful attacks <Paper corpusId=\"227152059\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"2656445\" paperTitle=\"(Yeom et al._1, 2017)\" isShortName></Paper>.\n\nMembership inference attacks have been applied to various model types beyond standard classification models, including generative models like GANs (Generative Adversarial Networks) and VAEs (Variational Autoencoders) <Paper corpusId=\"266362863\" paperTitle=\"(Al-Kaswan et al., 2023)\" isShortName></Paper> <Paper corpusId=\"199546273\" paperTitle=\"(Hilprecht et al., 2019)\" isShortName></Paper>, as well as language models and image diffusion models <Paper corpusId=\"266362863\" paperTitle=\"(Al-Kaswan et al., 2023)\" isShortName></Paper>. Each model type may require specialized attack techniques, but the fundamental principles remain similar <Paper corpusId=\"251664530\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper>.\n\nRecent developments have focused on improving attack efficacy through more sophisticated methods. Carlini et al. developed the Likelihood Ratio Attack (LiRA) that combines multiple techniques to achieve better performance, especially at low false-positive rates <Paper corpusId=\"270199828\" paperTitle=\"(Anderson et al., 2024)\" isShortName></Paper> <Paper corpusId=\"244920593\" paperTitle=\"(Carlini et al., 2021)\" isShortName></Paper>. Other advances include attacks that utilize the entire training trajectory of a model rather than just its final state <Paper corpusId=\"272423578\" paperTitle=\"(Wen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"251953448\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Song et al., 2019)", "paper": {"corpus_id": 165163934, "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples", "year": 2019, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Liwei Song", "authorId": "144173853"}, {"name": "R. Shokri", "authorId": "2520493"}, {"name": "Prateek Mittal", "authorId": "143615345"}], "n_citations": 244}, "snippets": ["For a target machine learning model, the membership inference attacks aim to determine whether a given data point was used to train the model or not (Hayes et al., 2017)32,(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Yeom et al., 2017). The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. (Shokri et al., 2016) design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. (Yeom et al., 2017) suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership."], "score": 0.99853515625}, {"id": "(Farokhi et al., 2020)", "paper": {"corpus_id": 210942951, "title": "Modelling and Quantifying Membership Information Leakage in Machine Learning", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "F. Farokhi", "authorId": "1803792"}, {"name": "M. K\u00e2afar", "authorId": "1708760"}], "n_citations": 24}, "snippets": ["Membership inference attacks, a class of adversarial inference algorithms designed to distinguish data used for training a machine learning model, have recently gained much attention (Shokri et al., 2016)(Truex et al., 2018)(Salem et al., 2018)(Sablayrolles et al., 2019). These attacks have been deployed on various machine learning models; see, e.g., (Shokri et al., 2016)(Hayes et al., 2017)(Chen et al., 2019)(Hilprecht et al., 2019)(Liu et al., 2019)(Backes et al., 2016). The success of the attacks is often attributed to that a machine learning model behaves differently on the training dataset and the test dataset, e.g., it shows higher confidence on the training dataset due to an array of reasons, such as over-fitting."], "score": 0.9970703125}, {"id": "(Yeom et al., 2017)", "paper": {"corpus_id": 195346528, "title": "The Unintended Consequences of Overfitting: Training Data Inference Attacks", "year": 2017, "venue": "arXiv.org", "authors": [{"name": "Samuel Yeom", "authorId": "26378728"}, {"name": "Matt Fredrikson", "authorId": "2623167"}, {"name": "S. Jha", "authorId": "1680133"}], "n_citations": 40}, "snippets": ["In a membership inference attack, the adversary attempts to infer whether a specific point was included in the dataset used to train a given model. The adversary is given a data point z = (x, y), access to a model A S , the size of the model's training set |S| = n, and the distribution D that the training set was drawn from. With this information the adversary must decide whether z \u2208 S. For the purposes of this discussion, we do not distinguish whether A's access to A S is \"black-box\", i.e., consisting only of input/output queries, or \"white-box\", i.e., involving the internal structure of the model itself. However, unless otherwise noted all of the attacks presented in this section assume black-box access."], "score": 0.9970703125}, {"id": "(He et al._1, 2022)", "paper": {"corpus_id": 251066729, "title": "Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning", "year": 2022, "venue": "European Conference on Computer Vision", "authors": [{"name": "Xinlei He", "authorId": "2116553732"}, {"name": "Hongbin Liu", "authorId": "2110279247"}, {"name": "N. Gong", "authorId": "144516687"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 16}, "snippets": ["In membership inference attack, the adversary aims to determine whether a given data sample x belongs to the target model T 's training dataset or not given the adversary's background knowledge K. A data sample x is called member (or non-member ) if it belongs to (or does not belong to) the training dataset of the target model T . Formally, we define the membership inference attack as A : x, T , K \u2192 {0, 1}, where the attack A is essentially a mapping function and 1 (or 0) means the data sample x is a member (or non-member)."], "score": 0.99951171875}, {"id": "(Shokri et al., 2016)", "paper": {"corpus_id": 10488675, "title": "Membership Inference Attacks Against Machine Learning Models", "year": 2016, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "R. Shokri", "authorId": "2520493"}, {"name": "M. Stronati", "authorId": "34828439"}, {"name": "Congzheng Song", "authorId": "3469125"}, {"name": "Vitaly Shmatikov", "authorId": "1723945"}], "n_citations": 4160}, "snippets": ["We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset."], "score": 0.998046875}, {"id": "(Liu et al., 2020)", "paper": {"corpus_id": 227152059, "title": "When Machine Learning Meets Privacy", "year": 2020, "venue": "ACM Computing Surveys", "authors": [{"name": "B. Liu", "authorId": "145306564"}, {"name": "Ming Ding", "authorId": "145633124"}, {"name": "Sina Shaham", "authorId": "40221713"}, {"name": "Wenny Rahayu", "authorId": "2352525282"}, {"name": "F. Farokhi", "authorId": "1803792"}, {"name": "Zihuai Lin", "authorId": "1740858"}], "n_citations": 290}, "snippets": ["Membership inference attack refers to acquiring the knowledge about whether a certain data record ( \u00ec  \u2605 ,  \u2605 ) belongs to the model's training dataset  or not (Melis et al., 2018)(Shokri et al., 2016). An illustration of such an attack can be found in Fig. 5(c). \n\nShokri et al. (Shokri et al., 2016) introduced a \"black-box membership inference\" that used a shadow training technique to imitate the behavior of the target model. The trained inference model is used \"to recognize differences in the target model's predictions\" on training and non-training inputs. They also found that overfitting, the structure and type of the model are the main factors that cause a model to be vulnerable to membership inference attack. Long et al. [89] and Yeom et al. (Yeom et al., 2017) investigated \"the relationship between overfitting and privacy leakage\". Salem et al. (Salem et al., 2018) proposed a membership inference attack method using an unsupervised binary classification, \"which does not need to train any shadow model and does not assume knowledge of model or data distribution\"."], "score": 0.99853515625}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 258236265, "title": "Selective and Collaborative Influence Function for Efficient Recommendation Unlearning", "year": 2023, "venue": "Expert systems with applications", "authors": [{"name": "Yuyuan Li", "authorId": "1527113700"}, {"name": "Chaochao Chen", "authorId": "1694815"}, {"name": "Xiaolin Zheng", "authorId": "1687974"}, {"name": "Yizhao Zhang", "authorId": "2214803049"}, {"name": "Biao Gong", "authorId": "2211782518"}, {"name": "Jun Wang", "authorId": "2152811080"}], "n_citations": 26}, "snippets": ["Membership inference is a well-acknowledged method used to analyze information leakage from a trained model (Yu et al., 2021). Specifically, given a trained model (target) and a data point (query), membership inference determines whether this point was in the model's training dataset. Membership inference attack against machine learning models was pioneered by (Shokri et al., 2016). The main idea is regarding the membership inference problem as a binary classification task, and using machine learning classifiers to attack the target machine learning model."], "score": 0.9970703125}, {"id": "(Zhou et al., 2022)", "paper": {"corpus_id": 246706163, "title": "PPA: Preference Profiling Attack Against Federated Learning", "year": 2022, "venue": "Network and Distributed System Security Symposium", "authors": [{"name": "Chunyi Zhou", "authorId": "1845880105"}, {"name": "Yansong Gao", "authorId": "39922366"}, {"name": "Anmin Fu", "authorId": "2068511826"}, {"name": "Kai Chen", "authorId": "2157740727"}, {"name": "Zhiyang Dai", "authorId": "151498397"}, {"name": "Zhi Zhang", "authorId": "2116763991"}, {"name": "Minhui Xue", "authorId": "2837434"}, {"name": "Yuqing Zhang", "authorId": "2155342827"}], "n_citations": 23}, "snippets": ["The membership inference attack (Shokri et al., 2016) proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. (Salem et al., 2018) optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. (Nasr et al., 2018) designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. (Chen et al., 2019) provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. (Leino et al., 2019) utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. (Pyrgelis et al., 2017) focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks (Hu et al., 2021), (Hayes et al., 2017), (Hilprecht et al., 2019) attacked generative model under the white-box and blackbox settings."], "score": 0.99755859375}, {"id": "(Salem et al., 2018)", "paper": {"corpus_id": 46933970, "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models", "year": 2018, "venue": "Network and Distributed System Security Symposium", "authors": [{"name": "A. Salem", "authorId": "66697271"}, {"name": "Yang Zhang", "authorId": "2145954003"}, {"name": "Mathias Humbert", "authorId": "144887171"}, {"name": "Mario Fritz", "authorId": "1739548"}, {"name": "M. Backes", "authorId": "144588806"}], "n_citations": 950}, "snippets": ["Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications.\n\nIn this paper, we concentrate on one such attack, namely membership inference attack. In this setting, an adversary aims to determine whether a data item (also referred to as a data point) was used to train an ML model or not. Successful membership inference attacks can cause severe consequences. For instance, if a machine learning model is trained on the data collected from people with a certain disease, by knowing that a victim's data belong to the training data of the model, the attacker can immediately learn this victim's health status."], "score": 0.99853515625}, {"id": "(Yeom et al._1, 2017)", "paper": {"corpus_id": 2656445, "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting", "year": 2017, "venue": "IEEE Computer Security Foundations Symposium", "authors": [{"name": "Samuel Yeom", "authorId": "26378728"}, {"name": "Irene Giacomelli", "authorId": "3025831"}, {"name": "Matt Fredrikson", "authorId": "2623167"}, {"name": "S. Jha", "authorId": "1680133"}], "n_citations": 1133}, "snippets": ["Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."], "score": 0.0}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 250048543, "title": "Debiasing Learning for Membership Inference Attacks Against Recommender Systems", "year": 2022, "venue": "Knowledge Discovery and Data Mining", "authors": [{"name": "Zihan Wang", "authorId": null}, {"name": "Na Huang", "authorId": "2173325427"}, {"name": "Fei Sun", "authorId": "143770118"}, {"name": "Pengjie Ren", "authorId": "1749477"}, {"name": "Zhumin Chen", "authorId": "1721165"}, {"name": "Hengliang Luo", "authorId": "2889630"}, {"name": "M. de Rijke", "authorId": "1696030"}, {"name": "Z. Ren", "authorId": "2780667"}], "n_citations": 14}, "snippets": ["The goal of membership inference attacks is to infer the membership of individual training samples for a target model. Shokri et al. (Shokri et al., 2016) specify the first membership inference attack against machine learning models. The authors propose a general formulation of membership inference attack against machine learning models, and train multiple shadow models to simulate the target model's behavior. In that case, the training sets for multiple attack models (one for each class) are generated. Salem et al. (Salem et al., 2018) further relax several key assumptions from (Shokri et al., 2016), including knowledge of the target model architecture and target dataset distribution. Yeom et al. (Yeom et al., 2017) explore the relationship between attack performance and overfitting, and propose the first decisionbased attack. Nasr et al. (Nasr et al., 2018) study membership inference attacks in both black-box and white-box settings. Instead of using output scores, several recent membership attacks (Choquette-Choo et al., 2020)(Li et al., 2020) assume only predicted hard labels of models are exposed, and demonstrate that label-only exposures are also vulnerable to membership leakage. In addition, Zhang et al. (Zhang et al., 2021) investigate MIA against recommender systems, leveraging the differences between user history behaviors and output items from recommenders."], "score": 0.99755859375}, {"id": "(Huang et al., 2020)", "paper": {"corpus_id": 218674569, "title": "DAMIA: Leveraging Domain Adaptation as a Defense Against Membership Inference Attacks", "year": 2020, "venue": "IEEE Transactions on Dependable and Secure Computing", "authors": [{"name": "Hongwei Huang", "authorId": "2146281955"}, {"name": "Weiqi Luo", "authorId": "144644709"}, {"name": "Guoqiang Zeng", "authorId": "48035994"}, {"name": "J. Weng", "authorId": "145369053"}, {"name": "Yue Zhang", "authorId": "2145913542"}, {"name": "Anjia Yang", "authorId": "2145970"}], "n_citations": 25}, "snippets": ["Membership inference attack is a type of attack against deep learning models, which can be deployed to determine whether a sample is from the training set of a victim model. The basic idea of the attack is that the information exposed by the model contains the abundant information of the training data, based on which an attacker may perform membership inferences. Theoretically, all characteristics of the victim model such as activation values, affine outputs, gradients or even the model's transparency report, can be utilized by attackers to deploy the attack [15,19]20]. Given that most of the above characteristics are not publicly accessible, attackers may solely rely on the outputs of the model to deploy the attack in practice."], "score": 0.9990234375}, {"id": "(Choquette-Choo et al., 2020)", "paper": {"corpus_id": 220831381, "title": "Label-Only Membership Inference Attacks", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Christopher A. Choquette-Choo", "authorId": "1415982317"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}, {"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Nicolas Papernot", "authorId": "1967156"}], "n_citations": 516}, "snippets": ["Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model."], "score": 0.998046875}, {"id": "(Bertran et al., 2023)", "paper": {"corpus_id": 259375769, "title": "Scalable Membership Inference Attacks via Quantile Regression", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Mart\u00edn Bertr\u00e1n", "authorId": "37335063"}, {"name": "Shuai Tang", "authorId": "46321498"}, {"name": "Michael Kearns", "authorId": "81338045"}, {"name": "Jamie Morgenstern", "authorId": "144848816"}, {"name": "Aaron Roth", "authorId": "1682008"}, {"name": "Zhiwei Steven Wu", "authorId": "1768074"}], "n_citations": 50}, "snippets": ["Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data."], "score": 0.99853515625}, {"id": "(Nasr et al., 2018)", "paper": {"corpus_id": 133091488, "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning", "year": 2018, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "Milad Nasr", "authorId": "3490923"}, {"name": "R. Shokri", "authorId": "2520493"}, {"name": "Amir Houmansadr", "authorId": "1972973"}], "n_citations": 1452}, "snippets": ["Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."], "score": 0.0}, {"id": "(Leino et al., 2019)", "paper": {"corpus_id": 195699554, "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference", "year": 2019, "venue": "USENIX Security Symposium", "authors": [{"name": "Klas Leino", "authorId": "35802340"}, {"name": "Matt Fredrikson", "authorId": "2623167"}], "n_citations": 272}, "snippets": ["Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model."], "score": 0.99658203125}, {"id": "(Tanuwidjaja et al., 2020)", "paper": {"corpus_id": 221725546, "title": "Privacy-Preserving Deep Learning on Machine Learning as a Service\u2014a Comprehensive Survey", "year": 2020, "venue": "IEEE Access", "authors": [{"name": "Harry Chandra Tanuwidjaja", "authorId": "3385931"}, {"name": "Rakyong Choi", "authorId": "36105968"}, {"name": "Seunggeun Baek", "authorId": "30708702"}, {"name": "Kwangjo Kim", "authorId": "1741995"}], "n_citations": 82}, "snippets": ["Generally, membership inference means deciding whether given data were used for generating some aggregation of the data (or not). In the context of deep learning, a model itself (including the model parameters) can be regarded as the 'aggregation' of the training data. Therefore, membership inference attacks on DL models indicate attacks to decide whether given data belong to the training dataset (or not). Shokri et al. (Shokri et al., 2016) provided one of the first suggestions of membership inference attacks. \n\nMembership inference attacks are the attacks for the models violating the first security goal of PPDL. Stronger versions of membership inference attacks include extraction of some properties of sensitive training data or even recovery of the training data, which can be reduced to normal membership inference attacks. Usually, membership inference attacks harness overfitting during training, producing a difference in accuracy between the training data and the other data."], "score": 0.99658203125}, {"id": "(Al-Kaswan et al., 2023)", "paper": {"corpus_id": 266362863, "title": "Traces of Memorisation in Large Language Models for Code", "year": 2023, "venue": "International Conference on Software Engineering", "authors": [{"name": "Ali Al-Kaswan", "authorId": "2199249915"}, {"name": "M. Izadi", "authorId": "145774460"}, {"name": "Arie van Deursen", "authorId": "10734708"}], "n_citations": 17}, "snippets": ["Membership inference attacks are a type of attack that aims to determine whether a specific data point was included in the training data of a machine learning model. The goal of these attacks is to infer whether a given data point was used to train the model or not, without having access to the training data itself.\n\nThe first membership inference attack against machine learning models was proposed by Shokri et al. to target classification models deployed by Machine Learning as a Service (MLaaS) providers [45]. Since then the field has expanded and attacks have been proposed that target generative models [24] and LLMs [25]. Recently, membership inference attacks have been proposed against transformerbased image diffusion models such as Stable Diffusion [18]."], "score": 0.9990234375}, {"id": "(Hilprecht et al., 2019)", "paper": {"corpus_id": 199546273, "title": "Monte Carlo and Reconstruction Membership Inference Attacks against Generative Models", "year": 2019, "venue": "Proceedings on Privacy Enhancing Technologies", "authors": [{"name": "Benjamin Hilprecht", "authorId": "81786870"}, {"name": "Martin H\u00e4rterich", "authorId": "2736329"}, {"name": "Daniel Bernau", "authorId": "13047311"}], "n_citations": 191}, "snippets": ["Abstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples."], "score": 0.0}, {"id": "(Lu et al., 2022)", "paper": {"corpus_id": 251664530, "title": "Label\u2010only membership inference attacks on machine unlearning without dependence of posteriors", "year": 2022, "venue": "International Journal of Intelligent Systems", "authors": [{"name": "Zhaobo Lu", "authorId": "2143551615"}, {"name": "Hai Liang", "authorId": "145844506"}, {"name": "Minghao Zhao", "authorId": "2152527649"}, {"name": "Qingzhe Lv", "authorId": "2143047511"}, {"name": "Tiancai Liang", "authorId": "47716189"}, {"name": "Yilei Wang", "authorId": "3057294"}], "n_citations": 21}, "snippets": ["Membership inference attack is a privacy attack against machine learning models, which exposes users' data. Formally, given a data sample x and a trained model , the adversary  can calculate the membership state according to the additional knowledge \u03a9: \n\nwhere 1 means that x belongs to the training set of , otherwise not. Membership inference attacks have been extensively studied in various fields, such as generative adversarial networks, 13,14,23 distributed recommender systems, 12 natural language processing, 29 and computer vision segmentation. 27"], "score": 0.9990234375}, {"id": "(Anderson et al., 2024)", "paper": {"corpus_id": 270199828, "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation", "year": 2024, "venue": "International Conference on Information Systems Security and Privacy", "authors": [{"name": "Maya Anderson", "authorId": "39868788"}, {"name": "Guy Amit", "authorId": "2291066051"}, {"name": "Abigail Goldsteen", "authorId": "2652502"}], "n_citations": 19}, "snippets": ["Membership inference attacks (Shokri et al., 2016)(Hu et al., 2021) are a type of privacy threat, where an attacker aims to determine whether a specific data record was used in the training set of a machine learning model. This carries significant privacy implications as it can potentially reveal sensitive information about individuals, even if the model does not directly release any personal data. \n\nFormally, an attacker aims to determine the membership of a sample x in the training data D m of a target model m, i.e., to check if x \u2208 D m . This is known as sample-level membership inference. Typically, these attacks involve calculating one or more metrics on the target model's outputs that reflect the probability of the sample being a part of the training set, such as the model outputs' entropy or log-probabilities (Carlini et al., 2021). Several metrics may be computed for each sample and then fused together using a machine learning model, known as an attack model, which in turn outputs the probability of a sample being a member of the training set."], "score": 0.99853515625}, {"id": "(Carlini et al., 2021)", "paper": {"corpus_id": 244920593, "title": "Membership Inference Attacks From First Principles", "year": 2021, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Steve Chien", "authorId": "2059189068"}, {"name": "Milad Nasr", "authorId": "3490923"}, {"name": "Shuang Song", "authorId": "144206374"}, {"name": "A. Terzis", "authorId": "1763579"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}], "n_citations": 708}, "snippets": ["The objective of a membership inference attack (MIA) [60] is to predict if a specific training example was, or was not, used as training data in a particular model. This makes MIAs the simplest and most widely deployed attack for auditing training data privacy."], "score": 0.9990234375}, {"id": "(Wen et al., 2024)", "paper": {"corpus_id": 272423578, "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?", "year": 2024, "venue": "Network and Distributed System Security Symposium", "authors": [{"name": "Rui Wen", "authorId": "2054749404"}, {"name": "Michael Backes", "authorId": "2257034706"}, {"name": "Yang Zhang", "authorId": "2257291195"}], "n_citations": 2}, "snippets": ["Membership Inference Attack (MIA) [12,31,34,50,55,70] is a prominent privacy attack utilized to determine whether a specific data sample belongs to a training dataset. This attack is widely employed to assess the privacy of training data due to its simplicity and broad applicability.\n\nIn the attack scenario, the adversary A is granted access to a target model and is tasked with determining the membership status of a given data sample (x, y). Formally, the membership inference attack can be defined as a security game, referred to as Membership Inference Security Game, which is described as follows: Definition 4.1 (Membership Inference Security Game [7])."], "score": 0.99853515625}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 251953448, "title": "Membership Inference Attacks by Exploiting Loss Trajectory", "year": 2022, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Yiyong Liu", "authorId": "2182511319"}, {"name": "Zhengyu Zhao", "authorId": "2277275"}, {"name": "M. Backes", "authorId": "144588806"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 111}, "snippets": ["Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios."], "score": 0.0}], "table": null}, {"title": "Privacy Implications and Risks", "tldr": "Membership inference attacks pose serious privacy risks by potentially revealing sensitive personal information about individuals whose data was used to train machine learning models. These attacks are particularly concerning in domains like healthcare, where revealing membership in a training dataset could expose an individual's medical conditions. (11 sources)", "text": "\nMembership inference attacks represent a significant privacy threat that extends beyond simply determining whether a data sample was in a training set. The privacy implications are particularly severe when the training data contains sensitive information about individuals <Paper corpusId=\"253244424\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>. For example, if an attacker can infer that a clinical record or an individual was used to train a model associated with a specific disease, this directly exposes private health information about that individual <Paper corpusId=\"253244424\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>.\n\nThese attacks enable adversaries to access confidential training data or even replicate processing models by using only publicly accessible services <Paper corpusId=\"218473513\" paperTitle=\"(Xue et al., 2020)\" isShortName></Paper>. The fundamental privacy concern is that machine learning models can unintentionally memorize and leak information about the data they were trained on <Paper corpusId=\"267069190\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. This \"unintended membership exposure\" can lead to \"catastrophic privacy loss for individuals\" in real-world applications <Paper corpusId=\"253244424\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>.\n\nDue to these serious privacy implications, membership inference attacks have become widely adopted as basic metrics to quantify privacy exposure in statistical data analysis algorithms <Paper corpusId=\"253244424\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>. These attacks demonstrate that, under mild assumptions about the target model, it is possible to identify portions of the training dataset, potentially leading to leakage of private data <Paper corpusId=\"269756904\" paperTitle=\"(Galichin et al., 2024)\" isShortName></Paper>.\n\nRecent research has expanded the understanding of membership inference risks by exploring new attack vectors. Pawelczyk et al. introduced \"counterfactual distance-based attacks\" that leverage algorithmic recourse to determine if an instance belongs to a model's training data <Paper corpusId=\"253446930\" paperTitle=\"(Pawelczyk et al., 2022)\" isShortName></Paper>. Such advances highlight the evolving nature of privacy threats in machine learning.\n\nThe vulnerability to membership inference attacks also implies a model's potential to leak other private information outside the specific context of membership, an idea that aligns closely with the definition of differential privacy as a worst-case certification <Paper corpusId=\"275471364\" paperTitle=\"(Hong et al., 2025)\" isShortName></Paper>. This means that models vulnerable to membership inference may be susceptible to other types of privacy breaches as well.\n\nMembership inference attacks are particularly problematic in certain domains and settings. In federated learning environments, where data remains distributed across multiple devices or organizations, these attacks pose a significant threat to the confidentiality of local training data <Paper corpusId=\"274869679\" paperTitle=\"(Xia et al., 2024)\" isShortName></Paper>. Similarly, in healthcare applications, membership inference attacks could reveal whether a patient's data was used to train a model related to sensitive conditions <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>.\n\nThe privacy risks extend to various model types beyond standard classification models. Research has shown that generative models, including GANs (Generative Adversarial Networks), can also be vulnerable to membership inference attacks <Paper corpusId=\"52211986\" paperTitle=\"(Hayes et al., 2017)\" isShortName></Paper>. These attacks against generative models exploit the models' capacity to learn statistical differences in distributions, enabling adversaries to detect overfitting and recognize inputs that were part of training datasets <Paper corpusId=\"52211986\" paperTitle=\"(Hayes et al., 2017)\" isShortName></Paper>.\n\nAs machine learning models become more integrated into privacy-critical applications like face recognition and medical image analysis, the potential impact of membership inference attacks grows <Paper corpusId=\"237563320\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. To address these risks, researchers have developed various defense mechanisms, though finding effective protections that maintain model utility remains an active area of research <Paper corpusId=\"256627812\" paperTitle=\"(Matsumoto et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Chen et al., 2022)", "paper": {"corpus_id": 253244424, "title": "Amplifying Membership Exposure via Data Poisoning", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yufei Chen", "authorId": "1711982"}, {"name": "Chao Shen", "authorId": "2088079836"}, {"name": "Yun Shen", "authorId": "2117688523"}, {"name": "Cong Wang", "authorId": "2116638601"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 32}, "snippets": ["In a membership inference attack, an attacker aims to infer whether a specific sample (x, y) belongs to the training dataset D train at the test time (Li et al., 2020)(Salem et al., 2018)(Shokri et al., 2016). Unintended membership exposure causes catastrophic privacy loss for individuals. For example, in the real world, a data sample x can be a clinical record or an individual. Membership inference attacks enable the attackers to infer whether this clinical record or individual has been used to train a model associated with a certain disease. As such, these attacks are widely adopted as basic metrics to quantify privacy exposure in statistical data analysis algorithms [13](Li et al., 2013)."], "score": 0.99658203125}, {"id": "(Shokri et al., 2016)", "paper": {"corpus_id": 10488675, "title": "Membership Inference Attacks Against Machine Learning Models", "year": 2016, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "R. Shokri", "authorId": "2520493"}, {"name": "M. Stronati", "authorId": "34828439"}, {"name": "Congzheng Song", "authorId": "3469125"}, {"name": "Vitaly Shmatikov", "authorId": "1723945"}], "n_citations": 4160}, "snippets": ["We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset."], "score": 0.998046875}, {"id": "(Xue et al., 2020)", "paper": {"corpus_id": 218473513, "title": "Machine Learning Security: Threats, Countermeasures, and Evaluations", "year": 2020, "venue": "IEEE Access", "authors": [{"name": "Mingfu Xue", "authorId": "46301056"}, {"name": "Chengxiang Yuan", "authorId": "1984508"}, {"name": "Heyi Wu", "authorId": "1421758751"}, {"name": "Yushu Zhang", "authorId": "2108051942"}, {"name": "Weiqiang Liu", "authorId": "2109335717"}], "n_citations": 124}, "snippets": ["Liu et al. (Liu et al., 2015) illustrate security threats in cognitive systems. Specifically, they show that attacker can access to confidential training data or replicate the processing model by using only the public accessible services of the model. Shokri et al. (Shokri et al., 2016) propose the so called membership inference attack, in which the adversary can estimate whether a given data is in the training set of a target model. Particularly, they use the target model's prediction of training and nontraining data to train a membership inference model (Shokri et al., 2016). According to the output of the target model, the generated membership inference model can identify the differences in the prediction of the target model on its training data and the data that hasn't been used for its training. The membership inference attack proposed in (Shokri et al., 2016) is generic, but the success of member inference attacks depends on the overfitting of the model (Yeom et al., 2017), (Shokri et al., 2016). If it is a well-generalized model, the success rate of the membership inference attack is low."], "score": 0.9970703125}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 267069190, "title": "Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chao Liu", "authorId": "2281289264"}, {"name": "Boxi Chen", "authorId": "2280335814"}, {"name": "Wei Shao", "authorId": "2280146329"}, {"name": "Chris Zhang", "authorId": "2280200066"}, {"name": "Kelvin Wong", "authorId": "2264107865"}, {"name": "Yi Zhang", "authorId": "2280349425"}], "n_citations": 3}, "snippets": ["Membership Inference Attacks have emerged as a noteworthy concern. At the core of membership inference attacks is the attacker's ability to infer whether a specific data point was part of the training dataset used to train an ML model. A visual representation of this process can be found in Fig. 6. In other words, membership inference attacks are based on methods in which the target model learns features in the training dataset. The attacker constructs his training dataset and observes the model's output on these samples to infer whether the samples in the test dataset belong to the membership samples. This might sound innocuous at first, but consider scenarios where the training data contains sensitive information -revealing whether a particular data point (like a patient's medical record) was used in training could lead to significant privacy breaches."], "score": 0.998046875}, {"id": "(Galichin et al., 2024)", "paper": {"corpus_id": 269756904, "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Andrey V. Galichin", "authorId": "2301151068"}, {"name": "Mikhail Aleksandrovich Pautov", "authorId": "134451470"}, {"name": "Alexey Zhavoronkin", "authorId": "2301152807"}, {"name": "Oleg Y. Rogov", "authorId": "2279751953"}, {"name": "Ivan Oseledets", "authorId": "2257279038"}], "n_citations": 2}, "snippets": ["The objective of the membership inference attack (MIA) (Shokri et al., 2016) is to determine whether a specific data sample was presented in the training data of the target model or no.MIAs demonstrate that, under mild assumptions about the target model, it is possible to identify a part of its training dataset, leading to possible leakage of private data.To broaden the scope of practical applications of neural networks, it is important to have a reliable tool to assess their vulnerability to the leakage of private training data."], "score": 0.9970703125}, {"id": "(Pawelczyk et al., 2022)", "paper": {"corpus_id": 253446930, "title": "On the Privacy Risks of Algorithmic Recourse", "year": 2022, "venue": "International Conference on Artificial Intelligence and Statistics", "authors": [{"name": "Martin Pawelczyk", "authorId": "89583148"}, {"name": "Himabindu Lakkaraju", "authorId": "1892673"}, {"name": "Seth Neel", "authorId": "5880154"}], "n_citations": 31}, "snippets": ["In this work, we address the aforementioned gaps by initiating a study of if and how an adversary can leverage algorithmic recourses to leak sensitive information about the training data of the underlying model. To this end, we introduce a general class of membership inference attacks called counterfactual distance-based attacks which leverage algorithmic recourse to determine if an instance belongs to the training data of the underlying model or not."], "score": 0.99755859375}, {"id": "(Hong et al., 2025)", "paper": {"corpus_id": 275471364, "title": "Understanding and Mitigating Membership Inference Risks of Neural Ordinary Differential Equations", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Sanghyun Hong", "authorId": "2275287112"}, {"name": "Fan Wu", "authorId": "2327330670"}, {"name": "Anthony Gruber", "authorId": "2303406541"}, {"name": "Kookjin Lee", "authorId": "2265425696"}], "n_citations": 0}, "snippets": ["Membership inference attacks aim to determine whether a specific example is a member of the training data. To do this, the attacker exploits the difference in the target model's response to the specific example when it is a member versus a non-member. Membership inference can be considered a threat on its own, but a model's vulnerability to inference attacks also implies its potential to leak other private information outside of this context, an idea which aligns closely with the definition of differential privacy (Dwork, 2006) as a worst-case certification."], "score": 0.99755859375}, {"id": "(Xia et al., 2024)", "paper": {"corpus_id": 274869679, "title": "Leveraging Multiple Adversarial Perturbation Distances for Enhanced Membership Inference Attack in Federated Learning", "year": 2024, "venue": "Symmetry", "authors": [{"name": "Fan Xia", "authorId": "2336171446"}, {"name": "Yuhao Liu", "authorId": "2336189090"}, {"name": "Bo Jin", "authorId": "2336172334"}, {"name": "Zheng Yu", "authorId": "2297431838"}, {"name": "Xingwei Cai", "authorId": "2336264478"}, {"name": "Hao Li", "authorId": "2336144088"}, {"name": "Zhiyong Zha", "authorId": "2318042338"}, {"name": "Dai Hou", "authorId": "2336109193"}, {"name": "Kai Peng", "authorId": "2323202813"}], "n_citations": 1}, "snippets": ["Membership inference attacks (MIAs), which aim to determine whether a specific sample is part of the training dataset, pose a significant threat to federated learning."], "score": 0.998046875}, {"id": "(Hayes et al., 2017)", "paper": {"corpus_id": 52211986, "title": "LOGAN: Membership Inference Attacks Against Generative Models", "year": 2017, "venue": "Proceedings on Privacy Enhancing Technologies", "authors": [{"name": "Jamie Hayes", "authorId": "9200194"}, {"name": "Luca Melis", "authorId": "145557680"}, {"name": "G. Danezis", "authorId": "1722262"}, {"name": "Emiliano De Cristofaro", "authorId": "1728207"}], "n_citations": 515}, "snippets": ["Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."], "score": 0.0}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 237563320, "title": "Membership Leakage in Label-Only Exposures", "year": 2020, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Zheng Li", "authorId": "2146247989"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 246}, "snippets": ["Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision."], "score": 0.998046875}, {"id": "(Matsumoto et al., 2023)", "paper": {"corpus_id": 256627812, "title": "Membership Inference Attacks against Diffusion Models", "year": 2023, "venue": "2023 IEEE Security and Privacy Workshops (SPW)", "authors": [{"name": "Tomoya Matsumoto", "authorId": "2166181346"}, {"name": "Takayuki Miura", "authorId": "2204651737"}, {"name": "Naoto Yanai", "authorId": "34800838"}], "n_citations": 60}, "snippets": ["A membership inference attack is a kind of attack whereby an adversary infers whether a particular example was contained in the training dataset of a model (Shokri et al., 2016)- (Carlini et al., 2021). A model vulnerable to the attack potentially contains threats to privacy leakage, and hence recent works discuss membership inference attacks for various machine learning models (Conti et al., 2022), (Li et al., 2022), (Hayes et al., 2017). There are two settings (Shokri et al., 2016), i.e., the white-box setting where an adversary has access to model parameters, and the blackbox setting where he/she utilizes only outputs of the model."], "score": 0.9990234375}], "table": null}, {"title": "Attack Settings and Scenarios", "tldr": "Membership inference attacks can occur in various settings including black-box scenarios where attackers only access model outputs, white-box attacks where model parameters are available, and collaborative learning environments where semi-honest participants can exploit shared model information. (7 sources)", "text": "\nMembership inference attacks can be executed in several distinct settings, each with different adversarial capabilities and access levels:\n\n1. **Black-box Attack Setting**: In this most common scenario, the adversary only has access to the target model through a prediction API without knowledge of the model's architecture or parameters. The attacker queries the model with data samples and analyzes the prediction outputs to determine membership status <Paper corpusId=\"231861713\" paperTitle=\"(He et al._1, 2021)\" isShortName></Paper> <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper> <Paper corpusId=\"46933970\" paperTitle=\"(Salem et al., 2018)\" isShortName></Paper>. This setting is particularly relevant for Machine Learning as a Service (MLaaS) platforms where users can only interact with models through APIs.\n\n2. **White-box Attack Setting**: Here, the adversary has complete access to the target model's architecture and parameters. This provides significantly more information for the attack, allowing for analysis of internal model behavior beyond just the final predictions. White-box attacks can exploit vulnerabilities in training algorithms like stochastic gradient descent <Paper corpusId=\"231861713\" paperTitle=\"(He et al._1, 2021)\" isShortName></Paper> <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>. These attacks are particularly powerful as they can detect subtle patterns in how the model processes different data points.\n\n3. **Collaborative/Federated Learning Setting**: In scenarios where multiple parties jointly train a model while keeping their data private, a semi-honest participant can perform membership inference attacks against other participants' data. The semi-honest party follows the protocol correctly but attempts to learn about others' training data <Paper corpusId=\"244809678\" paperTitle=\"(Shin et al., 2021)\" isShortName></Paper> <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>. Since all participants share model architecture and parameters, a semi-honest party can create ideal shadow models that closely mimic the target model's behavior.\n\n4. **Active vs. Passive Attacks**: Membership inference can be conducted passively by simply observing model outputs, or actively by manipulating inputs or training processes to extract more information. Active attackers in federated learning can even modify their contributions to enhance the success of membership inference attacks <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>.\n\n5. **Generative Model Setting**: Beyond classification models, membership inference attacks target generative sequence models that may memorize and potentially leak training data. These attacks are particularly concerning when such models are trained on sensitive data like private messages or personal information <Paper corpusId=\"170076423\" paperTitle=\"(Carlini et al., 2018)\" isShortName></Paper>.\n\n6. **Defense-aware Setting**: As defenses against membership inference have emerged, researchers have developed more sophisticated attacks designed to circumvent these protections. This creates an ongoing arms race between attack and defense mechanisms <Paper corpusId=\"49863840\" paperTitle=\"(Nasr et al._1, 2018)\" isShortName></Paper>.\n\n7. **Transfer Attack Setting**: In this scenario, the attacker doesn't have direct access to the target model but instead trains shadow models on data from a similar distribution to simulate the target model's behavior. This approach, pioneered by Shokri et al., allows for attacks even with limited knowledge about the target model <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper> <Paper corpusId=\"46933970\" paperTitle=\"(Salem et al., 2018)\" isShortName></Paper>.", "citations": [{"id": "(He et al._1, 2021)", "paper": {"corpus_id": 231861713, "title": "Node-Level Membership Inference Attacks Against Graph Neural Networks", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Xinlei He", "authorId": "2116553732"}, {"name": "Rui Wen", "authorId": "2054749404"}, {"name": "Yixin Wu", "authorId": "2127727861"}, {"name": "M. Backes", "authorId": "144588806"}, {"name": "Yun Shen", "authorId": "2117688523"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 98}, "snippets": ["Membership inference attacks aim at inferring membership of individual training samples of a target model to which an adversary has black-box access through a prediction API [5,9,19,28,30,31,35,36,39]51]."], "score": 0.9990234375}, {"id": "(Shokri et al., 2016)", "paper": {"corpus_id": 10488675, "title": "Membership Inference Attacks Against Machine Learning Models", "year": 2016, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "R. Shokri", "authorId": "2520493"}, {"name": "M. Stronati", "authorId": "34828439"}, {"name": "Congzheng Song", "authorId": "3469125"}, {"name": "Vitaly Shmatikov", "authorId": "1723945"}], "n_citations": 4160}, "snippets": ["We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset."], "score": 0.998046875}, {"id": "(Salem et al., 2018)", "paper": {"corpus_id": 46933970, "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models", "year": 2018, "venue": "Network and Distributed System Security Symposium", "authors": [{"name": "A. Salem", "authorId": "66697271"}, {"name": "Yang Zhang", "authorId": "2145954003"}, {"name": "Mathias Humbert", "authorId": "144887171"}, {"name": "Mario Fritz", "authorId": "1739548"}, {"name": "M. Backes", "authorId": "144588806"}], "n_citations": 950}, "snippets": ["Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications.\n\nIn this paper, we concentrate on one such attack, namely membership inference attack. In this setting, an adversary aims to determine whether a data item (also referred to as a data point) was used to train an ML model or not. Successful membership inference attacks can cause severe consequences. For instance, if a machine learning model is trained on the data collected from people with a certain disease, by knowing that a victim's data belong to the training data of the model, the attacker can immediately learn this victim's health status."], "score": 0.99853515625}, {"id": "(Nasr et al., 2018)", "paper": {"corpus_id": 133091488, "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning", "year": 2018, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "Milad Nasr", "authorId": "3490923"}, {"name": "R. Shokri", "authorId": "2520493"}, {"name": "Amir Houmansadr", "authorId": "1972973"}], "n_citations": 1452}, "snippets": ["Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."], "score": 0.0}, {"id": "(Shin et al., 2021)", "paper": {"corpus_id": 244809678, "title": "Is Homomorphic Encryption-Based Deep Learning Secure Enough?", "year": 2021, "venue": "Italian National Conference on Sensors", "authors": [{"name": "Jinmyeong Shin", "authorId": "2111246337"}, {"name": "Seok-Hwan Choi", "authorId": "2100675"}, {"name": "Yoon-Ho Choi", "authorId": "2111227849"}], "n_citations": 4}, "snippets": ["Similar to a reconstruction attack, the membership inference attack requires some model information such as model architecture and hyper-parameters. Therefore, a membership inference attack cannot be performed in usual situations. However, when we consider a semi-honest party in the training phase that is operating honestly but curious about others data, a membership inference attack can be performed. As shown in Figure 3, when three parties including one semi-honest party train a model, these parties share all information about the model except their own data and train the model when all parties agree with the model. In this scenario, the semi-honest party can gain access to all the information needed to perform a membership inference attack. After the training is completed, the semi-honest party can obtain extra data including some data similar to the data used in the training phase. As the semi-honest party already knows the model information, they can generate the most desirable shadow model, which reproduces the target model's behavior and performs an ideal membership inference attack. For example, when a semi-honest adversary performs Shokri et al.'s membership inference attack (Shokri et al., 2016), the adversary can construct the ideal shadow model because the model parameters and architecture are already shared to all parties. After constructing the shadow model, the adversary trains the shadow model and an attack model using their own data. Then, the adversary can analyze the classification result of arbitrary data to perform a membership inference attack. Furthermore, the membership inference attack performed by the adversary cannot be detected since there is no interaction with other parties."], "score": 0.99755859375}, {"id": "(Carlini et al., 2018)", "paper": {"corpus_id": 170076423, "title": "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks", "year": 2018, "venue": "USENIX Security Symposium", "authors": [{"name": "Nicholas Carlini", "authorId": "2288725419"}, {"name": "Chang Liu", "authorId": "2118484320"}, {"name": "\u00da. Erlingsson", "authorId": "1758110"}, {"name": "Jernej Kos", "authorId": "36426383"}, {"name": "D. Song", "authorId": "143711382"}], "n_citations": 1148}, "snippets": ["This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. \nIn experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages."], "score": 0.0}, {"id": "(Nasr et al._1, 2018)", "paper": {"corpus_id": 49863840, "title": "Machine Learning with Membership Privacy using Adversarial Regularization", "year": 2018, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Milad Nasr", "authorId": "3490923"}, {"name": "R. Shokri", "authorId": "2520493"}, {"name": "Amir Houmansadr", "authorId": "1972973"}], "n_citations": 474}, "snippets": ["Machine learning models leak significant amount of information about their training sets, through their predictions. This is a serious privacy concern for the users of machine learning as a service. To address this concern, in this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models. We introduce a mechanism to train models with membership privacy, which ensures indistinguishability between the predictions of a model on its training data and other data points (from the same distribution). This requires minimizing the accuracy of the best black-box membership inference attack against the model. We formalize this as a min-max game, and design an adversarial training algorithm that minimizes the prediction loss of the model as well as the maximum gain of the inference attacks. This strategy, which can guarantee membership privacy (as prediction indistinguishability), acts also as a strong regularizer and helps generalizing the model. We evaluate the practical feasibility of our privacy mechanism on training deep neural networks using benchmark datasets. We show that the min-max strategy can mitigate the risks of membership inference attacks (near random guess), and can achieve this with a negligible drop in the model's prediction accuracy (less than 4%)."], "score": 0.0}], "table": null}], "cost": 0.475908}}
