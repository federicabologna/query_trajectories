{"original_query": "How to detect or extract pretraining data from language models? Are there related works on this topic? ", "input": "detect or data from language models related works topic", "output": {"sections": [{"title": "Introduction to Language Models and Detection", "tldr": "Large language models (LLMs) have transformed natural language processing with their ability to generate highly coherent and contextually relevant text. This capability has raised concerns about potential misuse, leading to the development of methods for detecting machine-generated content. (4 sources)", "text": "\nLarge language models (LLMs) have revolutionized natural language processing by offering significant advantages over previous pre-trained models, including better characterization of complex patterns and dependencies in text, along with impressive in-context learning capabilities for solving downstream tasks with minimal examples <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. Notable models such as GPT-3, PaLM, and ChatGPT have demonstrated remarkable abilities to generate text with high coherence, fluency, and semantic relevance, even effectively addressing complex inquiries across various domains including science, mathematics, history, and current events <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper>.\n\nAs these powerful language models become more prevalent, there is increasing concern about the potential misuse of machine-generated text, which could flood the internet with spam, social media bots, and low-value content <Paper corpusId=\"259095643\" paperTitle=\"(Kirchenbauer et al., 2023)\" isShortName></Paper>. The importance of effectively regulating LLM use to prevent significant social issues has become evident <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. This has led to the development of various detection methods, which can be broadly categorized into two approaches: zero-shot detection methods that may require access to the source model's output logits or losses, and classifier-based methods that train on collected human-written and machine-generated texts <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. Additionally, watermarking has emerged as a simple and effective strategy for enabling the detection and documentation of LLM-generated text <Paper corpusId=\"259095643\" paperTitle=\"(Kirchenbauer et al., 2023)\" isShortName></Paper>.\n\nThe development of tools to support humans in detecting whether text was generated by a model has shown promising results, with one study demonstrating an improvement in human detection rates from 54% to 72% without prior training when using appropriate annotation schemes <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"182952848\" paperTitle=\"(Gehrmann et al., 2019)\" isShortName></Paper>. As LLMs continue to advance, the field of machine-generated text detection remains a critical area of research to mitigate potential harms while harnessing the benefits of these powerful models.", "citations": [{"id": "(Deng et al., 2023)", "paper": {"corpus_id": 258947640, "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhijie Deng", "authorId": "145114723"}, {"name": "Hongcheng Gao", "authorId": "2162081759"}, {"name": "Yibo Miao", "authorId": "2188993538"}, {"name": "Hao Zhang", "authorId": "46702482"}], "n_citations": 20}, "snippets": ["Large language models. LLMs (Radford et al., 2019)(Brown et al., 2020)Chowdhery et al., 2022;Zhang et al., 2022;OpenAI, 2022) have revolutionized the field of natural language processing by offering several advantages over previous pre-trained models (Devlin et al., 2018;Liu et al., 2019;Lan et al., 2019), including a better characterization of complex patterns and dependencies in the text, and the appealing in-context learning ability for solving downstream tasks with minimal examples. Representative models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and Chat-GPT (OpenAI, 2022) have showcased their remarkable ability to generate text with high coherence, fluency, and semantic relevance. They can even effectively address complex inquiries related to science, mathematics, history, current events, and social trends. Therefore, it is increasingly important to effectively regulate the use of LLMs to prevent significant social issues. \n\nLLM-generated text detection. Previous methods can be broadly categorized into two groups. The first group of methods performs detection in a zero-shot manner (Solaiman et al., 2019;(Gehrmann et al., 2019)Mitchell et al., 2023;Yang et al., 2023), but they require access to the source model that generates the texts to derive quantities like output logits or losses for detection. For instance, Solaiman et al. (2019) suggest that a higher log probability for each token indicates that the text will likely be machine-generated. When the output logits/losses of the source model are unavailable, these methods rely on a proxy model for detection. However, there is often a substantial gap between the proxy and source models from which the text is generated. Another group of methods trains DNN-based classifiers on col-lected human-written and machine-generated texts for detection (Guo et al., 2023;Uchendu et al., 2020;Ope-nAI, 2023b)."], "score": 0.52880859375}, {"id": "(Brown et al., 2020)", "paper": {"corpus_id": 218971783, "title": "Language Models are Few-Shot Learners", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tom B. Brown", "authorId": "31035595"}, {"name": "Benjamin Mann", "authorId": "2056658938"}, {"name": "Nick Ryder", "authorId": "39849748"}, {"name": "Melanie Subbiah", "authorId": "2065894334"}, {"name": "J. Kaplan", "authorId": "152724169"}, {"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Arvind Neelakantan", "authorId": "2072676"}, {"name": "Pranav Shyam", "authorId": "67311962"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Ariel Herbert-Voss", "authorId": "1404060687"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "T. Henighan", "authorId": "103143311"}, {"name": "R. Child", "authorId": "48422824"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Daniel M. Ziegler", "authorId": "2052152920"}, {"name": "Jeff Wu", "authorId": "49387725"}, {"name": "Clemens Winter", "authorId": "2059411355"}, {"name": "Christopher Hesse", "authorId": "144239765"}, {"name": "Mark Chen", "authorId": "2108828435"}, {"name": "Eric Sigler", "authorId": "2064673055"}, {"name": "Ma-teusz Litwin", "authorId": "1380985420"}, {"name": "Scott Gray", "authorId": "145565184"}, {"name": "Benjamin Chess", "authorId": "1490681878"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Christopher Berner", "authorId": "133740015"}, {"name": "Sam McCandlish", "authorId": "52238703"}, {"name": "Alec Radford", "authorId": "38909097"}, {"name": "I. Sutskever", "authorId": "1701686"}, {"name": "Dario Amodei", "authorId": "2698777"}], "n_citations": 42437}, "snippets": ["Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."], "score": 0.0}, {"id": "(Kirchenbauer et al., 2023)", "paper": {"corpus_id": 259095643, "title": "On the Reliability of Watermarks for Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "John Kirchenbauer", "authorId": "2166053502"}, {"name": "Jonas Geiping", "authorId": "8284185"}, {"name": "Yuxin Wen", "authorId": "123191916"}, {"name": "Manli Shu", "authorId": "1643697854"}, {"name": "Khalid Saifullah", "authorId": "2203810783"}, {"name": "Kezhi Kong", "authorId": "80253287"}, {"name": "Kasun Fernando", "authorId": "94971447"}, {"name": "Aniruddha Saha", "authorId": "2056290221"}, {"name": "Micah Goldblum", "authorId": "121592562"}, {"name": "T. Goldstein", "authorId": "1962083"}], "n_citations": 123}, "snippets": ["As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text."], "score": 0.5712890625}, {"id": "(Gehrmann et al., 2019)", "paper": {"corpus_id": 182952848, "title": "GLTR: Statistical Detection and Visualization of Generated Text", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Sebastian Gehrmann", "authorId": "3159346"}, {"name": "Hendrik Strobelt", "authorId": "2879705"}, {"name": "Alexander M. Rush", "authorId": "2531268"}], "n_citations": 546}, "snippets": ["The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs."], "score": 0.0}], "table": null}, {"title": "Machine-Generated Text Detection Methods", "tldr": "Various methods have been developed to detect machine-generated text, ranging from classifier-based approaches to zero-shot detection and watermarking techniques. These detection methods leverage linguistic patterns, statistical features, and model-specific characteristics to differentiate between human and AI-written content. (6 sources)", "text": "\nThe challenge of detecting machine-generated text has led to the development of diverse approaches that can be broadly categorized into two main groups. The first group employs zero-shot detection methods that often require access to the source model's output logits or losses <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. For instance, some researchers have suggested that higher log probability for each token can indicate machine-generated text. However, when access to the source model is unavailable, these methods must rely on proxy models, which can lead to significant gaps in detection accuracy <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>.\n\nThe second group consists of classifier-based methods that train on collected human-written and machine-generated texts <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. These approaches have shown promising results in distinguishing between human and AI-generated content. Research has demonstrated that even simple classifiers using n-gram and part-of-speech features can achieve robust performance on both in-domain and out-of-domain data <Paper corpusId=\"269982474\" paperTitle=\"(McGovern et al., 2024)\" isShortName></Paper>. This effectiveness stems from LLMs possessing unique \"fingerprints\" that manifest as slight differences in the frequency of certain lexical and morphosyntactic features in their outputs <Paper corpusId=\"269982474\" paperTitle=\"(McGovern et al., 2024)\" isShortName></Paper>.\n\nSome researchers have explored adapting pre-trained language models specifically for detection tasks. For example, Misra et al. developed two language models trained on 725,000 emails containing both phishing and legitimate messages, applying them through classification-based fine-tuning and a priming-based approach <Paper corpusId=\"258328237\" paperTitle=\"(Misra et al., 2022)\" isShortName></Paper>. This pre-train-then-fine-tune paradigm has proven effective for specialized detection scenarios.\n\nComprehensive investigations into cross-model detection have evaluated classifiers' abilities to detect text generated by different LLMs and attribute content to specific models across various sizes and model families <Paper corpusId=\"262465111\" paperTitle=\"(Antoun et al., 2023)\" isShortName></Paper>. These studies are particularly valuable as the landscape of language models continues to expand.\n\nWatermarking has emerged as another important strategy for enabling the detection of LLM-generated text. This simple yet effective approach helps mitigate potential harms by providing a means to document and identify machine-generated content, which is crucial as AI-generated text threatens to flood the internet with spam, social media bots, and low-value content <Paper corpusId=\"259095643\" paperTitle=\"(Kirchenbauer et al., 2023)\" isShortName></Paper>. Tools like GLTR (Giant Language model Test Room) have been developed to support humans in detecting generated text through baseline statistical methods that can identify generation artifacts across multiple sampling schemes <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"182952848\" paperTitle=\"(Gehrmann et al., 2019)\" isShortName></Paper>.", "citations": [{"id": "(Deng et al., 2023)", "paper": {"corpus_id": 258947640, "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhijie Deng", "authorId": "145114723"}, {"name": "Hongcheng Gao", "authorId": "2162081759"}, {"name": "Yibo Miao", "authorId": "2188993538"}, {"name": "Hao Zhang", "authorId": "46702482"}], "n_citations": 20}, "snippets": ["Large language models. LLMs (Radford et al., 2019)(Brown et al., 2020)Chowdhery et al., 2022;Zhang et al., 2022;OpenAI, 2022) have revolutionized the field of natural language processing by offering several advantages over previous pre-trained models (Devlin et al., 2018;Liu et al., 2019;Lan et al., 2019), including a better characterization of complex patterns and dependencies in the text, and the appealing in-context learning ability for solving downstream tasks with minimal examples. Representative models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and Chat-GPT (OpenAI, 2022) have showcased their remarkable ability to generate text with high coherence, fluency, and semantic relevance. They can even effectively address complex inquiries related to science, mathematics, history, current events, and social trends. Therefore, it is increasingly important to effectively regulate the use of LLMs to prevent significant social issues. \n\nLLM-generated text detection. Previous methods can be broadly categorized into two groups. The first group of methods performs detection in a zero-shot manner (Solaiman et al., 2019;(Gehrmann et al., 2019)Mitchell et al., 2023;Yang et al., 2023), but they require access to the source model that generates the texts to derive quantities like output logits or losses for detection. For instance, Solaiman et al. (2019) suggest that a higher log probability for each token indicates that the text will likely be machine-generated. When the output logits/losses of the source model are unavailable, these methods rely on a proxy model for detection. However, there is often a substantial gap between the proxy and source models from which the text is generated. Another group of methods trains DNN-based classifiers on col-lected human-written and machine-generated texts for detection (Guo et al., 2023;Uchendu et al., 2020;Ope-nAI, 2023b)."], "score": 0.52880859375}, {"id": "(McGovern et al., 2024)", "paper": {"corpus_id": 269982474, "title": "Your Large Language Models Are Leaving Fingerprints", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hope McGovern", "authorId": "2115144895"}, {"name": "Rickard Stureborg", "authorId": "2040711244"}, {"name": "Yoshi Suhara", "authorId": "2283136281"}, {"name": "Dimitris Alikaniotis", "authorId": "71152801"}], "n_citations": 14}, "snippets": ["It has been shown that finetuned transformers and other supervised detectors effectively distinguish between human and machine-generated text in some situations arXiv:2305.13242, but we find that even simple classifiers on top of n-gram and part-of-speech features can achieve very robust performance on both in- and out-of-domain data. To understand how this is possible, we analyze machine-generated output text in five datasets, finding that LLMs possess unique fingerprints that manifest as slight differences in the frequency of certain lexical and morphosyntactic features."], "score": 0.501953125}, {"id": "(Misra et al., 2022)", "paper": {"corpus_id": 258328237, "title": "LMs go Phishing: Adapting Pre-trained Language Models to Detect Phishing Emails", "year": 2022, "venue": "2022 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)", "authors": [{"name": "Kanishka Misra", "authorId": "145274478"}, {"name": "J. Rayz", "authorId": "10681993"}], "n_citations": 8}, "snippets": ["Mean-while, progress in natural language processing has established the universal usefulness of adapting pre-trained language models to perform downstream tasks, in a paradigm known as pre-train-then-fine-tune. In this work, we build on this paradigm, and propose two language models that are adapted on 725k emails containing phishing and legitimate messages. We use these two models in two ways: 1) by performing classification-based fine-tuning, and 2) by developing a simple priming-based approach."], "score": 0.5556640625}, {"id": "(Antoun et al., 2023)", "paper": {"corpus_id": 262465111, "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content", "year": 2023, "venue": "International Conference on Language Resources and Evaluation", "authors": [{"name": "Wissam Antoun", "authorId": "51040671"}, {"name": "Beno\u00eet Sagot", "authorId": "68990982"}, {"name": "Djam\u00e9 Seddah", "authorId": "1679170"}], "n_citations": 13}, "snippets": ["A comprehensive investigation into crossmodel detection, evaluating the classifier's ability to detect text generated by different LLMs, and in model attribution, encompassing a broad range of sizes and model families."], "score": 0.74267578125}, {"id": "(Kirchenbauer et al., 2023)", "paper": {"corpus_id": 259095643, "title": "On the Reliability of Watermarks for Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "John Kirchenbauer", "authorId": "2166053502"}, {"name": "Jonas Geiping", "authorId": "8284185"}, {"name": "Yuxin Wen", "authorId": "123191916"}, {"name": "Manli Shu", "authorId": "1643697854"}, {"name": "Khalid Saifullah", "authorId": "2203810783"}, {"name": "Kezhi Kong", "authorId": "80253287"}, {"name": "Kasun Fernando", "authorId": "94971447"}, {"name": "Aniruddha Saha", "authorId": "2056290221"}, {"name": "Micah Goldblum", "authorId": "121592562"}, {"name": "T. Goldstein", "authorId": "1962083"}], "n_citations": 123}, "snippets": ["As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text."], "score": 0.5712890625}, {"id": "(Gehrmann et al., 2019)", "paper": {"corpus_id": 182952848, "title": "GLTR: Statistical Detection and Visualization of Generated Text", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Sebastian Gehrmann", "authorId": "3159346"}, {"name": "Hendrik Strobelt", "authorId": "2879705"}, {"name": "Alexander M. Rush", "authorId": "2531268"}], "n_citations": 546}, "snippets": ["The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs."], "score": 0.0}], "table": null}, {"title": "Pre-training Data Detection and Contamination", "tldr": "Pre-training data detection has emerged as a critical area of research addressing concerns about privacy, copyright, and benchmark contamination in large language models. Researchers have developed various methods to determine whether specific text was part of an LLM's training data, ranging from perplexity-based approaches to more sophisticated techniques that examine model internals. (8 sources)", "text": "\nThe growing capabilities of large language models have raised significant concerns about the potential contamination of their pre-training data, leading to increased research on methods to detect whether specific text was included in a model's training corpus. This research area has become particularly important due to implications for copyright violations, privacy issues, and test data contamination <Paper corpusId=\"268889777\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. A comprehensive review of 50 papers on data contamination detection highlights the need for formal definitions and critical assessment of the assumptions underlying different detection approaches <Paper corpusId=\"273549214\" paperTitle=\"(Fu et al., 2024)\" isShortName></Paper>.\n\nTo establish reliable evaluation standards, researchers have developed benchmarks such as WIKIMIA, which leverages the temporal nature of Wikipedia data to create accurate member and non-member datasets for testing detection methods. This dynamic benchmark can be applied to various models trained on Wikipedia and is automatically updated with newer non-member data as events occur <Paper corpusId=\"264451585\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>.\n\nSeveral innovative approaches have been proposed for pre-training data detection. The Data Contamination Quiz (DCQ) frames contamination detection as multiple-choice questions, creating word-level perturbations of dataset instances and observing whether models gravitate toward selecting the original version <Paper corpusId=\"265128736\" paperTitle=\"(Golchin et al., 2023)\" isShortName></Paper>. Another approach utilizes probing techniques to examine model internal activations rather than relying on superficial features like perplexities, which may not be reliable <Paper corpusId=\"270217411\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nTraditional methods have often relied on techniques from Membership Inference Attacks (MIAs), which depend heavily on LLMs' verbatim memorization capabilities. However, this reliance presents challenges given the vast amount of training data and limited effective training epochs <Paper corpusId=\"271570943\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>. To address these limitations, researchers have developed adaptive detection methods and divergence-based calibration approaches that compute the cross-entropy between token probability distributions and frequency distributions <Paper corpusId=\"272827783\" paperTitle=\"(Zhang et al._2, 2024)\" isShortName></Paper>.\n\nRecent findings suggest that a combination of different probing methods can successfully identify text known to both open-source and proprietary black-box LLMs. Importantly, different methods may identify different examples of memorized text, indicating that the development of diverse, complementary probes can provide better insights into how data was used during model training <Paper corpusId=\"277065772\" paperTitle=\"(Ravichander et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 268889777, "title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jingyang Zhang", "authorId": "2267788653"}, {"name": "Jingwei Sun", "authorId": "2156016815"}, {"name": "Eric C. Yeats", "authorId": "2119236605"}, {"name": "Ouyang Yang", "authorId": "2294877492"}, {"name": "Martin Kuo", "authorId": "2211526996"}, {"name": "Jianyi Zhang", "authorId": "2265652686"}, {"name": "Hao k Yang", "authorId": "2297270173"}, {"name": "Hai Li", "authorId": "2294901591"}], "n_citations": 54}, "snippets": ["The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations."], "score": 0.74365234375}, {"id": "(Fu et al., 2024)", "paper": {"corpus_id": 273549214, "title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Yujuan Fu", "authorId": "2294828739"}, {"name": "\u00d6zlem Uzuner", "authorId": "2298238369"}, {"name": "Meliha Yetisgen-Yildiz", "authorId": "1398215463"}, {"name": "Fei Xia", "authorId": "2294363357"}], "n_citations": 8}, "snippets": ["To bridge this gap, we (1) systematically review 50 papers on data contamination detection for LMs, (2) present the formal, mathematical definitions for different levels of data contamination, (3) categorize the underlying requirements and assumptions associated with each approach and critically assess whether these assumptions have been rigorously validated, and (4) demonstrate through case studies that some unverified assumptions can be wrong in multiple scenarios."], "score": 0.62744140625}, {"id": "(Shi et al., 2023)", "paper": {"corpus_id": 264451585, "title": "Detecting Pretraining Data from Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Weijia Shi", "authorId": "2254168373"}, {"name": "Anirudh Ajith", "authorId": "2218438150"}, {"name": "Mengzhou Xia", "authorId": "67284811"}, {"name": "Yangsibo Huang", "authorId": "108053318"}, {"name": "Daogao Liu", "authorId": "2261780806"}, {"name": "Terra Blevins", "authorId": "3443287"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Luke S. Zettlemoyer", "authorId": "2137813791"}], "n_citations": 201}, "snippets": ["Our first step towards addressing these challenges is to establish a reliable benchmark. We introduce WIKIMIA, a dynamic benchmark designed to periodically and automatically evaluate detection methods on any newly released pretrained LLMs. By leveraging the Wikipedia data timestamp and the model release date, we select old Wikipedia event data as our member data (i.e, seen data during pretraining) and recent Wikipedia event data (e.g., after 2023) as our non-member data (unseen). Our datasets thus exhibit three desirable properties: (1) Accurate: events that occur after LLM pretraining are guaranteed not to be present in the pretraining data. The temporal nature of events ensures that non-member data is indeed unseen and not mentioned in the pretraining data. (2) General: our benchmark is not confined to any specific model and can be applied to various models pretrained using Wikipedia (e.g., OPT, LLaMA, GPT-Neo) since Wikipedia is a commonly used pretraining data source. (3) Dynamic: we will continually update our benchmark by gathering newer non-member data (i.e., more recent events) from Wikipedia since our data construction pipeline is fully automated."], "score": 0.6083984375}, {"id": "(Golchin et al., 2023)", "paper": {"corpus_id": 265128736, "title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Shahriar Golchin", "authorId": "65754049"}, {"name": "M. Surdeanu", "authorId": "1760868"}], "n_citations": 26}, "snippets": ["We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions, devising a quiz format wherein three perturbed versions of each instance, subsampled from a specific dataset partition, are created. These changes only include word-level perturbations. The generated perturbations, along with the original dataset instance, form the options in the DCQ, with an extra option accommodating the selection of none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an LLM, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it."], "score": 0.5849609375}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 270217411, "title": "Probing Language Models for Pre-training Data Detection", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhenhua Liu", "authorId": "2294376388"}, {"name": "Tong Zhu", "authorId": "1914586128"}, {"name": "Chuanyuan Tan", "authorId": "2186374155"}, {"name": "Haonan Lu", "authorId": "2304460083"}, {"name": "Bing Liu", "authorId": "2330946427"}, {"name": "Wenliang Chen", "authorId": "2265943980"}], "n_citations": 13}, "snippets": ["Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations."], "score": 0.72412109375}, {"id": "(Zhang et al._1, 2024)", "paper": {"corpus_id": 271570943, "title": "Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Anqi Zhang", "authorId": "2313922585"}, {"name": "Chaofeng Wu", "authorId": "2314513929"}], "n_citations": 6}, "snippets": ["While large language models (LLMs) are extensively used, there are raising concerns regarding privacy, security, and copyright due to their opaque training data, which brings the problem of detecting pre-training data on the table. Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification."], "score": 0.78466796875}, {"id": "(Zhang et al._2, 2024)", "paper": {"corpus_id": 272827783, "title": "Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Weichao Zhang", "authorId": "2322456143"}, {"name": "Ruqing Zhang", "authorId": "2109960367"}, {"name": "Jiafeng Guo", "authorId": "70414094"}, {"name": "M. D. Rijke", "authorId": "2265490493"}, {"name": "Yixing Fan", "authorId": "7888704"}, {"name": "Xueqi Cheng", "authorId": "2244825947"}], "n_citations": 16}, "snippets": ["To address this issue, we introduce a divergence-based calibration method, inspired by the divergence-from-randomness concept, to calibrate token probabilities for pretraining data detection. We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score."], "score": 0.59521484375}, {"id": "(Ravichander et al., 2025)", "paper": {"corpus_id": 277065772, "title": "Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models", "year": 2025, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Abhilasha Ravichander", "authorId": "3023068"}, {"name": "Jillian R. Fisher", "authorId": "33772445"}, {"name": "Taylor Sorensen", "authorId": "122436831"}, {"name": "Ximing Lu", "authorId": "50085131"}, {"name": "Yuchen Lin", "authorId": "2284130273"}, {"name": "Maria Antoniak", "authorId": "2266838583"}, {"name": "Niloofar Mireshghallah", "authorId": "2254272878"}, {"name": "Chandra Bhagavatula", "authorId": "1857797"}, {"name": "Yejin Choi", "authorId": "2266363632"}], "n_citations": 1}, "snippets": ["In our work, we find that we are able to identify text that is known to even proprietary black-box LLMs, and that the examples of memorized text that were successfully identified can differ between probing methods. This indicates that the community would benefit from a range of such approaches, and that focusing on state-of-the-art detection performance should not be the only goal. Further, recent work has investigating combining signals for various training data identification methods in order to determine if a model was trained on a given document (Longpre et al., 2024). This suggests that developing diverse, complementary, probes can help us better understand how data was used to train models."], "score": 0.62548828125}], "table": null}, {"title": "Out-of-Distribution (OOD) Detection", "tldr": "Out-of-distribution detection identifies when input text falls outside a model's training distribution, which is crucial for safe deployment of language models. Methods range from embedding-based distance measurements to leveraging pre-trained models for zero-shot detection, helping ensure reliable model performance on unseen data. (4 sources)", "text": "\nOut-of-distribution (OOD) detection has emerged as a critical component in safeguarding natural language processing systems from producing unreliable outputs when confronted with inputs that differ significantly from their training data. This capability is essential for practical applications, as demonstrated in cases where a sentiment classification model trained on formal restaurant reviews might produce invalid results when processing informal social media posts <Paper corpusId=\"265351715\" paperTitle=\"(Pollano et al., 2023)\" isShortName></Paper>.\n\nA common approach to OOD detection involves converting textual inputs into an embedding space where distances can be measured. Researchers have utilized transformer-based language models such as BERT to extract embedding vectors for input text (e.g., the hidden representation of the special token) and then measure the distance between these vectors and those from in-distribution validation sets <Paper corpusId=\"265351715\" paperTitle=\"(Pollano et al., 2023)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>. When this distance exceeds a calibrated threshold, the input text is flagged as out-of-distribution. More sophisticated methods tap into the internal states of transformer-based models, which contain valuable information that can provide richer representations than simply using embeddings from the last or penultimate layers <Paper corpusId=\"265351715\" paperTitle=\"(Pollano et al., 2023)\" isShortName></Paper>.\n\nRecent advancements have leveraged large language models (LLMs) and multi-modal LLMs to transform OOD detection capabilities. These models can perform detection tasks in zero-shot or few-shot settings by utilizing pre-trained models like CLIP, representing a significant shift from traditional methods that typically require training classifiers on entire in-distribution datasets <Paper corpusId=\"272366479\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. This approach allows for detection of OOD samples with minimal additional training, enabling better generalization to unseen data.\n\nThe robustness of OOD detection methods has been demonstrated in cross-model experiments. In one study, researchers trained a detector on data from two language models and tested it on content generated by a third, previously unseen model. Despite an expected performance drop, the method remained effective in detecting content from the unseen model, achieving F1 scores of up to 91 points <Paper corpusId=\"267095281\" paperTitle=\"(Mao et al., 2024)\" isShortName></Paper>. This finding highlights the adaptability of modern OOD detection approaches and their potential for real-world applications where models may encounter content from various unknown sources.", "citations": [{"id": "(Pollano et al., 2023)", "paper": {"corpus_id": 265351715, "title": "Detecting out-of-distribution text using topological features of transformer-based language models", "year": 2023, "venue": "AISafety@IJCAI", "authors": [{"name": "Andres Pollano", "authorId": "2267725677"}, {"name": "Anupam Chaudhuri", "authorId": "2267728923"}, {"name": "Anj Simmons", "authorId": "2267725822"}], "n_citations": 1}, "snippets": ["In this paper, we focus on OOD detection for textual inputs to safeguard ML models that perform natural language processing (NLP) tasks. For example, a sentiment classification model trained on formal restaurant reviews may not produce valid results when applied to informal posts from social media. Determining that an input is OOD requires a way to measure the distance between an input and the in-distribution data. This in turn requires a method to convert textual data into an embedding space in which we can measure distance. One approach to this is to input the text to a transformer-based language model, such as BERT (Devlin et al., 2019), to extract an embedding vector for the input text (e.g., the hidden representation of the special [] token). We can then measure the distance of the embedding vector for an input text to the nearest (or k-nearest) embedding vector of a text from an in-distribution validation set. When this distance is beyond some threshold (which needs to be calibrated for the application), the input text is flagged as out of distribution. The internal state of transformer-based language models contains important information, which may be able to offer richer representations than only using the embedding obtained from the last or penultimate layer. For example, Azaria and Mitchell [3] demonstrated that it is possible to train a classifier on the activation values of the hidden layers of large language models to predict when they are generating false information rather than true information."], "score": 0.51904296875}, {"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 272366479, "title": "Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ruiyao Xu", "authorId": "2319451284"}, {"name": "Kaize Ding", "authorId": "2319332607"}], "n_citations": 7}, "snippets": ["Large Language Models (LLMs) and Multi-Modal LLMs (MLLMs) have transformed Out-of-Distribution (OOD) detection by leveraging pretrained models like CLIP to perform downstream detection tasks. These models are capable of detecting OOD samples in zero-shot or few-shot settings, meaning they can generalize to unseen data with little to no additional training. This represents a shift from traditional OOD detection methods, which typically rely on training classifiers using the entire in-distribution (ID) dataset."], "score": 0.62109375}, {"id": "(Mao et al., 2024)", "paper": {"corpus_id": 267095281, "title": "Raidar: geneRative AI Detection viA Rewriting", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Chengzhi Mao", "authorId": "7700460"}, {"name": "Carl Vondrick", "authorId": "1856025"}, {"name": "Hao Wang", "authorId": "2281126484"}, {"name": "Junfeng Yang", "authorId": "2110694456"}], "n_citations": 31}, "snippets": ["In the out-of-distribution experiment, we train the detector on data from two language models, assuming it is unaware that the test text will be generated from the third model. Despite a performance drop on detecting the out-of-distribution test data generated from the third model, our method remains effective in detecting content from this unseen model, underscoring our approach's robustness and adaptability, with up to 91 points on F1 score. The number shows the number of data, reflecting by the size of the dot."], "score": 0.51318359375}], "table": null}, {"title": "Data Generation and Annotation Approaches", "tldr": "Language models are increasingly being used to generate and annotate data for training specialized models and creating benchmarks. These approaches range from expert-guided data creation to automated statistical model discovery, providing new ways to address data scarcity in specialized domains. (2 sources)", "text": "\nThe development of effective language models often requires high-quality labeled data, which can be scarce or expensive to obtain in specialized domains. Researchers have addressed this challenge by developing innovative approaches to data generation and annotation that leverage language models themselves. For instance, Li et al. demonstrated a comprehensive approach to creating datasets for Alzheimer's disease (AD) detection by developing a novel pragmatic taxonomy and generating three complementary datasets with varying levels of human involvement: a gold dataset annotated by human experts on longitudinal electronic health records (EHRs), a silver dataset created through a data-to-label method that automatically labels sentences from public EHR collections, and a bronze dataset produced through a label-to-data method that generates sentences based on predefined label definitions <Paper corpusId=\"266176903\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nBeyond simple data generation, language models have also been employed for more complex tasks such as statistical model discovery. Li et al. introduced a method for language model-driven automated statistical model discovery that leverages the domain knowledge and programming capabilities of large language models. This approach follows Box's Loop framework, where the language model alternates between proposing statistical models represented as probabilistic programs (acting as a modeler) and critiquing those models (acting as a domain expert), enabling automated discovery of appropriate statistical models for given datasets <Paper corpusId=\"268041863\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. These approaches demonstrate how language models can serve not only as tools for text generation but also as sophisticated aids in data creation, annotation, and model development processes, potentially addressing challenges related to data scarcity and domain expertise requirements.", "citations": [{"id": "(Li et al., 2023)", "paper": {"corpus_id": 266176903, "title": "Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Rumeng Li", "authorId": "2237106751"}, {"name": "Xun Wang", "authorId": "2221230794"}, {"name": "Hong Yu", "authorId": "2273658683"}], "n_citations": 26}, "snippets": ["We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge and generated three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method, which labels sentences from a public EHR collection with AD-related signs and symptoms; and (3) a bronze dataset created by the label-to-data method which generates sentences with AD-related signs and symptoms based on the label definition."], "score": 0.61474609375}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 268041863, "title": "Automated Statistical Model Discovery with Language Models", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Michael Y. Li", "authorId": "2288065395"}, {"name": "Emily B. Fox", "authorId": "2287933421"}, {"name": "Noah D. Goodman", "authorId": "2280334415"}], "n_citations": 19}, "snippets": ["Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert."], "score": 0.52783203125}], "table": null}, {"title": "Applications of Detection Methods", "tldr": "Detection methods for language models have been applied across diverse domains including hate speech identification, network security, and educational text assessment. These applications leverage the capabilities of language models to address domain-specific challenges through both zero-shot approaches and specialized techniques. (3 sources)", "text": "\n- **Hate Speech Detection**: Researchers have explored zero-shot learning with prompting techniques to address hate speech detection across multiple languages. This approach is particularly valuable given the limited availability of labeled data and the high variability of hate speech across different contexts and languages. By injecting models with task-specific knowledge without relying on labeled data, prompting methods offer promising solutions to these persistent challenges in content moderation. <Paper corpusId=\"259376532\" paperTitle=\"(Plaza-del-Arco et al., 2023)\" isShortName></Paper>\n\n- **Network Security**: Language models have been applied to detect malicious network activity, demonstrating their potential beyond traditional text analysis. For example, researchers have investigated how large language models can understand and analyze non-language network data to identify unknown malicious flows, using Carpet Bombing attacks as a case study. This application showcases how LLMs' capabilities can be leveraged in specialized technical domains like networking security. <Paper corpusId=\"269757388\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Educational Content Assessment**: Detection methods based on token and token sequence probabilities have been utilized to predict text readability for educational purposes. By extracting log probabilities from models like GPT-2, researchers have created tools that can predict teachers' text readability ratings, helping to identify appropriate educational materials for different learning levels. This application demonstrates how language model detection capabilities can support educational content development and selection. <Paper corpusId=\"271880412\" paperTitle=\"(Hussain et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Plaza-del-Arco et al., 2023)", "paper": {"corpus_id": 259376532, "title": "Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech", "year": 2023, "venue": "WOAH", "authors": [{"name": "F. Plaza-del-Arco", "authorId": "1410406981"}, {"name": "Debora Nozza", "authorId": "2101317501"}, {"name": "Dirk Hovy", "authorId": "2022288"}], "n_citations": 57}, "snippets": ["Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data."], "score": 0.54541015625}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 269757388, "title": "DoLLM: How Large Language Models Understanding Network Flow Data to Detect Carpet Bombing DDoS", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qingyang Li", "authorId": "2301171718"}, {"name": "Yihang Zhang", "authorId": "2257101405"}, {"name": "Zhidong Jia", "authorId": "2301200359"}, {"name": "Yannan Hu", "authorId": "2216800907"}, {"name": "Lei Zhang", "authorId": "2301176195"}, {"name": "Jianrong Zhang", "authorId": "2301169914"}, {"name": "Yongming Xu", "authorId": "2301199102"}, {"name": "Yong Cui", "authorId": "2301175274"}, {"name": "Zongming Guo", "authorId": "2112582246"}, {"name": "Xinggong Zhang", "authorId": "1714130"}], "n_citations": 8}, "snippets": ["It is an interesting question Can and How Large Language Models (LLMs) understand non-language network data, and help us detect unknown malicious flows. This paper takes Carpet Bombing as a case study and shows how to exploit LLMs' powerful capability in the networking area."], "score": 0.51123046875}, {"id": "(Hussain et al., 2024)", "paper": {"corpus_id": 271880412, "title": "A tutorial on open-source large language models for behavioral science", "year": 2024, "venue": "Behavior Research Methods", "authors": [{"name": "Zak Hussain", "authorId": "2302450822"}, {"name": "Marcel Binz", "authorId": "32354733"}, {"name": "Rui Mata", "authorId": "2184033930"}, {"name": "Dirk U. Wulff", "authorId": "2302732683"}], "n_citations": 25}, "snippets": ["Measures based on token and token sequence probabilities have thus been used to, for instance, investigate how language models capture grammatical gender (An et al., 2019) and to predict human reading times (e.g., Merkx & Frank, 2020). The present example demonstrates how the log probabilities extracted from GPT-2 can be used to predict teachers' text readability ratings."], "score": 0.541015625}], "table": null}], "cost": 0.258882}}
