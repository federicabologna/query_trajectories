{"original_query": "Hi everyone, I want to use LLM to evaluate the quality of my synthetically generated data using GPT-4. I was wondering if I should use GPT-4 as the evaluator LLM or should I use a different LLM so that it does not bias its own generation. Can someone kindly explain the best practice right now for this? thanks!", "input": "want LLM to evaluate the quality generated data GPT-4 wondering use GPT-4 the evaluator LLM or different LLM so that it does own generation the best practice thanks", "output": {"sections": [{"title": "Background on LLMs as Evaluators", "tldr": "Large Language Models (LLMs) have emerged as powerful tools for evaluating text quality, including their own outputs. They can serve as automated judges that assess various aspects of generated content like coherence, factuality, and creativity. (LLM Memory)", "text": "\nLarge Language Models (LLMs) like GPT-4 are increasingly being used not only to generate content but also to evaluate the quality of text. This practice has become particularly important as organizations seek efficient ways to assess LLM-generated outputs at scale. LLMs can be prompted to act as evaluators that judge various dimensions of text quality, including factual accuracy, coherence, relevance, creativity, and adherence to instructions. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe use of LLMs as evaluators stems from their ability to understand context, detect nuances in language, and apply complex criteria when assessing text. This capability has created a new paradigm where machines can provide human-like feedback on content quality without the need for extensive human evaluation, which is often costly and time-consuming. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nLLMs can evaluate text through various methods, including direct scoring (e.g., rating on a scale of 1-10), comparative judgments (deciding which of two texts is better), or detailed qualitative feedback. Researchers have found that when properly prompted, advanced LLMs can achieve evaluation results that correlate strongly with human judgments across many tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nHowever, using LLMs to evaluate their own or other LLMs' outputs presents both opportunities and challenges. While it offers a scalable evaluation solution, concerns exist about potential biases, the \"self-rater\" problem (where models may be biased toward their own outputs), and the reliability of such evaluations for highly specialized or novel content. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Approaches to LLM-based Evaluation", "tldr": "LLMs can evaluate text quality through several established approaches, with self-reflection and using separate \"oracle\" LLMs being two primary strategies. (1 source)", "text": "\nResearchers and practitioners have developed several methods for using LLMs as evaluators of text quality:\n\n1. **Self-Reflection**: In this approach, after generating content, the same LLM is prompted to reconsider and critique its own output to identify potential shortcomings. This technique has been shown to improve accuracy by enabling the model to catch and correct its own errors. <Paper corpusId=\"268032876\" paperTitle=\"(Ghaisas et al., 2024)\" isShortName></Paper>\n\n2. **Oracle-LLM Evaluation**: This strategy employs a separate, often more powerful LLM as an external evaluator or \"oracle.\" For example, GPT-4 has been used as an oracle to compare and rank outputs from various other models like LLAMA, Alpaca, and ChatGPT, effectively measuring relative differences in generation quality. <Paper corpusId=\"268032876\" paperTitle=\"(Ghaisas et al., 2024)\" isShortName></Paper>\n\n3. **Comparative Judgments**: LLMs can be tasked with comparing two or more text samples and determining which one is superior based on specific criteria such as coherence, accuracy, or relevance. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n4. **Rubric-Based Evaluation**: Models can be provided with detailed rubrics or criteria to systematically evaluate text against specific dimensions of quality. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n5. **Numerical Scoring**: LLMs can assign numerical scores to text samples based on predefined quality metrics, creating quantifiable evaluation results. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThese approaches can be used individually or in combination to create robust evaluation frameworks that leverage the linguistic understanding capabilities of LLMs while attempting to mitigate potential biases.", "citations": [{"id": "(Ghaisas et al., 2024)", "paper": {"corpus_id": 268032876, "title": "Dealing with Data for RE: Mitigating Challenges while using NLP and Generative AI", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "S. Ghaisas", "authorId": "2107155"}, {"name": "Anmol Singhal", "authorId": "2120316520"}], "n_citations": 0}, "snippets": ["Beyond utilizing domain experts to validate generated data, practitioners have also begun deploying LLMs themselves for evaluation purposes. Two predominant strategies for using LLMs as evaluators are as follows: \n\n-Self-Reflection [24]: Following the completion of a task by an LLM, the model is prompted to reconsider its generated output and identify any potential shortcomings. This method has demonstrated an enhancement in accuracy. \n\n-Engaging an Oracle-LLM for Evaluation [15]: This strategy involves the use of an auxiliary, typically more powerful, LLM for evaluation. For instance, the authors of Vicuna deployed GPT-4 as an 'oracle' LLM to compare and rank the outputs of various LLMs, including LLAMA, Alpaca, and Chat-GPT, thereby ascertaining the relative differences in generation quality."], "score": 0.76708984375}], "table": null}, {"title": "Using GPT-4 as an Evaluator: Benefits and Limitations", "tldr": "GPT-4 is widely used as an evaluator due to its advanced capabilities, but researchers have identified significant limitations including potential bias when evaluating its own outputs and the \"evaluator-generator\" capability paradox. (5 sources)", "text": "\nGPT-4 has emerged as a popular choice for text evaluation tasks due to its advanced capabilities and reasoning abilities. Researchers frequently utilize GPT-4 as an evaluator, leveraging its status as one of the most sophisticated LLMs currently available <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This preference is reflected in the growing body of research that uses GPT-4's judgments as the standard for assessing the quality of text generation, including in specialized domains like medical question answering <Paper corpusId=\"266690837\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\nHowever, using GPT-4 as an evaluator presents several notable limitations. One significant concern is bias when GPT-4 evaluates outputs generated by itself or models with comparable capabilities. This creates what researchers describe as a \"chicken-and-egg dilemma\" - developing better LLMs requires robust evaluation methods, but the most capable evaluators are themselves advanced LLMs <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. GPT-4 has been observed to exhibit \"egocentric bias,\" potentially favoring its own generated responses over those from other models <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThe evaluation landscape is further complicated by the challenge of impartiality. When the evaluating model (LLM-as-evaluator) has similar capabilities to the model being evaluated (LLM-as-generator), the objectivity of the assessment becomes questionable <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This has led to calls for more diverse evaluation approaches that go beyond single-model judgments.\n\nTo address these limitations, researchers recommend using a broader spectrum of evaluation methods, including various benchmarks, diverse evaluation criteria <Paper corpusId=\"215548699\" paperTitle=\"(Sellam et al., 2020)\" isShortName></Paper>, and human feedback <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>. The NLP community has also begun exploring approaches that train specialized critique generation models based on evaluation data labeled through GPT-4's direct prompting, creating a more standardized evaluation framework <Paper corpusId=\"270738200\" paperTitle=\"(Ke et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Li et al., 2024)", "paper": {"corpus_id": 270391675, "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Zhen Li", "authorId": "2145256331"}, {"name": "Xiaohan Xu", "authorId": "2279658967"}, {"name": "Tao Shen", "authorId": "2279548827"}, {"name": "Can Xu", "authorId": "2284826718"}, {"name": "Jia-Chen Gu", "authorId": "2308241851"}, {"name": "Yuxuan Lai", "authorId": "2308073132"}, {"name": "Chongyang Tao", "authorId": "2287928517"}, {"name": "Shuai Ma", "authorId": "2307142498"}], "n_citations": 15}, "snippets": ["LLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;(Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."], "score": 0.57275390625}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 266690837, "title": "EHR Interaction Between Patients and AI: NoteAid EHR Interaction", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xiaocheng Zhang", "authorId": "2277238665"}, {"name": "Zonghai Yao", "authorId": "1576489304"}, {"name": "Hong Yu", "authorId": "2261455807"}], "n_citations": 2}, "snippets": ["We utilized the LLMs' ability in medical questions reasoning to evaluate the performance of collected conversation. We randomly selected 100 instances from the 876 generated based on MADE and combined them with 1000 test and 1000 validation data generated from MIMIC-III for LLM evaluation (totally 2100 cases). The LLMs used for evaluation were GPT-3.5-Turbo (referred to as Turbo in this section) and GPT-4. We aggregated the criteria into an evaluation prompt Table 4 to guide the LLMs in scoring each conversation. The results are shown in the Table 2. From the figure, it is evident that Turbo received higher scores than GPT-4, particularly in the Turbo NIP Explanation task. This discrepancy can be attributed to the strict evaluation criteria outlined in our evaluation prompt. Turbo, while slightly less capable than GPT-4, did not fully meet these criteria. Additionally, we observed that data generated by GPT-4 NIP, whether in the Q&A task or the Explanation task, exhibited more stable scores across both Turbo and GPT-4 evaluations. Notably, GPT-4 received a noticeably lower score in the Turbo NIP Explanation task when compared to Turbo."], "score": 0.55810546875}, {"id": "(Sellam et al., 2020)", "paper": {"corpus_id": 215548699, "title": "BLEURT: Learning Robust Metrics for Text Generation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Thibault Sellam", "authorId": "145450400"}, {"name": "Dipanjan Das", "authorId": "143790066"}, {"name": "Ankur P. Parikh", "authorId": "144729897"}], "n_citations": 1505}, "snippets": ["Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution."], "score": 0.0}, {"id": "(Ouyang et al., 2022)", "paper": {"corpus_id": 246426909, "title": "Training language models to follow instructions with human feedback", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Long Ouyang", "authorId": "31793034"}, {"name": "Jeff Wu", "authorId": "49387725"}, {"name": "Xu Jiang", "authorId": "2115903168"}, {"name": "Diogo Almeida", "authorId": "2061137049"}, {"name": "Carroll L. Wainwright", "authorId": "2064084601"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Chong Zhang", "authorId": null}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Katarina Slama", "authorId": "2117680841"}, {"name": "Alex Ray", "authorId": "2064770039"}, {"name": "John Schulman", "authorId": "47971768"}, {"name": "Jacob Hilton", "authorId": "2052366271"}, {"name": "Fraser Kelton", "authorId": "2151735262"}, {"name": "Luke E. Miller", "authorId": "2142365973"}, {"name": "Maddie Simens", "authorId": "2151735251"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "P. Welinder", "authorId": "2930640"}, {"name": "P. Christiano", "authorId": "145791315"}, {"name": "Jan Leike", "authorId": "2990741"}, {"name": "Ryan J. Lowe", "authorId": "49407415"}], "n_citations": 13203}, "snippets": ["Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."], "score": 0.0}, {"id": "(Ke et al., 2023)", "paper": {"corpus_id": 270738200, "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Pei Ke", "authorId": "1886879"}, {"name": "Bosi Wen", "authorId": "2122225897"}, {"name": "Andrew Feng", "authorId": "2316634875"}, {"name": "Xiao Liu", "authorId": "2308072332"}, {"name": "Xuanyu Lei", "authorId": "2181283109"}, {"name": "Jiale Cheng", "authorId": "2308160059"}, {"name": "Shengyuan Wang", "authorId": "2151486382"}, {"name": "Aohan Zeng", "authorId": "2051712753"}, {"name": "Yuxiao Dong", "authorId": "2243402027"}, {"name": "Hongning Wang", "authorId": "2253869803"}, {"name": "Jie Tang", "authorId": "2260595820"}, {"name": "Minlie Huang", "authorId": "2254009342"}], "n_citations": 34}, "snippets": ["Since the NLP community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting."], "score": 0.63037109375}], "table": null}, {"title": "Best Practices for LLM Evaluation", "tldr": "To mitigate inherent biases when using LLMs as evaluators, practitioners should implement multiple evaluation methods and employ diverse benchmark datasets. Using separate models for generation and evaluation helps reduce egocentric bias and improve assessment reliability. (3 sources)", "text": "\nTo address the challenges associated with LLM-based evaluation, researchers have identified several best practices that can help ensure more reliable and unbiased assessments:\n\n1. **Use diverse evaluation methods**: Rather than relying solely on a single LLM as an evaluator, employ a broader spectrum of evaluation approaches including various benchmarks, different evaluation criteria, and human feedback <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"215548699\" paperTitle=\"(Sellam et al., 2020)\" isShortName></Paper>. This multi-method approach helps mitigate the limitations of any single evaluation technique.\n\n2. **Separate generation from evaluation**: When possible, use different models for content generation and evaluation to reduce the \"egocentric bias\" problem where models favor their own outputs. This separation helps address the fundamental \"chicken-and-egg dilemma\" in LLM evaluation <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\n3. **Incorporate human judgment**: Human feedback remains a crucial component of effective evaluation frameworks. Models fine-tuned with human feedback, like InstructGPT, have demonstrated improved alignment with user intent and reduced toxic output generation <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>.\n\n4. **Define clear evaluation criteria**: Providing well-defined rubrics and specific evaluation dimensions helps guide the LLM evaluator toward more consistent and targeted assessments. This approach allows for more structured feedback on particular aspects of text quality <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\n5. **Consider model capability alignment**: Be aware of the relative capabilities between the evaluator and generator models. The most reliable evaluations typically come from using more capable models to evaluate less capable ones, rather than having models of similar capabilities evaluate each other <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\n6. **Validate evaluation results**: Cross-validate LLM evaluations against established benchmarks and, when possible, human judgments to ensure the evaluations align with recognized quality standards <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nBy implementing these best practices, practitioners can develop more robust evaluation frameworks that leverage the strengths of LLMs while minimizing their inherent limitations as evaluators.", "citations": [{"id": "(Li et al., 2024)", "paper": {"corpus_id": 270391675, "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Zhen Li", "authorId": "2145256331"}, {"name": "Xiaohan Xu", "authorId": "2279658967"}, {"name": "Tao Shen", "authorId": "2279548827"}, {"name": "Can Xu", "authorId": "2284826718"}, {"name": "Jia-Chen Gu", "authorId": "2308241851"}, {"name": "Yuxuan Lai", "authorId": "2308073132"}, {"name": "Chongyang Tao", "authorId": "2287928517"}, {"name": "Shuai Ma", "authorId": "2307142498"}], "n_citations": 15}, "snippets": ["LLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;(Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."], "score": 0.57275390625}, {"id": "(Sellam et al., 2020)", "paper": {"corpus_id": 215548699, "title": "BLEURT: Learning Robust Metrics for Text Generation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Thibault Sellam", "authorId": "145450400"}, {"name": "Dipanjan Das", "authorId": "143790066"}, {"name": "Ankur P. Parikh", "authorId": "144729897"}], "n_citations": 1505}, "snippets": ["Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution."], "score": 0.0}, {"id": "(Ouyang et al., 2022)", "paper": {"corpus_id": 246426909, "title": "Training language models to follow instructions with human feedback", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Long Ouyang", "authorId": "31793034"}, {"name": "Jeff Wu", "authorId": "49387725"}, {"name": "Xu Jiang", "authorId": "2115903168"}, {"name": "Diogo Almeida", "authorId": "2061137049"}, {"name": "Carroll L. Wainwright", "authorId": "2064084601"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Chong Zhang", "authorId": null}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Katarina Slama", "authorId": "2117680841"}, {"name": "Alex Ray", "authorId": "2064770039"}, {"name": "John Schulman", "authorId": "47971768"}, {"name": "Jacob Hilton", "authorId": "2052366271"}, {"name": "Fraser Kelton", "authorId": "2151735262"}, {"name": "Luke E. Miller", "authorId": "2142365973"}, {"name": "Maddie Simens", "authorId": "2151735251"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "P. Welinder", "authorId": "2930640"}, {"name": "P. Christiano", "authorId": "145791315"}, {"name": "Jan Leike", "authorId": "2990741"}, {"name": "Ryan J. Lowe", "authorId": "49407415"}], "n_citations": 13203}, "snippets": ["Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."], "score": 0.0}], "table": null}], "cost": 0.120396}}
