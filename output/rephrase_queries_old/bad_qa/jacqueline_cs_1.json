{"original_query": "What exactly is the \"dark knowledge\" that gets distilled in the standard formulation of knowledge distillation, wherein information is transferred from a teacher network to a student network? ", "input": "dark knowledge that gets standard formulation knowledge distillation information from a teacher network to student network", "output": {"sections": [{"title": "Introduction to Dark Knowledge and Knowledge Distillation", "tldr": "Knowledge distillation is a technique that transfers \"dark knowledge\" from a larger teacher network to a smaller student network, enabling efficient model compression. This approach allows smaller models to achieve performance comparable to their larger counterparts while requiring fewer computational resources. (9 sources)", "text": "\nKnowledge distillation (KD) has emerged as a pivotal technique in the field of machine learning, particularly for model compression and transfer learning. At its core, knowledge distillation involves extracting \"dark knowledge\" from a complex teacher network and transferring it to guide the learning of a more lightweight student network <Paper corpusId=\"219636179\" paperTitle=\"(Xu et al., 2020)\" isShortName></Paper>. This approach enables smaller models to approximate the performance of their larger counterparts while requiring significantly fewer computational resources <Paper corpusId=\"274655700\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277754927\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>.\n\nThe concept of knowledge distillation was initially proposed in 2006, though it wasn't explicitly explained at that time. In 2014, the method was formally defined as a process where a student network learns from the soft targets output by a teacher network <Paper corpusId=\"248683566\" paperTitle=\"(Xu et al., 2022)\" isShortName></Paper>. Since then, knowledge distillation has gained substantial attention as an effective strategy for enhancing the generalization capabilities of more compact neural networks <Paper corpusId=\"273227005\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\nThe fundamental principle behind knowledge distillation is that a smaller student model can benefit not only from ground-truth labels but also from the \"dark knowledge\" embedded in a larger teacher model <Paper corpusId=\"265444951\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>. This dark knowledge represents implicit insights not present in the ground-truth labels alone, allowing the student model to achieve performance levels that would be difficult to reach through conventional training methods <Paper corpusId=\"276249293\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nIn practical terms, knowledge distillation creates a pathway for transferring the sophisticated learning patterns of complex networks to simpler, more deployable architectures <Paper corpusId=\"264372297\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. This has significant implications for applications where computational efficiency is crucial, as it enables the development of models that maintain high accuracy while being much more efficient than their teacher counterparts <Paper corpusId=\"267657497\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Xu et al., 2020)", "paper": {"corpus_id": 219636179, "title": "Knowledge Distillation Meets Self-Supervision", "year": 2020, "venue": "European Conference on Computer Vision", "authors": [{"name": "Guodong Xu", "authorId": "46538811"}, {"name": "Ziwei Liu", "authorId": "2117940996"}, {"name": "Xiaoxiao Li", "authorId": "2108536754"}, {"name": "Chen Change Loy", "authorId": "1717179"}], "n_citations": 285}, "snippets": ["Knowledge distillation, which involves extracting the \"dark knowledge\" from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning."], "score": 0.93212890625}, {"id": "(Zhou et al., 2024)", "paper": {"corpus_id": 274655700, "title": "All You Need in Knowledge Distillation Is a Tailored Coordinate System", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Junjie Zhou", "authorId": "2331698741"}, {"name": "Ke Zhu", "authorId": "2273931950"}, {"name": "Jianxin Wu", "authorId": "2274078411"}], "n_citations": 3}, "snippets": ["Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy."], "score": 0.65869140625}, {"id": "(Zhou et al., 2025)", "paper": {"corpus_id": 277754927, "title": "All You Need in Knowledge Distillation Is a Tailored Coordinate System", "year": 2025, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Junjie Zhou", "authorId": "2331698741"}, {"name": "Ke Zhu", "authorId": "2273931950"}, {"name": "Jianxin Wu", "authorId": "2274078411"}], "n_citations": 0}, "snippets": ["Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy."], "score": 0.66015625}, {"id": "(Xu et al., 2022)", "paper": {"corpus_id": 248683566, "title": "Teacher-student collaborative knowledge distillation for image classification", "year": 2022, "venue": "Applied intelligence (Boston)", "authors": [{"name": "Chuanyun Xu", "authorId": "2817613"}, {"name": "Wenjian Gao", "authorId": "2164051968"}, {"name": "Tian Li", "authorId": "2164318078"}, {"name": "Nanlan Bai", "authorId": "2164821600"}, {"name": "Gang Li", "authorId": "2155121570"}, {"name": "Yang Zhang", "authorId": "2145954082"}], "n_citations": 44}, "snippets": ["Knowledge distillation (KD), an important method of model compression (Tan et al., 2019)(Cheng et al., 2018)(Bashir et al., 2020), is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model (Yim et al., 2017)(Kim et al., 2016)(Gou et al., 2020). This concept was first proposed in (Bucila et al., 2006), but then was not explicitly explained. In 2014, [10] proposed an approach that enables a student network to learn the soft targets output by a teacher network and defined the method as knowledge distillation."], "score": 0.6728515625}, {"id": "(Yin et al., 2024)", "paper": {"corpus_id": 273227005, "title": "Two-Stage Approach for Targeted Knowledge Transfer in Self-Knowledge Distillation", "year": 2024, "venue": "IEEE/CAA Journal of Automatica Sinica", "authors": [{"name": "Zimo Yin", "authorId": "2290035516"}, {"name": "Jian Pu", "authorId": "2142824615"}, {"name": "Yijie Zhou", "authorId": "2261082542"}, {"name": "Xiangyang Xue", "authorId": "2251995827"}], "n_citations": 0}, "snippets": ["Knowledge distillation (KD) enhances student network generalization by transferring dark knowledge from a complex teacher network."], "score": 0.650390625}, {"id": "(Su et al., 2023)", "paper": {"corpus_id": 265444951, "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation", "year": 2023, "venue": "", "authors": [{"name": "Chi-Ping Su", "authorId": "2268314644"}, {"name": "Ching-Hsun Tseng", "authorId": "1569686364"}, {"name": "Bin Pu", "authorId": "2338265886"}, {"name": "Lei Zhao", "authorId": "2338506240"}, {"name": "Zhuangzhuang Chen", "authorId": "2328588941"}, {"name": "Shin-Jye Lee", "authorId": "2116351339"}], "n_citations": 2}, "snippets": ["Knowledge Distillation (KD) enables a smaller\"student\"model to mimic a larger\"teacher\"model by transferring knowledge from the teacher's output or features", "Knowledge Distillation (KD) [8] offers a solution by enabling a compact \"student\" model to mimic a larger \"teacher\" model, allowing the student to learn from both ground-truth labels and the teacher's \"dark knowledge\" -the implicit insights not present in the ground-truth labels -enabling it to approach the teacher's performance in a compact form."], "score": 0.56494140625}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 276249293, "title": "Contrastive Representation Distillation via Multi-Scale Feature Decoupling", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Cuipeng Wang", "authorId": "2344789398"}, {"name": "Tieyuan Chen", "authorId": "2344967066"}, {"name": "Haipeng Wang", "authorId": "2344788684"}], "n_citations": 0}, "snippets": ["Knowledge distillation transfers the \"dark knowledge\" of a complex teacher network to a lightweight student network, enhancing the performance of the student network."], "score": 0.6796875}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 264372297, "title": "Improving Knowledge Distillation via Head and Tail Categories", "year": 2024, "venue": "IEEE transactions on circuits and systems for video technology (Print)", "authors": [{"name": "Liuchi Xu", "authorId": "2215812572"}, {"name": "Jin Ren", "authorId": "2157467240"}, {"name": "Zhenhua Huang", "authorId": "2151325820"}, {"name": "Weishi Zheng", "authorId": "2265252477"}, {"name": "Yunwen Chen", "authorId": "2261262093"}], "n_citations": 9}, "snippets": ["Knowledge distillation (KD) is a technique that transfers \"dark knowledge\" from a deep teacher network (teacher) to a shallow student network (student)."], "score": 0.61083984375}, {"id": "(Kim et al., 2023)", "paper": {"corpus_id": 267657497, "title": "Maximizing discrimination capability of knowledge distillation with energy function", "year": 2023, "venue": "Knowledge-Based Systems", "authors": [{"name": "Seonghak Kim", "authorId": "2268350036"}, {"name": "Gyeongdo Ham", "authorId": "2156910329"}, {"name": "Suin Lee", "authorId": "2268370058"}, {"name": "Donggon Jang", "authorId": "2268310103"}, {"name": "Daeshik Kim", "authorId": "2145154407"}], "n_citations": 4}, "snippets": ["Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks."], "score": 0.68896484375}], "table": null}, {"title": "Nature of Dark Knowledge", "tldr": "Dark knowledge refers to the rich information embedded in a teacher model's output probability distributions that goes beyond one-hot labels. This includes class similarity relationships, uncertainty estimates, and implicit insights that help student models generalize better. (19 sources)", "text": "\nDark knowledge, a term popularized by Hinton et al., represents the subtle information contained within a neural network's output probability distribution that is not captured by simple one-hot encoded labels <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. This knowledge primarily manifests in the relative probabilities assigned to incorrect classes, revealing how the teacher model perceives similarities between different categories <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper> <Paper corpusId=\"237091534\" paperTitle=\"(Ren et al., 2021)\" isShortName></Paper>.\n\nAt its core, dark knowledge encompasses the instance-to-class similarities that provide richer supervision signals for student networks <Paper corpusId=\"224801420\" paperTitle=\"(Deng et al., 2020)\" isShortName></Paper>. When a teacher network processes an image, it might assign a high probability to the correct class (e.g., \"cat\") but also non-zero probabilities to visually similar classes (e.g., \"dog\"), capturing nuanced visual relationships that are absent in binary ground-truth labels <Paper corpusId=\"236984375\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. This creates a more informative training signal that helps student models develop better generalization capabilities <Paper corpusId=\"267413204\" paperTitle=\"(Jin et al., 2024)\" isShortName></Paper>.\n\nThe nature of dark knowledge extends beyond just inter-class relationships. It encompasses the decision-making processes and uncertainty estimates of the teacher model <Paper corpusId=\"269921267\" paperTitle=\"(Mei et al., 2024)\" isShortName></Paper>. For instance, in complex discrimination tasks, dark knowledge can help refine noisy labels and provide insights into the underlying data structure that might not be explicitly present in the original training dataset <Paper corpusId=\"203642142\" paperTitle=\"(Dong et al., 2019)\" isShortName></Paper> <Paper corpusId=\"252668749\" paperTitle=\"(Biehler et al., 2022)\" isShortName></Paper>.\n\nTemperature scaling plays a critical role in revealing dark knowledge. By increasing the temperature parameter in the softmax function, the probability distribution becomes smoother, allowing more nuanced information about category interrelationships to be transferred to the student model <Paper corpusId=\"265384964\" paperTitle=\"(Xie et al., 2023)\" isShortName></Paper>. This smoother distribution yields smaller variance in gradients, often leading to faster convergence during student network training <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nDark knowledge can manifest at various network levels. While high-layer features typically relate to specific task objectives, lower-layer features represent more generic attributes like edges and corners. All these features contain valuable dark knowledge that can be transferred to guide the student network's training process <Paper corpusId=\"198179767\" paperTitle=\"(Zhao et al., 2019)\" isShortName></Paper>. This multi-level knowledge transfer creates a comprehensive learning framework where students benefit from both task-specific and general feature representations developed by the teacher.\n\nIn practical applications, dark knowledge serves as an additional source of supervision alongside ground-truth labels <Paper corpusId=\"232269823\" paperTitle=\"(Zhao et al., 2021)\" isShortName></Paper>. This dual supervision approach enables student networks to learn both the primary task objectives and the subtle class relationships captured by the teacher <Paper corpusId=\"247521335\" paperTitle=\"(Ye et al., 2022)\" isShortName></Paper>. The integration of dark knowledge has proven particularly effective in various contexts, including image classification, metric learning, and even lossy compression tasks where perfect reconstruction is rarely achievable <Paper corpusId=\"249017724\" paperTitle=\"(Bai et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260447668\" paperTitle=\"(Cui et al., 2022)\" isShortName></Paper>.\n\nThe effectiveness of dark knowledge transfer explains why student models often outperform models trained directly on hard labels alone. By learning from the teacher's probability distributions, student networks acquire information about unobserved intents and subtle data relationships that facilitate better decision boundaries and improved generalization <Paper corpusId=\"269033278\" paperTitle=\"(Hu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275920765\" paperTitle=\"(Takanami et al., 2025)\" isShortName></Paper> <Paper corpusId=\"264555654\" paperTitle=\"(Jung et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Hinton et al., 2015)", "paper": {"corpus_id": 7200347, "title": "Distilling the Knowledge in a Neural Network", "year": 2015, "venue": "arXiv.org", "authors": [{"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "J. Dean", "authorId": "49959210"}], "n_citations": 19742}, "snippets": ["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."], "score": 0.0}, {"id": "(Nayak et al., 2019)", "paper": {"corpus_id": 159041346, "title": "Zero-Shot Knowledge Distillation in Deep Networks", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Gaurav Kumar Nayak", "authorId": "143747407"}, {"name": "Konda Reddy Mopuri", "authorId": "2217000"}, {"name": "Vaisakh Shaj", "authorId": "7155274"}, {"name": "R. Venkatesh Babu", "authorId": "144682140"}, {"name": "Anirban Chakraborty", "authorId": "1429640900"}], "n_citations": 245}, "snippets": ["The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability."], "score": 0.6123046875}, {"id": "(Ren et al., 2021)", "paper": {"corpus_id": 237091534, "title": "Online Multi-Granularity Distillation for GAN Compression", "year": 2021, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Yuxi Ren", "authorId": null}, {"name": "Jie Wu", "authorId": "2118432533"}, {"name": "Xuefeng Xiao", "authorId": "2118724465"}, {"name": "Jianchao Yang", "authorId": "1706007"}], "n_citations": 39}, "snippets": ["In this way, the student network attempts to learn dark knowledge (Hinton et al., 2015) that contains the similarities between different classes, which can not be provided by the ground truth labels."], "score": 0.53564453125}, {"id": "(Deng et al., 2020)", "paper": {"corpus_id": 224801420, "title": "Locally Linear Region Knowledge Distillation", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Xiang Deng", "authorId": "2150478789"}, {"name": "Zhongfei Zhang", "authorId": "2118748124"}], "n_citations": 0}, "snippets": ["The soft targets contain the information about instance-to-class similarities (i..e, dark knowledge) that can improve the student performance."], "score": 0.67919921875}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 236984375, "title": "Combine-Net: An Improved Filter Pruning Algorithm", "year": 2021, "venue": "Inf.", "authors": [{"name": "Jinghan Wang", "authorId": "2109643902"}, {"name": "Guangyue Li", "authorId": "2151302904"}, {"name": "Wenzhao Zhang", "authorId": "2107940856"}], "n_citations": 3}, "snippets": ["The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics."], "score": 0.60888671875}, {"id": "(Jin et al., 2024)", "paper": {"corpus_id": 267413204, "title": "Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Can Jin", "authorId": "2308012469"}, {"name": "Tong Che", "authorId": "2282541442"}, {"name": "Hongwu Peng", "authorId": "2282596442"}, {"name": "Yiyuan Li", "authorId": "2282543805"}, {"name": "Marco Pavone", "authorId": "2237790577"}], "n_citations": 47}, "snippets": ["The dark knowledge method (Hinton et al., 2015) further develops KD, where a student model aims to fully match the output distribution of the teacher. Intuitively, distillation is effective because the teacher's output distribution over classes provides a more informative training signal than a one-hot label."], "score": 0.53662109375}, {"id": "(Mei et al., 2024)", "paper": {"corpus_id": 269921267, "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks", "year": 2024, "venue": "2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)", "authors": [{"name": "Taiyuan Mei", "authorId": "2298916636"}, {"name": "Yun Zi", "authorId": "2298918720"}, {"name": "X. Cheng", "authorId": "2222987403"}, {"name": "Zijun Gao", "authorId": "2297725659"}, {"name": "Qi Wang", "authorId": "2297735971"}, {"name": "Haowei Yang", "authorId": "2302372513"}], "n_citations": 20}, "snippets": ["Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model"], "score": 0.56396484375}, {"id": "(Dong et al., 2019)", "paper": {"corpus_id": 203642142, "title": "Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network", "year": 2019, "venue": "arXiv.org", "authors": [{"name": "Bin Dong", "authorId": "145496882"}, {"name": "Jikai Hou", "authorId": "104162954"}, {"name": "Yiping Lu", "authorId": "48518029"}, {"name": "Zhihua Zhang", "authorId": "47294286"}], "n_citations": 41}, "snippets": ["To explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels."], "score": 0.64013671875}, {"id": "(Biehler et al., 2022)", "paper": {"corpus_id": 252668749, "title": "Using Knowledge Distillation to improve interpretable models in a retail banking context", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Maxime Biehler", "authorId": "2186740719"}, {"name": "Mohamed Guermazi", "authorId": "2186740508"}, {"name": "C'elim Starck", "authorId": "2186740498"}], "n_citations": 2}, "snippets": ["Dark knowledge refers to information not directly encoded in the original training dataset, which nevertheless is relevant to the prediction task at hand. It is made explicit by a teacher model, then passed down through knowledge distillation. Although the term has been coined in the frame of soft targets (Hinton, Vinyals, and Dean, 2015), we argue that the teacher-student framework, along with the general idea of transmitting information from the one to the other through model outputs, can in practice be used in other settings while still being referred to as knowledge distillation."], "score": 0.67138671875}, {"id": "(Xie et al., 2023)", "paper": {"corpus_id": 265384964, "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation", "year": 2023, "venue": "Fire", "authors": [{"name": "Jinzhou Xie", "authorId": "2268047558"}, {"name": "Hongmin Zhao", "authorId": "2268031861"}], "n_citations": 6}, "snippets": ["With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation."], "score": 0.630859375}, {"id": "(Zhang et al., 2020)", "paper": {"corpus_id": 212855595, "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Zhenyu Zhang", "authorId": "47295143"}, {"name": "Xiaobo Shu", "authorId": "2269366"}, {"name": "Yu Bowen", "authorId": "48613402"}, {"name": "Tingwen Liu", "authorId": "2079682"}, {"name": "Jiapeng Zhao", "authorId": "48019474"}, {"name": "Quangang Li", "authorId": "2108645146"}, {"name": "Li Guo", "authorId": "48358041"}], "n_citations": 35}, "snippets": ["In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model", "The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al., 2017))."], "score": 0.818359375}, {"id": "(Zhao et al., 2019)", "paper": {"corpus_id": 198179767, "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching", "year": 2019, "venue": "IEEE Transactions on Cybernetics", "authors": [{"name": "Haoran Zhao", "authorId": "50981688"}, {"name": "Xin Sun", "authorId": "144326521"}, {"name": "Junyu Dong", "authorId": "1964397"}, {"name": "Changrui Chen", "authorId": "10944885"}, {"name": "Zihe Dong", "authorId": "2087106420"}], "n_citations": 59}, "snippets": ["Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process", "Hinton et al. (Hinton et al., 2015) extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks."], "score": 0.55322265625}, {"id": "(Zhao et al., 2021)", "paper": {"corpus_id": 232269823, "title": "Similarity Transfer for Knowledge Distillation", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Haoran Zhao", "authorId": "50981688"}, {"name": "Kun Gong", "authorId": "2058266174"}, {"name": "Xin Sun", "authorId": "144326521"}, {"name": "Junyu Dong", "authorId": "1964397"}, {"name": "Hui Yu", "authorId": "145429878"}], "n_citations": 3}, "snippets": ["Note that, the outputs of the teacher network are defined as the dark knowledge in KD, which provides similarity information as extra supervisions compared with one-hot labels. In other words, there are two sources of supervision used to train the student in KD, one from the ground-truth label, and the other from the soft targets of teacher network."], "score": 0.81787109375}, {"id": "(Ye et al., 2022)", "paper": {"corpus_id": 247521335, "title": "Generalized Knowledge Distillation via Relationship Matching", "year": 2022, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Han-Jia Ye", "authorId": "2151459740"}, {"name": "Su Lu", "authorId": "2115435395"}, {"name": "De-chuan Zhan", "authorId": "1721819"}], "n_citations": 21}, "snippets": ["In detail, a strong classifier, e.g., a neural network trained with deeper architectures [5], high-quality images [6], or precise optimization strategies [7], [8], acts as a \"teacher\" and guides the training of a \"student\" model by richer supervision, so that the learning experience from a related task is reused in the current task.\n\nThe teacher's class posterior probability over an instance is the most common dark knowledge, as it indicates the teacher's estimation of how similar an instance is to candidate categories. Besides the extreme \"black or white\" supervision, the student is asked to align its posterior with the teacher during its training progress."], "score": 0.771484375}, {"id": "(Bai et al., 2022)", "paper": {"corpus_id": 249017724, "title": "Improving the Latent Space of Image Style Transfer", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Yun-Hao Bai", "authorId": "48442720"}, {"name": "Cairong Wang", "authorId": "47073960"}, {"name": "C. Yuan", "authorId": "2117729099"}, {"name": "Yanbo Fan", "authorId": "2140245719"}, {"name": "Jue Wang", "authorId": "2144537318"}], "n_citations": 0}, "snippets": ["Knowledge distillation (KD) [26,27](Yu et al., 2019) is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability (Yu et al., 2019)(Peng et al., 2019), which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted."], "score": 0.71826171875}, {"id": "(Cui et al., 2022)", "paper": {"corpus_id": 260447668, "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback", "year": 2022, "venue": "IEEE Transactions on Communications", "authors": [{"name": "Yiming Cui", "authorId": "2172485573"}, {"name": "Jiajia Guo", "authorId": "47093519"}, {"name": "Zheng Cao", "authorId": "2113999930"}, {"name": "Huaze Tang", "authorId": "120710335"}, {"name": "Chao-Kai Wen", "authorId": "2257212132"}, {"name": "Shi Jin", "authorId": "2227268421"}, {"name": "Xin Wang", "authorId": "2288090155"}, {"name": "Xiaolin Hou", "authorId": "2240356164"}], "n_citations": 3}, "snippets": ["Compared to direct learning with labels, the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio."], "score": 0.66015625}, {"id": "(Hu et al., 2024)", "paper": {"corpus_id": 269033278, "title": "CrimeAlarm: Towards Intensive Intent Dynamics in Fine-grained Crime Prediction", "year": 2024, "venue": "International Conference on Database Systems for Advanced Applications", "authors": [{"name": "Kaixi Hu", "authorId": "2000918408"}, {"name": "Lin Li", "authorId": "2155688849"}, {"name": "Qing Xie", "authorId": "2027162851"}, {"name": "Xiaohui Tao", "authorId": "2070898550"}, {"name": "Guandong Xu", "authorId": "2256933615"}], "n_citations": 1}, "snippets": ["Different from one-hot labels (hard target), the probability distributions of event classes (soft target) provide knowledge among unobserved intents (a.k.a.dark knowledge [6]).Knowledge distillation (KD) is an effective paradigm to transfer such knowledge from teacher networks to student networks and obtain better generalization performance."], "score": 0.794921875}, {"id": "(Takanami et al., 2025)", "paper": {"corpus_id": 275920765, "title": "The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Kaito Takanami", "authorId": "2281746039"}, {"name": "Takashi Takahashi", "authorId": "2342462960"}, {"name": "Ayaka Sakata", "authorId": "2342406436"}], "n_citations": 1}, "snippets": ["Two main hypotheses have been proposed to explain such seemingly puzzling performance gains. The first suggests that the soft labels generated by the teacher provide dark knowledge Hinton et al. [2015]. \n\nHere, dark knowledge refers to the information implicitly embedded in the prediction probability distribution of the teacher model's output, which is absent in hard labels. It provides the student with additional information that captures subtle relationships within the data."], "score": 0.64404296875}, {"id": "(Jung et al., 2023)", "paper": {"corpus_id": 264555654, "title": "Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yeongseo Jung", "authorId": "2262406221"}, {"name": "Eunseo Jung", "authorId": "2262217139"}, {"name": "Lei Chen", "authorId": "2262372006"}], "n_citations": 5}, "snippets": ["In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge."], "score": 0.720703125}], "table": null}, {"title": "Technical Implementation of Dark Knowledge Transfer", "tldr": "Dark knowledge transfer is implemented using temperature-scaled softmax outputs from a teacher model to guide student learning. The process involves a combined loss function that balances distillation loss (comparing teacher and student soft probabilities) with conventional loss from ground-truth labels. (18 sources)", "text": "\nThe technical implementation of dark knowledge transfer centers around extracting and transferring the probability distributions from a teacher network to a student network through a carefully designed training procedure. This process begins with both networks processing the same input data, producing their respective logits (pre-softmax outputs) <Paper corpusId=\"203593636\" paperTitle=\"(Xie et al., 2019)\" isShortName></Paper> <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nA crucial element in this transfer is the temperature parameter (\u03c4), which controls the \"softness\" of the probability distributions <Paper corpusId=\"264516404\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>. When applied to the softmax function, this parameter produces softer probability distributions by dividing the logits before applying softmax: P = softmax(Z/\u03c4) <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"268857025\" paperTitle=\"(Su et al., 2024)\" isShortName></Paper>. Higher temperature values lead to smoother distributions that better reveal the subtle class relationships captured by the teacher network <Paper corpusId=\"269317596\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\nThe knowledge distillation loss function typically combines two components: a distillation loss and a conventional task loss <Paper corpusId=\"235489777\" paperTitle=\"(Zhao et al._1, 2021)\" isShortName></Paper>. The distillation loss measures the difference between the teacher's and student's soft probability distributions, commonly using Kullback-Leibler (KL) divergence <Paper corpusId=\"271244914\" paperTitle=\"(Giakoumoglou et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257504799\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This can be formulated as:\n\n```\nL_KD = KL(softmax(Z_T/\u03c4) || softmax(Z_S/\u03c4))\n```\n\nWhere Z_T and Z_S represent the logits from the teacher and student networks, respectively <Paper corpusId=\"231648215\" paperTitle=\"(Feng et al., 2021)\" isShortName></Paper>. For regression problems, this loss might be reformulated as the mean squared error between the teacher's and student's logits <Paper corpusId=\"231648215\" paperTitle=\"(Feng et al., 2021)\" isShortName></Paper>.\n\nThe conventional task loss typically uses cross-entropy between the student's predictions and ground-truth labels. The final training objective combines these losses with a weighting parameter (\u03b1) to balance their contributions <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>:\n\n```\nL_total = \u03b1 * L_KD + (1-\u03b1) * L_CE\n```\n\nThis dual supervision approach enables the student model to simultaneously learn from both ground-truth labels and the teacher's dark knowledge <Paper corpusId=\"254044469\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"263789814\" paperTitle=\"(Ding et al., 2020)\" isShortName></Paper>. The dark knowledge transfer occurs primarily through the soft labels, which provide additional supervisory signals beyond what's available in one-hot encoded ground-truth labels <Paper corpusId=\"233210081\" paperTitle=\"(Zhao et al._2, 2021)\" isShortName></Paper> <Paper corpusId=\"233407431\" paperTitle=\"(Ge et al., 2021)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nDuring training, the student network is optimized to match both the ground-truth labels and the probability outputs of the teacher model <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. This regularizes the student's learning process <Paper corpusId=\"233407431\" paperTitle=\"(Ge et al., 2021)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper> and helps it converge faster by leveraging the smaller variance in gradients produced by the teacher's soft distributions <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nThe entire knowledge distillation process can be viewed as a teaching mechanism where information flows from the teacher to the student through soft labels (logits) <Paper corpusId=\"273811396\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"215745611\" paperTitle=\"(Wang et al., 2020)\" isShortName></Paper>. This transfer of dark knowledge allows the student model to learn more efficiently and achieve performance closer to that of the teacher model, despite having fewer parameters <Paper corpusId=\"270389751\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.", "citations": [{"id": "(Xie et al., 2019)", "paper": {"corpus_id": 203593636, "title": "Training convolutional neural networks with cheap convolutions and online distillation", "year": 2019, "venue": "arXiv.org", "authors": [{"name": "Jiao Xie", "authorId": "2212036699"}, {"name": "Shaohui Lin", "authorId": "3431378"}, {"name": "Yichen Zhang", "authorId": "2121310989"}, {"name": "Linkai Luo", "authorId": "39378434"}], "n_citations": 12}, "snippets": ["Dark Knowledge. Let t and s be a teacher network and a student network with the final output features Z L (t) and Z L (s) , respectively. a (t) and a (s) are the logits of teacher and student networks, which can be computed respectively by: \n\nGiven an input image x, the probabilistic class posterior of teacher and student network p(c|x, K (t) ) and p(c|x, K (s) ) over a class c can be computed respectively as: \n\nwhere K (t) and K (s) are the parameters in the teacher and student networks. \n\nTo perform dark knowledge, we train the student network to minimize the following loss function: \n\nwhere \u03c4 is a temperature parameter to soften the distribution of predictions. Different from fine-tuning just by class labels, dark knowledge is to learn the student network from the soft outputs of teacher network and ground-truth labels."], "score": 0.7490234375}, {"id": "(Zhang et al., 2020)", "paper": {"corpus_id": 212855595, "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Zhenyu Zhang", "authorId": "47295143"}, {"name": "Xiaobo Shu", "authorId": "2269366"}, {"name": "Yu Bowen", "authorId": "48613402"}, {"name": "Tingwen Liu", "authorId": "2079682"}, {"name": "Jiapeng Zhao", "authorId": "48019474"}, {"name": "Quangang Li", "authorId": "2108645146"}, {"name": "Li Guo", "authorId": "48358041"}], "n_citations": 35}, "snippets": ["In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model", "The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al., 2017))."], "score": 0.818359375}, {"id": "(Yang et al., 2023)", "paper": {"corpus_id": 264516404, "title": "Attention and feature transfer based knowledge distillation", "year": 2023, "venue": "Scientific Reports", "authors": [{"name": "Guoliang Yang", "authorId": "2243407272"}, {"name": "Shuaiying Yu", "authorId": "2220900849"}, {"name": "Yangyang Sheng", "authorId": "2261922204"}, {"name": "Hao Yang", "authorId": "2257352809"}], "n_citations": 4}, "snippets": ["In order to make full use of the \"dark knowledge\" contained in soft labels, the concept of temperature was introduced."], "score": 0.56689453125}, {"id": "(Su et al., 2024)", "paper": {"corpus_id": 268857025, "title": "Task Integration Distillation for Object Detectors", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hai Su", "authorId": "2152173244"}, {"name": "Zhenwen Jian", "authorId": "2294574600"}, {"name": "Songsen Yu", "authorId": "2112454661"}], "n_citations": 1}, "snippets": ["The technique primarily transfers \"dark knowledge\" to the student model through the soft labels of the teacher model.To smoothly extract this \"dark knowledge\", a hyperparameter known as temperature is introduced."], "score": 0.71826171875}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 269317596, "title": "Shared Knowledge Distillation Network for Object Detection", "year": 2024, "venue": "Electronics", "authors": [{"name": "Zhen Guo", "authorId": "2158003499"}, {"name": "Pengzhou Zhang", "authorId": "2297246012"}, {"name": "Peng Liang", "authorId": "2297096848"}], "n_citations": 3}, "snippets": ["Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2)."], "score": 0.63720703125}, {"id": "(Zhao et al._1, 2021)", "paper": {"corpus_id": 235489777, "title": "Knowledge Distillation via Instance-level Sequence Learning", "year": 2021, "venue": "Knowledge-Based Systems", "authors": [{"name": "Haoran Zhao", "authorId": "50981688"}, {"name": "Xin Sun", "authorId": "144326521"}, {"name": "Junyu Dong", "authorId": "1964397"}, {"name": "Zihe Dong", "authorId": "2087106420"}, {"name": "Qiong Li", "authorId": "2108144626"}], "n_citations": 24}, "snippets": ["The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels.\n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss:"], "score": 0.8056640625}, {"id": "(Giakoumoglou et al., 2024)", "paper": {"corpus_id": 271244914, "title": "Relational Representation Distillation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Nikolaos Giakoumoglou", "authorId": "2196360101"}, {"name": "Tania Stathaki", "authorId": "2292259667"}], "n_citations": 0}, "snippets": ["The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs."], "score": 0.66064453125}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 257504799, "title": "MetaMixer: A Regularization Strategy for Online Knowledge Distillation", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Maorong Wang", "authorId": "2041323382"}, {"name": "L. Xiao", "authorId": "49948838"}, {"name": "T. Yamasaki", "authorId": "145572097"}], "n_citations": 1}, "snippets": ["The idea of transferring dark knowledge from the highcapacity teacher model to the compact student model was first proposed in (Bucila et al., 2006). However, it did not gain significant attention from researchers until the work by Hinton et al. [13], where the Kullback-Leibler (KL) divergence loss is used to minimize the difference between the probability distribution generated by a student network and the soft targets generated by a pre-trained teacher network."], "score": 0.55712890625}, {"id": "(Feng et al., 2021)", "paper": {"corpus_id": 231648215, "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation", "year": 2021, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Lingyun Feng", "authorId": "2152751276"}, {"name": "Minghui Qiu", "authorId": "2642333"}, {"name": "Yaliang Li", "authorId": "2110479359"}, {"name": "Haitao Zheng", "authorId": "16215052"}, {"name": "Ying Shen", "authorId": "2115382645"}], "n_citations": 10}, "snippets": ["For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits."], "score": 0.71337890625}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 256900863, "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xiongtao Zhang", "authorId": "2135847074"}, {"name": "Zezong Yin", "authorId": "2206403890"}, {"name": "Yunliang Jiang", "authorId": "3247526"}, {"name": "Yizhang Jiang", "authorId": "1390650781"}, {"name": "Da-Song Sun", "authorId": "2340356"}, {"name": "Yong Liu", "authorId": "2189281"}], "n_citations": 1}, "snippets": ["Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1."], "score": 0.80419921875}, {"id": "(Zhang et al., 2022)", "paper": {"corpus_id": 254044469, "title": "Class-aware Information for Logit-based Knowledge Distillation", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Shuoxi Zhang", "authorId": null}, {"name": "Hanpeng Liu", "authorId": "29901869"}, {"name": "J. Hopcroft", "authorId": "1706504"}, {"name": "Kun He", "authorId": "1702188"}], "n_citations": 2}, "snippets": ["The concept of KD was first proposed by Hinton et al. [10]. KD directs the student training by leveraging the dark knowledge of teacher model, and enhances the performance of student model successfully. Dark knowledge, which can provide additional information to supervise the training process compared to simply utilizing ground-truth labels, is obtained from teacher networks in features or soft logits."], "score": 0.72900390625}, {"id": "(Ding et al., 2020)", "paper": {"corpus_id": 263789814, "title": "Multi-level Knowledge Distillation via Knowledge Alignment and Correlation", "year": 2020, "venue": "", "authors": [{"name": "Fei Ding", "authorId": "2064424445"}, {"name": "Yin Yang", "authorId": "2257087597"}, {"name": "Hongxin Hu", "authorId": "2256778126"}, {"name": "V. Krovi", "authorId": "2095713717"}, {"name": "Feng Luo", "authorId": "2140495064"}], "n_citations": 4}, "snippets": ["Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals."], "score": 0.58935546875}, {"id": "(Zhao et al._2, 2021)", "paper": {"corpus_id": 233210081, "title": "Dual discriminator adversarial distillation for data-free model compression", "year": 2021, "venue": "International Journal of Machine Learning and Cybernetics", "authors": [{"name": "Haoran Zhao", "authorId": "50981688"}, {"name": "Xin Sun", "authorId": "144326521"}, {"name": "Junyu Dong", "authorId": "1964397"}, {"name": "Milos Manic", "authorId": "2185595070"}, {"name": "Huiyu Zhou", "authorId": "46544755"}, {"name": "Hui Yu", "authorId": "145429878"}], "n_citations": 20}, "snippets": ["It transfers the dark knowledge from the pre-trained teacher network to the compact student network by mimicking the class probabilities outputs, which are softened by setting a temperature hyperparameter in softmax. Then the performance of compact student can be retained and close to the performance of the teacher."], "score": 0.57421875}, {"id": "(Ge et al., 2021)", "paper": {"corpus_id": 233407431, "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Yixiao Ge", "authorId": "152988335"}, {"name": "Ching Lam Choi", "authorId": "2087535968"}, {"name": "Xiao Zhang", "authorId": "2115477215"}, {"name": "Peipei Zhao", "authorId": "46737362"}, {"name": "Feng Zhu", "authorId": "2075369514"}, {"name": "Rui Zhao", "authorId": "145638781"}, {"name": "Hongsheng Li", "authorId": "47893312"}], "n_citations": 26}, "snippets": ["Knowledge distillation (Hinton et al., 2015) can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network."], "score": 0.56103515625}, {"id": "(Hinton et al., 2015)", "paper": {"corpus_id": 7200347, "title": "Distilling the Knowledge in a Neural Network", "year": 2015, "venue": "arXiv.org", "authors": [{"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "J. Dean", "authorId": "49959210"}], "n_citations": 19742}, "snippets": ["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."], "score": 0.0}, {"id": "(Yu et al., 2024)", "paper": {"corpus_id": 273811396, "title": "Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment", "year": 2024, "venue": "IEEE Transactions on Artificial Intelligence", "authors": [{"name": "Chengting Yu", "authorId": "2163284473"}, {"name": "Fengzhao Zhang", "authorId": "2274194299"}, {"name": "Ruizhe Chen", "authorId": "2255346632"}, {"name": "Zuozhu Liu", "authorId": "2311458018"}, {"name": "Shurun Tan", "authorId": "2110408145"}, {"name": "Erping Li", "authorId": "2326182494"}, {"name": "Aili Wang", "authorId": "2115787618"}], "n_citations": 2}, "snippets": ["The working mechanism of vanilla KD is often comprehended via the process of \"teaching\" (Wang et al., 2020), whereby the transfer of \"dark knowledge\" occurs through the use of soft labels, known as logits, provided by the teacher to the student."], "score": 0.6953125}, {"id": "(Wang et al., 2020)", "paper": {"corpus_id": 215745611, "title": "Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks", "year": 2020, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Lin Wang", "authorId": "2144734901"}, {"name": "Kuk-Jin Yoon", "authorId": "51182421"}], "n_citations": 700}, "snippets": ["Deep neural models, in recent years, have been successful in almost every field, even solving the most complex problem statements. However, these models are huge in size with millions (and even billions) of parameters, demanding heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of labeled data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called \u2018Student-Teacher\u2019 (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically used for vision tasks. In general, we investigate some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning."], "score": 0.0}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 270389751, "title": "Multistage feature fusion knowledge distillation", "year": 2024, "venue": "Scientific Reports", "authors": [{"name": "Gang Li", "authorId": "2307186661"}, {"name": "Kun Wang", "authorId": "2307436738"}, {"name": "Pengfei Lv", "authorId": "2305765306"}, {"name": "Pan He", "authorId": "2305752302"}, {"name": "Zheng Zhou", "authorId": "2287501699"}, {"name": "Chuanyun Xu", "authorId": "2817613"}], "n_citations": 1}, "snippets": ["Knowledge distillation (KD), as initially proposed by Hinton et al. (Hinton et al., 2015) , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge."], "score": 0.53515625}], "table": null}, {"title": "Benefits of Dark Knowledge Transfer", "tldr": "Dark knowledge transfer provides numerous advantages including enhanced generalization, faster convergence, improved handling of noisy labels, and the ability to transfer semantic class relationships that aren't captured in one-hot encodings. (15 sources)", "text": "\nThe transfer of dark knowledge from teacher to student networks offers significant benefits beyond simply creating smaller models. One of the primary advantages is enhanced generalization capability. By learning from soft labels rather than one-hot vectors, student networks inherit the teacher's ability to generalize better across diverse data <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper> <Paper corpusId=\"233407431\" paperTitle=\"(Ge et al., 2021)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. This improved generalization stems from the implicit class relationship information embedded in the teacher's probability distributions.\n\nDark knowledge transfer also leads to faster training convergence for student networks. The soft distributions provided by the teacher yield smaller variance in gradients, which often results in more stable and expedited training <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. This efficiency benefit is particularly valuable when training large networks or working with limited computational resources.\n\nAnother important advantage is the ability to refine noisy labels in the training data. The dark knowledge from teacher models can help correct inconsistencies or errors in ground-truth labels, providing a more robust training signal <Paper corpusId=\"203642142\" paperTitle=\"(Dong et al., 2019)\" isShortName></Paper>. This label refinement capability makes knowledge distillation particularly valuable in scenarios where obtaining perfect ground-truth labels is challenging or prohibitively expensive.\n\nDark knowledge transfer also provides a powerful regularization effect on student networks. By requiring students to match both ground-truth labels and teacher distributions, the training process becomes more constrained and less prone to overfitting <Paper corpusId=\"233407431\" paperTitle=\"(Ge et al., 2021)\" isShortName></Paper>. This regularization helps students develop more robust decision boundaries.\n\nIn domains involving lossy compression, dark knowledge offers particularly valuable benefits. Since perfect reconstruction is rarely achievable in such tasks, learning from a teacher's outputs provides a more feasible sub-optimal solution compared to learning directly from ground-truth data <Paper corpusId=\"260447668\" paperTitle=\"(Cui et al., 2022)\" isShortName></Paper>. The teacher effectively demonstrates achievable performance levels given specific constraints.\n\nThe benefits of dark knowledge extend beyond traditional applications, with recent research showing its effectiveness across various tasks including image classification, object detection, and even part segmentation <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"229363322\" paperTitle=\"(Touvron et al., 2020)\" isShortName></Paper> <Paper corpusId=\"245006036\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper> <Paper corpusId=\"252918735\" paperTitle=\"(Cardace et al., 2022)\" isShortName></Paper>. Remarkably, even weak teachers with lower accuracy than their students can still provide beneficial dark knowledge through regularization effects <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"219962714\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper>.\n\nPerhaps most importantly, dark knowledge provides semantic similarity information between different classes that cannot be captured by traditional one-hot encoded labels <Paper corpusId=\"249017724\" paperTitle=\"(Bai et al., 2022)\" isShortName></Paper> <Paper corpusId=\"237091534\" paperTitle=\"(Ren et al., 2021)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. By transferring this relational understanding, student networks develop more nuanced decision boundaries that better reflect the underlying structure of the data <Paper corpusId=\"265444951\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>. This enables students to make more informed predictions, particularly in cases where class distinctions are subtle or overlapping.\n\nThrough these combined benefits, dark knowledge transfer creates a pathway for developing more efficient, accurate, and robust neural networks that maintain high performance despite their reduced size and computational requirements <Paper corpusId=\"251066725\" paperTitle=\"(Liang et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Nayak et al., 2019)", "paper": {"corpus_id": 159041346, "title": "Zero-Shot Knowledge Distillation in Deep Networks", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Gaurav Kumar Nayak", "authorId": "143747407"}, {"name": "Konda Reddy Mopuri", "authorId": "2217000"}, {"name": "Vaisakh Shaj", "authorId": "7155274"}, {"name": "R. Venkatesh Babu", "authorId": "144682140"}, {"name": "Anirban Chakraborty", "authorId": "1429640900"}], "n_citations": 245}, "snippets": ["The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability."], "score": 0.6123046875}, {"id": "(Ge et al., 2021)", "paper": {"corpus_id": 233407431, "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Yixiao Ge", "authorId": "152988335"}, {"name": "Ching Lam Choi", "authorId": "2087535968"}, {"name": "Xiao Zhang", "authorId": "2115477215"}, {"name": "Peipei Zhao", "authorId": "46737362"}, {"name": "Feng Zhu", "authorId": "2075369514"}, {"name": "Rui Zhao", "authorId": "145638781"}, {"name": "Hongsheng Li", "authorId": "47893312"}], "n_citations": 26}, "snippets": ["Knowledge distillation (Hinton et al., 2015) can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network."], "score": 0.56103515625}, {"id": "(Hinton et al., 2015)", "paper": {"corpus_id": 7200347, "title": "Distilling the Knowledge in a Neural Network", "year": 2015, "venue": "arXiv.org", "authors": [{"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "J. Dean", "authorId": "49959210"}], "n_citations": 19742}, "snippets": ["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."], "score": 0.0}, {"id": "(Zhang et al., 2020)", "paper": {"corpus_id": 212855595, "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Zhenyu Zhang", "authorId": "47295143"}, {"name": "Xiaobo Shu", "authorId": "2269366"}, {"name": "Yu Bowen", "authorId": "48613402"}, {"name": "Tingwen Liu", "authorId": "2079682"}, {"name": "Jiapeng Zhao", "authorId": "48019474"}, {"name": "Quangang Li", "authorId": "2108645146"}, {"name": "Li Guo", "authorId": "48358041"}], "n_citations": 35}, "snippets": ["In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model", "The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al., 2017))."], "score": 0.818359375}, {"id": "(Dong et al., 2019)", "paper": {"corpus_id": 203642142, "title": "Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network", "year": 2019, "venue": "arXiv.org", "authors": [{"name": "Bin Dong", "authorId": "145496882"}, {"name": "Jikai Hou", "authorId": "104162954"}, {"name": "Yiping Lu", "authorId": "48518029"}, {"name": "Zhihua Zhang", "authorId": "47294286"}], "n_citations": 41}, "snippets": ["To explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels."], "score": 0.64013671875}, {"id": "(Cui et al., 2022)", "paper": {"corpus_id": 260447668, "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback", "year": 2022, "venue": "IEEE Transactions on Communications", "authors": [{"name": "Yiming Cui", "authorId": "2172485573"}, {"name": "Jiajia Guo", "authorId": "47093519"}, {"name": "Zheng Cao", "authorId": "2113999930"}, {"name": "Huaze Tang", "authorId": "120710335"}, {"name": "Chao-Kai Wen", "authorId": "2257212132"}, {"name": "Shi Jin", "authorId": "2227268421"}, {"name": "Xin Wang", "authorId": "2288090155"}, {"name": "Xiaolin Hou", "authorId": "2240356164"}], "n_citations": 3}, "snippets": ["Compared to direct learning with labels, the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio."], "score": 0.66015625}, {"id": "(Wang et al._1, 2023)", "paper": {"corpus_id": 264590688, "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jiatai Wang", "authorId": "2186275686"}, {"name": "Zhiwei Xu", "authorId": "2244022079"}, {"name": "Xin Wang", "authorId": "2262805047"}], "n_citations": 1}, "snippets": ["The specific experimental settings are shown in Table I. The corresponding distillation methods are as follows: 1) FT (Kim et al., 2018) uses convolutional operations to transfer dark knowledge; 2) DeiT (Touvron et al., 2020) proposes the distillation token and uses its representation with the teacher model's dark knowledge to compute the distillation loss; 3) KD-RP (Li et al., 2021) exploits the differences in student and teacher networks to guide dark knowledge distillation; 4) KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation; 5) SD (Cardace et al., 2022) exploits self-distillation to learn effective representations to group point clouds in the target domain. \n\nThe experimental results are shown in Fig. 2, with the corresponding distillation methods highlighted in red. The five tasks can all improve the performance of their backbone networks after exploiting the Knowledge distillation. Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes (Yuan et al., 2020). With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge (Yuan et al., 2020). This demonstrates that dark knowledge of knowledge distillation plays a positive role in different learning tasks."], "score": 0.7421875}, {"id": "(Touvron et al., 2020)", "paper": {"corpus_id": 229363322, "title": "Training data-efficient image transformers & distillation through attention", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Hugo Touvron", "authorId": "2113243762"}, {"name": "M. Cord", "authorId": "51021910"}, {"name": "Matthijs Douze", "authorId": "3271933"}, {"name": "Francisco Massa", "authorId": "1403239967"}, {"name": "Alexandre Sablayrolles", "authorId": "3469062"}, {"name": "Herv'e J'egou", "authorId": "2065248680"}], "n_citations": 6805}, "snippets": ["Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models."], "score": 0.0}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 245006036, "title": "Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-guided Feature Imitation", "year": 2021, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Gang Li", "authorId": "2155120199"}, {"name": "Xiang Li", "authorId": "2144440085"}, {"name": "Yujie Wang", "authorId": "2115657967"}, {"name": "Shanshan Zhang", "authorId": "2145441956"}, {"name": "Yichao Wu", "authorId": "47095791"}, {"name": "Ding Liang", "authorId": "152335674"}], "n_citations": 81}, "snippets": ["Knowledge Distillation (KD) is a widely-used technology to inherit information from cumbersome teacher models to compact student models, consequently realizing model compression and acceleration. Compared with image classification, object detection is a more complex task, and designing specific KD methods for object detection is non-trivial. In this work, we elaborately study the behaviour difference between the teacher and student detection models, and obtain two intriguing observations: First, the teacher and student rank their detected candidate boxes quite differently, which results in their precision discrepancy. Second, there is a considerable gap between the feature response differences and prediction differences between teacher and student, indicating that equally imitating all the feature maps of the teacher is the sub-optimal choice for improving the student's accuracy. Based on the two observations, we propose Rank Mimicking (RM) and Prediction-guided Feature Imitation (PFI) for distilling one-stage detectors, respectively. RM takes the rank of candidate boxes from teachers as a new form of knowledge to distill, which consistently outperforms the traditional soft label distillation. PFI attempts to correlate feature differences with prediction differences, making feature imitation directly help to improve the student's accuracy. On MS COCO and PASCAL VOC benchmarks, extensive experiments are conducted on various detectors with different backbones to validate the effectiveness of our method. Specifically, RetinaNet with ResNet50 achieves 40.4% mAP on MS COCO, which is 3.5% higher than its baseline, and also outperforms previous KD methods."], "score": 0.0}, {"id": "(Cardace et al., 2022)", "paper": {"corpus_id": 252918735, "title": "Self-Distillation for Unsupervised 3D Domain Adaptation", "year": 2022, "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "authors": [{"name": "Adriano Cardace", "authorId": "2131012006"}, {"name": "Riccardo Spezialetti", "authorId": "46297738"}, {"name": "Pierluigi Zama Ramirez", "authorId": "80804241"}, {"name": "Samuele Salti", "authorId": "2607607"}, {"name": "L. D. Stefano", "authorId": "9395079"}], "n_citations": 16}, "snippets": ["Point cloud classification is a popular task in 3D vision. However, previous works, usually assume that point clouds at test time are obtained with the same procedure or sensor as those at training time. Unsupervised Domain Adaptation (UDA) instead, breaks this assumption and tries to solve the task on an unlabeled target domain, leveraging only on a supervised source domain. For point cloud classification, recent UDA methods try to align features across domains via auxiliary tasks such as point cloud reconstruction, which however do not optimize the discriminative power in the target domain in feature space. In contrast, in this work, we focus on obtaining a discriminative feature space for the target domain enforcing consistency between a point cloud and its augmented version. We then propose a novel iterative self-training methodology that exploits Graph Neural Networks in the UDA context to refine pseudo-labels. We perform extensive experiments and set the new state-of-the art in standard UDA benchmarks for point cloud classification. Finally, we show how our approach can be extended to more complex tasks such as part segmentation."], "score": 0.0}, {"id": "(Yuan et al., 2020)", "paper": {"corpus_id": 219962714, "title": "Revisiting Knowledge Distillation via Label Smoothing Regularization", "year": 2020, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Li Yuan", "authorId": "2087091296"}, {"name": "Francis E. H. Tay", "authorId": "40983412"}, {"name": "Guilin Li", "authorId": "2108696460"}, {"name": "Tao Wang", "authorId": "2155456820"}, {"name": "Jiashi Feng", "authorId": "33221685"}], "n_citations": 527}, "snippets": ["Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. In this work, we challenge this common belief by following experimental observations: 1) beyond the acknowledgment that the teacher can improve the student, the student can also enhance the teacher significantly by reversing the KD procedure; 2) a poorly-trained teacher with much lower accuracy than the student can still improve the latter significantly. To explain these observations, we provide a theoretical analysis of the relationships between KD and label smoothing regularization. We prove that 1) KD is a type of learned label smoothing regularization and 2) label smoothing regularization provides a virtual teacher model for KD. From these results, we argue that the success of KD is not fully due to the similarity information between categories from teachers, but also to the regularization of soft targets, which is equally or even more important. Based on these analyses, we further propose a novel Teacher-free Knowledge Distillation (Tf-KD) framework, where a student model learns from itself or manually-designed regularization distribution. The Tf-KD achieves comparable performance with normal KD from a superior teacher, which is well applied when a stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly deployed for training deep neural networks. Without any extra computation cost, Tf-KD achieves up to 0.65\\% improvement on ImageNet over well-established baseline models, which is superior to label smoothing regularization."], "score": 0.0}, {"id": "(Bai et al., 2022)", "paper": {"corpus_id": 249017724, "title": "Improving the Latent Space of Image Style Transfer", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Yun-Hao Bai", "authorId": "48442720"}, {"name": "Cairong Wang", "authorId": "47073960"}, {"name": "C. Yuan", "authorId": "2117729099"}, {"name": "Yanbo Fan", "authorId": "2140245719"}, {"name": "Jue Wang", "authorId": "2144537318"}], "n_citations": 0}, "snippets": ["Knowledge distillation (KD) [26,27](Yu et al., 2019) is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability (Yu et al., 2019)(Peng et al., 2019), which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted."], "score": 0.71826171875}, {"id": "(Ren et al., 2021)", "paper": {"corpus_id": 237091534, "title": "Online Multi-Granularity Distillation for GAN Compression", "year": 2021, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Yuxi Ren", "authorId": null}, {"name": "Jie Wu", "authorId": "2118432533"}, {"name": "Xuefeng Xiao", "authorId": "2118724465"}, {"name": "Jianchao Yang", "authorId": "1706007"}], "n_citations": 39}, "snippets": ["In this way, the student network attempts to learn dark knowledge (Hinton et al., 2015) that contains the similarities between different classes, which can not be provided by the ground truth labels."], "score": 0.53564453125}, {"id": "(Su et al., 2023)", "paper": {"corpus_id": 265444951, "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation", "year": 2023, "venue": "", "authors": [{"name": "Chi-Ping Su", "authorId": "2268314644"}, {"name": "Ching-Hsun Tseng", "authorId": "1569686364"}, {"name": "Bin Pu", "authorId": "2338265886"}, {"name": "Lei Zhao", "authorId": "2338506240"}, {"name": "Zhuangzhuang Chen", "authorId": "2328588941"}, {"name": "Shin-Jye Lee", "authorId": "2116351339"}], "n_citations": 2}, "snippets": ["Knowledge Distillation (KD) enables a smaller\"student\"model to mimic a larger\"teacher\"model by transferring knowledge from the teacher's output or features", "Knowledge Distillation (KD) [8] offers a solution by enabling a compact \"student\" model to mimic a larger \"teacher\" model, allowing the student to learn from both ground-truth labels and the teacher's \"dark knowledge\" -the implicit insights not present in the ground-truth labels -enabling it to approach the teacher's performance in a compact form."], "score": 0.56494140625}, {"id": "(Liang et al., 2022)", "paper": {"corpus_id": 251066725, "title": "Efficient One Pass Self-distillation with Zipf's Label Smoothing", "year": 2022, "venue": "European Conference on Computer Vision", "authors": [{"name": "Jiajun Liang", "authorId": "1387852255"}, {"name": "Linze Li", "authorId": "2304362074"}, {"name": "Z. Bing", "authorId": "2056410266"}, {"name": "Borui Zhao", "authorId": "2112526021"}, {"name": "Yao Tang", "authorId": "2179286812"}, {"name": "Bo Lin", "authorId": "2179192518"}, {"name": "Haoqiang Fan", "authorId": "1934546"}], "n_citations": 19}, "snippets": ["Instead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,(Cho et al., 2019)(Mirzadeh et al., 2019)(Zhu et al., 2021), designing student-friendly architecture [16,(Park et al., 2021), improving the distillation efficiency (Furlanello et al., 2018)(Kim et al., 2020)(Xu et al., 2019)(Yun et al., 2020) and explaining the distillation's working mechanism [1](Stanton et al., 2021)."], "score": 0.7392578125}], "table": null}, {"title": "Applications and Variations of Dark Knowledge", "tldr": "Dark knowledge transfer has been applied across diverse tasks beyond classification, including object detection, image segmentation, and multimodal learning. Various implementations have emerged, from feature-based distillation approaches to specialized techniques for handling missing modalities. (19 sources)", "text": "\nDark knowledge transfer has evolved beyond its initial applications, finding utility in numerous domains and specialized implementations:\n\n* **Classification Tasks**: The most common application of dark knowledge remains in image classification, where both intermediate feature embeddings and soft logits serve as the primary knowledge transferred from teacher to student networks <Paper corpusId=\"260704230\" paperTitle=\"(Hu et al., 2023)\" isShortName></Paper>.\n\n* **Feature-based Distillation**: Multiple approaches focus on transferring knowledge through intermediate features rather than logits. Examples include:\n - Convolutional paraphrasing (FT) which uses convolutional operations to transform teacher's knowledge for student consumption <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"3608236\" paperTitle=\"(Kim et al., 2018)\" isShortName></Paper>.\n - Feature transform with margin ReLU, which redesigns distillation feature positions and uses partial L2 distance functions <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"102483181\" paperTitle=\"(Heo et al., 2019)\" isShortName></Paper>.\n - Activation boundary distillation, which transfers knowledge by focusing on the separating hyperplanes that determine neuron activation states <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"53213211\" paperTitle=\"(Heo et al., 2018)\" isShortName></Paper>.\n\n* **Logit-based Distillation**: These approaches focus on transferring knowledge through the model's output layers, with implementations including:\n - Self-distillation techniques where models learn from themselves <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>.\n - Teacher-student matching with distillation tokens, as in DeiT which introduces a specific token to compute distillation loss <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"229363322\" paperTitle=\"(Touvron et al., 2020)\" isShortName></Paper>.\n\n* **Relation-based Distillation**: Beyond instance-level knowledge, some approaches transfer relationships between examples:\n - Correlation congruence methods that capture and transfer correlations between instances <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"102483463\" paperTitle=\"(Peng et al., 2019)\" isShortName></Paper>.\n - Relational knowledge distillation approaches that transfer mutual relations between data examples rather than individual activations <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"131765296\" paperTitle=\"(Park et al., 2019)\" isShortName></Paper>.\n - Similarity-preserving distillation that guides students to preserve pairwise similarities from teacher networks <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"198179476\" paperTitle=\"(Tung et al., 2019)\" isShortName></Paper>.\n\n* **Object Detection**: Dark knowledge transfer has been adapted for object detection tasks, with specialized techniques like Rank Mimicking (RM) and Prediction-guided Feature Imitation (PFI) that address the unique challenges of detection compared to classification <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"245006036\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\n* **Medical Imaging**: Knowledge distillation has proven particularly valuable in medical applications, especially for brain tumor segmentation where transferring knowledge from multi-modal networks to mono-modal ones helps address the clinical reality of limited modality availability <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"221543802\" paperTitle=\"(Hu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"245445463\" paperTitle=\"(Rahimpour et al., 2021)\" isShortName></Paper>.\n\n* **Multimodal Learning with Missing Modalities**: Dark knowledge transfer provides solutions for scenarios where some modalities are unavailable during inference:\n - Cross-modal distillation approaches that leverage multi-sequence MRI data for training while using only single-sequence data for inference <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"245445463\" paperTitle=\"(Rahimpour et al., 2021)\" isShortName></Paper>.\n - Learnable Cross-modal Knowledge Distillation (LCKD) that adaptively identifies important modalities and distills knowledge between them <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"263605398\" paperTitle=\"(Wang et al._2, 2023)\" isShortName></Paper>.\n - Teacher-student frameworks for Visual Question Answering with missing ground truth answers <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"233219684\" paperTitle=\"(Cho et al., 2021)\" isShortName></Paper>.\n - Transfer of \"dark knowledge\" from teacher networks trained on complete modalities to student networks trained on missing modalities <Paper corpusId=\"269362788\" paperTitle=\"(Li et al._2, 2024)\" isShortName></Paper>.\n\n* **Point Cloud Classification**: Self-distillation techniques have been applied to point cloud classification for unsupervised domain adaptation, helping to obtain discriminative feature spaces for target domains <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"252918735\" paperTitle=\"(Cardace et al., 2022)\" isShortName></Paper>.\n\n* **Sequential Knowledge Distillation**: Approaches like teacher assistants that bridge the gap between large teachers and small students when direct distillation is ineffective <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>.", "citations": [{"id": "(Hu et al., 2023)", "paper": {"corpus_id": 260704230, "title": "Teacher-Student Architecture for Knowledge Distillation: A Survey", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Chengming Hu", "authorId": "11577774"}, {"name": "Xuan Li", "authorId": "1870442533"}, {"name": "Danyang Liu", "authorId": "2822590"}, {"name": "Haolun Wu", "authorId": "107747459"}, {"name": "Xi Chen", "authorId": "2145308240"}, {"name": "Ju Wang", "authorId": "2125037263"}, {"name": "Xue Liu", "authorId": "2151061048"}], "n_citations": 19}, "snippets": ["Currently, most teacher-student architectures are employed on classification tasks, where intermediate feature embeddings and soft logits can be commonly represented as dark knowledge transferred to student networks."], "score": 0.6376953125}, {"id": "(Wang et al._1, 2023)", "paper": {"corpus_id": 264590688, "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jiatai Wang", "authorId": "2186275686"}, {"name": "Zhiwei Xu", "authorId": "2244022079"}, {"name": "Xin Wang", "authorId": "2262805047"}], "n_citations": 1}, "snippets": ["The specific experimental settings are shown in Table I. The corresponding distillation methods are as follows: 1) FT (Kim et al., 2018) uses convolutional operations to transfer dark knowledge; 2) DeiT (Touvron et al., 2020) proposes the distillation token and uses its representation with the teacher model's dark knowledge to compute the distillation loss; 3) KD-RP (Li et al., 2021) exploits the differences in student and teacher networks to guide dark knowledge distillation; 4) KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation; 5) SD (Cardace et al., 2022) exploits self-distillation to learn effective representations to group point clouds in the target domain. \n\nThe experimental results are shown in Fig. 2, with the corresponding distillation methods highlighted in red. The five tasks can all improve the performance of their backbone networks after exploiting the Knowledge distillation. Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes (Yuan et al., 2020). With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge (Yuan et al., 2020). This demonstrates that dark knowledge of knowledge distillation plays a positive role in different learning tasks."], "score": 0.7421875}, {"id": "(Kim et al., 2018)", "paper": {"corpus_id": 3608236, "title": "Paraphrasing Complex Network: Network Compression via Factor Transfer", "year": 2018, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jangho Kim", "authorId": "49476045"}, {"name": "Seonguk Park", "authorId": "35869519"}, {"name": "Nojun Kwak", "authorId": "3160425"}], "n_citations": 551}, "snippets": ["Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods."], "score": 0.0}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 273821996, "title": "Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Mingcheng Li", "authorId": "2188978724"}, {"name": "Dingkang Yang", "authorId": "2143920085"}, {"name": "Yang Liu", "authorId": "2290474772"}, {"name": "Shunli Wang", "authorId": "2108655879"}, {"name": "Jiawei Chen", "authorId": "2279095231"}, {"name": "Shuai Wang", "authorId": "2186872968"}, {"name": "Jinjie Wei", "authorId": "2290653228"}, {"name": "Yue Jiang", "authorId": "2278886236"}, {"name": "Qingyao Xu", "authorId": "2303841599"}, {"name": "Xiaolu Hou", "authorId": "2298268194"}, {"name": "Mingyang Sun", "authorId": "2216487730"}, {"name": "Ziyun Qian", "authorId": "2202592845"}, {"name": "Dongliang Kou", "authorId": "2218981837"}, {"name": "Lihua Zhang", "authorId": "2278978362"}], "n_citations": 1}, "snippets": ["Knowledge distillation leverages additional supervisory signals from a pre-trained teacher network to aid in training a student network [15]. There are generally two categories of knowledge distillation methods: distillation from intermediate features (Heo et al., 2019)(Heo et al., 2018)(Kim et al., 2018)(Park et al., 2019)(Peng et al., 2019)39,(Tung et al., 2019)43,(Yim et al., 2017)[73] and distillation from logits (Cho et al., 2019)(Furlanello et al., 2018)(Mirzadeh et al., 2019)(Yang et al., 2018)(Zhao et al., 2016). Many studies (Cho et al., 2021)(Hu et al., 2020)(Rahimpour et al., 2021)21,(Wang et al., 2023)[51] utilize knowledge distillation for MSA tasks with missing modalities. These approaches aim to transfer dark knowledge from teacher networks trained on complete modalities to student networks trained by missing modalities."], "score": 0.6357421875}, {"id": "(Heo et al., 2019)", "paper": {"corpus_id": 102483181, "title": "A Comprehensive Overhaul of Feature Distillation", "year": 2019, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Byeongho Heo", "authorId": "3086596"}, {"name": "Jeesoo Kim", "authorId": "47965071"}, {"name": "Sangdoo Yun", "authorId": "2151587"}, {"name": "Hyojin Park", "authorId": "46904404"}, {"name": "Nojun Kwak", "authorId": "3160425"}, {"name": "J. Choi", "authorId": "46174575"}], "n_citations": 584}, "snippets": ["We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L2 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65% of top-1 error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at project page."], "score": 0.0}, {"id": "(Heo et al., 2018)", "paper": {"corpus_id": 53213211, "title": "Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons", "year": 2018, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Byeongho Heo", "authorId": "3086596"}, {"name": "Minsik Lee", "authorId": "2646766"}, {"name": "Sangdoo Yun", "authorId": "2151587"}, {"name": "J. Choi", "authorId": "46174575"}], "n_citations": 527}, "snippets": ["An activation boundary for a neuron refers to a separating hyperplane that determines whether the neuron is activated or deactivated. It has been long considered in neural networks that the activations of neurons, rather than their exact output values, play the most important role in forming classificationfriendly partitions of the hidden feature space. However, as far as we know, this aspect of neural networks has not been considered in the literature of knowledge transfer. In this paper, we propose a knowledge transfer method via distillation of activation boundaries formed by hidden neurons. For the distillation, we propose an activation transfer loss that has the minimum value when the boundaries generated by the student coincide with those by the teacher. Since the activation transfer loss is not differentiable, we design a piecewise differentiable loss approximating the activation transfer loss. By the proposed method, the student learns a separating boundary between activation region and deactivation region formed by each neuron in the teacher. Through the experiments in various aspects of knowledge transfer, it is verified that the proposed method outperforms the current state-of-the-art."], "score": 0.0}, {"id": "(Furlanello et al., 2018)", "paper": {"corpus_id": 4110009, "title": "Born Again Neural Networks", "year": 2018, "venue": "International Conference on Machine Learning", "authors": [{"name": "Tommaso Furlanello", "authorId": "2067208583"}, {"name": "Zachary Chase Lipton", "authorId": "32219137"}, {"name": "Michael Tschannen", "authorId": "143902495"}, {"name": "L. Itti", "authorId": "7326223"}, {"name": "Anima Anandkumar", "authorId": "2047844"}], "n_citations": 1034}, "snippets": ["Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."], "score": 0.0}, {"id": "(Touvron et al., 2020)", "paper": {"corpus_id": 229363322, "title": "Training data-efficient image transformers & distillation through attention", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Hugo Touvron", "authorId": "2113243762"}, {"name": "M. Cord", "authorId": "51021910"}, {"name": "Matthijs Douze", "authorId": "3271933"}, {"name": "Francisco Massa", "authorId": "1403239967"}, {"name": "Alexandre Sablayrolles", "authorId": "3469062"}, {"name": "Herv'e J'egou", "authorId": "2065248680"}], "n_citations": 6805}, "snippets": ["Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models."], "score": 0.0}, {"id": "(Peng et al., 2019)", "paper": {"corpus_id": 102483463, "title": "Correlation Congruence for Knowledge Distillation", "year": 2019, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Baoyun Peng", "authorId": "153916114"}, {"name": "Xiao Jin", "authorId": "153104092"}, {"name": "Jiaheng Liu", "authorId": "2108421213"}, {"name": "Shunfeng Zhou", "authorId": "5454120"}, {"name": "Yichao Wu", "authorId": "47095791"}, {"name": "Yu Liu", "authorId": "2146400394"}, {"name": "Dongsheng Li", "authorId": "144032853"}, {"name": "Zhaoning Zhang", "authorId": "2362362"}], "n_citations": 513}, "snippets": ["Most teacher-student frameworks based on knowledge distillation (KD) depend on a strong congruent constraint on instance level. However, they usually ignore the correlation between multiple instances, which is also valuable for knowledge transfer. In this work, we propose a new framework named correlation congruence for knowledge distillation (CCKD), which transfers not only the instance-level information but also the correlation between instances. Furthermore, a generalized kernel method based on Taylor series expansion is proposed to better capture the correlation between instances. Empirical experiments and ablation studies on image classification tasks (including CIFAR-100, ImageNet-1K) and metric learning tasks (including ReID and Face Recognition) show that the proposed CCKD substantially outperforms the original KD and other SOTA KD-based methods. The CCKD can be easily deployed in the majority of the teacher-student framework such as KD and hint-based learning methods."], "score": 0.0}, {"id": "(Park et al., 2019)", "paper": {"corpus_id": 131765296, "title": "Relational Knowledge Distillation", "year": 2019, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Wonpyo Park", "authorId": "107950764"}, {"name": "Dongju Kim", "authorId": "2111840349"}, {"name": "Yan Lu", "authorId": "144574822"}, {"name": "Minsu Cho", "authorId": "72643925"}], "n_citations": 1424}, "snippets": ["Knowledge distillation aims at transferring knowledge acquired in one model (a teacher) to another model (a student) that is typically smaller. Previous approaches can be expressed as a form of training the student to mimic output activations of individual data examples represented by the teacher. We introduce a novel approach, dubbed relational knowledge distillation (RKD), that transfers mutual relations of data examples instead. For concrete realizations of RKD, we propose distance-wise and angle-wise distillation losses that penalize structural differences in relations. Experiments conducted on different tasks show that the proposed method improves educated student models with a significant margin. In particular for metric learning, it allows students to outperform their teachers' performance, achieving the state of the arts on standard benchmark datasets."], "score": 0.0}, {"id": "(Tung et al., 2019)", "paper": {"corpus_id": 198179476, "title": "Similarity-Preserving Knowledge Distillation", "year": 2019, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Frederick Tung", "authorId": "2065573607"}, {"name": "Greg Mori", "authorId": "10771328"}], "n_citations": 981}, "snippets": ["Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach."], "score": 0.0}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 245006036, "title": "Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-guided Feature Imitation", "year": 2021, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Gang Li", "authorId": "2155120199"}, {"name": "Xiang Li", "authorId": "2144440085"}, {"name": "Yujie Wang", "authorId": "2115657967"}, {"name": "Shanshan Zhang", "authorId": "2145441956"}, {"name": "Yichao Wu", "authorId": "47095791"}, {"name": "Ding Liang", "authorId": "152335674"}], "n_citations": 81}, "snippets": ["Knowledge Distillation (KD) is a widely-used technology to inherit information from cumbersome teacher models to compact student models, consequently realizing model compression and acceleration. Compared with image classification, object detection is a more complex task, and designing specific KD methods for object detection is non-trivial. In this work, we elaborately study the behaviour difference between the teacher and student detection models, and obtain two intriguing observations: First, the teacher and student rank their detected candidate boxes quite differently, which results in their precision discrepancy. Second, there is a considerable gap between the feature response differences and prediction differences between teacher and student, indicating that equally imitating all the feature maps of the teacher is the sub-optimal choice for improving the student's accuracy. Based on the two observations, we propose Rank Mimicking (RM) and Prediction-guided Feature Imitation (PFI) for distilling one-stage detectors, respectively. RM takes the rank of candidate boxes from teachers as a new form of knowledge to distill, which consistently outperforms the traditional soft label distillation. PFI attempts to correlate feature differences with prediction differences, making feature imitation directly help to improve the student's accuracy. On MS COCO and PASCAL VOC benchmarks, extensive experiments are conducted on various detectors with different backbones to validate the effectiveness of our method. Specifically, RetinaNet with ResNet50 achieves 40.4% mAP on MS COCO, which is 3.5% higher than its baseline, and also outperforms previous KD methods."], "score": 0.0}, {"id": "(Hu et al., 2020)", "paper": {"corpus_id": 221543802, "title": "Knowledge Distillation from Multi-modal to Mono-modal Segmentation Networks", "year": 2020, "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "authors": [{"name": "Minhao Hu", "authorId": "2145917899"}, {"name": "Matthis Maillard", "authorId": "1987237543"}, {"name": "Ya Zhang", "authorId": "2129507184"}, {"name": "Tommaso Ciceri", "authorId": "1987210827"}, {"name": "Giammarco La Barbera", "authorId": "146771548"}, {"name": "I. Bloch", "authorId": "1695917"}, {"name": "P. Gori", "authorId": "1742163277"}], "n_citations": 129}, "snippets": ["The joint use of multiple imaging modalities for medical image segmentation has been widely studied in recent years. The fusion of information from different modalities has demonstrated to improve the segmentation accuracy, with respect to mono-modal segmentations, in several applications. However, acquiring multiple modalities is usually not possible in a clinical setting due to a limited number of physicians and scanners, and to limit costs and scan time. Most of the time, only one modality is acquired. In this paper, we propose KD-Net, a framework to transfer knowledge from a trained multi-modal network (teacher) to a mono-modal one (student). The proposed method is an adaptation of the generalized distillation framework where the student network is trained on a subset (1 modality) of the teacher\u2019s inputs (n modalities). We illustrate the effectiveness of the proposed framework in brain tumor segmentation with the BraTS 2018 dataset. Using different architectures, we show that the student network effectively learns from the teacher and always outperforms the baseline mono-modal network in terms of segmentation accuracy."], "score": 0.0}, {"id": "(Rahimpour et al., 2021)", "paper": {"corpus_id": 245445463, "title": "Cross-Modal Distillation to Improve MRI-Based Brain Tumor Segmentation With Missing MRI Sequences", "year": 2021, "venue": "IEEE Transactions on Biomedical Engineering", "authors": [{"name": "Masoomeh Rahimpour", "authorId": "32163043"}, {"name": "J. Bertels", "authorId": "27011321"}, {"name": "A. Radwan", "authorId": "152985178"}, {"name": "Henri Vandermeulen", "authorId": "2135672010"}, {"name": "S. Sunaert", "authorId": "2680672"}, {"name": "D. Vandermeulen", "authorId": "143908467"}, {"name": "F. Maes", "authorId": "1709659"}, {"name": "K. Goffin", "authorId": "46523475"}, {"name": "M. Koole", "authorId": "2502283"}], "n_citations": 30}, "snippets": ["Convolutional neural networks (CNNs) for brain tumor segmentation are generally developed using complete sets of magnetic resonance imaging (MRI) sequences for both training and inference. As such, these algorithms are not trained for realistic, clinical scenarios where parts of the MRI sequences which were used for training, are missing during inference. To increase clinical applicability, we proposed a cross-modal distillation approach to leverage the availability of multi-sequence MRI data for training and generate an enriched CNN model which uses only single-sequence MRI data for inference but outperforms a single-sequence CNN model. We assessed the performance of the proposed method for whole tumor and tumor core segmentation with multi-sequence MRI data available for training but only <inline-formula><tex-math notation=\"LaTeX\">$T_{1}$</tex-math></inline-formula>-weighted (<inline-formula><tex-math notation=\"LaTeX\">$T_{\\text{1}w}$</tex-math></inline-formula>) sequence data available for inference, using BraTS 2018, and in-house datasets. Results showed that cross-modal distillation significantly improved the Dice score for both whole tumor and tumor core segmentation when only <inline-formula><tex-math notation=\"LaTeX\">$T_{\\text{1}w}$</tex-math></inline-formula> sequence data were available for inference. For the evaluation using the in-house dataset, cross-modal distillation achieved an average Dice score of 79.04% and 69.39% for whole tumor and tumor core segmentation, respectively, while a single-sequence U-Net model using <inline-formula><tex-math notation=\"LaTeX\">$T_{\\text{1}w}$</tex-math></inline-formula> sequence data for both training and inference achieved an average Dice score of 73.60% and 62.62%, respectively. These findings confirmed cross-modal distillation as an effective method to increase the potential of single-sequence CNN models such that segmentation performance is less compromised by missing MRI sequences or having only one MRI sequence available for segmentation."], "score": 0.0}, {"id": "(Wang et al._2, 2023)", "paper": {"corpus_id": 263605398, "title": "Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality", "year": 2023, "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "authors": [{"name": "Hu Wang", "authorId": "2253875173"}, {"name": "Yuanhong Chen", "authorId": "50581035"}, {"name": "Congbo Ma", "authorId": "2132565465"}, {"name": "Jodie Avery", "authorId": "2179001232"}, {"name": "Louise Hull", "authorId": "2178996607"}, {"name": "Gustavo Carneiro", "authorId": "2253394660"}], "n_citations": 33}, "snippets": ["The problem of missing modalities is both critical and non-trivial to be handled in multi-modal models. It is common for multi-modal tasks that certain modalities contribute more compared to other modalities, and if those important modalities are missing, the model performance drops significantly. Such fact remains unexplored by current multi-modal approaches that recover the representation from missing modalities by feature reconstruction or blind feature aggregation from other modalities, instead of extracting useful information from the best performing modalities. In this paper, we propose a Learnable Cross-modal Knowledge Distillation (LCKD) model to adaptively identify important modalities and distil knowledge from them to help other modalities from the cross-modal perspective for solving the missing modality issue. Our approach introduces a teacher election procedure to select the most ``qualified'' teachers based on their single modality performance on certain tasks. Then, cross-modal knowledge distillation is performed between teacher and student modalities for each task to push the model parameters to a point that is beneficial for all tasks. Hence, even if the teacher modalities for certain tasks are missing during testing, the available student modalities can accomplish the task well enough based on the learned knowledge from their automatically elected teacher modalities. Experiments on the Brain Tumour Segmentation Dataset 2018 (BraTS2018) shows that LCKD outperforms other methods by a considerable margin, improving the state-of-the-art performance by 3.61% for enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in terms of segmentation Dice score."], "score": 0.0}, {"id": "(Cho et al., 2021)", "paper": {"corpus_id": 233219684, "title": "Dealing with Missing Modalities in the Visual Question Answer-Difference Prediction Task through Knowledge Distillation", "year": 2021, "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "authors": [{"name": "Jae-Won Cho", "authorId": "2191802"}, {"name": "Dong-Jin Kim", "authorId": "40622539"}, {"name": "Jinsoo Choi", "authorId": "2118888090"}, {"name": "Yunjae Jung", "authorId": "2116160249"}, {"name": "I. Kweon", "authorId": "98758720"}], "n_citations": 17}, "snippets": ["In this work, we address the issues of the missing modalities that have arisen from the Visual Question Answer-Difference prediction task and find a novel method to solve the task at hand. We address the missing modality\u2013the ground truth answers\u2013that are not present at test time and use a privileged knowledge distillation scheme to deal with the issue of the missing modality. In order to efficiently do so, we first introduce a model, the \"Big\" Teacher, that takes the image/question/answer triplet as its input and out-performs the baseline, then use a combination of models to distill knowledge to a target network (student) that only takes the image/question pair as its inputs. We experiment our models on the VizWiz and VQA-V2 Answer Difference datasets and show through extensive experimentation and ablation the performance of our method and a diverse possibility for future research."], "score": 0.0}, {"id": "(Li et al._2, 2024)", "paper": {"corpus_id": 269362788, "title": "Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities", "year": 2024, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Mingcheng Li", "authorId": "2188978724"}, {"name": "Dingkang Yang", "authorId": "2143920085"}, {"name": "Xiao Zhao", "authorId": "2212046663"}, {"name": "Shuai Wang", "authorId": "2186872968"}, {"name": "Yan Wang", "authorId": "2298314652"}, {"name": "Kun Yang", "authorId": "2288061973"}, {"name": "Mingyang Sun", "authorId": "2216487730"}, {"name": "Dongliang Kou", "authorId": "2218981837"}, {"name": "Ziyun Qian", "authorId": "2202592845"}, {"name": "Lihua Zhang", "authorId": "2278978362"}], "n_citations": 12}, "snippets": ["The core concept of these efforts is to transfer \"dark knowledge\" from teacher networks trained by complete modalities to student networks trained by missing modalities.The teacher model typically produces more valuable feature presentations than the student model."], "score": 0.62451171875}, {"id": "(Cardace et al., 2022)", "paper": {"corpus_id": 252918735, "title": "Self-Distillation for Unsupervised 3D Domain Adaptation", "year": 2022, "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "authors": [{"name": "Adriano Cardace", "authorId": "2131012006"}, {"name": "Riccardo Spezialetti", "authorId": "46297738"}, {"name": "Pierluigi Zama Ramirez", "authorId": "80804241"}, {"name": "Samuele Salti", "authorId": "2607607"}, {"name": "L. D. Stefano", "authorId": "9395079"}], "n_citations": 16}, "snippets": ["Point cloud classification is a popular task in 3D vision. However, previous works, usually assume that point clouds at test time are obtained with the same procedure or sensor as those at training time. Unsupervised Domain Adaptation (UDA) instead, breaks this assumption and tries to solve the task on an unlabeled target domain, leveraging only on a supervised source domain. For point cloud classification, recent UDA methods try to align features across domains via auxiliary tasks such as point cloud reconstruction, which however do not optimize the discriminative power in the target domain in feature space. In contrast, in this work, we focus on obtaining a discriminative feature space for the target domain enforcing consistency between a point cloud and its augmented version. We then propose a novel iterative self-training methodology that exploits Graph Neural Networks in the UDA context to refine pseudo-labels. We perform extensive experiments and set the new state-of-the art in standard UDA benchmarks for point cloud classification. Finally, we show how our approach can be extended to more complex tasks such as part segmentation."], "score": 0.0}, {"id": "(Mirzadeh et al., 2019)", "paper": {"corpus_id": 212908749, "title": "Improved Knowledge Distillation via Teacher Assistant", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Seyed Iman Mirzadeh", "authorId": "145156788"}, {"name": "Mehrdad Farajtabar", "authorId": "1682124"}, {"name": "Ang Li", "authorId": "2112839418"}, {"name": "Nir Levine", "authorId": "153898744"}, {"name": "Akihiro Matsukawa", "authorId": "2063980545"}, {"name": "H. Ghasemzadeh", "authorId": "144600887"}], "n_citations": 1081}, "snippets": ["Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."], "score": 0.0}], "table": null}], "cost": 0.436332}}
