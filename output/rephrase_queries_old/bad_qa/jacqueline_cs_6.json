{"original_query": "Social bias in large language models is an endemic problem, as models often inherit and amplify stereotypical judgments due to naturally occurring, yet undesirable statistical associations from training corpora. What are some recent techniques on post-trained, aligned LMs that mitigate social bias?", "input": "Social bias large language models is an endemic problem models inherit amplify stereotypical judgments undesirable statistical associations from training corpora recent techniques post trained LMs mitigate social bias", "output": {"sections": [{"title": "Introduction to Social Bias in LLMs", "tldr": "Large Language Models (LLMs) frequently exhibit social biases that reflect and sometimes amplify stereotypes present in their training data. These biases can perpetuate harmful generalizations related to gender, race, religion, and other social constructs, raising significant concerns as these models become increasingly deployed in real-world applications. (11 sources)", "text": "\nSocial bias in Large Language Models represents a persistent and endemic problem in natural language processing. These models inherently \"inherit and amplify stereotypical judgments and undesirable statistical associations from training corpora\" <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper>. The fundamental issue stems from how LLMs are designed to learn patterns from the vast amounts of human-generated text they are trained on, inevitably absorbing the biases present in those data sources. As demonstrated by Caliskan et al., language models trained on large quantities of human-generated texts encode human-like social biases that mirror implicit associations found in human cognition <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"23163324\" paperTitle=\"(Caliskan et al., 2016)\" isShortName></Paper>.\n\nThese biases manifest as \"harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs\" <Paper corpusId=\"235623756\" paperTitle=\"(Liang et al., 2021)\" isShortName></Paper>. For instance, research has shown that language models can exhibit persistent anti-Muslim bias, with GPT-3 associating \"Muslim\" with \"terrorist\" in 23% of test cases, a rate significantly higher than stereotypical associations for other religious groups <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"231603388\" paperTitle=\"(Abid et al., 2021)\" isShortName></Paper>.\n\nThe prevalence of these biases becomes increasingly concerning as LLMs are rapidly deployed in various real-world applications <Paper corpusId=\"270878706\" paperTitle=\"(Raj et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271310069\" paperTitle=\"(Allam, 2024)\" isShortName></Paper>. Social biases encoded in these models are reflected in downstream tasks such as machine translation, sentiment classification, and natural language generation <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"250391069\" paperTitle=\"(Jentzsch et al., 2023)\" isShortName></Paper> <Paper corpusId=\"190000105\" paperTitle=\"(Kurita et al., 2019)\" isShortName></Paper>. For example, when analyzing gender bias in BERT models used for movie classification, researchers found significant gender biases across multiple training conditions, indicating that \"reflected biases stem from public BERT models rather than task-specific data\" <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"250391069\" paperTitle=\"(Jentzsch et al., 2023)\" isShortName></Paper>.\n\nAs LLMs continue to advance in their capabilities to grasp complex contextual information, harmful biases are \"likely increasingly intertwined with those models\" <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"250391069\" paperTitle=\"(Jentzsch et al., 2023)\" isShortName></Paper>. The widespread adoption of these models makes addressing such biases an \"emerging and important task\" <Paper corpusId=\"248780440\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>, particularly as they have been consistently shown to \"exhibit various biases and stereotypes in their generated content\" <Paper corpusId=\"275336873\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270924184\" paperTitle=\"(Hida et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Mei et al., 2023)", "paper": {"corpus_id": 259129801, "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks", "year": 2023, "venue": "Conference on Fairness, Accountability and Transparency", "authors": [{"name": "Katelyn Mei", "authorId": "2189183000"}, {"name": "Sonia Fereidooni", "authorId": "2196943720"}, {"name": "Aylin Caliskan", "authorId": "144537437"}], "n_citations": 51}, "snippets": ["Social bias large language models is an endemic problem models inherit amplify stereotypical judgments undesirable statistical associations from training corpora", "Caliskan et al. (Caliskan et al., 2016) demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation (Abid et al., 2021)(Jentzsch et al., 2023)[23](Kurita et al., 2019). As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities."], "score": 0.97216796875}, {"id": "(Caliskan et al., 2016)", "paper": {"corpus_id": 23163324, "title": "Semantics derived automatically from language corpora contain human-like biases", "year": 2016, "venue": "Science", "authors": [{"name": "Aylin Caliskan", "authorId": "144537437"}, {"name": "J. Bryson", "authorId": "145315445"}, {"name": "Arvind Narayanan", "authorId": "47735253"}], "n_citations": 2673}, "snippets": ["Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology."], "score": 0.0}, {"id": "(Liang et al., 2021)", "paper": {"corpus_id": 235623756, "title": "Towards Understanding and Mitigating Social Biases in Language Models", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Paul Pu Liang", "authorId": "28130078"}, {"name": "Chiyu Wu", "authorId": "2115397918"}, {"name": "Louis-philippe Morency", "authorId": "49933077"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}], "n_citations": 394}, "snippets": ["Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation."], "score": 0.9677734375}, {"id": "(Abid et al., 2021)", "paper": {"corpus_id": 231603388, "title": "Persistent Anti-Muslim Bias in Large Language Models", "year": 2021, "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "authors": [{"name": "Abubakar Abid", "authorId": "144948925"}, {"name": "Maheen Farooqi", "authorId": "77751476"}, {"name": "James Y. Zou", "authorId": "145085305"}], "n_citations": 555}, "snippets": ["It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups."], "score": 0.0}, {"id": "(Raj et al., 2024)", "paper": {"corpus_id": 270878706, "title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis", "year": 2024, "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "authors": [{"name": "Chahat Raj", "authorId": "2261742076"}, {"name": "A. Mukherjee", "authorId": "2125631153"}, {"name": "Aylin Caliskan", "authorId": "2306632484"}, {"name": "Antonios Anastasopoulos", "authorId": "2261741456"}, {"name": "Ziwei Zhu", "authorId": "2261887816"}], "n_citations": 14}, "snippets": ["Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities."], "score": 0.9765625}, {"id": "(Allam, 2024)", "paper": {"corpus_id": 271310069, "title": "BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ahmed Allam", "authorId": "2312204915"}], "n_citations": 10}, "snippets": ["Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns."], "score": 0.96923828125}, {"id": "(Jentzsch et al., 2023)", "paper": {"corpus_id": 250391069, "title": "Gender Bias in BERT - Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task", "year": 2023, "venue": "GEBNLP", "authors": [{"name": "Sophie F. Jentzsch", "authorId": "151209594"}, {"name": "Cigdem Turan", "authorId": "13671251"}], "n_citations": 33}, "snippets": ["Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT?s biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage."], "score": 0.0}, {"id": "(Kurita et al., 2019)", "paper": {"corpus_id": 190000105, "title": "Measuring Bias in Contextualized Word Representations", "year": 2019, "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing", "authors": [{"name": "Keita Kurita", "authorId": "147225682"}, {"name": "Nidhi Vyas", "authorId": "47963068"}, {"name": "Ayush Pareek", "authorId": "18081101"}, {"name": "A. Black", "authorId": "1690706"}, {"name": "Yulia Tsvetkov", "authorId": "145317727"}], "n_citations": 451}, "snippets": ["Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases."], "score": 0.0}, {"id": "(Guo et al., 2022)", "paper": {"corpus_id": 248780440, "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yue Guo", "authorId": null}, {"name": "Yi Yang", "authorId": "46285693"}, {"name": "A. Abbasi", "authorId": "144849629"}], "n_citations": 167}, "snippets": ["Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task."], "score": 0.97119140625}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 275336873, "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yachao Zhao", "authorId": "2233316526"}, {"name": "Bo Wang", "authorId": "2266189718"}, {"name": "Yan Wang", "authorId": "2301797252"}], "n_citations": 4}, "snippets": ["Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content."], "score": 0.970703125}, {"id": "(Hida et al., 2024)", "paper": {"corpus_id": 270924184, "title": "Social Bias Evaluation for Large Language Models Requires Prompt Variations", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Rem Hida", "authorId": "46186371"}, {"name": "Masahiro Kaneko", "authorId": "143655216"}, {"name": "Naoaki Okazaki", "authorId": "2269460776"}], "n_citations": 20}, "snippets": ["Large Language Models (LLMs) exhibit considerable social biases, and various studies have tried to evaluate and mitigate these biases accurately."], "score": 0.9775390625}], "table": null}, {"title": "Sources and Manifestations of Social Bias", "tldr": "Large Language Models (LLMs) inherit social biases from their training data that primarily stem from unfiltered internet content, reflecting and sometimes amplifying societal stereotypes. These biases manifest across multiple dimensions including gender, race, religion, disability, and other demographic attributes, affecting various downstream applications. (14 sources)", "text": "\nSocial biases in Large Language Models originate primarily from their training methodology and data sources. LLMs are typically \"trained on enormous scale of uncurated Internet-based data\" <Paper corpusId=\"261530629\" paperTitle=\"(Gallegos et al., 2023)\" isShortName></Paper>, which inherently contains biased information reflecting historical and structural power imbalances in society. This uncurated approach means that \"biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models\" <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper>.\n\nThe sources of these biases are multifaceted. They stem from \"historical inequalities in training data, linguistic imbalances, and adversarial manipulation\" <Paper corpusId=\"277667520\" paperTitle=\"(Cantini et al., 2025)\" isShortName></Paper>, as well as \"data availability, selection, language, and social contexts\" <Paper corpusId=\"271097745\" paperTitle=\"(Cantini et al., 2024)\" isShortName></Paper>. When exposed to such large unstructured datasets, LLMs \"learn and sometimes even amplify the biases present in such data\" <Paper corpusId=\"253762006\" paperTitle=\"(Garimella et al., 2022)\" isShortName></Paper>.\n\nThese biases manifest across multiple demographic dimensions. Studies have consistently demonstrated that LLMs exhibit:\n\n1. **Gender bias**: Models like GPT-2 \"tend to generate more negative texts towards females\" <Paper corpusId=\"270214849\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\n2. **Religious bias**: GPT-3 has been shown to \"frequently associate Muslims with violent contexts\" <Paper corpusId=\"277667666\" paperTitle=\"(Xiao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231603388\" paperTitle=\"(Abid et al., 2021)\" isShortName></Paper>, with Muslims being \"analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases\" <Paper corpusId=\"277667666\" paperTitle=\"(Xiao et al., 2025)\" isShortName></Paper>.\n\n3. **Racial bias**: Language models embody \"covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded\" <Paper corpusId=\"277667666\" paperTitle=\"(Xiao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"272214842\" paperTitle=\"(Hofmann et al., 2024)\" isShortName></Paper>.\n\n4. **Disability bias**: Models show \"undesirable biases towards mentions of disability\" <Paper corpusId=\"265212726\" paperTitle=\"(Manerba et al., 2023)\" isShortName></Paper>. For instance, \"gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness\" <Paper corpusId=\"265212726\" paperTitle=\"(Manerba et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218487466\" paperTitle=\"(Hutchinson et al., 2020)\" isShortName></Paper>.\n\n5. **Stigma-related bias**: Studies examining bias against 93 stigmatized groups found that \"when prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions\" <Paper corpusId=\"267411833\" paperTitle=\"(Gallegos et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper>.\n\nThese biases are not merely contained within the models themselves but are \"perpetuated to downstream tasks\" <Paper corpusId=\"265212726\" paperTitle=\"(Manerba et al., 2023)\" isShortName></Paper>. For example, \"bias against stigmatized groups is also reflected in four downstream sentiment classifiers\" where \"sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative\" <Paper corpusId=\"267411833\" paperTitle=\"(Gallegos et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper>.\n\nEven with advancements in alignment techniques such as Reinforcement Learning from Human Feedback (RLHF), the problem persists. Current practices intended to alleviate racial bias \"exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level\" <Paper corpusId=\"277667666\" paperTitle=\"(Xiao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"272214842\" paperTitle=\"(Hofmann et al., 2024)\" isShortName></Paper>.\n\nThe manifestation of these biases becomes particularly concerning as LLMs are increasingly \"deployed in applications that affect millions of people\" <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper>, where their \"inherent biases are harmful to the targeted social groups\" <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper>. This harmful impact extends to various practical applications, with biases potentially leading to \"unfair predictions based on attributes like race and gender\" <Paper corpusId=\"271859735\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper> across multimodal applications and other domains.", "citations": [{"id": "(Gallegos et al., 2023)", "paper": {"corpus_id": 261530629, "title": "Bias and Fairness in Large Language Models: A Survey", "year": 2023, "venue": "Computational Linguistics", "authors": [{"name": "Isabel O. Gallegos", "authorId": "2237806749"}, {"name": "Ryan A. Rossi", "authorId": "2066337266"}, {"name": "Joe Barrow", "authorId": "40080808"}, {"name": "Md. Mehrab Tanjim", "authorId": "35631602"}, {"name": "Sungchul Kim", "authorId": "2109571021"}, {"name": "Franck Dernoncourt", "authorId": "2462276"}, {"name": "Tong Yu", "authorId": "1500399016"}, {"name": "Ruiyi Zhang", "authorId": "1940556"}, {"name": "Nesreen Ahmed", "authorId": "47699955"}], "n_citations": 594}, "snippets": ["Typically trained on an enormous scale of uncurated Internet-based data, LLMs inherit stereotypes, misrepresentations, derogatory and exclusionary language, and other denigrating behaviors that disproportionately affect already-vulnerable and marginalized communities Dodge et al., 2021;Sheng et al., 2021b). These harms are forms of \"social bias,\" a subjective and normative term we broadly use to refer to disparate treatment or outcomes between social groups that arise from historical and structural power asymmetries, which we define and discuss in Section 2. Though LLMs often reflect existing biases, they can amplify these biases too; in either case, the automated reproduction of injustice can reinforce systems of inequity (Benjamin, 2020)."], "score": 0.9892578125}, {"id": "(Sharma et al., 2023)", "paper": {"corpus_id": 258170403, "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Swapnil Sharma", "authorId": "2214583125"}, {"name": "Nikita Anand", "authorId": "2214521713"}, {"name": "V. KranthiKiranG.", "authorId": "1415341297"}, {"name": "Alind Jain", "authorId": "2214565507"}], "n_citations": 0}, "snippets": ["Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups."], "score": 0.96923828125}, {"id": "(Cantini et al., 2025)", "paper": {"corpus_id": 277667520, "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Riccardo Cantini", "authorId": "1585232914"}, {"name": "A. Orsino", "authorId": "96934840"}, {"name": "Massimo Ruggiero", "authorId": "2354558125"}, {"name": "Domenico Talia", "authorId": "2299780920"}], "n_citations": 4}, "snippets": ["Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation."], "score": 0.970703125}, {"id": "(Cantini et al., 2024)", "paper": {"corpus_id": 271097745, "title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation", "year": 2024, "venue": "IFIP Working Conference on Database Semantics", "authors": [{"name": "Riccardo Cantini", "authorId": "1585232914"}, {"name": "Giada Cosenza", "authorId": "2310699340"}, {"name": "A. Orsino", "authorId": "96934840"}, {"name": "Domenico Talia", "authorId": "2299780920"}], "n_citations": 7}, "snippets": ["Biases in data availability, selection, language, and social contexts may collectively reflect prejudices, disparities, and stereotypes that can inadvertently be learned and perpetuated by LLMs, leading to unfair and harmful responses."], "score": 0.9736328125}, {"id": "(Garimella et al., 2022)", "paper": {"corpus_id": 253762006, "title": "Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique", "year": 2022, "venue": "AACL", "authors": [{"name": "Aparna Garimella", "authorId": "31099365"}, {"name": "Rada Mihalcea", "authorId": "2105984203"}, {"name": "Akhash Amarnath", "authorId": "2121347719"}], "n_citations": 21}, "snippets": ["BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others."], "score": 0.98046875}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 270214849, "title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Tianci Liu", "authorId": "1803285"}, {"name": "Haoyu Wang", "authorId": "51225422"}, {"name": "Shiyang Wang", "authorId": "1486407811"}, {"name": "Yu Cheng", "authorId": "2304607213"}, {"name": "Jing Gao", "authorId": "2284861474"}], "n_citations": 1}, "snippets": ["Notwithstanding, despite their remarkable performance, LMs suffer from the fairness issue, i.e., they may generate negative texts that are biased against underrepresented demographic groups (e.g., female) in our society (Sheng et al., 2019). For instance, GPT-2 (Radford et al., 2019) tends to generate more negative texts towards females (Huang et al., 2019). Such social biases, termed as global biases (Sheng et al., 2020), stem from the real world corpora due to historical reasons (Basta et al., 2019). Unsurprisingly, LMs reproduce or amplify the biases from the data whereon they are trained (Gehman et al., 2020; Schick et al., 2021)."], "score": 0.97021484375}, {"id": "(Xiao et al., 2025)", "paper": {"corpus_id": 277667666, "title": "Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias in Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yisong Xiao", "authorId": "2276489683"}, {"name": "Aishan Liu", "authorId": "2257572247"}, {"name": "Siyuan Liang", "authorId": "2325884825"}, {"name": "Xianglong Liu", "authorId": "2237942988"}, {"name": "Dacheng Tao", "authorId": "2237906923"}], "n_citations": 3}, "snippets": ["LLMs often inherit social stereotypes and biases [96] from the training data (Hofmann et al., 2024)83,92], leading to biased behavior toward specific social groups, particularly in relation to protected attributes such as religion, race, and gender. For instance, GPT-3 (Brown et al., 2020) has been shown to frequently associate Muslims with violent contexts (Abid et al., 2021)[39], and Microsoft's AI chatbot Tay infamously produced racist and inappropriate content after interacting with users on social media [8]."], "score": 0.9853515625}, {"id": "(Abid et al., 2021)", "paper": {"corpus_id": 231603388, "title": "Persistent Anti-Muslim Bias in Large Language Models", "year": 2021, "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "authors": [{"name": "Abubakar Abid", "authorId": "144948925"}, {"name": "Maheen Farooqi", "authorId": "77751476"}, {"name": "James Y. Zou", "authorId": "145085305"}], "n_citations": 555}, "snippets": ["It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups."], "score": 0.0}, {"id": "(Hofmann et al., 2024)", "paper": {"corpus_id": 272214842, "title": "AI generates covertly racist decisions about people based on their dialect", "year": 2024, "venue": "The Naturalist", "authors": [{"name": "Valentin Hofmann", "authorId": "2289842227"}, {"name": "Pratyusha Kalluri", "authorId": "13014201"}, {"name": "Dan Jurafsky", "authorId": "2256674786"}, {"name": "Sharese King", "authorId": "2289843030"}], "n_citations": 77}, "snippets": ["Hundreds of millions of people now interact with language models, with uses ranging from help with writing1,2 to informing hiring decisions3. However, these language models are known to perpetuate systematic racial prejudices, making their judgements biased in problematic ways about groups such as African Americans4\u20137. Although previous research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time, particularly in the United States after the civil rights movement8,9. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded. By contrast, the language models\u2019 overt stereotypes about African Americans are more positive. Dialect prejudice has the potential for harmful consequences: language models are more likely to suggest that speakers of AAE be assigned less-prestigious jobs, be convicted of crimes and be sentenced to death. Finally, we show that current practices of alleviating racial bias in language models, such as human preference alignment, exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level. Our findings have far-reaching implications for the fair and safe use of language technology."], "score": 0.0}, {"id": "(Manerba et al., 2023)", "paper": {"corpus_id": 265212726, "title": "Social Bias Probing: Fairness Benchmarking for Language Models", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Marta Marchiori Manerba", "authorId": "2121386115"}, {"name": "Karolina Sta\u0144czak", "authorId": "82563120"}, {"name": "Riccardo Guidotti", "authorId": "2257013371"}, {"name": "Isabelle Augenstein", "authorId": "1736067"}], "n_citations": 20}, "snippets": ["The unparalleled ability of language models (LMs) to generalize from vast corpora is tinged by an inherent reinforcement of social biases. These biases are not merely encoded within LMs' representations but are also perpetuated to downstream tasks (Blodgett et al., 2021)Sta\u0144czak and Augenstein, 2021), where they can manifest in an uneven treatment of different demographic groups (Rudinger et al., 2018)(Stanovsky et al., 2019)(Kiritchenko et al., 2018)(Venkit et al., 2022)."], "score": 0.966796875}, {"id": "(Hutchinson et al., 2020)", "paper": {"corpus_id": 218487466, "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ben Hutchinson", "authorId": "2044655623"}, {"name": "Vinodkumar Prabhakaran", "authorId": "3331141"}, {"name": "Emily L. Denton", "authorId": "40081727"}, {"name": "Kellie Webster", "authorId": "20825661"}, {"name": "Yu Zhong", "authorId": "2112887022"}, {"name": "Stephen Denuyl", "authorId": "1667883461"}], "n_citations": 313}, "snippets": ["Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness."], "score": 0.0}, {"id": "(Gallegos et al., 2024)", "paper": {"corpus_id": 267411833, "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Isabel O. Gallegos", "authorId": "2237806749"}, {"name": "Ryan A. Rossi", "authorId": "2066337266"}, {"name": "Joe Barrow", "authorId": "40080808"}, {"name": "Md. Mehrab Tanjim", "authorId": "35631602"}, {"name": "Tong Yu", "authorId": "1500399016"}, {"name": "Hanieh Deilamsalehy", "authorId": "1787977"}, {"name": "Ruiyi Zhang", "authorId": "2283147661"}, {"name": "Sungchul Kim", "authorId": "2261424174"}, {"name": "Franck Dernoncourt", "authorId": "2462276"}], "n_citations": 23}, "snippets": ["At the same time as this success, however, LLMs have been shown to learn, reproduce, and even amplify denigrating, stereotypical, and exclusionary social behaviors (e.g., (Bender et al., 2021)(Hutchinson et al., 2020)(Mei et al., 2023)(Sheng et al., 2021)Weidinger et al., 2022). We refer to this class of harms as \"social bias,\" a normative term that characterizes disparate representations, treatments, or outcomes between social groups due to historical and structural power imbalances."], "score": 0.9892578125}, {"id": "(Mei et al., 2023)", "paper": {"corpus_id": 259129801, "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks", "year": 2023, "venue": "Conference on Fairness, Accountability and Transparency", "authors": [{"name": "Katelyn Mei", "authorId": "2189183000"}, {"name": "Sonia Fereidooni", "authorId": "2196943720"}, {"name": "Aylin Caliskan", "authorId": "144537437"}], "n_citations": 51}, "snippets": ["Social bias large language models is an endemic problem models inherit amplify stereotypical judgments undesirable statistical associations from training corpora", "Caliskan et al. (Caliskan et al., 2016) demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation (Abid et al., 2021)(Jentzsch et al., 2023)[23](Kurita et al., 2019). As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities."], "score": 0.97216796875}, {"id": "(Cheng et al., 2024)", "paper": {"corpus_id": 271859735, "title": "Social Debiasing for Fair Multi-modal LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Harry Cheng", "authorId": "2149241557"}, {"name": "Yangyang Guo", "authorId": "1390575046"}, {"name": "Qingpei Guo", "authorId": "2273322768"}, {"name": "Ming Yang", "authorId": "2249834712"}, {"name": "Tian Gan", "authorId": "2247906706"}, {"name": "Liqiang Nie", "authorId": "2284688853"}], "n_citations": 2}, "snippets": ["Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender."], "score": 0.982421875}], "table": null}, {"title": "Impacts and Harms of Social Bias", "tldr": "Social biases in Large Language Models (LLMs) result in significant real-world harms that disproportionately affect marginalized communities. These harms manifest as both representational damage\u2014reinforcing negative stereotypes about certain groups\u2014and allocational damage\u2014potentially denying opportunities or resources to members of disadvantaged communities. (11 sources)", "text": "\nSocial biases embedded in Large Language Models (LLMs) pose serious concerns as these systems increasingly influence public discourse and decision-making processes <Paper corpusId=\"269449709\" paperTitle=\"(Narayan et al., 2024)\" isShortName></Paper>. The impact of these biases extends beyond theoretical concerns into tangible, real-world harms that disproportionately affect marginalized communities. These harms generally fall into two primary categories: representational and allocational <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper>.\n\nRepresentational harms occur when LLMs \"portray some groups negatively or fail to represent them at all\" <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper>. This negative portrayal perpetuates stereotypes and reinforces discriminatory attitudes toward specific groups. For instance, studies have shown that biases in LLMs can \"perpetuate stereotypes, reinforce discrimination, and negatively impact real-world decision-making\" <Paper corpusId=\"276647994\" paperTitle=\"(Pan et al., 2025)\" isShortName></Paper>.\n\nAllocational harms, on the other hand, involve \"denying certain groups opportunities or resources\" <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper>. These harms have significant implications in critical domains such as \"hiring, law enforcement, and content moderation,\" where biased LLM outputs \"may disproportionately harm marginalized individuals and communities\" <Paper corpusId=\"276647994\" paperTitle=\"(Pan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"222090785\" paperTitle=\"(Nangia et al., 2020)\" isShortName></Paper> <Paper corpusId=\"239010011\" paperTitle=\"(Parrish et al., 2021)\" isShortName></Paper>.\n\nThe impacts of these biases become particularly concerning as LLMs are deployed in applications affecting millions of people <Paper corpusId=\"276317810\" paperTitle=\"(Narnaware et al., 2025)\" isShortName></Paper>. In practical applications, fairness and inclusivity are essential for equitable outcomes, yet \"existing biases in training data often manifest in model responses, leading to unintended but impactful consequences\" <Paper corpusId=\"276317810\" paperTitle=\"(Narnaware et al., 2025)\" isShortName></Paper>. For example, when used in question-answering systems with under-informative contexts, models often rely on stereotypes, consistently reproducing harmful biases <Paper corpusId=\"239010011\" paperTitle=\"(Parrish et al., 2021)\" isShortName></Paper>.\n\nThe consequences of these biases can be particularly dire, resulting in \"amplification of bias, discrimination, and detrimental effects on marginalized groups\" <Paper corpusId=\"271923841\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. Even more concerning is evidence that LLMs not only reflect existing biases but may actually amplify them beyond what is reflected in human perceptions or ground truth statistics <Paper corpusId=\"261276445\" paperTitle=\"(Kotek et al., 2023)\" isShortName></Paper>. This amplification effect poses a significant risk as these models are \"integrated into commonplace technology,\" potentially perpetuating \"negative preconceptions and social injustices\" on a broader scale <Paper corpusId=\"277758223\" paperTitle=\"(Kamruzzaman, 2025)\" isShortName></Paper>.\n\nFurthermore, the impact of biases extends beyond static representations to generation tasks. As Garimella et al. note, \"mitigating biases in only the representations may not suffice to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation\" <Paper corpusId=\"236477795\" paperTitle=\"(Garimella et al., 2021)\" isShortName></Paper>. This indicates that even when models perform well on traditional bias benchmarks, they may still produce biased outputs in practical applications.\n\nPerhaps most troubling is the evidence that current bias mitigation techniques, such as human preference alignment, may actually \"exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level\" <Paper corpusId=\"272214842\" paperTitle=\"(Hofmann et al., 2024)\" isShortName></Paper>. This suggests that addressing the harms of social bias in LLMs requires more fundamental approaches than simply filtering out overtly biased responses.", "citations": [{"id": "(Narayan et al., 2024)", "paper": {"corpus_id": 269449709, "title": "Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "M. Narayan", "authorId": "2232497"}, {"name": "John Pasmore", "authorId": "2298907347"}, {"name": "Elton Sampaio", "authorId": "2298908887"}, {"name": "Vijay Raghavan", "authorId": "2298907296"}, {"name": "Gabriella Waters", "authorId": "2298907100"}], "n_citations": 1}, "snippets": ["The burgeoning influence of Large Language Models (LLMs) in shaping public discourse and decision-making underscores the imperative to address inherent biases within these AI systems."], "score": 0.96728515625}, {"id": "(Kolisko et al., 2023)", "paper": {"corpus_id": 259716055, "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Skylar Kolisko", "authorId": "2222666109"}, {"name": "Carolyn Jane Anderson", "authorId": "144901955"}], "n_citations": 11}, "snippets": ["Social bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017)."], "score": 0.9833984375}, {"id": "(Pan et al., 2025)", "paper": {"corpus_id": 276647994, "title": "Beneath the Surface: How Large Language Models Reflect Hidden Bias", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jinhao Pan", "authorId": "2294143417"}, {"name": "Chahat Raj", "authorId": "2261742076"}, {"name": "Ziyu Yao", "authorId": "2323112615"}, {"name": "Ziwei Zhu", "authorId": "2220183419"}], "n_citations": 0}, "snippets": ["The remarkable performance of Large Language Models (LLMs) is frequently accompanied by the propagation of social bias inherent in their training data (Gallegos et al., 2023)(Hofmann et al., 2024)(Navigli et al., 2023)Cui et al., 2024). These biases raise serious ethical concerns, as they perpetuate stereotypes, reinforce discrimination, and negatively impact real-world decision-making. In domains such as hiring, law enforcement, and content moderation, the use of these models in realworld applications may disproportionately harm marginalized individuals and communities (Parrish et al., 2021)(Nangia et al., 2020)(Nadeem et al., 2020)(Manerba et al., 2023)Bi et al., 2023;del Arco et al., 2024;(Kotek et al., 2023)."], "score": 0.9697265625}, {"id": "(Nangia et al., 2020)", "paper": {"corpus_id": 222090785, "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Nikita Nangia", "authorId": "10666396"}, {"name": "Clara Vania", "authorId": "3054462"}, {"name": "Rasika Bhalerao", "authorId": "49550275"}, {"name": "Samuel R. Bowman", "authorId": "3644767"}], "n_citations": 685}, "snippets": ["Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress."], "score": 0.0}, {"id": "(Parrish et al., 2021)", "paper": {"corpus_id": 239010011, "title": "BBQ: A hand-built bias benchmark for question answering", "year": 2021, "venue": "Findings", "authors": [{"name": "Alicia Parrish", "authorId": "119389860"}, {"name": "Angelica Chen", "authorId": "13336152"}, {"name": "Nikita Nangia", "authorId": "10666396"}, {"name": "Vishakh Padmakumar", "authorId": "2044959912"}, {"name": "Jason Phang", "authorId": "80842917"}, {"name": "Jana Thompson", "authorId": "2148444557"}, {"name": "Phu Mon Htut", "authorId": "41022736"}, {"name": "Sam Bowman", "authorId": "1799822"}], "n_citations": 424}, "snippets": ["It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model\u2019s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model\u2019s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested."], "score": 0.0}, {"id": "(Narnaware et al., 2025)", "paper": {"corpus_id": 276317810, "title": "SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Vishal Narnaware", "authorId": "2345186336"}, {"name": "Ashmal Vayani", "authorId": "2287846115"}, {"name": "Rohit Gupta", "authorId": "2110003398"}, {"name": "S. Swetha", "authorId": "143951905"}, {"name": "Mubarak Shah", "authorId": "2287971163"}], "n_citations": 3}, "snippets": ["Bias in LMMs is particularly concerning in real-world applications, where fairness and inclusivity are essential for equi- table outcomes. Existing biases in training data often manifest in model responses, leading to unintended but impactful consequences. Addressing these biases is crucial to ensuring that LMMs contribute positively to society while minimizing potential harms."], "score": 0.96240234375}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 271923841, "title": "Identifying and Mitigating Social Bias Knowledge in Language Models", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ruizhe Chen", "authorId": "2255346632"}, {"name": "Yichen Li", "authorId": "2301405716"}, {"name": "Jianfei Yang", "authorId": "2260614480"}, {"name": "Yang Feng", "authorId": "2253854049"}, {"name": "J. Zhou", "authorId": "2253900280"}, {"name": "Jian Wu", "authorId": "2253868642"}, {"name": "Zuozhu Liu", "authorId": "2311458018"}], "n_citations": 7}, "snippets": ["Issues related to fairness in LLMs can have dire outcomes, such as the amplification of bias, discrimination, and detrimental effects on marginalized groups. Consequently, substantial efforts are being made to assess and address biases in large language models (Gallegos et al., 2023;Li et al., 2023b;Fan et al., 2024a;Luo et al., 2024)."], "score": 0.978515625}, {"id": "(Kotek et al., 2023)", "paper": {"corpus_id": 261276445, "title": "Gender bias and stereotypes in Large Language Models", "year": 2023, "venue": "International Conference on Climate Informatics", "authors": [{"name": "Hadas Kotek", "authorId": "3365389"}, {"name": "Rikker Dockum", "authorId": "90166394"}, {"name": "David Q. Sun", "authorId": "32100412"}], "n_citations": 236}, "snippets": ["Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs\u2019 behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women\u2019s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person\u2019s gender; (b) these choices align with people\u2019s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably."], "score": 0.0}, {"id": "(Kamruzzaman, 2025)", "paper": {"corpus_id": 277758223, "title": "Investigating and Mitigating Undesirable Biases in Large Language Models", "year": 2025, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "M. Kamruzzaman", "authorId": "2077526744"}], "n_citations": 0}, "snippets": ["The widespread integration of these models into commonplace technology has brought to light deep concerns about the biases they encompass, which could serve to perpetuate negative preconceptions and social injustices."], "score": 0.9833984375}, {"id": "(Garimella et al., 2021)", "paper": {"corpus_id": 236477795, "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation", "year": 2021, "venue": "Findings", "authors": [{"name": "Aparna Garimella", "authorId": "31099365"}, {"name": "Akhash Amarnath", "authorId": "2121347719"}, {"name": "K. Kumar", "authorId": "2110632520"}, {"name": "Akash Pramod Yalla", "authorId": "2121368400"}, {"name": "Anandhavelu Natarajan", "authorId": "3365985"}, {"name": "Niyati Chhaya", "authorId": "2954043"}, {"name": "Balaji Vasan Srinivasan", "authorId": "2881425"}], "n_citations": 59}, "snippets": ["Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classi\ufb01cation tasks, mitigating biases in only the representations may not suf-\ufb01ce to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation."], "score": 0.974609375}, {"id": "(Hofmann et al., 2024)", "paper": {"corpus_id": 272214842, "title": "AI generates covertly racist decisions about people based on their dialect", "year": 2024, "venue": "The Naturalist", "authors": [{"name": "Valentin Hofmann", "authorId": "2289842227"}, {"name": "Pratyusha Kalluri", "authorId": "13014201"}, {"name": "Dan Jurafsky", "authorId": "2256674786"}, {"name": "Sharese King", "authorId": "2289843030"}], "n_citations": 77}, "snippets": ["Hundreds of millions of people now interact with language models, with uses ranging from help with writing1,2 to informing hiring decisions3. However, these language models are known to perpetuate systematic racial prejudices, making their judgements biased in problematic ways about groups such as African Americans4\u20137. Although previous research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time, particularly in the United States after the civil rights movement8,9. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded. By contrast, the language models\u2019 overt stereotypes about African Americans are more positive. Dialect prejudice has the potential for harmful consequences: language models are more likely to suggest that speakers of AAE be assigned less-prestigious jobs, be convicted of crimes and be sentenced to death. Finally, we show that current practices of alleviating racial bias in language models, such as human preference alignment, exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level. Our findings have far-reaching implications for the fair and safe use of language technology."], "score": 0.0}], "table": null}, {"title": "Bias Mitigation Techniques", "tldr": "Researchers have developed various techniques to mitigate social biases in Large Language Models, including fine-tuning approaches, model editing methods, prompt-based debiasing, and projection-based approaches that modify representations. Each approach offers different trade-offs between effectiveness, computational cost, and preservation of model performance on downstream tasks. (12 sources)", "text": "\nA wide range of techniques have been developed to address social biases in Large Language Models. These approaches generally fall into several categories:\n\n1. **Fine-tuning-based approaches**:\n - **Counterfactual Data Augmentation (CDA)**: This technique involves creating balanced training datasets by generating counterfactual examples that swap demographic attributes, helping models learn more equitable representations <Paper corpusId=\"184486914\" paperTitle=\"(Zmigrod et al., 2019)\" isShortName></Paper> <Paper corpusId=\"269773271\" paperTitle=\"(Chen et al._1, 2024)\" isShortName></Paper>.\n - **Re-balanced corpus pre-training**: Using demographically balanced datasets during pre-training or fine-tuning to reduce the impact of imbalanced representation in original training data <Paper corpusId=\"269773271\" paperTitle=\"(Chen et al._1, 2024)\" isShortName></Paper>.\n - **Contrastive learning**: Teaching models to distinguish between biased and unbiased content through contrastive objectives <Paper corpusId=\"269773271\" paperTitle=\"(Chen et al._1, 2024)\" isShortName></Paper>.\n\n2. **Representation modification approaches**:\n - **Iterative Null-space Projection (INLP)**: This method involves repeatedly training linear classifiers to predict protected attributes, then projecting representations onto their null-space to remove information related to those attributes <Paper corpusId=\"215786522\" paperTitle=\"(Ravfogel et al., 2020)\" isShortName></Paper> <Paper corpusId=\"266054040\" paperTitle=\"(Prakash et al., 2023)\" isShortName></Paper>.\n - **Debiasing layers**: Adding specialized layers to model architectures specifically designed to reduce bias in representations <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper>.\n - **Model editing methods**: Post-hoc modification techniques that alter specific model weights or activations to reduce stereotypical associations without requiring complete retraining <Paper corpusId=\"267770177\" paperTitle=\"(Yan et al., 2024)\" isShortName></Paper>.\n\n3. **Prompt-based approaches**:\n - **Prompt tuning**: Using specially designed prompts that guide models away from producing biased outputs <Paper corpusId=\"248780440\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper> <Paper corpusId=\"269773271\" paperTitle=\"(Chen et al._1, 2024)\" isShortName></Paper>.\n - **Auto-Debias**: Automatically searching for biased prompts and then applying distribution alignment loss to mitigate differences in completions across demographic groups <Paper corpusId=\"248780440\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>.\n - **Continuous prompt tuning**: Unlike discrete prompts with fixed semantic meanings, continuous prompts use unfixed mathematical representations at the token level to debias models more effectively <Paper corpusId=\"253446867\" paperTitle=\"(Yang et al., 2022)\" isShortName></Paper>.\n\n4. **Feedback-based approaches**:\n - **Reinforcement Learning from Human Feedback (RLHF)**: Using human evaluations to guide model outputs away from biased responses <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper>.\n - **Ethical prompting**: Instructing models to behave ethically, though this approach often results in performance degradation <Paper corpusId=\"274965310\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.\n\n5. **Evaluation and benchmarking tools**:\n - **Specialized reference datasets**: Resources like Winogender and Winobias provide standardized benchmarks for measuring and addressing gender bias <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper>.\n - **Stereotype measurement datasets**: Tools like StereoSet <Paper corpusId=\"215828184\" paperTitle=\"(Nadeem et al., 2020)\" isShortName></Paper> and CrowS-Pairs <Paper corpusId=\"222090785\" paperTitle=\"(Nangia et al., 2020)\" isShortName></Paper> enable systematic evaluation of stereotypical biases across different domains and protected attributes.\n\nEach approach involves trade-offs between debiasing effectiveness, computational cost, and preservation of model performance. As bias mitigation becomes increasingly critical for responsible AI deployment in high-stakes contexts like healthcare and hiring <Paper corpusId=\"268379141\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>, developing techniques that effectively reduce bias without compromising model utility remains an active area of research.", "citations": [{"id": "(Zmigrod et al., 2019)", "paper": {"corpus_id": 184486914, "title": "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ran Zmigrod", "authorId": "51044403"}, {"name": "Sabrina J. Mielke", "authorId": "27689253"}, {"name": "Hanna M. Wallach", "authorId": "1831395"}, {"name": "Ryan Cotterell", "authorId": "1750769"}], "n_citations": 283}, "snippets": ["Gender stereotypes are manifest in most of the world\u2019s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality."], "score": 0.0}, {"id": "(Chen et al._1, 2024)", "paper": {"corpus_id": 269773271, "title": "Large Language Model Bias Mitigation from the Perspective of Knowledge Editing", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ruizhe Chen", "authorId": "2255346632"}, {"name": "Yichen Li", "authorId": "2301405716"}, {"name": "Zikai Xiao", "authorId": "2257215912"}, {"name": "Zuo-Qiang Liu", "authorId": "2155333146"}], "n_citations": 14}, "snippets": ["Pre-trained Large Language Models (LLMs) have demonstrated exceptional performance on many tasks (Devlin et al., 2018;Floridi & Chiriatti, 2020;(Brown et al., 2020).However, the encoded social stereotypes and human-like biases inevitably cause undesired behaviors when deploying LLMs in practice (Zhao et al., 2019;(Navigli et al., 2023)Sheng et al., 2021).Existing approaches to mitigate biases in LLMs are mainly categorized into: (1) Fine-tuning (Zmigrod et al., 2019;Webster et al., 2020;He et al., 2022;Liang et al., 2020;Lauscher et al., 2021), which includes techniques such as re-balanced corpus pre-training, contrastive learning, projection methods, and efficient parameter tuning.(2) Prompt-tuning (Guo et al., 2022)(Yang et al., 2022)Li et al., 2023b;Dong et al., 2023), which involves creating prompts to address social biases."], "score": 0.98046875}, {"id": "(Ravfogel et al., 2020)", "paper": {"corpus_id": 215786522, "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Shauli Ravfogel", "authorId": "51432464"}, {"name": "Yanai Elazar", "authorId": "51131518"}, {"name": "Hila Gonen", "authorId": "1821892"}, {"name": "Michael Twiton", "authorId": "102707804"}, {"name": "Yoav Goldberg", "authorId": "79775260"}], "n_citations": 388}, "snippets": ["The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification."], "score": 0.0}, {"id": "(Prakash et al., 2023)", "paper": {"corpus_id": 266054040, "title": "Layered Bias: Interpreting Bias in Pretrained Large Language Models", "year": 2023, "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP", "authors": [{"name": "Nirmalendu Prakash", "authorId": "2218633063"}, {"name": "Roy Ka-Wei Lee", "authorId": "2261922609"}], "n_citations": 3}, "snippets": ["In recent years, the NLP community has prioritized studying biases in LLMs. Early work by (Bolukbasi et al., 2016) revealed gender and ethnic biases in word embeddings like Word2Vec and GloVe. This trend of identifying biases continued with more complex models like BERT, where researchers examined how biases are encoded and propagated (Kurita et al., 2019)(May et al., 2019). Researchers have also developed datasets, such as StereoSet (Nadeem et al., 2020) and CrowS-Pairs (Nangia et al., 2020), specifically to measure and understand these biases. (Sap et al., 2019) delved into the effects of biased data, especially from human annotators, on the behavior of models. Alongside identification, efforts have been geared towards the mitigation of bias in LLMs. Techniques such as iterative nullspace projection (INLP) (Ravfogel et al., 2020) and Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019) have been proposed and implemented to mitigate biases in LLMs."], "score": 0.97509765625}, {"id": "(Mirza et al., 2024)", "paper": {"corpus_id": 272826949, "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Vishal Mirza", "authorId": "2322445184"}, {"name": "Rahul Kulkarni", "authorId": "2322445481"}, {"name": "Aakanksha Jadhav", "authorId": "2322445728"}], "n_citations": 2}, "snippets": ["Recent advancements in Large Language Models(LLMs) have been notable, yet widespread enterprise adoption remains limited due to various constraints. This paper examines bias in LLMs-a crucial issue affecting their usability, reliability, and fairness. Researchers are developing strategies to mitigate bias, including debiasing layers, specialized reference datasets like Winogender and Winobias, and reinforcement learning with human feedback (RLHF). These techniques have been integrated into the latest LLMs."], "score": 0.98095703125}, {"id": "(Yan et al., 2024)", "paper": {"corpus_id": 267770177, "title": "Potential and Challenges of Model Editing for Social Debiasing", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jianhao Yan", "authorId": "134233854"}, {"name": "Futing Wang", "authorId": "2285582246"}, {"name": "Yafu Li", "authorId": "2110450452"}, {"name": "Yue Zhang", "authorId": "2249762135"}], "n_citations": 9}, "snippets": ["Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing."], "score": 0.97119140625}, {"id": "(Guo et al., 2022)", "paper": {"corpus_id": 248780440, "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yue Guo", "authorId": null}, {"name": "Yi Yang", "authorId": "46285693"}, {"name": "A. Abbasi", "authorId": "144849629"}], "n_citations": 167}, "snippets": ["Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task."], "score": 0.97119140625}, {"id": "(Yang et al., 2022)", "paper": {"corpus_id": 253446867, "title": "ADEPT: A DEbiasing PrompT Framework", "year": 2022, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Ke Yang", "authorId": "2277527247"}, {"name": "Charles Yu", "authorId": "2110963190"}, {"name": "Y. Fung", "authorId": "51135899"}, {"name": "Manling Li", "authorId": "2118482058"}, {"name": "Heng Ji", "authorId": "2113323573"}], "n_citations": 25}, "snippets": ["Several works have proven that finetuning is an applicable approach for debiasing contextualized word embeddings. Similarly, discrete prompts with semantic meanings have shown to be effective in debiasing tasks. With unfixed mathematical representation at the token level, continuous prompts usually surpass discrete ones at providing a pre-trained language model (PLM) with additional task-specific information. Despite this, relatively few efforts have been made to debias PLMs by prompt tuning with continuous prompts compared to its discrete counterpart. Furthermore, for most debiasing methods that alter a PLM's original parameters, a major problem is the need to not only decrease the bias in the PLM but also to ensure that the PLM does not lose its representation ability. Finetuning methods typically have a hard time maintaining this balance, as they tend to violently remove meanings of attribute words (like the words developing our concepts of \"male\" and \"female\" for gender), which also leads to an unstable and unpredictable training process. In this paper, we propose ADEPT, a method to debias PLMs using prompt tuning while maintaining the delicate balance between removing biases and ensuring representation ability. To achieve this, we propose a new training criterion inspired by manifold learning and equip it with an explicit debiasing term to optimize prompt tuning. In addition, we conduct several experiments with regard to the reliability, quality, and quantity of a previously proposed attribute training corpus in order to obtain a clearer prototype of a certain attribute, which indicates the attribute's position and relative distances to other words on the manifold. We evaluate ADEPT on several widely acknowledged debiasing benchmarks and downstream tasks, and find that it achieves competitive results while maintaining (and in some cases even improving) the PLM's representation ability. We further visualize words' correlation before and after debiasing a PLM, and give some possible explanations for the visible effects."], "score": 0.0}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 274965310, "title": "Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhenjie Xu", "authorId": "2336859072"}, {"name": "Wenqing Chen", "authorId": "2279760195"}, {"name": "Yi Tang", "authorId": "2336730514"}, {"name": "Xuanying Li", "authorId": "2336828833"}, {"name": "Cheng Hu", "authorId": "2336830483"}, {"name": "Zhixuan Chu", "authorId": "2303254578"}, {"name": "Kui Ren", "authorId": "2302800539"}, {"name": "Zibin Zheng", "authorId": "2316508020"}, {"name": "Zhichao Lu School of Software Engineering", "authorId": "2336738374"}, {"name": "Sun Yat-sen University", "authorId": "89574632"}, {"name": "S. O. Physics", "authorId": "89909107"}, {"name": "Astronomy", "authorId": "2063003285"}, {"name": "School of Materials Science", "authorId": "102700312"}, {"name": "Technology", "authorId": "2321564437"}, {"name": "Zhejiang University", "authorId": "103231213"}, {"name": "Department of Computer Science", "authorId": "2212824158"}, {"name": "City University of Hong Kong", "authorId": "120391927"}], "n_citations": 1}, "snippets": ["Natural language processing (NLP) has seen remarkable advancements with the development of large language models (LLMs). Despite these advancements, LLMs often produce socially biased outputs. Recent studies have mainly addressed this problem by prompting LLMs to behave ethically, but this approach results in unacceptable performance degradation."], "score": 0.97900390625}, {"id": "(Nadeem et al., 2020)", "paper": {"corpus_id": 215828184, "title": "StereoSet: Measuring stereotypical bias in pretrained language models", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Moin Nadeem", "authorId": "50411022"}, {"name": "Anna Bethke", "authorId": "78850252"}, {"name": "Siva Reddy", "authorId": "145732771"}], "n_citations": 1015}, "snippets": ["A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu."], "score": 0.0}, {"id": "(Nangia et al., 2020)", "paper": {"corpus_id": 222090785, "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Nikita Nangia", "authorId": "10666396"}, {"name": "Clara Vania", "authorId": "3054462"}, {"name": "Rasika Bhalerao", "authorId": "49550275"}, {"name": "Samuel R. Bowman", "authorId": "3644767"}], "n_citations": 685}, "snippets": ["Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress."], "score": 0.0}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 268379141, "title": "Prompting Fairness: Integrating Causality to Debias Large Language Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Jingling Li", "authorId": "2291078963"}, {"name": "Zeyu Tang", "authorId": "2125563094"}, {"name": "Xiaoyu Liu", "authorId": "2181841542"}, {"name": "P. Spirtes", "authorId": "143648560"}, {"name": "Kun Zhang", "authorId": "2268848576"}, {"name": "Liu Leqi", "authorId": "51435222"}, {"name": "Yang Liu", "authorId": "2268439518"}], "n_citations": 12}, "snippets": ["Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical."], "score": 0.98779296875}], "table": null}, {"title": "Challenges in Bias Mitigation", "tldr": "Despite significant progress in developing bias mitigation techniques for LLMs, researchers face substantial challenges including computational costs, environmental impact, and the persistence of biases through fine-tuning. Complete bias elimination remains elusive as improvements in one dimension can lead to new biases or performance degradation in other areas. (4 sources)", "text": "\nMitigating social bias in Large Language Models faces several significant challenges that complicate implementation of effective solutions. One of the most substantial barriers is the prohibitive cost associated with retraining large models. As Goncalves et al. note, \"biased pretrained models are hard to fix, as retraining is prohibitively expensive both financially and environmentally\" <Paper corpusId=\"266163873\" paperTitle=\"(Goncalves et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253397743\" paperTitle=\"(Hessenthaler et al., 2022)\" isShortName></Paper>. This creates a practical limitation for researchers and organizations without access to extensive computational resources.\n\nThe challenge of bias persistence across model development stages presents another significant obstacle. Research has shown that \"social biases in LLMs are an ongoing problem that is propagated from pretraining to finetuning\" <Paper corpusId=\"266163873\" paperTitle=\"(Goncalves et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258378241\" paperTitle=\"(Ladhak et al., 2023)\" isShortName></Paper> <Paper corpusId=\"248780268\" paperTitle=\"(Gira et al., 2022)\" isShortName></Paper>. This means that biases embedded during pre-training can persist even after fine-tuning for specific downstream tasks, making comprehensive bias mitigation particularly difficult.\n\nCurrent approaches to bias mitigation often face trade-offs between effectiveness and performance. Many existing methods \"either fail to remove the bias completely, degrade performance ('catastrophic forgetting'), or are costly to execute\" <Paper corpusId=\"266163873\" paperTitle=\"(Goncalves et al., 2023)\" isShortName></Paper> <Paper corpusId=\"248780268\" paperTitle=\"(Gira et al., 2022)\" isShortName></Paper>. This creates a challenging balance between reducing bias and maintaining model utility for intended applications.\n\nThe interplay between fairness and environmental sustainability presents another complex challenge. There is \"increasing evidence that an exclusive focus on fairness can actually hinder environmental sustainability, and vice versa\" <Paper corpusId=\"266163873\" paperTitle=\"(Goncalves et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253397743\" paperTitle=\"(Hessenthaler et al., 2022)\" isShortName></Paper>. This tension between different ethical considerations complicates the development of holistic approaches to bias mitigation.\n\nAdditionally, biases in models can manifest in unexpected ways across different applications. For example, in summarization tasks, name-nationality biases from pre-training can lead to factual hallucinations in generated summaries, with \"more abstractive models allow biases to propagate more directly to downstream tasks as hallucinated facts\" <Paper corpusId=\"266163873\" paperTitle=\"(Goncalves et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258378241\" paperTitle=\"(Ladhak et al., 2023)\" isShortName></Paper>. This highlights the challenge of comprehensively addressing biases across diverse applications and model architectures.\n\nEven well-intentioned mitigation approaches can have unexpected consequences. For instance, knowledge distillation\u2014a technique often used to reduce the environmental impact of large models\u2014can \"actually decrease model fairness\" in some contexts <Paper corpusId=\"266163873\" paperTitle=\"(Goncalves et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253397743\" paperTitle=\"(Hessenthaler et al., 2022)\" isShortName></Paper>. This demonstrates how solutions targeting one aspect of AI ethics might inadvertently exacerbate problems in another dimension.\n\nThese challenges underscore the need for more holistic, efficient, and accessible approaches to bias mitigation that can address the multifaceted nature of social biases in large language models while considering broader ethical and practical constraints.", "citations": [{"id": "(Goncalves et al., 2023)", "paper": {"corpus_id": 266163873, "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Gustavo Gon\u00e7alves", "authorId": "2273536347"}, {"name": "Emma Strubell", "authorId": "2268272"}], "n_citations": 11}, "snippets": ["Social biases in LLMs are an ongoing problem that is propagated from pretraining to finetuning (Ladhak et al., 2023)(Gira et al., 2022). Biased pretrained models are hard to fix, as retraining is prohibitively expensive both financially and environmentally (Hessenthaler et al., 2022)."], "score": 0.97265625}, {"id": "(Hessenthaler et al., 2022)", "paper": {"corpus_id": 253397743, "title": "Bridging Fairness and Environmental Sustainability in Natural Language Processing", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Marius Hessenthaler", "authorId": "2190173964"}, {"name": "Emma Strubell", "authorId": "2268272"}, {"name": "Dirk Hovy", "authorId": "2022288"}, {"name": "Anne Lauscher", "authorId": "29891652"}], "n_citations": 8}, "snippets": ["Fairness and environmental impact are important research directions for the sustainable development of artificial intelligence. However, while each topic is an active research area in natural language processing (NLP), there is a surprising lack of research on the interplay between the two fields. This lacuna is highly problematic, since there is increasing evidence that an exclusive focus on fairness can actually hinder environmental sustainability, and vice versa. In this work, we shed light on this crucial intersection in NLP by (1) investigating the efficiency of current fairness approaches through surveying example methods for reducing unfair stereotypical bias from the literature, and (2) evaluating a common technique to reduce energy consumption (and thus environmental impact) of English NLP models, knowledge distillation (KD), for its impact on fairness. In this case study, we evaluate the effect of important KD factors, including layer and dimensionality reduction, with respect to: (a) performance on the distillation task (natural language inference and semantic similarity prediction), and (b) multiple measures and dimensions of stereotypical bias (e.g., gender bias measured via the Word Embedding Association Test). Our results lead us to clarify current assumptions regarding the effect of KD on unfair bias: contrary to other findings, we show that KD can actually decrease model fairness."], "score": 0.0}, {"id": "(Ladhak et al., 2023)", "paper": {"corpus_id": 258378241, "title": "When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization", "year": 2023, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Faisal Ladhak", "authorId": "8759332"}, {"name": "Esin Durmus", "authorId": "41152329"}, {"name": "Mirac Suzgun", "authorId": "51903517"}, {"name": "Tianyi Zhang", "authorId": "2146332311"}, {"name": "Dan Jurafsky", "authorId": "1746807"}, {"name": "K. McKeown", "authorId": "145590324"}, {"name": "Tatsunori Hashimoto", "authorId": "2117567142"}], "n_citations": 58}, "snippets": ["Large language models (LLMs) are subject to sociocultural and other biases previously identified using intrinsic evaluations. However, when and how these intrinsic biases in pre-trained LM representations propagate to downstream, fine-tuned NLP tasks like summarization is not well understood. In this work, we investigate one type of bias\u2014name-nationality bias\u2014and trace it from the pre-training stage to a downstream summarization task across multiple summarization modeling choices. We show that these biases manifest themselves as hallucinations in summarization, leading to factually incorrect summaries. We also find that this propagation of biases is algorithm-dependent: more abstractive models allow biases to propagate more directly to downstream tasks as hallucinated facts. Building on these observations, we further analyze how changes to the adaptation method and fine-tuning data set affect name nationality biases and show that while they can reduce the overall rate of hallucinations, they do not change the types of biases that do appear."], "score": 0.0}, {"id": "(Gira et al., 2022)", "paper": {"corpus_id": 248780268, "title": "Debiasing Pre-Trained Language Models via Efficient Fine-Tuning", "year": 2022, "venue": "LTEDI", "authors": [{"name": "Michael Gira", "authorId": "2165226185"}, {"name": "Ruisu Zhang", "authorId": "2165321339"}, {"name": "Kangwook Lee", "authorId": "2115495251"}], "n_citations": 44}, "snippets": ["An explosion in the popularity of transformer-based language models (such as GPT-3, BERT, RoBERTa, and ALBERT) has opened the doors to new machine learning applications involving language modeling, text generation, and more. However, recent scrutiny reveals that these language models contain inherent biases towards certain demographics reflected in their training data. While research has tried mitigating this problem, existing approaches either fail to remove the bias completely, degrade performance (\"catastrophic forgetting\"), or are costly to execute. This work examines how to reduce gender bias in a GPT-2 language model by fine-tuning less than 1% of its parameters. Through quantitative benchmarks, we show that this is a viable way to reduce prejudice in pre-trained language models while remaining cost-effective at scale."], "score": 0.0}], "table": null}], "cost": 0.397941}}
