{"original_query": "How to evaluate copyright infringement of image generation models? ", "input": "copyright infringement of image generation models", "output": {"sections": [{"title": "Introduction to Copyright Concerns in Generative AI", "tldr": "Generative AI models for images raise significant copyright concerns as they can potentially reproduce content similar to copyrighted works in their training data. These issues arise both during model training and when generating outputs, creating legal challenges for creators, developers, and users. (8 sources)", "text": "\nGenerative AI models, particularly image generation systems like diffusion models, have demonstrated remarkable capabilities in creating high-quality, realistic images from textual descriptions. However, these advancements have introduced significant copyright challenges that affect multiple stakeholders in the creative ecosystem <Paper corpusId=\"260155285\" paperTitle=\"(Leotta et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268532352\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. The fundamental problem stems from these models being trained on massive datasets that frequently contain copyrighted works, making it practically impossible to ensure training data is completely free of protected material <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>.\n\nCopyright infringement concerns in generative AI emerge through two primary mechanisms. First, during the training phase, algorithms directly access copyrighted material, potentially encoding verbatim copies within the model's weights. Second, during deployment, when users prompt the model to generate content, the outputs may be substantially similar to copyrighted training data without any clear provenance tracking <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. This is particularly problematic as models like Stable Diffusion and Midjourney can generate images closely resembling protected content, raising questions about whether these similarities constitute copyright violations <Paper corpusId=\"272146279\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper>.\n\nAs the boundary between human creativity and machine-generated content becomes increasingly blurred, determining copyright ownership and liability has become a complex challenge <Paper corpusId=\"260155285\" paperTitle=\"(Leotta et al., 2023)\" isShortName></Paper>. While existing copyright laws grant creators exclusive rights to reproduce, distribute, and monetize their creative works, these frameworks may be insufficient to address the unique characteristics of AI-generated content <Paper corpusId=\"268532352\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. The issue is further complicated by the impracticality of using only non-copyrighted content for training these models, as developers seek to leverage high-quality data to improve generation capabilities <Paper corpusId=\"273654195\" paperTitle=\"(Shi et al._1, 2024)\" isShortName></Paper>.\n\nDespite attempts to develop methods that mitigate copyright infringement risks, significant challenges remain <Paper corpusId=\"270258236\" paperTitle=\"(Chiba-Okabe et al., 2024)\" isShortName></Paper>. Recent legal cases involving image generation platforms highlight growing concerns about whether the high-quality content synthesized by these models might be excessively similar to copyrighted training data, potentially violating the rights of numerous artists and copyright holders <Paper corpusId=\"272146279\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258297834\" paperTitle=\"(Dogoulis et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Leotta et al., 2023)", "paper": {"corpus_id": 260155285, "title": "Not with my name! Inferring artists' names of input strings employed by Diffusion Models", "year": 2023, "venue": "International Conference on Image Analysis and Processing", "authors": [{"name": "R. Leotta", "authorId": "47253043"}, {"name": "Oliver Giudice", "authorId": "1797543"}, {"name": "Luca Guarnera", "authorId": "40010524"}, {"name": "S. Battiato", "authorId": "1742452"}], "n_citations": 7}, "snippets": ["The rapid advancement of generative models, particularly Diffusion Models [7], has led to a surge in high-quality, realistic image generation. These models have demonstrated immense potential for creative applications across various domains, including art, design, and advertising. However, their ability to replicate the styles of specific artists raises concerns about Intellectual Property (IP) rights and potential copyright infringements [20,25,19]. \n\nAs the boundary between human creativity and machine-generated content becomes increasingly blurred, it is crucial to address the legal and ethical implications of using generative models to produce art, as well as to develop methods that evaluate the extent to which generated images are influenced by the works of real artists and ensure the protection of their intellectual property. \n\nSeveral studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights. In an article dated 2021 the challenges in determining copyright ownership for AI-generated works were discussed and authors emphasized the need for legal frameworks that could adequately address the unique nature of generative models. MLQ.AI also reported on a copyright infringement case involving generative AI, which sparked debates on the responsibilities of AI developers and users in protecting original creators' rights. In response to these concerns, legal scholars have delved into the complexities of copyright law as it pertains to AI-generated artworks. Gillotte [8] examined the challenges in assigning liability and protecting IP rights, arguing that existing copyright laws may not be sufficient to address the unique characteristics of AI-generated contents. Indeed, it is became a copyright dilemma and the need for a balance between innovation and IP protection is arising to ensure that creative works are safeguarded without stifling technological advancements [16]."], "score": 0.982421875}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 268532352, "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models", "year": 2024, "venue": "", "authors": [{"name": "Rui Ma", "authorId": "2210435658"}, {"name": "Qiang Zhou", "authorId": "2257338567"}, {"name": "Yizhu Jin", "authorId": "2268733263"}, {"name": "Daquan Zhou", "authorId": "2292161412"}, {"name": "Bangjun Xiao", "authorId": "2292177829"}, {"name": "Xiuyu Li", "authorId": "2292217065"}, {"name": "Yi Qu", "authorId": "2292690089"}, {"name": "Aishani Singh", "authorId": "2261448160"}, {"name": "Kurt Keutzer", "authorId": "2242659602"}, {"name": "Jingtong Hu", "authorId": "2328473437"}, {"name": "Xiaodong Xie", "authorId": "2307915260"}, {"name": "Zhen Dong", "authorId": "2293731776"}, {"name": "Shanghang Zhang", "authorId": "2257020214"}, {"name": "Shiji Zhou", "authorId": "2275298300"}], "n_citations": 2}, "snippets": ["Copyright law confers upon creators the exclusive rights to reproduce, distribute, and monetize their creative works. However, recent progress in text-to-image generation has introduced formidable challenges to copyright enforcement. These technologies enable the unauthorized learning and replication of copyrighted content, artistic creations, and likenesses, leading to the proliferation of unregulated content. Notably, models like stable diffusion, which excel in text-to-image synthesis, heighten the risk of copyright infringement and unauthorized distribution", ".Text-to-image generative models have recently emerged as a significant topic in computer vision, demonstrating remarkable results in the area of generative modeling (Goodfellow et al., 2021)(Rombach et al., 2021).These models bridge the gap between language and visual contents by generating realistic images from textual descriptions.However, rapid advancements in text-to-image generation techniques have raised concerns about copyright protection, particularly unauthorized reproduction of content, artistic creations, and portraits [4].A specific concern arises from the use of Stable Diffusion (SD), a state-of-the-art text-conditional latent diffusion model, which has sparked global discussions on copyright."], "score": 0.99072265625}, {"id": "(Vyas et al., 2023)", "paper": {"corpus_id": 257050406, "title": "On Provable Copyright Protection for Generative Models", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Nikhil Vyas", "authorId": "145603901"}, {"name": "S. Kakade", "authorId": "144695232"}, {"name": "B. Barak", "authorId": "1697211"}], "n_citations": 94}, "snippets": ["Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models.\n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material."], "score": 0.9892578125}, {"id": "(Shi et al., 2024)", "paper": {"corpus_id": 272146279, "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhuan Shi", "authorId": "2319419519"}, {"name": "Jing Yan", "authorId": "2318391138"}, {"name": "Xiaoli Tang", "authorId": "2318236128"}, {"name": "Lingjuan Lyu", "authorId": "2287820224"}, {"name": "Boi Faltings", "authorId": "2054858128"}], "n_citations": 1}, "snippets": ["These models utilize extensive training data that may include copyrighted works, which they are sometimes capable of memorizing (Carlini et al. 2023). This ability can result in the pro- duction of images that closely resemble protected content (See in Figure 1), posing significant challenges to copyright protection (Dogoulis et al., 2023). Recent legal cases, such as those involving Stable Diffusion (Rombach et al., 2021) and Midjourney(Mansour 2023), highlight concerns over the use of copyrighted data in AI training, where the models potentially infringe on the rights of numerous artists. These cases highlight a growing concern: Could the high-quality content synthesized by these generative AIs be excessively similar to copyrighted training data, potentially violating the rights of copyright holders?"], "score": 0.98779296875}, {"id": "(Rombach et al., 2021)", "paper": {"corpus_id": 245335280, "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Robin Rombach", "authorId": "1660819540"}, {"name": "A. Blattmann", "authorId": "119843260"}, {"name": "Dominik Lorenz", "authorId": "2053482699"}, {"name": "Patrick Esser", "authorId": "35175531"}, {"name": "B. Ommer", "authorId": "1796707"}], "n_citations": 15768}, "snippets": ["By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."], "score": 0.0}, {"id": "(Shi et al._1, 2024)", "paper": {"corpus_id": 273654195, "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhuan Shi", "authorId": "2319419519"}, {"name": "Yifei Song", "authorId": "2328664079"}, {"name": "Xiaoli Tang", "authorId": "2318236128"}, {"name": "Lingjuan Lyu", "authorId": "2287820224"}, {"name": "Boi Faltings", "authorId": "2054858128"}], "n_citations": 1}, "snippets": ["The widespread deployment of such generative art models has brought about significant challenges concerning the risk for copyright infringement. The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28]."], "score": 0.99267578125}, {"id": "(Chiba-Okabe et al., 2024)", "paper": {"corpus_id": 270258236, "title": "Tackling copyright issues in AI image generation through originality estimation and genericization", "year": 2024, "venue": "Scientific Reports", "authors": [{"name": "Hiroaki Chiba-Okabe", "authorId": "2297800322"}, {"name": "Weijie J. Su", "authorId": "2278306561"}], "n_citations": 1}, "snippets": ["Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners. Although some methods have been found effective to some extent, significant risks of copyright infringement remains."], "score": 0.98583984375}, {"id": "(Dogoulis et al., 2023)", "paper": {"corpus_id": 258297834, "title": "Improving Synthetically Generated Image Detection in Cross-Concept Settings", "year": 2023, "venue": "MAD@ICMR", "authors": [{"name": "Pantelis Dogoulis", "authorId": "97760096"}, {"name": "Giorgos Kordopatis-Zilos", "authorId": "1403953272"}, {"name": "I. Kompatsiaris", "authorId": "119661806"}, {"name": "S. Papadopoulos", "authorId": "144178604"}], "n_citations": 19}, "snippets": ["New advancements for the detection of synthetic images are critical for fighting disinformation, as the capabilities of generative AI models continuously evolve and can lead to hyper-realistic synthetic imagery at unprecedented scale and speed. In this paper, we focus on the challenge of generalizing across different concept classes, e.g., when training a detector on human faces and testing on synthetic animal images \u2013 highlighting the ineffectiveness of existing approaches that randomly sample generated images to train their models. By contrast, we propose an approach based on the premise that the robustness of the detector can be enhanced by training it on realistic synthetic images that are selected based on their quality scores according to a probabilistic quality estimation model. We demonstrate the effectiveness of the proposed approach by conducting experiments with generated images from two seminal architectures, StyleGAN2 and Latent Diffusion, and using three different concepts for each, so as to measure the cross-concept generalization ability. Our results show that our quality-based sampling method leads to higher detection performance for nearly all concepts, improving the overall effectiveness of the synthetic image detectors."], "score": 0.0}], "table": null}, {"title": "How Copyright Infringement Occurs in Generative Models", "tldr": "Copyright infringement in generative AI models occurs through two main mechanisms: memorization of training data leading to reproduction of copyrighted content, and the generation of outputs substantially similar to copyrighted works without proper attribution or permission. (11 sources)", "text": "\nGenerative models like Stable Diffusion and Midjourney can potentially infringe on copyright through several distinct mechanisms. The first and most direct mechanism is memorization, where models encode and reproduce verbatim copies of copyrighted material from their training data <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper> <Paper corpusId=\"254366634\" paperTitle=\"(Somepalli et al., 2022)\" isShortName></Paper>. This goes beyond simple inspiration, as research has shown that diffusion models are capable of completely replicating their training data samples, producing outputs that are nearly identical to copyrighted works <Paper corpusId=\"267412857\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>.\n\nThe second mechanism involves more subtle forms of infringement, where models create outputs that are \"substantially similar\" to copyrighted works even without verbatim copying <Paper corpusId=\"276558342\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. This is particularly concerning because U.S. copyright law, which influences laws in many countries, considers substantial similarity as grounds for infringement <Paper corpusId=\"276558342\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. Unlike human artists who might be inspired by existing works but create distinctly new content, generative models can inadvertently reproduce the characteristic elements of an artist's style across their body of work <Paper corpusId=\"269137659\" paperTitle=\"(Moayeri et al., 2024)\" isShortName></Paper>.\n\nCopyright infringement can occur at two distinct phases of the generative AI pipeline. During the training phase, algorithms directly access copyrighted material, potentially encoding verbatim copies within the model's weights <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. Then, during the deployment phase, when users prompt the model to generate content, the outputs may be substantially similar to copyrighted training data without any clear provenance tracking, making it difficult for users to verify if outputs infringe on existing copyrights <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>.\n\nWhat makes this problem particularly concerning is that infringement can occur unintentionally, without either the user or developer specifically aiming to replicate copyrighted material <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. Additionally, the uninterpretable nature of generative models makes it challenging to provide direct evidence of copying, with access and similarity serving as primary evidence in potential legal disputes <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>.\n\nRecent research has uncovered even more nuanced forms of potential copyright infringement. For instance, researchers have demonstrated that infringement can occur through disguised training samples that look drastically different from copyrighted content yet still induce the same effect when used to train latent diffusion models <Paper corpusId=\"269033217\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. Such concealed methods of infringement are particularly problematic as they can easily circumvent current auditing tools <Paper corpusId=\"269033217\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>.\n\nThe risk of copyright infringement is amplified by the massive scale of training datasets like LAION, which contain billions of web-scale images that are impractical to thoroughly review or filter manually <Paper corpusId=\"278129333\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>. This has led to real-world legal consequences, with several artists filing lawsuits against companies like Stability AI, DeviantArt, Midjourney, and Runway AI, arguing that their models function as \"21st-century collage tools\" that remix copyrighted works without permission <Paper corpusId=\"278129333\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>.\n\nThe problem extends beyond just image reproduction to include fine-tuning methods such as DreamBooth, which can efficiently enable unauthorized parties to directly edit or modify source data to create new samples, potentially infringing on the copyright of original works <Paper corpusId=\"267412857\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>. These developments underscore the urgent need for responsible AI image generation that addresses copyright concerns <Paper corpusId=\"265351912\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper> and highlights the limitations of traditional copyright protection measures like watermarks and metadata in this new technological landscape <Paper corpusId=\"277955485\" paperTitle=\"(Rakib et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Carlini et al., 2023)", "paper": {"corpus_id": 256389993, "title": "Extracting Training Data from Diffusion Models", "year": 2023, "venue": "USENIX Security Symposium", "authors": [{"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Jamie Hayes", "authorId": "9200194"}, {"name": "Milad Nasr", "authorId": "3490923"}, {"name": "Matthew Jagielski", "authorId": "40844378"}, {"name": "Vikash Sehwag", "authorId": "3482535"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}, {"name": "Borja Balle", "authorId": "1718064"}, {"name": "Daphne Ippolito", "authorId": "7975935"}, {"name": "Eric Wallace", "authorId": "145217343"}], "n_citations": 617}, "snippets": ["Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."], "score": 0.0}, {"id": "(Somepalli et al., 2022)", "paper": {"corpus_id": 254366634, "title": "Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Gowthami Somepalli", "authorId": "2003112028"}, {"name": "Vasu Singla", "authorId": "1824188732"}, {"name": "Micah Goldblum", "authorId": "121592562"}, {"name": "Jonas Geiping", "authorId": "8284185"}, {"name": "T. Goldstein", "authorId": "1962083"}], "n_citations": 329}, "snippets": ["Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html"], "score": 0.0}, {"id": "(Ren et al., 2024)", "paper": {"corpus_id": 267412857, "title": "Copyright Protection in Generative AI: A Technical Perspective", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jie Ren", "authorId": "2256589810"}, {"name": "Han Xu", "authorId": "2253881697"}, {"name": "Pengfei He", "authorId": "2185740224"}, {"name": "Yingqian Cui", "authorId": "2218740984"}, {"name": "Shenglai Zeng", "authorId": "2253682835"}, {"name": "Jiankun Zhang", "authorId": "2282560420"}, {"name": "Hongzhi Wen", "authorId": "2256788829"}, {"name": "Jiayuan Ding", "authorId": "46496977"}, {"name": "Hui Liu", "authorId": "2253533415"}, {"name": "Yi Chang", "authorId": "2267019992"}, {"name": "Jiliang Tang", "authorId": "2115879611"}], "n_citations": 42}, "snippets": ["For the source data owner, which refers to the party or individual who owns the originality of image works, their data can be intentionally or unintentionally collected by model trainers as training samples to construct DGMs as introduced above. For example, recent studies [16]134] have demonstrated that popular DGMs are highly possible to completely replicate their training data samples, which is called memorization. \n\nThe possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works."], "score": 0.99169921875}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 276558342, "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shunchang Liu", "authorId": "2346891526"}, {"name": "Zhuan Shi", "authorId": "2319419519"}, {"name": "Lingjuan Lyu", "authorId": "2287820224"}, {"name": "Yaochu Jin", "authorId": "2344619001"}, {"name": "Boi Faltings", "authorId": "2054858128"}], "n_citations": 2}, "snippets": ["Text-to-image generative models (Rombach et al., 2021)(Betker et al., 0)Team et al., 2023;(Esser et al., 2024)Hurst et al., 2024;(Zhang et al., 2023)a;Hintersdorf et al., 2024) have transformed creative industries by producing detailed visuals from text prompts. However, these models have been found to sometimes memorize and reproduce content from their training data (Carlini et al., 2023)(Somepalli et al., 2022)Ren et al., 2024;Wang et al., 2024c;Shi et al., 2024b;a;Zhang et al., 2024). This raises significant concerns about copyright infringement, especially when the generated images closely resemble existing copyrighted works. According to U.S. law (rot, 1970), also referenced by most countries, a work can be considered infringing if it constitutes substantial similarity to another work1 . Therefore, determining whether AI-generated images infringe on copyright requires a clear and reliable method to compare them with copyrighted materials to identify substantial similarity."], "score": 0.99169921875}, {"id": "(Moayeri et al., 2024)", "paper": {"corpus_id": 269137659, "title": "Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mazda Moayeri", "authorId": "104644443"}, {"name": "Samyadeep Basu", "authorId": "2114710333"}, {"name": "S. Balasubramanian", "authorId": "144021807"}, {"name": "Priyatham Kattakinda", "authorId": "1962835975"}, {"name": "Atoosa Malemir Chegini", "authorId": "2188780004"}, {"name": "R. Brauneis", "authorId": "69357142"}, {"name": "S. Feizi", "authorId": "34389431"}], "n_citations": 4}, "snippets": ["Recent text-to-image generative models such as Stable Diffusion are extremely adept at mimicking and generating copyrighted content, raising concerns amongst artists that their unique styles may be improperly copied. Understanding how generative models copy\"artistic style\"is more complex than duplicating a single image, as style is comprised by a set of elements (or signature) that frequently co-occurs across a body of work, where each individual work may vary significantly."], "score": 0.99267578125}, {"id": "(Vyas et al., 2023)", "paper": {"corpus_id": 257050406, "title": "On Provable Copyright Protection for Generative Models", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Nikhil Vyas", "authorId": "145603901"}, {"name": "S. Kakade", "authorId": "144695232"}, {"name": "B. Barak", "authorId": "1697211"}], "n_citations": 94}, "snippets": ["Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models.\n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material."], "score": 0.9892578125}, {"id": "(Chiba-Okabe, 2024)", "paper": {"corpus_id": 273023255, "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hiroaki Chiba-Okabe", "authorId": "2297800322"}], "n_citations": 2}, "snippets": ["In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content (Carlini et al., 2020)(Carlini et al., 2023)[39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works."], "score": 0.98876953125}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 269033217, "title": "Disguised Copyright Infringement of Latent Diffusion Models", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Yiwei Lu", "authorId": "2275053301"}, {"name": "Matthew Y.R. Yang", "authorId": "2284800079"}, {"name": "Zuoqiu Liu", "authorId": "2295948127"}, {"name": "Gautam Kamath", "authorId": "2284763541"}, {"name": "Yaoliang Yu", "authorId": "2274963165"}], "n_citations": 8}, "snippets": ["Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools."], "score": 0.9833984375}, {"id": "(Chen et al., 2025)", "paper": {"corpus_id": 278129333, "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Chen Chen", "authorId": "1696291"}, {"name": "Daochang Liu", "authorId": "51023221"}, {"name": "Mubarak Shah", "authorId": "2302950741"}, {"name": "Chang Xu", "authorId": "2288626806"}], "n_citations": 1}, "snippets": ["Leveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion (Rombach et al., 2021) and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research (Carlini et al., 2023)8,(Somepalli et al., 2022)[34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as (Wen et al., 2023) demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images."], "score": 0.99169921875}, {"id": "(Zhou et al., 2023)", "paper": {"corpus_id": 265351912, "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Junlei Zhou", "authorId": "2267879159"}, {"name": "Jiashi Gao", "authorId": "2149258131"}, {"name": "Ziwei Wang", "authorId": "2184222659"}, {"name": "Xuetao Wei", "authorId": "2255554914"}], "n_citations": 2}, "snippets": ["However, these AI-generated artworks inherit the characteristics of the images that are used to train models [33,40,43], which might be pretty similar to the original ones, as shown in Figure 1. Such similarity has aroused concerns about copyright infringement disputes. For example, three artists (Sarah Andersen, Kelly McKernan, and Karla Ortiz) have recently accused Stable Diffusion of unlawfully scraping copyrighted images from the Internet to mimic their art styles [11]. To this end, research on responsible AI image generation is in urgent need to address such copyright issues."], "score": 0.998046875}, {"id": "(Rakib et al., 2025)", "paper": {"corpus_id": 277955485, "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mazharul Islam Rakib", "authorId": "2313107575"}, {"name": "Showrin Rahman", "authorId": "2357491645"}, {"name": "J. Mondal", "authorId": "2113939573"}, {"name": "Xi Xiao", "authorId": "2349232014"}, {"name": "David Lewis", "authorId": "2357414290"}, {"name": "Alessandra Mileo", "authorId": "2356548661"}, {"name": "Meem Arafat Manab", "authorId": "2210516789"}], "n_citations": 1}, "snippets": ["In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard", ".Diffusion models have revolutionized image generation, surpassing predecessors like GANs and VAEs in both fidelity and performance. However, this progress has been shadowed by a critical challenge: source copying. This issue raises serious concerns regarding privacy and intellectual property rights, demanding innovative solutions that preserve content quality while enhancing copyright protection."], "score": 0.994140625}], "table": null}, {"title": "Legal Cases and Lawsuits", "tldr": "Several high-profile lawsuits have been filed against generative AI companies for copyright infringement, with artists, photographers, and stock image providers seeking legal remedies for unauthorized use of their works in model training. These cases highlight the growing legal tensions between copyright holders and AI developers, with some courts already allowing claims to proceed. (8 sources)", "text": "\n## Notable Copyright Infringement Lawsuits\n\n* **Getty Images v. Stability AI**: One of the most prominent legal challenges came from Getty Images, which sued Stability AI (creators of Stable Diffusion) for allegedly reproducing over 12 million Getty Images along with their captions and metadata without permission or compensation <Paper corpusId=\"267400526\" paperTitle=\"(Zhuang, 2024)\" isShortName></Paper>. This case represents a significant commercial entity taking action against AI developers <Paper corpusId=\"266900037\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n* **Artists' Collective Lawsuits**: Multiple artists have filed collective lawsuits against Stability AI, DeviantArt, and Midjourney, claiming these companies trained their models on artists' works without consent, allowing the AI to replicate their distinctive styles and content <Paper corpusId=\"265352103\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. In one notable case, painter Erin Hanson provided substantial evidence that AI-generated images plagiarized his work <Paper corpusId=\"267400526\" paperTitle=\"(Zhuang, 2024)\" isShortName></Paper>.\n\n* **Legal Progress**: In August 2024, U.S. District Judge Orrick ruled that artists could proceed with copyright infringement claims against Stability AI, Midjourney, and DeviantArt, signaling that courts are taking these concerns seriously <Paper corpusId=\"278338968\" paperTitle=\"(Reissinger et al., 2025)\" isShortName></Paper>. This ruling marked a significant development in establishing legal precedent for AI copyright cases.\n\n* **Character Copyright Cases**: Studies have shown that image generation models can be prompted to reproduce copyrighted characters (such as Mario or Batman), which has already resulted in at least one lawsuit awarding damages based on the generation of such characters <Paper corpusId=\"270620122\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>. This has prompted commercial services like DALL-E to implement protective interventions.\n\n* **Emerging Legal Framework**: These lawsuits highlight growing concerns about whether the high-quality content synthesized by generative AI models is excessively similar to copyrighted training data, potentially violating the rights of numerous artists and copyright holders <Paper corpusId=\"272146279\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper>. The cases are forcing courts to address the unique legal challenges posed by AI models that can memorize and reproduce aspects of their training data.\n\n* **Broader Implications**: These legal challenges are occurring as the boundary between human creativity and machine-generated content becomes increasingly blurred, raising crucial questions about legal and ethical implications of using generative models to produce art <Paper corpusId=\"260155285\" paperTitle=\"(Leotta et al., 2023)\" isShortName></Paper>. The outcomes of these cases will likely influence how copyright law adapts to AI-generated content.", "citations": [{"id": "(Zhuang, 2024)", "paper": {"corpus_id": 267400526, "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists", "year": 2024, "venue": "Applied and Computational Engineering", "authors": [{"name": "Lyulin Zhuang", "authorId": "2282411198"}], "n_citations": 2}, "snippets": ["Stable Diffusion can quickly generate high-quality images based on simple text descriptions, thanks to its training on billions of sample images collected online. However, not all of these sample images fall within the public domain; some are even protected by copyright. As one can imagine, the authors and photographers of these copyrighted images are not pleased with the actions of Stable Diffusion.\n\nStable Diffusion has faced multiple lawsuits due to copyright issues. Artists and photographers have initiated collective lawsuits against this technology, and one of the world's leading image suppliers, Getty Images Holdings Inc., has also filed a similar case. According to Getty's (2023) complaint, \"Stability AI has reproduced over 12 million Getty Images, along with their related captions and metadata, without permission or compensation.\"", "After exploring how artificial intelligence (AI)-generated platforms work, learn what processes may involve human artist infringement in the course of their work. So let's talk about where these workflows infringe on human artists' copyrighted works -Artificial intelligence (AI)-generated content raises concerns about copyright infringement of human artist works in several ways.\n\nThe researchers summarized three aspects of human artist infringement: model training using only copyrighted works without permission. Take Stable Diffusion, which has been involved in many copyright disputes. It uses the image database containing hundreds of millions-LAION-5B (Kaixin Zhu, & Yiqun Zhang. ( 2023)) as the source of training data, and it does not need the consent of the copyright owner [5].\n\nSecond, there is a lot of plagiarism and adaptation of copyrighted works in AI-generated images. In a case in which an artist sued an AI-generated platform, the painter Erin Hanson provided strong evidence that the AI-generated images plagiarized his work. And there are many such lawsuits now, not a few. Third, the images generated by the AI generation platform, such as Stable Diffusion, if works infringe copyright works, do not have any watermarks or source marks, making it even more difficult for copyright owners to accept."], "score": 0.9912109375}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 266900037, "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Haonan Wang", "authorId": "2267866973"}, {"name": "Qianli Shen", "authorId": "2257038423"}, {"name": "Yao Tong", "authorId": "2278794984"}, {"name": "Yang Zhang", "authorId": "2267877984"}, {"name": "Kenji Kawaguchi", "authorId": "2256995496"}], "n_citations": 32}, "snippets": ["In a copyright infringement attack, the attacker, who is the copyright owner of some creations (such as images, poems, etc.), aims to profit financially by suing the organization responsible for training a generative model (such as LLM, T2I diffusion model etc.) for copyright infringement. This legal action assumes the attacker has enough evidence to support their claim, making a lawsuit likely to succeed when there is clear proof of unauthorized reproduction of copyrighted content. A real-world example illustrating this scenario is the lawsuit filed by Getty Images against the AI art generator Stable Diffusion in the United States for copyright infringement (Vincent, 2023)."], "score": 0.98828125}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 265352103, "title": "On Copyright Risks of Text-to-Image Diffusion Models", "year": 2023, "venue": "", "authors": [{"name": "Yang Zhang", "authorId": "2267877984"}, {"name": "Teoh Tze Tzun", "authorId": "2267728071"}, {"name": "Lim Wei Hern", "authorId": "2267727392"}, {"name": "Haonan Wang", "authorId": "2267866973"}, {"name": "Kenji Kawaguchi", "authorId": "2256995496"}], "n_citations": 10}, "snippets": ["Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts.\n\nOur research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues.\n\nThe apprehension surrounding copyright protection in diffusion models has also evolved into a tangible threat, as multiple lawsuits related to copyright infringement have been initiated against companies that utilize diffusion models for commercial purposes. Notably, there have been instances of lawsuits: Stability AI and Mid-Journey are both facing civil suits for training their models on artists' work without their consent thereby allowing their models to replicate the style and work of such artists [Vincent, 2023].\n\nFor copyright infringement, we focus on the copyright regulation in the US [Copyright Office, 2022]. Nevertheless, the concept of copyright applies to the regulation in other countries. In the context of copyright law in the US, whether a piece of copyrighted material can be used by others is governed by the concept of Fair Use which permits the use of copyrighted material only in a transformative manner that is distinct from the original work. There have been legal precedents which demonstrate that structural similarity can lead to infringement claims. Given that such generative models are trained on datasets such as LAION-5B [Schuhmann et al., 2022] which contains publicly available copyrighted data, if the models generate images with visual features that have substantial structural similarity to the original copyrighted images, this would likely be grounds for claims of copyright infringement."], "score": 0.9970703125}, {"id": "(Reissinger et al., 2025)", "paper": {"corpus_id": 278338968, "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Lena Reissinger", "authorId": "2359254079"}, {"name": "Yuanyuan Li", "authorId": "2256011059"}, {"name": "Anna Haensch", "authorId": "23107750"}, {"name": "Neeraj Sarna", "authorId": "2254269177"}], "n_citations": 0}, "snippets": ["As generative AI (GenAI) becomes increasingly prevalent in realworld applications, concerns about its potential risks continue to grow. We focus on the risks associated with the replication of copyrighted material and restrict ourselves to image generation. The tangible legal and financial implications of these risks have sparked our current research. For instance, artists sued Stability AI (Midjourney and DevianArt) claiming that the companies' AI image generators produce images that are strikingly similar to their artworks [2]. In August 2024, a U.S. district judge (Orrick) ruled that the artists could proceed with the copyright infringement claims, underscoring the ongoing legal uncertainties surrounding AI-generated content [6]."], "score": 0.99267578125}, {"id": "(He et al., 2024)", "paper": {"corpus_id": 270620122, "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Luxi He", "authorId": "2294507804"}, {"name": "Yangsibo Huang", "authorId": "2283305597"}, {"name": "Weijia Shi", "authorId": "2304129935"}, {"name": "Tinghao Xie", "authorId": "2144071564"}, {"name": "Haotian Liu", "authorId": "2308072184"}, {"name": "Yue Wang", "authorId": "2307621989"}, {"name": "Luke S. Zettlemoyer", "authorId": "2137813791"}, {"name": "Chiyuan Zhang", "authorId": "2309481623"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Peter Henderson", "authorId": "2254262712"}], "n_citations": 12}, "snippets": ["Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL-E have started deploying interventions."], "score": 0.99267578125}, {"id": "(Shi et al., 2024)", "paper": {"corpus_id": 272146279, "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhuan Shi", "authorId": "2319419519"}, {"name": "Jing Yan", "authorId": "2318391138"}, {"name": "Xiaoli Tang", "authorId": "2318236128"}, {"name": "Lingjuan Lyu", "authorId": "2287820224"}, {"name": "Boi Faltings", "authorId": "2054858128"}], "n_citations": 1}, "snippets": ["These models utilize extensive training data that may include copyrighted works, which they are sometimes capable of memorizing (Carlini et al. 2023). This ability can result in the pro- duction of images that closely resemble protected content (See in Figure 1), posing significant challenges to copyright protection (Dogoulis et al., 2023). Recent legal cases, such as those involving Stable Diffusion (Rombach et al., 2021) and Midjourney(Mansour 2023), highlight concerns over the use of copyrighted data in AI training, where the models potentially infringe on the rights of numerous artists. These cases highlight a growing concern: Could the high-quality content synthesized by these generative AIs be excessively similar to copyrighted training data, potentially violating the rights of copyright holders?"], "score": 0.98779296875}, {"id": "(Rombach et al., 2021)", "paper": {"corpus_id": 245335280, "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Robin Rombach", "authorId": "1660819540"}, {"name": "A. Blattmann", "authorId": "119843260"}, {"name": "Dominik Lorenz", "authorId": "2053482699"}, {"name": "Patrick Esser", "authorId": "35175531"}, {"name": "B. Ommer", "authorId": "1796707"}], "n_citations": 15768}, "snippets": ["By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."], "score": 0.0}, {"id": "(Leotta et al., 2023)", "paper": {"corpus_id": 260155285, "title": "Not with my name! Inferring artists' names of input strings employed by Diffusion Models", "year": 2023, "venue": "International Conference on Image Analysis and Processing", "authors": [{"name": "R. Leotta", "authorId": "47253043"}, {"name": "Oliver Giudice", "authorId": "1797543"}, {"name": "Luca Guarnera", "authorId": "40010524"}, {"name": "S. Battiato", "authorId": "1742452"}], "n_citations": 7}, "snippets": ["The rapid advancement of generative models, particularly Diffusion Models [7], has led to a surge in high-quality, realistic image generation. These models have demonstrated immense potential for creative applications across various domains, including art, design, and advertising. However, their ability to replicate the styles of specific artists raises concerns about Intellectual Property (IP) rights and potential copyright infringements [20,25,19]. \n\nAs the boundary between human creativity and machine-generated content becomes increasingly blurred, it is crucial to address the legal and ethical implications of using generative models to produce art, as well as to develop methods that evaluate the extent to which generated images are influenced by the works of real artists and ensure the protection of their intellectual property. \n\nSeveral studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights. In an article dated 2021 the challenges in determining copyright ownership for AI-generated works were discussed and authors emphasized the need for legal frameworks that could adequately address the unique nature of generative models. MLQ.AI also reported on a copyright infringement case involving generative AI, which sparked debates on the responsibilities of AI developers and users in protecting original creators' rights. In response to these concerns, legal scholars have delved into the complexities of copyright law as it pertains to AI-generated artworks. Gillotte [8] examined the challenges in assigning liability and protecting IP rights, arguing that existing copyright laws may not be sufficient to address the unique characteristics of AI-generated contents. Indeed, it is became a copyright dilemma and the need for a balance between innovation and IP protection is arising to ensure that creative works are safeguarded without stifling technological advancements [16]."], "score": 0.982421875}], "table": null}, {"title": "Proposed Solutions and Defenses", "tldr": "Multiple technical approaches have been developed to address copyright infringement in generative AI, including watermarking techniques, content filtering systems, and constraint-based generation methods that prevent models from reproducing protected content. (11 sources)", "text": "\nAs concerns about copyright infringement in generative AI grow, researchers and developers have proposed several technical solutions to mitigate these risks. One prominent approach involves digital watermarking techniques for neural networks. These methods aim to protect the intellectual property rights of model owners by enabling verification of model ownership and tracking potential misuse <Paper corpusId=\"265066820\" paperTitle=\"(Fei et al., 2023)\" isShortName></Paper>. This is particularly important in commercial settings where generative models are licensed to users who might violate license agreements or engage in malicious activities.\n\nContent filtering and concept removal represent another significant defense strategy. Researchers have developed methods to continuously remove improper concepts from generative models to prevent copyright infringement and other misuse, such as mimicking specific artistic styles or generating inappropriate content <Paper corpusId=\"274436153\" paperTitle=\"(Han et al., 2024)\" isShortName></Paper> <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper> <Paper corpusId=\"248986576\" paperTitle=\"(Saharia et al., 2022)\" isShortName></Paper> <Paper corpusId=\"253420366\" paperTitle=\"(Schramowski et al., 2022)\" isShortName></Paper>. These approaches are particularly important for preventing text-to-image models from generating content that infringes on artistic styles or intellectual properties.\n\nSome developers have focused on removing copyrighted images from training datasets entirely as a preventive measure <Paper corpusId=\"274436785\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. This approach aims to avoid potential copyright issues by ensuring models never learn from protected content. However, this can be challenging to implement comprehensively given the scale of training datasets and the difficulty in identifying all copyrighted materials.\n\nMore sophisticated approaches include constraint-based generation techniques that guide diffusion models away from producing content that resembles copyrighted material. For example, researchers have implemented latent constrained models that selectively modify generated content during the initial stages of denoising to prevent copyright infringement while maintaining overall image quality <Paper corpusId=\"276250558\" paperTitle=\"(Zampini et al., 2025)\" isShortName></Paper>. In one implementation, this method reduced the generation of protected cartoon characters from 33% to just 10% of outputs, with minimal impact on image quality as measured by FID scores.\n\nThe development of formal frameworks for copyright-safe generation is also emerging. Some researchers have proposed the concept of \"near access-freeness\" (NAF), which provides theoretical bounds on the probability that a model outputs samples similar to copyrighted data, even when such data was included in training <Paper corpusId=\"274436785\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. These frameworks offer mathematical guarantees about a model's behavior regarding protected content.\n\nDespite these advances, traditional copyright protection measures like watermarks and metadata have proven largely ineffective in the context of generative AI <Paper corpusId=\"277955485\" paperTitle=\"(Rakib et al., 2025)\" isShortName></Paper>. The challenge of balancing copyright protection with the creative capabilities of generative models remains significant, particularly as advanced few-shot generation techniques like DreamBooth enable efficient art imitation and style transfer with minimal training data <Paper corpusId=\"268513090\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"251800180\" paperTitle=\"(Ruiz et al., 2022)\" isShortName></Paper>.\n\nThe quest for effective defense mechanisms must continue as models become more sophisticated and backdoor attacks against diffusion models become more prevalent <Paper corpusId=\"274436785\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. These ongoing research efforts highlight the importance of developing solutions that preserve the generative capabilities of AI models while respecting copyright protections and intellectual property rights.", "citations": [{"id": "(Fei et al., 2023)", "paper": {"corpus_id": 265066820, "title": "Robust Retraining-free GAN Fingerprinting via Personalized Normalization", "year": 2023, "venue": "International Workshop on Information Forensics and Security", "authors": [{"name": "Jianwei Fei", "authorId": "1638005137"}, {"name": "Zhihua Xia", "authorId": "2072878384"}, {"name": "B. Tondi", "authorId": "2488892"}, {"name": "Mauro Barni", "authorId": "2259940667"}], "n_citations": 6}, "snippets": ["In recent years, there has been significant growth in the commercial applications of generative models, licensed and distributed by model developers to users, who in turn use them to offer services. In this scenario, there is a need to track and identify the responsible user in the presence of a violation of the license agreement or any kind of malicious usage.\n\nProtecting the Intellectual Property Rights (IPR) of model owners has become a pressing issue to avoid potential copyright infringements, such as unauthorized duplication or model theft, when these models are delivered to malicious users. Deep Neural Network (DNN) watermarking has been proposed as a solution to protect the IPR associated with DNN models [1].\n\nWith the exception of a few scattered works [3], the existing approaches for the watermarking of generative models, notably GANs, are designed for ownership verification, aiming at making it possible to retrieve the model authorship information from the generated images."], "score": 0.98193359375}, {"id": "(Han et al., 2024)", "paper": {"corpus_id": 274436153, "title": "Continuous Concepts Removal in Text-to-image Diffusion Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tingxu Han", "authorId": "2170360833"}, {"name": "Weisong Sun", "authorId": "3433022"}, {"name": "Yanrong Hu", "authorId": "2333453981"}, {"name": "Chunrong Fang", "authorId": "2239197945"}, {"name": "Yonglong Zhang", "authorId": "2333368518"}, {"name": "Shiqing Ma", "authorId": "2333472479"}, {"name": "Tao Zheng", "authorId": "2322486553"}, {"name": "Zhenyu Chen", "authorId": "2238950128"}, {"name": "Zhenting Wang", "authorId": "2154723145"}], "n_citations": 3}, "snippets": ["Advancements in Artificial Intelligence Generated Contents (AIGCs) [47] have revolutionized the field of image synthesis (Rombach et al., 2021)38,(Xue et al., 2023), among which text-to-image diffusion models enable the creation of high-quality images from textual descriptions (Saharia et al., 2022)(Zhang et al., 2023). However, this progress has also raised significant concerns regarding the potential misuse of these models (Carlini et al., 2023)13,(Sha et al., 2022)[43]. Such misuse includes generating content that infringes on copyrights, such as mimicking specific artistic styles [30], intellectual properties [45]46], or creating disturbing and improper subject matter, including eroticism and violence (Schramowski et al., 2022). Addressing these issues necessitates continuously removing those improper concepts from these models to prevent misuse and protect copyright from infringement."], "score": 0.98095703125}, {"id": "(Rombach et al., 2021)", "paper": {"corpus_id": 245335280, "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Robin Rombach", "authorId": "1660819540"}, {"name": "A. Blattmann", "authorId": "119843260"}, {"name": "Dominik Lorenz", "authorId": "2053482699"}, {"name": "Patrick Esser", "authorId": "35175531"}, {"name": "B. Ommer", "authorId": "1796707"}], "n_citations": 15768}, "snippets": ["By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."], "score": 0.0}, {"id": "(Saharia et al., 2022)", "paper": {"corpus_id": 248986576, "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Chitwan Saharia", "authorId": "51497543"}, {"name": "William Chan", "authorId": "144333684"}, {"name": "Saurabh Saxena", "authorId": "2054003577"}, {"name": "Lala Li", "authorId": "2111917831"}, {"name": "Jay Whang", "authorId": "21040156"}, {"name": "Emily L. Denton", "authorId": "40081727"}, {"name": "Seyed Kamyar Seyed Ghasemipour", "authorId": "81419386"}, {"name": "Burcu Karagol Ayan", "authorId": "143990191"}, {"name": "S. S. Mahdavi", "authorId": "1982213"}, {"name": "Raphael Gontijo Lopes", "authorId": "143826364"}, {"name": "Tim Salimans", "authorId": "2887364"}, {"name": "Jonathan Ho", "authorId": "2126278"}, {"name": "David J. Fleet", "authorId": "1793739"}, {"name": "Mohammad Norouzi", "authorId": "144739074"}], "n_citations": 6075}, "snippets": ["We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results."], "score": 0.0}, {"id": "(Schramowski et al., 2022)", "paper": {"corpus_id": 253420366, "title": "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "P. Schramowski", "authorId": "40896023"}, {"name": "Manuel Brack", "authorId": "2166299958"}, {"name": "Bjorn Deiseroth", "authorId": "2905059"}, {"name": "K. Kersting", "authorId": "2066493115"}], "n_citations": 309}, "snippets": ["Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.11Code available at https://huggingface.co/docs/diffusers/api/pipelines/stable.diffusion.safe"], "score": 0.0}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 274436785, "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhixiang Guo", "authorId": "2333463963"}, {"name": "Siyuan Liang", "authorId": "2325884825"}, {"name": "Aishan Liu", "authorId": "2257572247"}, {"name": "Dacheng Tao", "authorId": "2237906923"}], "n_citations": 3}, "snippets": ["However, as commercial text-to-image diffusion models become increasingly prevalent [57,61], copyright issues have emerged as a significant concern. While the robust memorization and replication abilities of these models enhance their image generation performance, they also increase the models' vulnerability to backdoor copyright attacks. By injecting concealed poisoned data into the training set, diffusion models can be compromised without the need for fine-tuning [62]. Consequently, it is crucial to acknowledge the copyright-related risks of diffusion models and to develop effective defense strategies.\n\nCurrent solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,61,76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74]77], which can result in copyright violations, have yet to be developed."], "score": 0.98828125}, {"id": "(Vyas et al., 2023)", "paper": {"corpus_id": 257050406, "title": "On Provable Copyright Protection for Generative Models", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Nikhil Vyas", "authorId": "145603901"}, {"name": "S. Kakade", "authorId": "144695232"}, {"name": "B. Barak", "authorId": "1697211"}], "n_citations": 94}, "snippets": ["Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models.\n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material."], "score": 0.9892578125}, {"id": "(Zampini et al., 2025)", "paper": {"corpus_id": 276250558, "title": "Training-Free Constrained Generation With Stable Diffusion Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "S. Zampini", "authorId": "2277539130"}, {"name": "Jacob K. Christopher", "authorId": "2267727785"}, {"name": "Luca Oneto", "authorId": "2255314313"}, {"name": "Davide Anguita", "authorId": "2300205982"}, {"name": "Ferdinando Fioretto", "authorId": "2141569789"}], "n_citations": 2}, "snippets": ["This method ensures that the generated images are guided away from resembling copyrighted material while still allowing the model to produce high-quality outputs. By selectively modifying the generated content during the initial stages of denoising, we can effectively prevent the model from producing images that infringe on copyrights without significantly affecting the overall image quality", "We implement a Conditional Diffusion Model baselines using and unconstrained stable diffusion model identical to the one used for our method. The conditional baseline generates the protected cartoon character (Mickey Mouse) 33% of the time, despite conditioning it against these generations. \n\nConversely, our Latent Constrained Model only generates the protected cartoon character 10% of the time, aligning with the expected bounds of the classifier's predictive accuracy. Our method has proven to be highly effective because it preserves the generative capabilities of the model while imposing the defined constraints. Notably, the difference between the original image and the corrected one primarily affects the areas near the figure that violate the constraint, while the rest of the image remains largely unchanged. The FID scores of the generated images, increasing only slightly from 61.2 to 65.1, remain largely unaltered by the gradientbased correction. This demonstrates that our approach can selectively modify generated content to avoid copyrighted material without compromising overall image quality."], "score": 0.98583984375}, {"id": "(Rakib et al., 2025)", "paper": {"corpus_id": 277955485, "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mazharul Islam Rakib", "authorId": "2313107575"}, {"name": "Showrin Rahman", "authorId": "2357491645"}, {"name": "J. Mondal", "authorId": "2113939573"}, {"name": "Xi Xiao", "authorId": "2349232014"}, {"name": "David Lewis", "authorId": "2357414290"}, {"name": "Alessandra Mileo", "authorId": "2356548661"}, {"name": "Meem Arafat Manab", "authorId": "2210516789"}], "n_citations": 1}, "snippets": ["In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard", ".Diffusion models have revolutionized image generation, surpassing predecessors like GANs and VAEs in both fidelity and performance. However, this progress has been shadowed by a critical challenge: source copying. This issue raises serious concerns regarding privacy and intellectual property rights, demanding innovative solutions that preserve content quality while enhancing copyright protection."], "score": 0.994140625}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 268513090, "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion", "year": 2024, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Xiaoyu Wu", "authorId": "2108069960"}, {"name": "Yang Hua", "authorId": "2147311278"}, {"name": "Chumeng Liang", "authorId": "2186858424"}, {"name": "Jiaru Zhang", "authorId": "2118001291"}, {"name": "Hao Wang", "authorId": "2144220882"}, {"name": "Tao Song", "authorId": "2055312951"}, {"name": "Haibing Guan", "authorId": "2292035375"}], "n_citations": 6}, "snippets": ["Recent years have witnessed the advancement of Diffusion Models (DMs) in computer vision.These models demonstrate exceptional capabilities across a diverse array of tasks, including image editing [14], and video editing [35], among others.Particularly, the emergence of few-shot generation techniques, exemplified by Dreambooth (Ruiz et al., 2022) and swiftly capturing the style or primary objects by fine-tuning a pretrained model on a small set of images.This process enables efficient and high-quality art imitation and art style transfer by utilizing the fine-tuned model.\n\nHowever, these advanced few-shot generation techniques also spark widespread concerns regarding the protection of copyright for human artworks and individual photographs.There is a growing fear that parties may exploit the generative capabilities of DMs to create and profit from derivative artworks based on existing artists' works, without obtaining proper authorization [6,19].Concurrently, concerns arise regarding the creation of fabricated images of individuals without their consent [32].All of these collectively pose a serious threat to the security of personal data and intellectual property rights."], "score": 0.984375}, {"id": "(Ruiz et al., 2022)", "paper": {"corpus_id": 251800180, "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Nataniel Ruiz", "authorId": "31601235"}, {"name": "Yuanzhen Li", "authorId": "2167749913"}, {"name": "Varun Jampani", "authorId": "2131639924"}, {"name": "Y. Pritch", "authorId": "1782328"}, {"name": "Michael Rubinstein", "authorId": "144544291"}, {"name": "Kfir Aberman", "authorId": "3451442"}], "n_citations": 2891}, "snippets": ["Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \"personalization\" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/"], "score": 0.0}], "table": null}, {"title": "Implications for Stakeholders", "tldr": "Copyright infringement in generative AI affects multiple groups, including original content creators who face unauthorized use of their work, AI developers who must manage legal risks, and users who may unknowingly generate infringing content. (8 sources)", "text": "\nThe rise of generative AI models for images has created a complex web of implications for various stakeholders in the creative ecosystem. For original content creators\u2014artists, photographers, and other visual content owners\u2014these models present significant challenges to their intellectual property rights. When their works are included in training datasets without permission or compensation, they face not only potential economic losses but also the unauthorized reproduction or adaptation of their distinctive styles and content <Paper corpusId=\"267400526\" paperTitle=\"(Zhuang, 2024)\" isShortName></Paper>. This problem is compounded by the fact that AI-generated outputs typically lack watermarks or source attribution, making it even more difficult for copyright owners to detect infringement or receive appropriate credit <Paper corpusId=\"267400526\" paperTitle=\"(Zhuang, 2024)\" isShortName></Paper>.\n\nFor AI model developers and companies, the stakes are equally high. They face mounting legal risks through multiple lawsuits, as evidenced by cases against Stability AI and other companies that have been accused of reproducing copyrighted works without permission <Paper corpusId=\"267400526\" paperTitle=\"(Zhuang, 2024)\" isShortName></Paper>. These legal challenges necessitate the development of protective measures such as DNN watermarking to verify model ownership and track potential misuse <Paper corpusId=\"265066820\" paperTitle=\"(Fei et al., 2023)\" isShortName></Paper>. Additionally, developers must navigate the complex landscape of licensing and distribution, particularly when their models are provided as services to users who might violate license agreements or engage in malicious usage <Paper corpusId=\"265066820\" paperTitle=\"(Fei et al., 2023)\" isShortName></Paper>.\n\nEnd users of generative AI services face their own set of challenges and responsibilities. They may potentially be considered direct or indirect infringers in copyright disputes, even without intending to replicate copyrighted material <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. This creates significant uncertainty for individuals and businesses using these technologies, as they must navigate copyright risks without clear guidelines on what constitutes infringement in AI-generated content. The uninterpretable nature of generative models further complicates matters, as it becomes difficult to provide direct evidence of copying when infringement is alleged <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>.\n\nThe advancement of few-shot generation techniques such as DreamBooth has intensified these concerns. These methods enable efficient art imitation and style transfer with minimal training data <Paper corpusId=\"268513090\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"251800180\" paperTitle=\"(Ruiz et al., 2022)\" isShortName></Paper>, making it easier for unauthorized parties to directly edit or modify source data to create derivative works <Paper corpusId=\"267412857\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>. This capability poses a serious threat to the security of personal data and intellectual property rights, as parties may exploit generative capabilities to create and profit from derivative artworks without proper authorization <Paper corpusId=\"268513090\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>.\n\nAs generative models continue to evolve, the tension between technological innovation and copyright protection will likely intensify. Studies have shown that exposure to certain types of content during training can lead to a high likelihood of models reproducing or closely imitating that content <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper> <Paper corpusId=\"229156229\" paperTitle=\"(Carlini et al., 2020)\" isShortName></Paper> <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper>, suggesting that the inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This presents an ongoing challenge for all stakeholders involved in the generative AI ecosystem as they navigate the complex intersection of technological capabilities, creative expression, and legal frameworks.", "citations": [{"id": "(Zhuang, 2024)", "paper": {"corpus_id": 267400526, "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists", "year": 2024, "venue": "Applied and Computational Engineering", "authors": [{"name": "Lyulin Zhuang", "authorId": "2282411198"}], "n_citations": 2}, "snippets": ["Stable Diffusion can quickly generate high-quality images based on simple text descriptions, thanks to its training on billions of sample images collected online. However, not all of these sample images fall within the public domain; some are even protected by copyright. As one can imagine, the authors and photographers of these copyrighted images are not pleased with the actions of Stable Diffusion.\n\nStable Diffusion has faced multiple lawsuits due to copyright issues. Artists and photographers have initiated collective lawsuits against this technology, and one of the world's leading image suppliers, Getty Images Holdings Inc., has also filed a similar case. According to Getty's (2023) complaint, \"Stability AI has reproduced over 12 million Getty Images, along with their related captions and metadata, without permission or compensation.\"", "After exploring how artificial intelligence (AI)-generated platforms work, learn what processes may involve human artist infringement in the course of their work. So let's talk about where these workflows infringe on human artists' copyrighted works -Artificial intelligence (AI)-generated content raises concerns about copyright infringement of human artist works in several ways.\n\nThe researchers summarized three aspects of human artist infringement: model training using only copyrighted works without permission. Take Stable Diffusion, which has been involved in many copyright disputes. It uses the image database containing hundreds of millions-LAION-5B (Kaixin Zhu, & Yiqun Zhang. ( 2023)) as the source of training data, and it does not need the consent of the copyright owner [5].\n\nSecond, there is a lot of plagiarism and adaptation of copyrighted works in AI-generated images. In a case in which an artist sued an AI-generated platform, the painter Erin Hanson provided strong evidence that the AI-generated images plagiarized his work. And there are many such lawsuits now, not a few. Third, the images generated by the AI generation platform, such as Stable Diffusion, if works infringe copyright works, do not have any watermarks or source marks, making it even more difficult for copyright owners to accept."], "score": 0.9912109375}, {"id": "(Fei et al., 2023)", "paper": {"corpus_id": 265066820, "title": "Robust Retraining-free GAN Fingerprinting via Personalized Normalization", "year": 2023, "venue": "International Workshop on Information Forensics and Security", "authors": [{"name": "Jianwei Fei", "authorId": "1638005137"}, {"name": "Zhihua Xia", "authorId": "2072878384"}, {"name": "B. Tondi", "authorId": "2488892"}, {"name": "Mauro Barni", "authorId": "2259940667"}], "n_citations": 6}, "snippets": ["In recent years, there has been significant growth in the commercial applications of generative models, licensed and distributed by model developers to users, who in turn use them to offer services. In this scenario, there is a need to track and identify the responsible user in the presence of a violation of the license agreement or any kind of malicious usage.\n\nProtecting the Intellectual Property Rights (IPR) of model owners has become a pressing issue to avoid potential copyright infringements, such as unauthorized duplication or model theft, when these models are delivered to malicious users. Deep Neural Network (DNN) watermarking has been proposed as a solution to protect the IPR associated with DNN models [1].\n\nWith the exception of a few scattered works [3], the existing approaches for the watermarking of generative models, notably GANs, are designed for ownership verification, aiming at making it possible to retrieve the model authorship information from the generated images."], "score": 0.98193359375}, {"id": "(Chiba-Okabe, 2024)", "paper": {"corpus_id": 273023255, "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hiroaki Chiba-Okabe", "authorId": "2297800322"}], "n_citations": 2}, "snippets": ["In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content (Carlini et al., 2020)(Carlini et al., 2023)[39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works."], "score": 0.98876953125}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 268513090, "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion", "year": 2024, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Xiaoyu Wu", "authorId": "2108069960"}, {"name": "Yang Hua", "authorId": "2147311278"}, {"name": "Chumeng Liang", "authorId": "2186858424"}, {"name": "Jiaru Zhang", "authorId": "2118001291"}, {"name": "Hao Wang", "authorId": "2144220882"}, {"name": "Tao Song", "authorId": "2055312951"}, {"name": "Haibing Guan", "authorId": "2292035375"}], "n_citations": 6}, "snippets": ["Recent years have witnessed the advancement of Diffusion Models (DMs) in computer vision.These models demonstrate exceptional capabilities across a diverse array of tasks, including image editing [14], and video editing [35], among others.Particularly, the emergence of few-shot generation techniques, exemplified by Dreambooth (Ruiz et al., 2022) and swiftly capturing the style or primary objects by fine-tuning a pretrained model on a small set of images.This process enables efficient and high-quality art imitation and art style transfer by utilizing the fine-tuned model.\n\nHowever, these advanced few-shot generation techniques also spark widespread concerns regarding the protection of copyright for human artworks and individual photographs.There is a growing fear that parties may exploit the generative capabilities of DMs to create and profit from derivative artworks based on existing artists' works, without obtaining proper authorization [6,19].Concurrently, concerns arise regarding the creation of fabricated images of individuals without their consent [32].All of these collectively pose a serious threat to the security of personal data and intellectual property rights."], "score": 0.984375}, {"id": "(Ruiz et al., 2022)", "paper": {"corpus_id": 251800180, "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Nataniel Ruiz", "authorId": "31601235"}, {"name": "Yuanzhen Li", "authorId": "2167749913"}, {"name": "Varun Jampani", "authorId": "2131639924"}, {"name": "Y. Pritch", "authorId": "1782328"}, {"name": "Michael Rubinstein", "authorId": "144544291"}, {"name": "Kfir Aberman", "authorId": "3451442"}], "n_citations": 2891}, "snippets": ["Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \"personalization\" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/"], "score": 0.0}, {"id": "(Ren et al., 2024)", "paper": {"corpus_id": 267412857, "title": "Copyright Protection in Generative AI: A Technical Perspective", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jie Ren", "authorId": "2256589810"}, {"name": "Han Xu", "authorId": "2253881697"}, {"name": "Pengfei He", "authorId": "2185740224"}, {"name": "Yingqian Cui", "authorId": "2218740984"}, {"name": "Shenglai Zeng", "authorId": "2253682835"}, {"name": "Jiankun Zhang", "authorId": "2282560420"}, {"name": "Hongzhi Wen", "authorId": "2256788829"}, {"name": "Jiayuan Ding", "authorId": "46496977"}, {"name": "Hui Liu", "authorId": "2253533415"}, {"name": "Yi Chang", "authorId": "2267019992"}, {"name": "Jiliang Tang", "authorId": "2115879611"}], "n_citations": 42}, "snippets": ["For the source data owner, which refers to the party or individual who owns the originality of image works, their data can be intentionally or unintentionally collected by model trainers as training samples to construct DGMs as introduced above. For example, recent studies [16]134] have demonstrated that popular DGMs are highly possible to completely replicate their training data samples, which is called memorization. \n\nThe possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works."], "score": 0.99169921875}, {"id": "(Carlini et al., 2020)", "paper": {"corpus_id": 229156229, "title": "Extracting Training Data from Large Language Models", "year": 2020, "venue": "USENIX Security Symposium", "authors": [{"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}, {"name": "Eric Wallace", "authorId": "145217343"}, {"name": "Matthew Jagielski", "authorId": "40844378"}, {"name": "Ariel Herbert-Voss", "authorId": "1404060687"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Tom B. Brown", "authorId": "31035595"}, {"name": "D. Song", "authorId": "143711382"}, {"name": "\u00da. Erlingsson", "authorId": "1758110"}, {"name": "Alina Oprea", "authorId": "3046437"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 1950}, "snippets": ["It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models."], "score": 0.0}, {"id": "(Carlini et al., 2023)", "paper": {"corpus_id": 256389993, "title": "Extracting Training Data from Diffusion Models", "year": 2023, "venue": "USENIX Security Symposium", "authors": [{"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Jamie Hayes", "authorId": "9200194"}, {"name": "Milad Nasr", "authorId": "3490923"}, {"name": "Matthew Jagielski", "authorId": "40844378"}, {"name": "Vikash Sehwag", "authorId": "3482535"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}, {"name": "Borja Balle", "authorId": "1718064"}, {"name": "Daphne Ippolito", "authorId": "7975935"}, {"name": "Eric Wallace", "authorId": "145217343"}], "n_citations": 617}, "snippets": ["Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."], "score": 0.0}], "table": null}], "cost": 0.411042}}
