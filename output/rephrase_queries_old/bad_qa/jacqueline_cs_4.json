{"original_query": "Data selection has emerged as an important problem for training good language models (i.e., which datapoints to include). Has there been any research on the ordering of datapoints for LMs (i.e., easy to hard curriculum sequence of data points), such that a LM may be trained in a more sample-efficient manner? ", "input": "Data selection as important problem training good language models which been research the ordering datapoints for LMs easy to hard curriculum sequence of data points such LM a sample efficient manner", "output": {"sections": [{"title": "Introduction to Curriculum Learning", "tldr": "Curriculum learning is an approach inspired by human learning that involves training models on progressively more difficult examples. It consists of determining data difficulty and designing appropriate training schedules to improve model performance and efficiency. (12 sources)", "text": "\nCurriculum learning (CL) is a training strategy inspired by how humans learn, where training samples are organized by difficulty levels and presented to models in an easy-to-hard progression <Paper corpusId=\"237553761\" paperTitle=\"(Agrawal et al., 2021)\" isShortName></Paper>. This approach was formally introduced to machine learning by Bengio et al. in 2009, though the concept traces back to Elman's 1993 \"starting small\" hypothesis <Paper corpusId=\"265506572\" paperTitle=\"(Chobey et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247762191\" paperTitle=\"(Mohiuddin et al., 2022)\" isShortName></Paper>. The core intuition is that by mimicking human education, where learning begins with simpler concepts before tackling more complex ones, neural networks can achieve better performance and faster convergence <Paper corpusId=\"208031414\" paperTitle=\"(Ma et al., 2019)\" isShortName></Paper>.\n\nIn recent years, curriculum learning has demonstrated significant benefits across various NLP tasks, particularly in machine translation, and has expanded to other natural language understanding applications <Paper corpusId=\"227905455\" paperTitle=\"(Laverghetta et al., 2020)\" isShortName></Paper> <Paper corpusId=\"220045816\" paperTitle=\"(Xu et al., 2020)\" isShortName></Paper>. The strategy has proven valuable as a plug-in approach for improving model generalization capacity and convergence rate in multiple domains beyond NLP, including computer vision <Paper corpusId=\"232362223\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper> <Paper corpusId=\"53295888\" paperTitle=\"(Zhang et al., 2018)\" isShortName></Paper>.\n\nImplementing curriculum learning involves two fundamental components: (1) a difficulty measurer that assigns scores to training examples based on their complexity, and (2) a training scheduler that determines the order of presentation based on these difficulty scores <Paper corpusId=\"265506572\" paperTitle=\"(Chobey et al., 2023)\" isShortName></Paper>. These components can be designed manually using prior knowledge or automatically derived during the training process <Paper corpusId=\"232362223\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. Recent research has explored various methodologies for both components, from transfer learning approaches to self-paced learning techniques that adapt to the learner's progress <Paper corpusId=\"102350936\" paperTitle=\"(Hacohen et al., 2019)\" isShortName></Paper> <Paper corpusId=\"10891229\" paperTitle=\"(Jiang et al., 2015)\" isShortName></Paper>.\n\nThe growing body of research consistently shows that the sequencing of training data significantly impacts both the efficiency of the learning process and the final performance of language models <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper> <Paper corpusId=\"231709290\" paperTitle=\"(Soviany et al., 2021)\" isShortName></Paper>. This realization has sparked diverse approaches to curriculum design, with researchers continually exploring new ways to determine example difficulty and optimal training schedules.", "citations": [{"id": "(Agrawal et al., 2021)", "paper": {"corpus_id": 237553761, "title": "On the Role of Corpus Ordering in Language Modeling", "year": 2021, "venue": "SUSTAINLP", "authors": [{"name": "Ameeta Agrawal", "authorId": "2628916"}, {"name": "Suresh Singh", "authorId": "2145771623"}, {"name": "Lauren Schneider", "authorId": "144958306"}, {"name": "Michael Samuels", "authorId": "151134500"}], "n_citations": 7}, "snippets": ["The intuition behind this approach is to mimic the manner in which humans learn. Training samples are organized by levels of difficulty and training proceeds in steps where the model is first trained on a subset of the corpus at a given difficulty level before being trained on another difficulty level, and so on."], "score": 0.79052734375}, {"id": "(Chobey et al., 2023)", "paper": {"corpus_id": 265506572, "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?", "year": 2023, "venue": "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning", "authors": [{"name": "Aryaman Chobey", "authorId": "2268760204"}, {"name": "Oliver Smith", "authorId": "2268760018"}, {"name": "Anzi Wang", "authorId": "2268796061"}, {"name": "Grusha Prasad", "authorId": "2268760229"}], "n_citations": 5}, "snippets": ["Curriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see (Soviany et al., 2021); (Wang et al., 2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021)."], "score": 0.69384765625}, {"id": "(Mohiuddin et al., 2022)", "paper": {"corpus_id": 247762191, "title": "Data Selection Curriculum for Neural Machine Translation", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Tasnim Mohiuddin", "authorId": "6838342"}, {"name": "Philipp Koehn", "authorId": "1755162"}, {"name": "Vishrav Chaudhary", "authorId": "113810201"}, {"name": "James Cross", "authorId": "2059363961"}, {"name": "Shruti Bhosale", "authorId": "2116473"}, {"name": "Shafiq R. Joty", "authorId": "2708940"}], "n_citations": 13}, "snippets": ["Curriculum Learning Inspired by human learners, (Elman, 1993) argues that optimization of neural network training can be accelerated by gradually increasing the difficulty of the concepts. (Bengio et al., 2009) were the first to use the term \"curricu-lum learning\" to refer to the easy-to-hard training strategies in the context of machine learning. Using an easy-to-hard curriculum based on increasing vocabulary size in language model training, they achieved performance improvement. Recent work (Jiang et al., 2015)(Hacohen et al., 2019)(Zhou et al., 2020) shows that manoeuvring the sequence of training data can improve both training efficiency and model accuracy."], "score": 0.71240234375}, {"id": "(Ma et al., 2019)", "paper": {"corpus_id": 208031414, "title": "Domain Adaptation with BERT-based Domain Classification and Data Selection", "year": 2019, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xiaofei Ma", "authorId": "47646605"}, {"name": "Peng Xu", "authorId": "2091437540"}, {"name": "Zhiguo Wang", "authorId": "40296541"}, {"name": "Ramesh Nallapati", "authorId": "1701451"}], "n_citations": 95}, "snippets": ["Our method is inspired by the work on curriculum learning and recent work on data selection for transfer learning. \n\nCurriculum Learning: Curriculum Learning (Bengio et al., 2009) deals with the question of how to use prior knowledge about the difficulty of the training examples, to boost the rate of learning and the performance of the final model. The ranking or weighting of the training examples is used to guide the order of presentation of examples to the learner. The idea is to build a curriculum of progressively harder samples in order to significantly accelerate a neural network's train-ing."], "score": 0.80029296875}, {"id": "(Laverghetta et al., 2020)", "paper": {"corpus_id": 227905455, "title": "Towards a Task-Agnostic Model of Difficulty Estimation for Supervised Learning Tasks", "year": 2020, "venue": "AACL", "authors": [{"name": "Antonio Laverghetta", "authorId": "2033736232"}, {"name": "Jamshidbek Mirzakhalov", "authorId": "1716200134"}, {"name": "John Licato", "authorId": "2143879"}], "n_citations": 2}, "snippets": ["Curriculum learning (Elman, 1993), a strategy where the model is trained on easier examples before harder ones, has recently been shown to improve performance and reduce training time on a variety of NLP tasks, especially machine translation (Liu et al., 2020;(Wang et al., 2019)(Zhou et al., 2020)(Xu et al., 2020). \n\nThe success of this research shows that the order in which training data is presented to a model is important, but how to best apply curriculum learning broadly to NLP and how to design effective measures of difficulty to create curricula remain relatively unexplored."], "score": 0.76953125}, {"id": "(Xu et al., 2020)", "paper": {"corpus_id": 220045816, "title": "Curriculum Learning for Natural Language Understanding", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Benfeng Xu", "authorId": "1754285124"}, {"name": "L. Zhang", "authorId": "48378753"}, {"name": "Zhendong Mao", "authorId": "1855978"}, {"name": "Quan Wang", "authorId": "143906199"}, {"name": "Hongtao Xie", "authorId": "143994657"}, {"name": "Yongdong Zhang", "authorId": "1699819"}], "n_citations": 206}, "snippets": ["With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks."], "score": 0.0}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 232362223, "title": "A Survey on Curriculum Learning", "year": 2021, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Xin Wang", "authorId": "2153687490"}, {"name": "Yudong Chen", "authorId": "51310474"}, {"name": "Wenwu Zhu", "authorId": "145583986"}], "n_citations": 611}, "snippets": ["Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=\"LaTeX\">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3069908.gif\"/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations."], "score": 0.0}, {"id": "(Zhang et al., 2018)", "paper": {"corpus_id": 53295888, "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "Xuan Zhang", "authorId": "49469742"}, {"name": "Manish Kumar", "authorId": "48387892"}, {"name": "Huda Khayrallah", "authorId": "3115181"}, {"name": "Kenton Murray", "authorId": "38730896"}, {"name": "Jeremy Gwinnup", "authorId": "3456371"}, {"name": "Marianna J. Martindale", "authorId": "3219152"}, {"name": "Paul McNamee", "authorId": "145324163"}, {"name": "Kevin Duh", "authorId": "1800354"}, {"name": "Marine Carpuat", "authorId": "2954727"}], "n_citations": 112}, "snippets": ["Curriculum learning (Bengio et al., 2009) hypothesizes that choosing the order in which training samples are presented to a learning system can help train better models faster. In particular, presenting samples that are easier to learn from before presenting difficult samples is an intuitively attractive idea, which has been applied in various ways in Machine Learning and Natural Language Processing tasks (Bengio et al., 2009)Tsvetkov et al., 2016;Cirik et al., 2016;Graves et al., 2017, inter alia)."], "score": 0.708984375}, {"id": "(Hacohen et al., 2019)", "paper": {"corpus_id": 102350936, "title": "On The Power of Curriculum Learning in Training Deep Networks", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Guy Hacohen", "authorId": "94064232"}, {"name": "D. Weinshall", "authorId": "1789171"}], "n_citations": 449}, "snippets": ["Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher\" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function."], "score": 0.0}, {"id": "(Jiang et al., 2015)", "paper": {"corpus_id": 10891229, "title": "Self-Paced Curriculum Learning", "year": 2015, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Lu Jiang", "authorId": "39978626"}, {"name": "Deyu Meng", "authorId": "1803714"}, {"name": "Qian Zhao", "authorId": "46317290"}, {"name": "S. Shan", "authorId": "145455919"}, {"name": "Alexander Hauptmann", "authorId": "7661726"}], "n_citations": 526}, "snippets": ["Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training. The two methods share a similar conceptual learning paradigm, but differ in specific learning schemes. In CL, the curriculum is predetermined by prior knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner. In SPL, the curriculum is dynamically determined to adjust to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a unified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training. In comparison to human education, SPCL is analogous to \"instructor-student-collaborative\" learning mode, as opposed to \"instructor-driven\" in CL or \"student-driven\" in SPL. Empirically, we show that the advantage of SPCL on two tasks."], "score": 0.0}, {"id": "(Zhou et al., 2020)", "paper": {"corpus_id": 220047761, "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yikai Zhou", "authorId": "2110349143"}, {"name": "Baosong Yang", "authorId": "21299583"}, {"name": "Derek F. Wong", "authorId": "1758353"}, {"name": "Yu Wan", "authorId": "153379180"}, {"name": "Lidia S. Chao", "authorId": "1774304"}], "n_citations": 96}, "snippets": ["Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."], "score": 0.0}, {"id": "(Soviany et al., 2021)", "paper": {"corpus_id": 231709290, "title": "Curriculum Learning: A Survey", "year": 2021, "venue": "International Journal of Computer Vision", "authors": [{"name": "Petru Soviany", "authorId": "40901288"}, {"name": "Radu Tudor Ionescu", "authorId": "1817759"}, {"name": "Paolo Rota", "authorId": "39337007"}, {"name": "N. Sebe", "authorId": "1703601"}], "n_citations": 359}, "snippets": ["(Zhou et al., 2020) introduce an uncertainty-based curriculum batching approach for neural machine translation. They propose using uncertainty at the data level, for establishing the easy-to-hard ordering, and the model level, to decide the right moment to enhance the training set with more difficult samples. To measure the difficulty of the examples, they start from the intuition that samples with higher crossentropy and uncertainty are more difficult to translate."], "score": 0.8095703125}], "table": null}, {"title": "Core Principles of Curriculum Learning", "tldr": "Curriculum learning is based on two key principles: organizing training data by difficulty and presenting it in a strategic sequence. The approach includes measuring sample difficulty, scheduling training progressively, and adapting the curriculum based on the model's learning state. (9 sources)", "text": "\nCurriculum learning is fundamentally built on the principle of organizing training samples based on their difficulty and presenting them to the model in a strategic sequence, typically from easy to hard <Paper corpusId=\"388785\" paperTitle=\"(Fan et al., 2017)\" isShortName></Paper>. This approach draws inspiration from human education systems, where learning progresses from basic concepts to more complex ones. The core components of curriculum learning include difficulty measurement and training scheduling, which together form the framework for implementing this learning strategy.\n\nA central principle of curriculum learning is difficulty assessment, which can be approached through various methods. Some researchers have grouped samples by their similarity to a specific domain of interest, allowing for targeted training schedules <Paper corpusId=\"155089817\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>. Others have proposed measuring sample difficulty from the model's perspective, using metrics like \"learning percentage\" to self-rank training data based on how quickly the model learns from different examples <Paper corpusId=\"267740312\" paperTitle=\"(Mekala et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249060677\" paperTitle=\"(Mekala et al., 2022)\" isShortName></Paper>.\n\nThe second key principle involves the training scheduler, which determines how and when different training examples are presented to the model. Traditional curriculum learning prioritizes easy points with low label noise before moving to all data points, enhancing convergence but potentially failing to address redundancy in the training data <Paper corpusId=\"261048902\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. More sophisticated approaches include dynamic data selection, which gradually transitions from generic to domain-specific training by increasing selection thresholds as training progresses <Paper corpusId=\"237581256\" paperTitle=\"(Iter et al., 2021)\" isShortName></Paper> <Paper corpusId=\"7921428\" paperTitle=\"(Wees et al., 2017)\" isShortName></Paper>.\n\nRecent implementations have expanded these principles to larger-scale language models, creating multi-stage training processes where data complexity increases incrementally. For instance, the Orion-14B model employed a data scheduling strategy that initially focused on common knowledge sources like web pages and news articles before gradually incorporating more complex data sources such as textbooks, academic papers, and source code <Paper corpusId=\"267095066\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\nAn important principle that complements curriculum learning is active learning, where the strategic ordering of training data selection can lead to more efficient learning from smaller datasets <Paper corpusId=\"226964591\" paperTitle=\"(Pillai et al., 2020)\" isShortName></Paper>. This approach recognizes that not all data points contribute equally to model learning, and selective sampling based on informativeness can enhance training efficiency.\n\nTogether, these principles form a framework for curriculum learning that moves beyond random data presentation, acknowledging that \"feeding data in a totally random order is not always a good choice\" <Paper corpusId=\"388785\" paperTitle=\"(Fan et al., 2017)\" isShortName></Paper>. By carefully structuring the learning process, curriculum approaches aim to improve both the efficiency of training and the final performance of language models.", "citations": [{"id": "(Fan et al., 2017)", "paper": {"corpus_id": 388785, "title": "Learning What Data to Learn", "year": 2017, "venue": "arXiv.org", "authors": [{"name": "Yang Fan", "authorId": "144566102"}, {"name": "Fei Tian", "authorId": "143853336"}, {"name": "Tao Qin", "authorId": "143826491"}, {"name": "Jiang Bian", "authorId": "152441498"}, {"name": "Tie-Yan Liu", "authorId": "2110264337"}], "n_citations": 79}, "snippets": ["Training data plays a critical role in machine learning. The data selection strategy along the training process could significantly impact the performance of the learned model. For example, an appropriate strategy of removing redundant data can lead to a better model by using less computational efforts. For another example, previous studies on curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010) reveal some principles of how tailoring data based on its 'hardness' can favor the model training; that is, easy data instances are important at the early age of model training, while at later age, harder training examples tend to be more effective to improve the model, since easy ones bring minor changes.\n\nThese facts reveal that how to feed training samples into a machine learning system is nontrivial and feeding data in a totally random order is not always a good choice. To explore a better data selection strategy for training, previous works including curriculum learning (CL) and self-paced learning (SPL) adopt simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010;Jiang et al., 2014a)."], "score": 0.88134765625}, {"id": "(Zhang et al., 2019)", "paper": {"corpus_id": 155089817, "title": "Curriculum Learning for Domain Adaptation in Neural Machine Translation", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Xuan Zhang", "authorId": "49469742"}, {"name": "Pamela Shapiro", "authorId": "38581144"}, {"name": "Manish Kumar", "authorId": "48387892"}, {"name": "Paul McNamee", "authorId": "145324163"}, {"name": "Marine Carpuat", "authorId": "2954727"}, {"name": "Kevin Duh", "authorId": "1800354"}], "n_citations": 124}, "snippets": ["We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule."], "score": 0.912109375}, {"id": "(Mekala et al., 2024)", "paper": {"corpus_id": 267740312, "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Dheeraj Mekala", "authorId": "7565696"}, {"name": "Alex Nguyen", "authorId": "2284673632"}, {"name": "Jingbo Shang", "authorId": "2284595153"}], "n_citations": 21}, "snippets": ["In this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model."], "score": 0.9140625}, {"id": "(Mekala et al., 2022)", "paper": {"corpus_id": 249060677, "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Dheeraj Mekala", "authorId": "7565696"}, {"name": "Chengyu Dong", "authorId": "2113540861"}, {"name": "Jingbo Shang", "authorId": "2884976"}], "n_citations": 20}, "snippets": ["Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."], "score": 0.0}, {"id": "(Deng et al., 2023)", "paper": {"corpus_id": 261048902, "title": "Towards Accelerated Model Training via Bayesian Data Selection", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zhijie Deng", "authorId": "145114723"}, {"name": "Peng Cui", "authorId": "2153522384"}, {"name": "Jun Zhu", "authorId": "145254043"}], "n_citations": 5}, "snippets": ["Curriculum learning, as advocated by (Bengio et al., 2009), prioritizes easy points with low label noise before uniformly training on all data points. While this strategy enhances convergence, it fails to address the issue of skipping redundant points already learned."], "score": 0.7568359375}, {"id": "(Iter et al., 2021)", "paper": {"corpus_id": 237581256, "title": "The Trade-offs of Domain Adaptation for Neural Language Models", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Dan Iter", "authorId": "3310951"}, {"name": "David Grangier", "authorId": "2529182"}], "n_citations": 22}, "snippets": ["Dynamic selection (Wees et al., 2017) proposes to increase the selection threshold \u03c4 t as training progresses, gradually transitioning from generic to in-domain training. This gradual adaptation of neural network is related to curriculum learning (Bengio et al., 2009) which studies the ordering of examples and tasks during model training."], "score": 0.9013671875}, {"id": "(Wees et al., 2017)", "paper": {"corpus_id": 7921428, "title": "Dynamic Data Selection for Neural Machine Translation", "year": 2017, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "M. V. D. Wees", "authorId": "3110566"}, {"name": "Arianna Bisazza", "authorId": "3242253"}, {"name": "Christof Monz", "authorId": "1696402"}], "n_citations": 154}, "snippets": ["Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce \u2018dynamic data selection\u2019 for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call \u2018gradual fine-tuning\u2019, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline."], "score": 0.0}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 267095066, "title": "Orion-14B: Open-source Multilingual Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Du Chen", "authorId": "2280380985"}, {"name": "Yi Huang", "authorId": "2280373513"}, {"name": "Xiaopu Li", "authorId": "2280377400"}, {"name": "Yongqiang Li", "authorId": "2280380419"}, {"name": "Yongqiang Liu", "authorId": "2280377865"}, {"name": "Haihui Pan", "authorId": "1557313084"}, {"name": "Leichao Xu", "authorId": "2280411885"}, {"name": "Dacheng Zhang", "authorId": "2109546143"}, {"name": "Zhipeng Zhang", "authorId": "2280368716"}, {"name": "Kun Han", "authorId": "2281415463"}], "n_citations": 4}, "snippets": ["Considering that humans acquire knowledge in a deliberate order (Evanson et al., 2023), it is plausible that language models might also benefit from a structured learning order when processing training data. Curriculum learning (Bengio et al., 2009) has been suggested as a method to organize the learning process by progressively increasing the complexity of the training data. However, most prior studies have concentrated on sample-level data and smaller datasets. Chen et al. ( 2023) employed a skills-based framework for training data selection and continuous pretraining with a 3B-parameter language model. This approach achieved greater accuracy compared to the baseline method of uniform data source sampling, suggesting the potential efficacy of strategic data scheduling.\n\nIn training the Orion-14B model, we intentionally develop a data scheduling strategy that organizes training data to incrementally increase its complexity. We divide the training data into several stages based on the data sources and their complexity. These stages are differentiated by the mix ratios of data sources. Initial stages primarily include data with common knowledge, such as web pages and news articles. In the subsequent stages, we gradually augment the proportion of data containing more complex knowledge, including textbooks, academic papers, and source code."], "score": 0.83740234375}, {"id": "(Pillai et al., 2020)", "paper": {"corpus_id": 226964591, "title": "Sampling Approach Matters: Active Learning for Robotic Language Acquisition", "year": 2020, "venue": "2020 IEEE International Conference on Big Data (Big Data)", "authors": [{"name": "Nisha Pillai", "authorId": "144339038"}, {"name": "Edward Raff", "authorId": "34885007"}, {"name": "Francis Ferraro", "authorId": "2064957151"}, {"name": "Cynthia Matuszek", "authorId": "2674440"}], "n_citations": 3}, "snippets": ["Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora."], "score": 0.72509765625}], "table": null}, {"title": "Implementation Approaches", "tldr": "Curriculum learning implementations range from uncertainty-based methods in machine translation to sophisticated data selection strategies for large language models. Approaches include clustering-based selection, hard-to-easy strategies, ordered skill learning, and dynamic reweighting based on model competence and data difficulty. (16 sources)", "text": "\nImplementing curriculum learning in language models requires designing effective mechanisms for data selection and training scheduling. One prominent approach is uncertainty-based curriculum learning for neural machine translation, which uses uncertainty at both the data and model levels to establish easy-to-hard ordering and determine when to introduce more difficult samples <Paper corpusId=\"231709290\" paperTitle=\"(Soviany et al., 2021)\" isShortName></Paper> <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>. This method leverages cross-entropy as a measure of data difficulty and exploits the variance of weight distributions to represent model uncertainty, resulting in improved translation quality and convergence speed <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>.\n\nFor text classification tasks, cold-start data selection strategies have been developed to optimize the fine-tuning of pre-trained language models under limited labeling budgets <Paper corpusId=\"252280753\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>. These strategies include using pre-training loss as a proxy for classification uncertainty <Paper corpusId=\"224724415\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper> and K-means clustering to select diverse and representative training instances <Paper corpusId=\"235755110\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>. The TypiClust approach, for example, is specifically designed for low-budget scenarios, selecting typical examples when few labeled examples are available <Paper corpusId=\"246634642\" paperTitle=\"(Hacohen et al., 2022)\" isShortName></Paper>.\n\nIn non-autoregressive translation, researchers have introduced a hard-to-easy learning strategy that combines pretrained models with distilled data <Paper corpusId=\"257901096\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This approach gradually decreases the ratio of raw data in the training process, building on insights that pretraining with raw data can improve performance by rejuvenating low-frequency words <Paper corpusId=\"229923128\" paperTitle=\"(Ding et al., 2020)\" isShortName></Paper>. Similar masked sequence-to-sequence models have been developed with n-gram loss functions to address specific translation challenges <Paper corpusId=\"220046693\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\nFor large language models, more sophisticated frameworks have emerged based on the hypothesis that models, like humans, acquire interdependent skills in a natural order <Paper corpusId=\"260203057\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. The Skill-It algorithm implements this insight by sampling data based on prerequisite skills, showing significant improvements in both continual pre-training and fine-tuning scenarios <Paper corpusId=\"260203057\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"276250237\" paperTitle=\"(Sow et al., 2025)\" isShortName></Paper>. Other curriculum learning approaches for LLMs structure training data using criteria such as prompt length, attention scores, and loss values to progress from simpler to more complex tasks <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>.\n\nRecent research has also explored explicit curriculum design for pretraining data selection, including methods based on decreasing gradient norms, least certainty, and increasing expertise <Paper corpusId=\"270371045\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper>. The QuRating method, for example, uses language models to rate pretraining data based on writing style, required expertise, factual content, and educational value, creating a training curriculum that improves performance without changing the training dataset <Paper corpusId=\"267681974\" paperTitle=\"(Wettig et al., 2024)\" isShortName></Paper>. Similarly, PRESENCE jointly reweights samples using self-influence scores as indicators of sample importance <Paper corpusId=\"264935129\" paperTitle=\"(Thakkar et al., 2023)\" isShortName></Paper>.\n\nAt a more fundamental level, implementing curriculum learning requires quantitative evaluation of each datapoint and an elaborated sampling mechanism to select the most informative subset of data within a given budget <Paper corpusId=\"271710435\" paperTitle=\"(Qin et al., 2024)\" isShortName></Paper>. The expected benefits include noise reduction by ignoring mislabeled data, rebalancing data distributions by adjusting the sampling of easy versus hard examples, and expediting training for more efficient model iterations <Paper corpusId=\"271710435\" paperTitle=\"(Qin et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Soviany et al., 2021)", "paper": {"corpus_id": 231709290, "title": "Curriculum Learning: A Survey", "year": 2021, "venue": "International Journal of Computer Vision", "authors": [{"name": "Petru Soviany", "authorId": "40901288"}, {"name": "Radu Tudor Ionescu", "authorId": "1817759"}, {"name": "Paolo Rota", "authorId": "39337007"}, {"name": "N. Sebe", "authorId": "1703601"}], "n_citations": 359}, "snippets": ["(Zhou et al., 2020) introduce an uncertainty-based curriculum batching approach for neural machine translation. They propose using uncertainty at the data level, for establishing the easy-to-hard ordering, and the model level, to decide the right moment to enhance the training set with more difficult samples. To measure the difficulty of the examples, they start from the intuition that samples with higher crossentropy and uncertainty are more difficult to translate."], "score": 0.8095703125}, {"id": "(Zhou et al., 2020)", "paper": {"corpus_id": 220047761, "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yikai Zhou", "authorId": "2110349143"}, {"name": "Baosong Yang", "authorId": "21299583"}, {"name": "Derek F. Wong", "authorId": "1758353"}, {"name": "Yu Wan", "authorId": "153379180"}, {"name": "Lidia S. Chao", "authorId": "1774304"}], "n_citations": 96}, "snippets": ["Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."], "score": 0.0}, {"id": "(Yu et al., 2023)", "paper": {"corpus_id": 252280753, "title": "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yue Yu", "authorId": "1633124736"}, {"name": "Rongzhi Zhang", "authorId": "46752897"}, {"name": "Ran Xu", "authorId": "2115801998"}, {"name": "Jieyu Zhang", "authorId": "47540245"}, {"name": "Jiaming Shen", "authorId": "3363642"}, {"name": "Chao Zhang", "authorId": "145657504"}], "n_citations": 23}, "snippets": ["We study cold-start data selection for text classification with c classes formulated as follows: Given a pool of unlabeled samples D u = {x j } U j=1 and an empty training set D l = \u2205, we aim to fine-tune a pre-trained language model M denoted as f (\u2022; \u03b8) under limited labeling budget |B| interactively: In each round, we use an acquisition function F(\u2022) to query b samples denoted as Q from D u . Next, the acquired samples are labeled and moved from D u to D l . Then we fine-tune the pre-trained language model f (\u2022; \u03b8) with D l to maximize the performance on downstream classification tasks. The above steps can either be one-round (Chang et al., 2021)(Hacohen et al., 2022)) (b = |B| in this case) or repeated for multiple rounds (Yuan et al., 2020) (b = |B|/|Rounds|) until reaching the budget |B|."], "score": 0.8447265625}, {"id": "(Yuan et al., 2020)", "paper": {"corpus_id": 224724415, "title": "Cold-start Active Learning through Self-Supervised Language Modeling", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Michelle Yuan", "authorId": "39879969"}, {"name": "Hsuan-Tien Lin", "authorId": "1798966"}, {"name": "Jordan L. Boyd-Graber", "authorId": "1389036863"}], "n_citations": 184}, "snippets": ["Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time."], "score": 0.0}, {"id": "(Chang et al., 2021)", "paper": {"corpus_id": 235755110, "title": "On Training Instance Selection for Few-Shot Neural Text Generation", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ernie Chang", "authorId": "48025720"}, {"name": "Xiaoyu Shen", "authorId": "2562211"}, {"name": "Hui-Syuan Yeh", "authorId": "2047999043"}, {"name": "Vera Demberg", "authorId": "2869436"}], "n_citations": 41}, "snippets": ["Large-scale pretrained language models have led to dramatic improvements in text generation. Impressive performance can be achieved by finetuning only on a small number of instances (few-shot setting). Nonetheless, almost all previous work simply applies random sampling to select the few-shot training instances. Little to no attention has been paid to the selection strategies and how they would affect model performance. In this work, we present a study on training instance selection in few-shot neural text generation. The selection decision is made based only on the unlabeled data so as to identify the most worthwhile data points that should be annotated under some budget of labeling cost. Based on the intuition that the few-shot training instances should be diverse and representative of the entire data distribution, we propose a simple selection strategy with K-means clustering. We show that even with the naive clustering-based approach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. The code and training data are made available. We hope that this work will call for more attention on this largely unexplored area."], "score": 0.0}, {"id": "(Hacohen et al., 2022)", "paper": {"corpus_id": 246634642, "title": "Active Learning on a Budget: Opposite Strategies Suit High and Low Budgets", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Guy Hacohen", "authorId": "94064232"}, {"name": "Avihu Dekel", "authorId": "2153469082"}, {"name": "D. Weinshall", "authorId": "1789171"}], "n_citations": 123}, "snippets": ["Investigating active learning, we focus on the relation between the number of labeled examples (budget size), and suitable querying strategies. Our theoretical analysis shows a behavior reminiscent of phase transition: typical examples are best queried when the budget is low, while unrepresentative examples are best queried when the budget is large. Combined evidence shows that a similar phenomenon occurs in common classification models. Accordingly, we propose TypiClust -- a deep active learning strategy suited for low budgets. In a comparative empirical investigation of supervised learning, using a variety of architectures and image datasets, TypiClust outperforms all other active learning strategies in the low-budget regime. Using TypiClust in the semi-supervised framework, performance gets an even more significant boost. In particular, state-of-the-art semi-supervised methods trained on CIFAR-10 with 10 labeled examples selected by TypiClust, reach 93.2% accuracy -- an improvement of 39.4% over random selection. Code is available at https://github.com/avihu111/TypiClust."], "score": 0.0}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 257901096, "title": "Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Min Liu", "authorId": "2226017688"}, {"name": "Yu Bao", "authorId": "145854784"}, {"name": "Chengqi Zhao", "authorId": "144562857"}, {"name": "Shujian Huang", "authorId": "2124946880"}], "n_citations": 3}, "snippets": ["Motivated by the success of curriculum learning (Qian et al., 2020)(Guo et al., 2020)Liu et al. 2020), we further introduce a hard-to-easy learning strategy to improve the performance. (Ding et al., 2020) show that pretraining with raw data can improve the performance of NAT by rejuvenating low-frequency words. To keep the merits of low-mode, they further trained the pretrained model on distilled data. We combine this idea with our data selection method by decreasing the ratio of raw data in the training process."], "score": 0.71142578125}, {"id": "(Ding et al., 2020)", "paper": {"corpus_id": 229923128, "title": "Understanding and Improving Lexical Choice in Non-Autoregressive Translation", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "Liang Ding", "authorId": "46573238"}, {"name": "Longyue Wang", "authorId": "1800190"}, {"name": "Xuebo Liu", "authorId": "1390611971"}, {"name": "Derek F. Wong", "authorId": "1758353"}, {"name": "D. Tao", "authorId": "143719920"}, {"name": "Zhaopeng Tu", "authorId": "2909321"}], "n_citations": 77}, "snippets": ["Knowledge distillation (KD) is essential for training non-autoregressive translation (NAT) models by reducing the complexity of the raw data with an autoregressive teacher model. In this study, we empirically show that as a side effect of this training, the lexical choice errors on low-frequency words are propagated to the NAT model from the teacher model. To alleviate this problem, we propose to expose the raw data to NAT models to restore the useful information of low-frequency words, which are missed in the distilled data. To this end, we introduce an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Experimental results across language pairs and model architectures demonstrate the effectiveness and universality of the proposed approach. Extensive analyses confirm our claim that our approach improves performance by reducing the lexical choice errors on low-frequency words. Encouragingly, our approach pushes the SOTA NAT performance on the WMT14 English-German and WMT16 Romanian-English datasets up to 27.8 and 33.8 BLEU points, respectively. The source code will be released."], "score": 0.0}, {"id": "(Guo et al., 2020)", "paper": {"corpus_id": 220046693, "title": "Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Junliang Guo", "authorId": "13838086"}, {"name": "Linli Xu", "authorId": "2230211"}, {"name": "Enhong Chen", "authorId": "2227868312"}], "n_citations": 79}, "snippets": ["The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model."], "score": 0.0}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 260203057, "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Mayee F. Chen", "authorId": "48622329"}, {"name": "Nicholas Roberts", "authorId": "2069037355"}, {"name": "K. Bhatia", "authorId": "144383716"}, {"name": "Jue Wang", "authorId": "39597242"}, {"name": "Ce Zhang", "authorId": "1776014"}, {"name": "Frederic Sala", "authorId": "2186982588"}, {"name": "Christopher R\u00e9", "authorId": "1803218"}], "n_citations": 65}, "snippets": ["The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training."], "score": 0.916015625}, {"id": "(Sow et al., 2025)", "paper": {"corpus_id": 276250237, "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining", "year": 2025, "venue": "International Conference on Learning Representations", "authors": [{"name": "Daouda Sow", "authorId": "143827145"}, {"name": "Herbert Woisetschl\u00e4ger", "authorId": "2220057323"}, {"name": "Saikiran Bulusu", "authorId": "35693339"}, {"name": "Shiqiang Wang", "authorId": "2255363698"}, {"name": "Hans-Arno Jacobsen", "authorId": "2254179381"}, {"name": "Yingbin Liang", "authorId": "2344954540"}], "n_citations": 6}, "snippets": ["Training Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023;(Chen et al., 2023)Fan et al., 2023;Thakkar et al., 2023) have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, Xie et al. ( 2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. (Chen et al., 2023) introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values", ".The approach in Fan & Jaggi (2023) offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE."], "score": 0.81494140625}, {"id": "(Kim et al., 2024)", "paper": {"corpus_id": 269756933, "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jisu Kim", "authorId": "2301165169"}, {"name": "Juhwan Lee", "authorId": "2301167177"}], "n_citations": 10}, "snippets": ["The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data."], "score": 0.67236328125}, {"id": "(Yu et al., 2024)", "paper": {"corpus_id": 270371045, "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zichun Yu", "authorId": "2275526493"}, {"name": "Spandan Das", "authorId": "2305649905"}, {"name": "Chenyan Xiong", "authorId": "2275170116"}], "n_citations": 37}, "snippets": ["On the other hand, many researchers have proposed curriculum learning strategies that dynamically adjust the data distribution in the pretraining [11; 24; 47; 51; 56; 62]. ELECTRA-style models (208229926) incorporated curriculum learning in their pretraining process by synchronously training the model with an auxiliary generator, which provided more and more difficult training signals for the discriminator. This implicit curriculum significantly improved the pretraining efficiency on denoising language models [4; 11; 19; 39; 65]. Other methods have explicitly designed the curriculum for pretraining data selection, such as decreasing gradient norms (Thakkar et al., 2023), least certainty [24; 51], and increasing expertise (Wettig et al., 2024), demonstrating the benefits of adapting the pretraining data signal according to the model's ever-changing preferences."], "score": 0.8916015625}, {"id": "(Wettig et al., 2024)", "paper": {"corpus_id": 267681974, "title": "QuRating: Selecting High-Quality Data for Training Language Models", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alexander Wettig", "authorId": "2127066887"}, {"name": "Aatmik Gupta", "authorId": "2284268826"}, {"name": "Saumya Malik", "authorId": "2323513320"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 79}, "snippets": ["Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications."], "score": 0.94970703125}, {"id": "(Thakkar et al., 2023)", "paper": {"corpus_id": 264935129, "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Megh Thakkar", "authorId": "2264977662"}, {"name": "Tolga Bolukbasi", "authorId": "2843215"}, {"name": "Sriram Ganapathy", "authorId": "1726355"}, {"name": "Shikhar Vashishth", "authorId": "3404827"}, {"name": "Sarath Chandar", "authorId": "123607932"}, {"name": "P. Talukdar", "authorId": "2408872"}], "n_citations": 26}, "snippets": ["Language Models (LMs) pre-trained with self-supervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pre-training data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in the research direction of sample reweighting for pre-training language models."], "score": 0.0}, {"id": "(Qin et al., 2024)", "paper": {"corpus_id": 271710435, "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Yulei Qin", "authorId": "2267903811"}, {"name": "Yuncheng Yang", "authorId": "2184288111"}, {"name": "Pengcheng Guo", "authorId": "2314919460"}, {"name": "Gang Li", "authorId": "2315086401"}, {"name": "Hang Shao", "authorId": "2314834176"}, {"name": "Yuchen Shi", "authorId": "2315154473"}, {"name": "Zihan Xu", "authorId": "2267376632"}, {"name": "Yun Gu", "authorId": "2258793023"}, {"name": "Ke Li", "authorId": "2257346950"}, {"name": "Xing Sun", "authorId": "2314886050"}], "n_citations": 13}, "snippets": ["We aim at finding the most informative subset S b \u2282 S from the entire set S under the given budget |S b | \u2264 b. Mathematically, the selection of S b requires the quantitative evaluation q(\u2022) on each datapoint x i and an elaborated sampling mechanism \u03c0: \n\nwhere \u03c0(\u2022, b) denotes the sampling process with a maximum b datapoints. With respect to the detailed implementation of \u03c0, either an iterative, greedy algorithm or a batch-wise heuristic rule can be adopted for compatibility with q(\u2022). The expected benefits of such selection include: 1) the reduction of noise by ignoring those mislabeled, mismatched instruction-response pairs, 2) the re-balance of data distributions by down-sampling those easy, common, and similar examples while up-sampling hard, rare ones, and 3) the expedition of training in return for efficient iterations of LLMs."], "score": 0.943359375}], "table": null}, {"title": "Difficulty Measurement Techniques", "tldr": "Difficulty measurement in curriculum learning employs various metrics to determine the complexity of training examples, from cross-entropy and uncertainty scores to learning percentages and model competence indicators. These techniques enable effective implementation of easy-to-hard training progressions by quantifying sample difficulty from both data-centric and model-centric perspectives. (8 sources)", "text": "\nMeasuring the difficulty of training examples is a fundamental component of curriculum learning, providing the basis for organizing training data in meaningful sequences. One prominent approach uses uncertainty at the data level to establish easy-to-hard ordering, with higher cross-entropy and uncertainty values indicating more difficult samples to translate <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper> <Paper corpusId=\"231709290\" paperTitle=\"(Soviany et al., 2021)\" isShortName></Paper>. This uncertainty-based difficulty assessment has proven particularly effective in neural machine translation, where it helps determine when to introduce more challenging examples based on the model's current capabilities.\n\nFrom a model-centric perspective, researchers have developed metrics that measure sample difficulty based on how the model itself interacts with the data. The learning percentage approach evaluates how quickly a model learns from different examples, with samples that yield more learning in earlier epochs being classified as easier <Paper corpusId=\"267740312\" paperTitle=\"(Mekala et al., 2024)\" isShortName></Paper>. This self-ranking methodology draws inspiration from the learning order metric that observes the memorization patterns of neural networks, which typically learn clean examples before noisy ones <Paper corpusId=\"249060677\" paperTitle=\"(Mekala et al., 2022)\" isShortName></Paper>.\n\nMore sophisticated difficulty measurement techniques incorporate multiple criteria to create structured training progressions. Some approaches use prompt length, attention scores, and loss values to determine the complexity of training data, enabling curriculum-based training that progresses from simpler to more complex tasks <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. These multi-dimensional difficulty metrics provide a more nuanced assessment of sample complexity than single-criterion approaches.\n\nThe difficulty measurement process is critical because \"feeding data in a totally random order is not always a good choice\" <Paper corpusId=\"388785\" paperTitle=\"(Fan et al., 2017)\" isShortName></Paper>. Effective difficulty metrics enable curriculum learning to distinguish between \"easy\" examples that benefit early training stages and \"harder\" examples that become more valuable as training progresses. This principle aligns with observations from curriculum learning and self-paced learning, where the appropriate \"hardness\" of training data changes throughout the model's development <Paper corpusId=\"388785\" paperTitle=\"(Fan et al., 2017)\" isShortName></Paper>.\n\nAs the field advances, difficulty measurement continues to evolve beyond simple heuristic rules, with researchers developing increasingly sophisticated techniques to quantify training example complexity. These measures form the foundation for the second key component of curriculum learning\u2014training schedulers that determine when and how different examples are presented to the model <Paper corpusId=\"232362223\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper> <Paper corpusId=\"265506572\" paperTitle=\"(Chobey et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Zhou et al., 2020)", "paper": {"corpus_id": 220047761, "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yikai Zhou", "authorId": "2110349143"}, {"name": "Baosong Yang", "authorId": "21299583"}, {"name": "Derek F. Wong", "authorId": "1758353"}, {"name": "Yu Wan", "authorId": "153379180"}, {"name": "Lidia S. Chao", "authorId": "1774304"}], "n_citations": 96}, "snippets": ["Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."], "score": 0.0}, {"id": "(Soviany et al., 2021)", "paper": {"corpus_id": 231709290, "title": "Curriculum Learning: A Survey", "year": 2021, "venue": "International Journal of Computer Vision", "authors": [{"name": "Petru Soviany", "authorId": "40901288"}, {"name": "Radu Tudor Ionescu", "authorId": "1817759"}, {"name": "Paolo Rota", "authorId": "39337007"}, {"name": "N. Sebe", "authorId": "1703601"}], "n_citations": 359}, "snippets": ["(Zhou et al., 2020) introduce an uncertainty-based curriculum batching approach for neural machine translation. They propose using uncertainty at the data level, for establishing the easy-to-hard ordering, and the model level, to decide the right moment to enhance the training set with more difficult samples. To measure the difficulty of the examples, they start from the intuition that samples with higher crossentropy and uncertainty are more difficult to translate."], "score": 0.8095703125}, {"id": "(Mekala et al., 2024)", "paper": {"corpus_id": 267740312, "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Dheeraj Mekala", "authorId": "7565696"}, {"name": "Alex Nguyen", "authorId": "2284673632"}, {"name": "Jingbo Shang", "authorId": "2284595153"}], "n_citations": 21}, "snippets": ["In this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model."], "score": 0.9140625}, {"id": "(Mekala et al., 2022)", "paper": {"corpus_id": 249060677, "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Dheeraj Mekala", "authorId": "7565696"}, {"name": "Chengyu Dong", "authorId": "2113540861"}, {"name": "Jingbo Shang", "authorId": "2884976"}], "n_citations": 20}, "snippets": ["Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."], "score": 0.0}, {"id": "(Kim et al., 2024)", "paper": {"corpus_id": 269756933, "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jisu Kim", "authorId": "2301165169"}, {"name": "Juhwan Lee", "authorId": "2301167177"}], "n_citations": 10}, "snippets": ["The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data."], "score": 0.67236328125}, {"id": "(Fan et al., 2017)", "paper": {"corpus_id": 388785, "title": "Learning What Data to Learn", "year": 2017, "venue": "arXiv.org", "authors": [{"name": "Yang Fan", "authorId": "144566102"}, {"name": "Fei Tian", "authorId": "143853336"}, {"name": "Tao Qin", "authorId": "143826491"}, {"name": "Jiang Bian", "authorId": "152441498"}, {"name": "Tie-Yan Liu", "authorId": "2110264337"}], "n_citations": 79}, "snippets": ["Training data plays a critical role in machine learning. The data selection strategy along the training process could significantly impact the performance of the learned model. For example, an appropriate strategy of removing redundant data can lead to a better model by using less computational efforts. For another example, previous studies on curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010) reveal some principles of how tailoring data based on its 'hardness' can favor the model training; that is, easy data instances are important at the early age of model training, while at later age, harder training examples tend to be more effective to improve the model, since easy ones bring minor changes.\n\nThese facts reveal that how to feed training samples into a machine learning system is nontrivial and feeding data in a totally random order is not always a good choice. To explore a better data selection strategy for training, previous works including curriculum learning (CL) and self-paced learning (SPL) adopt simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010;Jiang et al., 2014a)."], "score": 0.88134765625}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 232362223, "title": "A Survey on Curriculum Learning", "year": 2021, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Xin Wang", "authorId": "2153687490"}, {"name": "Yudong Chen", "authorId": "51310474"}, {"name": "Wenwu Zhu", "authorId": "145583986"}], "n_citations": 611}, "snippets": ["Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=\"LaTeX\">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3069908.gif\"/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations."], "score": 0.0}, {"id": "(Chobey et al., 2023)", "paper": {"corpus_id": 265506572, "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?", "year": 2023, "venue": "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning", "authors": [{"name": "Aryaman Chobey", "authorId": "2268760204"}, {"name": "Oliver Smith", "authorId": "2268760018"}, {"name": "Anzi Wang", "authorId": "2268796061"}, {"name": "Grusha Prasad", "authorId": "2268760229"}], "n_citations": 5}, "snippets": ["Curriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see (Soviany et al., 2021); (Wang et al., 2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021)."], "score": 0.69384765625}], "table": null}, {"title": "Benefits of Curriculum Learning", "tldr": "Curriculum learning offers multiple benefits for language model training, including improved model performance, faster convergence, and greater data efficiency. These advantages stem from the strategic sequencing of training examples that aligns with how models naturally learn. (10 sources)", "text": "\nCurriculum learning provides several key benefits for training language models:\n\n* **Improved Model Performance**: By presenting training data in an easy-to-hard sequence, curriculum learning has been shown to improve performance across a variety of NLP tasks, particularly in machine translation <Paper corpusId=\"227905455\" paperTitle=\"(Laverghetta et al., 2020)\" isShortName></Paper>. This structured approach can lead to better final model accuracy compared to random data presentation <Paper corpusId=\"247762191\" paperTitle=\"(Mohiuddin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"102350936\" paperTitle=\"(Hacohen et al., 2019)\" isShortName></Paper>.\n\n* **Faster Convergence**: Curriculum learning accelerates the training process by enabling models to learn more efficiently. Studies have demonstrated that carefully maneuvering the sequence of training data can improve both training efficiency and model accuracy <Paper corpusId=\"247762191\" paperTitle=\"(Mohiuddin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>.\n\n* **Enhanced Data Efficiency**: Curriculum approaches allow models to learn effectively from smaller datasets, making training more computationally efficient. Ordering the selection of training data using active learning principles can lead to improvements in learning efficiently from smaller corpora <Paper corpusId=\"226964591\" paperTitle=\"(Pillai et al., 2020)\" isShortName></Paper>.\n\n* **Reduced Computational Resources**: For large language models that require significant computational power, curriculum learning offers a way to achieve comparable or better results with less training data. This is particularly valuable given the challenges posed by LLMs in terms of computational resources <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>.\n\n* **Elimination of Redundant Data**: Curriculum approaches can help identify and remove redundant training examples, leading to a better model with less computational effort <Paper corpusId=\"388785\" paperTitle=\"(Fan et al., 2017)\" isShortName></Paper>.\n\n* **Knowledge Transfer Between Models**: Curriculum learning can facilitate knowledge transfer between models of different sizes. The SmallToLarge (S2L) method leverages training trajectories from small models to guide data selection for larger models, making supervised fine-tuning more efficient <Paper corpusId=\"268363364\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n* **Optimized Sample Combinations**: Beyond evaluating individual sample quality, curriculum approaches can address the combinatorial effects among samples, preventing issues that arise from intrinsic homogeneity or contradictions in the training data <Paper corpusId=\"271064746\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\n* **Adaptive Learning**: Unlike fixed training regimes, curriculum learning recognizes that the appropriate \"hardness\" of training data changes throughout a model's development. Easy examples benefit early training stages, while harder examples become more valuable as training progresses <Paper corpusId=\"388785\" paperTitle=\"(Fan et al., 2017)\" isShortName></Paper> <Paper corpusId=\"10891229\" paperTitle=\"(Jiang et al., 2015)\" isShortName></Paper>.", "citations": [{"id": "(Laverghetta et al., 2020)", "paper": {"corpus_id": 227905455, "title": "Towards a Task-Agnostic Model of Difficulty Estimation for Supervised Learning Tasks", "year": 2020, "venue": "AACL", "authors": [{"name": "Antonio Laverghetta", "authorId": "2033736232"}, {"name": "Jamshidbek Mirzakhalov", "authorId": "1716200134"}, {"name": "John Licato", "authorId": "2143879"}], "n_citations": 2}, "snippets": ["Curriculum learning (Elman, 1993), a strategy where the model is trained on easier examples before harder ones, has recently been shown to improve performance and reduce training time on a variety of NLP tasks, especially machine translation (Liu et al., 2020;(Wang et al., 2019)(Zhou et al., 2020)(Xu et al., 2020). \n\nThe success of this research shows that the order in which training data is presented to a model is important, but how to best apply curriculum learning broadly to NLP and how to design effective measures of difficulty to create curricula remain relatively unexplored."], "score": 0.76953125}, {"id": "(Mohiuddin et al., 2022)", "paper": {"corpus_id": 247762191, "title": "Data Selection Curriculum for Neural Machine Translation", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Tasnim Mohiuddin", "authorId": "6838342"}, {"name": "Philipp Koehn", "authorId": "1755162"}, {"name": "Vishrav Chaudhary", "authorId": "113810201"}, {"name": "James Cross", "authorId": "2059363961"}, {"name": "Shruti Bhosale", "authorId": "2116473"}, {"name": "Shafiq R. Joty", "authorId": "2708940"}], "n_citations": 13}, "snippets": ["Curriculum Learning Inspired by human learners, (Elman, 1993) argues that optimization of neural network training can be accelerated by gradually increasing the difficulty of the concepts. (Bengio et al., 2009) were the first to use the term \"curricu-lum learning\" to refer to the easy-to-hard training strategies in the context of machine learning. Using an easy-to-hard curriculum based on increasing vocabulary size in language model training, they achieved performance improvement. Recent work (Jiang et al., 2015)(Hacohen et al., 2019)(Zhou et al., 2020) shows that manoeuvring the sequence of training data can improve both training efficiency and model accuracy."], "score": 0.71240234375}, {"id": "(Hacohen et al., 2019)", "paper": {"corpus_id": 102350936, "title": "On The Power of Curriculum Learning in Training Deep Networks", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Guy Hacohen", "authorId": "94064232"}, {"name": "D. Weinshall", "authorId": "1789171"}], "n_citations": 449}, "snippets": ["Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher\" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function."], "score": 0.0}, {"id": "(Zhou et al., 2020)", "paper": {"corpus_id": 220047761, "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yikai Zhou", "authorId": "2110349143"}, {"name": "Baosong Yang", "authorId": "21299583"}, {"name": "Derek F. Wong", "authorId": "1758353"}, {"name": "Yu Wan", "authorId": "153379180"}, {"name": "Lidia S. Chao", "authorId": "1774304"}], "n_citations": 96}, "snippets": ["Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."], "score": 0.0}, {"id": "(Pillai et al., 2020)", "paper": {"corpus_id": 226964591, "title": "Sampling Approach Matters: Active Learning for Robotic Language Acquisition", "year": 2020, "venue": "2020 IEEE International Conference on Big Data (Big Data)", "authors": [{"name": "Nisha Pillai", "authorId": "144339038"}, {"name": "Edward Raff", "authorId": "34885007"}, {"name": "Francis Ferraro", "authorId": "2064957151"}, {"name": "Cynthia Matuszek", "authorId": "2674440"}], "n_citations": 3}, "snippets": ["Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora."], "score": 0.72509765625}, {"id": "(Kim et al., 2024)", "paper": {"corpus_id": 269756933, "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jisu Kim", "authorId": "2301165169"}, {"name": "Juhwan Lee", "authorId": "2301167177"}], "n_citations": 10}, "snippets": ["The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data."], "score": 0.67236328125}, {"id": "(Fan et al., 2017)", "paper": {"corpus_id": 388785, "title": "Learning What Data to Learn", "year": 2017, "venue": "arXiv.org", "authors": [{"name": "Yang Fan", "authorId": "144566102"}, {"name": "Fei Tian", "authorId": "143853336"}, {"name": "Tao Qin", "authorId": "143826491"}, {"name": "Jiang Bian", "authorId": "152441498"}, {"name": "Tie-Yan Liu", "authorId": "2110264337"}], "n_citations": 79}, "snippets": ["Training data plays a critical role in machine learning. The data selection strategy along the training process could significantly impact the performance of the learned model. For example, an appropriate strategy of removing redundant data can lead to a better model by using less computational efforts. For another example, previous studies on curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010) reveal some principles of how tailoring data based on its 'hardness' can favor the model training; that is, easy data instances are important at the early age of model training, while at later age, harder training examples tend to be more effective to improve the model, since easy ones bring minor changes.\n\nThese facts reveal that how to feed training samples into a machine learning system is nontrivial and feeding data in a totally random order is not always a good choice. To explore a better data selection strategy for training, previous works including curriculum learning (CL) and self-paced learning (SPL) adopt simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010;Jiang et al., 2014a)."], "score": 0.88134765625}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 268363364, "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yu Yang", "authorId": "2291083514"}, {"name": "Siddhartha Mishra", "authorId": "2290975932"}, {"name": "Jeffrey N Chiang", "authorId": "2290912016"}, {"name": "Baharan Mirzasoleiman", "authorId": "2389094"}], "n_citations": 23}, "snippets": ["Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models."], "score": 0.86669921875}, {"id": "(Yin et al., 2024)", "paper": {"corpus_id": 271064746, "title": "Entropy Law: The Story Behind Data Compression and LLM Performance", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mingjia Yin", "authorId": "2260649859"}, {"name": "Chuhan Wu", "authorId": "2276003321"}, {"name": "Yufei Wang", "authorId": "2283529247"}, {"name": "Hao Wang", "authorId": "2256768674"}, {"name": "Wei Guo", "authorId": "2260810851"}, {"name": "Yasheng Wang", "authorId": "2282544603"}, {"name": "Yong Liu", "authorId": "2293683997"}, {"name": "Ruiming Tang", "authorId": "2284295184"}, {"name": "Defu Lian", "authorId": "1862782"}, {"name": "Enhong Chen", "authorId": "2113754294"}], "n_citations": 27}, "snippets": ["Data is the cornerstone of large language models (LLMs), but not all data is useful for model learning. Carefully selected data can better elicit the capabilities of LLMs with much less computational overhead. Most methods concentrate on evaluating the quality of individual samples in data selection, while the combinatorial effects among samples are neglected. Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction."], "score": 0.85693359375}, {"id": "(Jiang et al., 2015)", "paper": {"corpus_id": 10891229, "title": "Self-Paced Curriculum Learning", "year": 2015, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Lu Jiang", "authorId": "39978626"}, {"name": "Deyu Meng", "authorId": "1803714"}, {"name": "Qian Zhao", "authorId": "46317290"}, {"name": "S. Shan", "authorId": "145455919"}, {"name": "Alexander Hauptmann", "authorId": "7661726"}], "n_citations": 526}, "snippets": ["Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training. The two methods share a similar conceptual learning paradigm, but differ in specific learning schemes. In CL, the curriculum is predetermined by prior knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner. In SPL, the curriculum is dynamically determined to adjust to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a unified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training. In comparison to human education, SPCL is analogous to \"instructor-student-collaborative\" learning mode, as opposed to \"instructor-driven\" in CL or \"student-driven\" in SPL. Empirically, we show that the advantage of SPCL on two tasks."], "score": 0.0}], "table": null}, {"title": "Applications in Language Models", "tldr": "Curriculum learning is applied across various language model tasks, from machine translation to large-scale pretraining. These applications demonstrate how strategic data sequencing enhances model performance through improved translation quality, more efficient fine-tuning, and better resource utilization. (19 sources)", "text": "\nCurriculum learning has found widespread applications across various language model tasks, most notably in machine translation. For non-autoregressive translation (NAT), researchers have implemented a hard-to-easy learning strategy that gradually decreases the ratio of raw data in the training process, building on evidence that pretraining with raw data can improve performance by rejuvenating low-frequency words <Paper corpusId=\"257901096\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"229923128\" paperTitle=\"(Ding et al., 2020)\" isShortName></Paper>. This approach has proven particularly effective for masked sequence-to-sequence models designed with n-gram loss functions to address specific translation challenges <Paper corpusId=\"220046693\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper> <Paper corpusId=\"221150562\" paperTitle=\"(Qian et al., 2020)\" isShortName></Paper>.\n\nFor large language model (LLM) pretraining, curriculum learning approaches have evolved beyond simple heuristics. Recent research has explored explicit curriculum design for pretraining data selection, including methods based on decreasing gradient norms, least certainty, and increasing expertise <Paper corpusId=\"270371045\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264935129\" paperTitle=\"(Thakkar et al., 2023)\" isShortName></Paper>. The QuRating method, for example, uses language models to rate pretraining data based on writing style, required expertise, factual content, and educational value, creating a training curriculum that improves performance without changing the training dataset <Paper corpusId=\"267681974\" paperTitle=\"(Wettig et al., 2024)\" isShortName></Paper>. Similarly, PRESENCE jointly reweights samples using self-influence scores as indicators of sample importance <Paper corpusId=\"264935129\" paperTitle=\"(Thakkar et al., 2023)\" isShortName></Paper>.\n\nIn the context of fine-tuning LLMs, curriculum learning has shown that high-quality data selection can dramatically reduce data requirements. LIMA demonstrated that by carefully curating just 1,000 high-quality data points, models can achieve remarkable performance that generalizes to unseen data <Paper corpusId=\"274130626\" paperTitle=\"(Khera et al., 2024)\" isShortName></Paper>. This aligns with findings from Zhou et al., who showed that almost all knowledge in large language models is learned during pretraining, with only limited instruction tuning data necessary to teach models to produce high-quality output <Paper corpusId=\"258822910\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n\nThe Skill-It algorithm implements curriculum learning by sampling data based on prerequisite skills, showing significant improvements in both continual pre-training and fine-tuning scenarios <Paper corpusId=\"260203057\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This approach is based on the hypothesis that models, like humans, acquire interdependent skills in a natural order <Paper corpusId=\"276250237\" paperTitle=\"(Sow et al., 2025)\" isShortName></Paper>. On the LEGO synthetic dataset in continual pre-training, Skill-It achieved 36.5 points higher accuracy than random sampling, while on the Natural Instructions dataset in fine-tuning, it reduced validation loss by 13.6% compared to training solely on target skill data <Paper corpusId=\"260203057\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\nTraditional online batch selection methods that select hard samples with high loss or high gradient norm often require a second forward/backward pass, introducing large computational costs that hurt scalability for large models <Paper corpusId=\"264439064\" paperTitle=\"(Fan et al., 2023)\" isShortName></Paper> <Paper corpusId=\"3663876\" paperTitle=\"(Katharopoulos et al., 2018)\" isShortName></Paper>. To address this challenge, researchers have explored group-level data selection approaches that consider samples in large groups with common characteristics, often derived from metadata such as web domains or source collection names <Paper corpusId=\"276575595\" paperTitle=\"(Belenki et al., 2025)\" isShortName></Paper>.\n\nIt's important to note that while data selection is crucial for effective training, simply filtering for examples that match human notions of data quality doesn't always improve model behavior. Research has found that selecting according to similarity with \"high-quality\" data sources may not increase (and can even hurt) performance compared to randomly selecting data <Paper corpusId=\"267094971\" paperTitle=\"(Engstrom et al., 2024)\" isShortName></Paper>. This underscores the complexity of curriculum design and the importance of empirical validation for any data selection strategy.\n\nThe efficiency gains from curriculum learning are particularly valuable for large language models, where the quality of training data significantly impacts the statistical efficiency of the training procedure and model performance <Paper corpusId=\"273233719\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"245124124\" paperTitle=\"(Du et al., 2021)\" isShortName></Paper> <Paper corpusId=\"247951931\" paperTitle=\"(Chowdhery et al., 2022)\" isShortName></Paper>. Recent approaches like DoReMi, which optimizes domain weights for training without knowledge of downstream tasks, have shown remarkable efficiency improvements, reaching baseline accuracy with 2.6x fewer training steps <Paper corpusId=\"258741043\" paperTitle=\"(Xie et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Liu et al., 2023)", "paper": {"corpus_id": 257901096, "title": "Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Min Liu", "authorId": "2226017688"}, {"name": "Yu Bao", "authorId": "145854784"}, {"name": "Chengqi Zhao", "authorId": "144562857"}, {"name": "Shujian Huang", "authorId": "2124946880"}], "n_citations": 3}, "snippets": ["Motivated by the success of curriculum learning (Qian et al., 2020)(Guo et al., 2020)Liu et al. 2020), we further introduce a hard-to-easy learning strategy to improve the performance. (Ding et al., 2020) show that pretraining with raw data can improve the performance of NAT by rejuvenating low-frequency words. To keep the merits of low-mode, they further trained the pretrained model on distilled data. We combine this idea with our data selection method by decreasing the ratio of raw data in the training process."], "score": 0.71142578125}, {"id": "(Ding et al., 2020)", "paper": {"corpus_id": 229923128, "title": "Understanding and Improving Lexical Choice in Non-Autoregressive Translation", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "Liang Ding", "authorId": "46573238"}, {"name": "Longyue Wang", "authorId": "1800190"}, {"name": "Xuebo Liu", "authorId": "1390611971"}, {"name": "Derek F. Wong", "authorId": "1758353"}, {"name": "D. Tao", "authorId": "143719920"}, {"name": "Zhaopeng Tu", "authorId": "2909321"}], "n_citations": 77}, "snippets": ["Knowledge distillation (KD) is essential for training non-autoregressive translation (NAT) models by reducing the complexity of the raw data with an autoregressive teacher model. In this study, we empirically show that as a side effect of this training, the lexical choice errors on low-frequency words are propagated to the NAT model from the teacher model. To alleviate this problem, we propose to expose the raw data to NAT models to restore the useful information of low-frequency words, which are missed in the distilled data. To this end, we introduce an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Experimental results across language pairs and model architectures demonstrate the effectiveness and universality of the proposed approach. Extensive analyses confirm our claim that our approach improves performance by reducing the lexical choice errors on low-frequency words. Encouragingly, our approach pushes the SOTA NAT performance on the WMT14 English-German and WMT16 Romanian-English datasets up to 27.8 and 33.8 BLEU points, respectively. The source code will be released."], "score": 0.0}, {"id": "(Guo et al., 2020)", "paper": {"corpus_id": 220046693, "title": "Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Junliang Guo", "authorId": "13838086"}, {"name": "Linli Xu", "authorId": "2230211"}, {"name": "Enhong Chen", "authorId": "2227868312"}], "n_citations": 79}, "snippets": ["The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model."], "score": 0.0}, {"id": "(Qian et al., 2020)", "paper": {"corpus_id": 221150562, "title": "Glancing Transformer for Non-Autoregressive Neural Machine Translation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Lihua Qian", "authorId": "2072789464"}, {"name": "Hao Zhou", "authorId": "2111824520"}, {"name": "Yu Bao", "authorId": "145854784"}, {"name": "Mingxuan Wang", "authorId": "50468534"}, {"name": "Lin Qiu", "authorId": "2068178455"}, {"name": "Weinan Zhang", "authorId": "2108309275"}, {"name": "Yong Yu", "authorId": "1811427"}, {"name": "Lei Li", "authorId": "143900005"}], "n_citations": 158}, "snippets": ["Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM) for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8\u00d7-15\u00d7 speedup. Note that GLAT does not modify the network architecture, which is a training method to learn word interdependency. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points."], "score": 0.0}, {"id": "(Yu et al., 2024)", "paper": {"corpus_id": 270371045, "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zichun Yu", "authorId": "2275526493"}, {"name": "Spandan Das", "authorId": "2305649905"}, {"name": "Chenyan Xiong", "authorId": "2275170116"}], "n_citations": 37}, "snippets": ["On the other hand, many researchers have proposed curriculum learning strategies that dynamically adjust the data distribution in the pretraining [11; 24; 47; 51; 56; 62]. ELECTRA-style models (208229926) incorporated curriculum learning in their pretraining process by synchronously training the model with an auxiliary generator, which provided more and more difficult training signals for the discriminator. This implicit curriculum significantly improved the pretraining efficiency on denoising language models [4; 11; 19; 39; 65]. Other methods have explicitly designed the curriculum for pretraining data selection, such as decreasing gradient norms (Thakkar et al., 2023), least certainty [24; 51], and increasing expertise (Wettig et al., 2024), demonstrating the benefits of adapting the pretraining data signal according to the model's ever-changing preferences."], "score": 0.8916015625}, {"id": "(Thakkar et al., 2023)", "paper": {"corpus_id": 264935129, "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Megh Thakkar", "authorId": "2264977662"}, {"name": "Tolga Bolukbasi", "authorId": "2843215"}, {"name": "Sriram Ganapathy", "authorId": "1726355"}, {"name": "Shikhar Vashishth", "authorId": "3404827"}, {"name": "Sarath Chandar", "authorId": "123607932"}, {"name": "P. Talukdar", "authorId": "2408872"}], "n_citations": 26}, "snippets": ["Language Models (LMs) pre-trained with self-supervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pre-training data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in the research direction of sample reweighting for pre-training language models."], "score": 0.0}, {"id": "(Wettig et al., 2024)", "paper": {"corpus_id": 267681974, "title": "QuRating: Selecting High-Quality Data for Training Language Models", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alexander Wettig", "authorId": "2127066887"}, {"name": "Aatmik Gupta", "authorId": "2284268826"}, {"name": "Saumya Malik", "authorId": "2323513320"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 79}, "snippets": ["Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications."], "score": 0.94970703125}, {"id": "(Khera et al., 2024)", "paper": {"corpus_id": 274130626, "title": "Efficient Alignment of Large Language Models via Data Sampling", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Amrit Khera", "authorId": "21302492"}, {"name": "Rajat Ghosh", "authorId": "2213553962"}, {"name": "Debojyoti Dutta", "authorId": "2267726934"}], "n_citations": 1}, "snippets": ["Data selection is a well known problem in literature with many algorithms such as filtering, coresets, importance sampling and more working towards the same goal [22]. Here, we present data selection in the domain of LLMs and natural language and how they can enable data efficient training. \n\nLIMA, proposed by [30] employs a small high quality dataset for fine-tuning. They show that by carefully curating only a 1000 data points, they are able to achieve remarkable performance which is generalizable to unseen data as well, thereby suggesting limited high quality tuning data is sufficient. \n\nAnother prominent study by [16] demonstrates that sample quality can reduce the data requirement without compromising the downstream performance. They investigate data engineering strategies in the fine-tuning paradigm from multiple facets to identify the characterises of good instruction tuning data. DEITA, the model family tuned by their proposed strategy to automatically select a complex and high quality dataset achieves comparable performance to open source models while using only a tenth of the data."], "score": 0.96337890625}, {"id": "(Zhou et al., 2023)", "paper": {"corpus_id": 258822910, "title": "LIMA: Less Is More for Alignment", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Chunting Zhou", "authorId": "2384711"}, {"name": "Pengfei Liu", "authorId": "144118452"}, {"name": "Puxin Xu", "authorId": "2214843767"}, {"name": "Srini Iyer", "authorId": "1900163"}, {"name": "Jiao Sun", "authorId": "145478138"}, {"name": "Yuning Mao", "authorId": "3375249"}, {"name": "Xuezhe Ma", "authorId": "2378954"}, {"name": "Avia Efrat", "authorId": "1388010852"}, {"name": "Ping Yu", "authorId": "2114104308"}, {"name": "L. Yu", "authorId": "49297123"}, {"name": "Susan Zhang", "authorId": "2108244542"}, {"name": "Gargi Ghosh", "authorId": "134007132"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Omer Levy", "authorId": "39455775"}], "n_citations": 850}, "snippets": ["Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."], "score": 0.0}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 260203057, "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Mayee F. Chen", "authorId": "48622329"}, {"name": "Nicholas Roberts", "authorId": "2069037355"}, {"name": "K. Bhatia", "authorId": "144383716"}, {"name": "Jue Wang", "authorId": "39597242"}, {"name": "Ce Zhang", "authorId": "1776014"}, {"name": "Frederic Sala", "authorId": "2186982588"}, {"name": "Christopher R\u00e9", "authorId": "1803218"}], "n_citations": 65}, "snippets": ["The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training."], "score": 0.916015625}, {"id": "(Sow et al., 2025)", "paper": {"corpus_id": 276250237, "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining", "year": 2025, "venue": "International Conference on Learning Representations", "authors": [{"name": "Daouda Sow", "authorId": "143827145"}, {"name": "Herbert Woisetschl\u00e4ger", "authorId": "2220057323"}, {"name": "Saikiran Bulusu", "authorId": "35693339"}, {"name": "Shiqiang Wang", "authorId": "2255363698"}, {"name": "Hans-Arno Jacobsen", "authorId": "2254179381"}, {"name": "Yingbin Liang", "authorId": "2344954540"}], "n_citations": 6}, "snippets": ["Training Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023;(Chen et al., 2023)Fan et al., 2023;Thakkar et al., 2023) have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, Xie et al. ( 2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. (Chen et al., 2023) introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values", ".The approach in Fan & Jaggi (2023) offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE."], "score": 0.81494140625}, {"id": "(Fan et al., 2023)", "paper": {"corpus_id": 264439064, "title": "Irreducible Curriculum for Language Model Pretraining", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Simin Fan", "authorId": "2261456109"}, {"name": "Martin Jaggi", "authorId": "2256984280"}], "n_citations": 11}, "snippets": ["Nevertheless, few research have succeeded applying traditional sample-level selection schemes for large langauge model pretraining. Online batch selection methods [Loshchilov and Hutter, 2015(Katharopoulos et al., 2018), Jiang et al., 2019, Schaul et al., 2015] select hard samples with high loss or high gradient norm, which require a second forward/backward pass. That introduces large extra computation costs when the model size is large, which hurts the scalability. On the other side, Campos [2021] shows that the linguistic-based curriculum learning failed to improve on causal language model pretraining."], "score": 0.88525390625}, {"id": "(Katharopoulos et al., 2018)", "paper": {"corpus_id": 3663876, "title": "Not All Samples Are Created Equal: Deep Learning with Importance Sampling", "year": 2018, "venue": "International Conference on Machine Learning", "authors": [{"name": "Angelos Katharopoulos", "authorId": "3493855"}, {"name": "F. Fleuret", "authorId": "2721983"}], "n_citations": 522}, "snippets": ["Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on \"informative\" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%."], "score": 0.0}, {"id": "(Belenki et al., 2025)", "paper": {"corpus_id": 276575595, "title": "Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Lior Belenki", "authorId": "2316558829"}, {"name": "Alekh Agarwal", "authorId": "2274120058"}, {"name": "Tianze Shi", "authorId": "2346974785"}, {"name": "Kristina Toutanova", "authorId": "2288931206"}], "n_citations": 0}, "snippets": ["There is an extensive body of work on data selection and mixture optimization for pretraining language models. Albalak et al. (2024) offer a comprehensive recent survey. Approaches for data selection and cleaning consider different granularities of data, such as token-level, sample-level (individual documents or sentences can be selected or weighted), and group-level (where we consider samples in large groups assumed to have common characteristics, often derived from meta-data such as the web domain (like Wikipedia) or source collection name (such as C4)."], "score": 0.90380859375}, {"id": "(Engstrom et al., 2024)", "paper": {"corpus_id": 267094971, "title": "DsDm: Model-Aware Dataset Selection with Datamodels", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Logan Engstrom", "authorId": "39468283"}, {"name": "Axel Feldmann", "authorId": "2280334462"}, {"name": "A. Ma\u0327dry", "authorId": "143826246"}], "n_citations": 61}, "snippets": ["When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with\"high quality\"data sources may not increase (and can even hurt) performance compared to randomly selecting data."], "score": 0.9541015625}, {"id": "(Bai et al., 2024)", "paper": {"corpus_id": 273233719, "title": "Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration", "year": 2024, "venue": "", "authors": [{"name": "Tianyi Bai", "authorId": "2318978696"}, {"name": "Ling Yang", "authorId": "2302788310"}, {"name": "Zhen Hao Wong", "authorId": "2325175258"}, {"name": "Fupeng Sun", "authorId": "2367277175"}, {"name": "Jiahui Peng", "authorId": "2233445161"}, {"name": "Xinlin Zhuang", "authorId": "2366068443"}, {"name": "Chi Zhang", "authorId": "2325489246"}, {"name": "Lijun Wu", "authorId": "2325195689"}, {"name": "Jiantao Qiu", "authorId": "2289911484"}, {"name": "Wentao Zhang", "authorId": "2302813081"}, {"name": "Binhang Yuan", "authorId": "2303407552"}, {"name": "Conghui He", "authorId": "2291040348"}], "n_citations": 6}, "snippets": ["Efficient data selection is crucial for the pretraining of large language models (LLMs), as the quality of training data significantly impacts the statistical efficiency of the training procedure and the model performance (Brown, 2020;(Du et al., 2021)(Chowdhery et al., 2022). Recently, we have witnessed numerous approaches, such as filtering high-quality data (Xie et al., 2023)(Wettig et al., 2024), mixing data from multiple domains (Xie et al., 2023)Liu et al., 2024), and selecting data that optimally boosts downstream task performance dynamically (Engstrom et al., 2024;Yu et al., 2024), which aim to improve data efficiency by prioritizing more informative training samples."], "score": 0.96337890625}, {"id": "(Du et al., 2021)", "paper": {"corpus_id": 245124124, "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Nan Du", "authorId": "2140321952"}, {"name": "Yanping Huang", "authorId": "2145438541"}, {"name": "Andrew M. Dai", "authorId": "2555924"}, {"name": "Simon Tong", "authorId": "2058177533"}, {"name": "Dmitry Lepikhin", "authorId": "150077954"}, {"name": "Yuanzhong Xu", "authorId": "2145139570"}, {"name": "M. Krikun", "authorId": "2048712"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Adams Wei Yu", "authorId": "40625240"}, {"name": "Orhan Firat", "authorId": "2345617"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "L. Fedus", "authorId": "2096916416"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Zongwei Zhou", "authorId": "1389392654"}, {"name": "Tao Wang", "authorId": null}, {"name": "Yu Emma Wang", "authorId": "2153608756"}, {"name": "Kellie Webster", "authorId": "20825661"}, {"name": "Marie Pellat", "authorId": "97905921"}, {"name": "Kevin Robinson", "authorId": "2148473059"}, {"name": "K. Meier-Hellstern", "authorId": "1398655031"}, {"name": "Toju Duke", "authorId": "2145151992"}, {"name": "Lucas Dixon", "authorId": "2065639113"}, {"name": "Kun Zhang", "authorId": "1556095165"}, {"name": "Quoc V. Le", "authorId": "2827616"}, {"name": "Yonghui Wu", "authorId": "48607963"}, {"name": "Z. Chen", "authorId": "2545358"}, {"name": "Claire Cui", "authorId": "2052275005"}], "n_citations": 826}, "snippets": ["Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks."], "score": 0.0}, {"id": "(Chowdhery et al., 2022)", "paper": {"corpus_id": 247951931, "title": "PaLM: Scaling Language Modeling with Pathways", "year": 2022, "venue": "Journal of machine learning research", "authors": [{"name": "Aakanksha Chowdhery", "authorId": "2841893"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Gaurav Mishra", "authorId": "2159632445"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "P. Barham", "authorId": "152399055"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Charles Sutton", "authorId": "152549864"}, {"name": "Sebastian Gehrmann", "authorId": "3159346"}, {"name": "Parker Schuh", "authorId": "2620528"}, {"name": "Kensen Shi", "authorId": "2362367"}, {"name": "Sasha Tsvyashchenko", "authorId": "2160888237"}, {"name": "Joshua Maynez", "authorId": "2124977868"}, {"name": "Abhishek Rao", "authorId": "1484043592"}, {"name": "Parker Barnes", "authorId": "80940648"}, {"name": "Yi Tay", "authorId": "144447820"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Vinodkumar Prabhakaran", "authorId": "3331141"}, {"name": "Emily Reif", "authorId": "49849144"}, {"name": "Nan Du", "authorId": "2140321952"}, {"name": "Ben Hutchinson", "authorId": "2044655623"}, {"name": "Reiner Pope", "authorId": "2161431901"}, {"name": "James Bradbury", "authorId": "2065251344"}, {"name": "Jacob Austin", "authorId": "2058365883"}, {"name": "M. Isard", "authorId": "2090818"}, {"name": "Guy Gur-Ari", "authorId": "2284681044"}, {"name": "Pengcheng Yin", "authorId": "38253388"}, {"name": "Toju Duke", "authorId": "2145151992"}, {"name": "Anselm Levskaya", "authorId": "6639036"}, {"name": "S. Ghemawat", "authorId": "1780892"}, {"name": "Sunipa Dev", "authorId": "50991767"}, {"name": "H. Michalewski", "authorId": "47407464"}, {"name": "Xavier Garc\u00eda", "authorId": "143936294"}, {"name": "Vedant Misra", "authorId": "40055795"}, {"name": "Kevin Robinson", "authorId": "2148473059"}, {"name": "L. Fedus", "authorId": "2096916416"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "Daphne Ippolito", "authorId": "7975935"}, {"name": "D. Luan", "authorId": "150970919"}, {"name": "Hyeontaek Lim", "authorId": "8939217"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "A. Spiridonov", "authorId": "1572884723"}, {"name": "Ryan Sepassi", "authorId": "35474601"}, {"name": "David Dohan", "authorId": "35363891"}, {"name": "Shivani Agrawal", "authorId": "3504647"}, {"name": "Mark Omernick", "authorId": "3175815"}, {"name": "Andrew M. Dai", "authorId": "2555924"}, {"name": "Thanumalayan Sankaranarayana Pillai", "authorId": "2598683"}, {"name": "Marie Pellat", "authorId": "97905921"}, {"name": "Aitor Lewkowycz", "authorId": "102549875"}, {"name": "Erica Moreira", "authorId": "2057453483"}, {"name": "R. Child", "authorId": "48422824"}, {"name": "Oleksandr Polozov", "authorId": "2636739"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Zongwei Zhou", "authorId": "2198519"}, {"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Brennan Saeta", "authorId": "4125424"}, {"name": "Mark D\u00edaz", "authorId": "2152965375"}, {"name": "Orhan Firat", "authorId": "2345617"}, {"name": "Michele Catasta", "authorId": "1754926"}, {"name": "Jason Wei", "authorId": "119640649"}, {"name": "K. Meier-Hellstern", "authorId": "1398655031"}, {"name": "D. Eck", "authorId": "2396681"}, {"name": "J. Dean", "authorId": "48448318"}, {"name": "Slav Petrov", "authorId": "1754497"}, {"name": "Noah Fiedel", "authorId": "22640071"}], "n_citations": 6293}, "snippets": ["Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."], "score": 0.0}, {"id": "(Xie et al., 2023)", "paper": {"corpus_id": 258741043, "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Sang Michael Xie", "authorId": "46215055"}, {"name": "Hieu Pham", "authorId": "143950636"}, {"name": "Xuanyi Dong", "authorId": "9929684"}, {"name": "Nan Du", "authorId": "2140321952"}, {"name": "Hanxiao Liu", "authorId": "2391802"}, {"name": "Yifeng Lu", "authorId": "2141538599"}, {"name": "Percy Liang", "authorId": "145419642"}, {"name": "Quoc V. Le", "authorId": "1397917613"}, {"name": "Tengyu Ma", "authorId": "2114186424"}, {"name": "Adams Wei Yu", "authorId": "40625240"}], "n_citations": 203}, "snippets": ["The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to set the domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks."], "score": 0.0}], "table": null}, {"title": "Challenges and Limitations", "tldr": "Despite its benefits, curriculum learning faces significant challenges including computational overhead for sample evaluation, failure to address data redundancy, and the counterintuitive finding that quality-focused selection doesn't always improve performance. Balancing data diversity with difficulty ranking remains a complex optimization problem. (6 sources)", "text": "\n* **Computational Overhead**: Traditional online batch selection methods that select hard samples based on high loss or gradient norm require a second forward/backward pass, introducing substantial computational costs that reduce scalability for large language models <Paper corpusId=\"264439064\" paperTitle=\"(Fan et al., 2023)\" isShortName></Paper> <Paper corpusId=\"3663876\" paperTitle=\"(Katharopoulos et al., 2018)\" isShortName></Paper>.\n\n* **Redundancy Management**: While curriculum learning enhances convergence by prioritizing easy points with low label noise, traditional approaches fail to address the issue of redundant training points that have already been learned <Paper corpusId=\"261048902\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>.\n\n* **Balancing Diversity and Difficulty**: Optimizing for data diversity tends to bias coresets toward easier samples, whereas selection based on difficulty ranking often omits easy samples that are necessary for effective training. These complementary factors must be jointly considered for optimal coreset selection <Paper corpusId=\"263909051\" paperTitle=\"(Maharana et al., 2023)\" isShortName></Paper>.\n\n* **Counter-intuitive Quality Effects**: Contrary to intuition, selecting data based on similarity to \"high-quality\" sources may not improve\u2014and can sometimes hurt\u2014performance compared to random data selection. This challenges the assumption that filtering for human notions of data quality necessarily improves model behavior <Paper corpusId=\"267094971\" paperTitle=\"(Engstrom et al., 2024)\" isShortName></Paper>.\n\n* **Combinatorial Effects**: Most data selection methods focus on evaluating individual sample quality while neglecting the combinatorial effects among samples. Even perfectly curated individual samples may create suboptimal learning conditions due to intrinsic homogeneity or contradictions when combined <Paper corpusId=\"271064746\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\n* **Domain-Specific Limitations**: Linguistic-based curriculum learning approaches have failed to demonstrate improvements in causal language model pretraining, suggesting that not all curriculum strategies transfer effectively across different language modeling tasks <Paper corpusId=\"264439064\" paperTitle=\"(Fan et al., 2023)\" isShortName></Paper>.\n\n* **Resource Constraints**: Implementing sophisticated curriculum learning approaches often requires additional computational resources for sample evaluation and curriculum design, creating a trade-off between the benefits of improved training and the costs of implementing the curriculum <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Fan et al., 2023)", "paper": {"corpus_id": 264439064, "title": "Irreducible Curriculum for Language Model Pretraining", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Simin Fan", "authorId": "2261456109"}, {"name": "Martin Jaggi", "authorId": "2256984280"}], "n_citations": 11}, "snippets": ["Nevertheless, few research have succeeded applying traditional sample-level selection schemes for large langauge model pretraining. Online batch selection methods [Loshchilov and Hutter, 2015(Katharopoulos et al., 2018), Jiang et al., 2019, Schaul et al., 2015] select hard samples with high loss or high gradient norm, which require a second forward/backward pass. That introduces large extra computation costs when the model size is large, which hurts the scalability. On the other side, Campos [2021] shows that the linguistic-based curriculum learning failed to improve on causal language model pretraining."], "score": 0.88525390625}, {"id": "(Katharopoulos et al., 2018)", "paper": {"corpus_id": 3663876, "title": "Not All Samples Are Created Equal: Deep Learning with Importance Sampling", "year": 2018, "venue": "International Conference on Machine Learning", "authors": [{"name": "Angelos Katharopoulos", "authorId": "3493855"}, {"name": "F. Fleuret", "authorId": "2721983"}], "n_citations": 522}, "snippets": ["Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on \"informative\" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%."], "score": 0.0}, {"id": "(Deng et al., 2023)", "paper": {"corpus_id": 261048902, "title": "Towards Accelerated Model Training via Bayesian Data Selection", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zhijie Deng", "authorId": "145114723"}, {"name": "Peng Cui", "authorId": "2153522384"}, {"name": "Jun Zhu", "authorId": "145254043"}], "n_citations": 5}, "snippets": ["Curriculum learning, as advocated by (Bengio et al., 2009), prioritizes easy points with low label noise before uniformly training on all data points. While this strategy enhances convergence, it fails to address the issue of skipping redundant points already learned."], "score": 0.7568359375}, {"id": "(Maharana et al., 2023)", "paper": {"corpus_id": 263909051, "title": "D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Adyasha Maharana", "authorId": "8785371"}, {"name": "Prateek Yadav", "authorId": "46841632"}, {"name": "Mohit Bansal", "authorId": "2253396640"}], "n_citations": 34}, "snippets": ["Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection."], "score": 0.74755859375}, {"id": "(Engstrom et al., 2024)", "paper": {"corpus_id": 267094971, "title": "DsDm: Model-Aware Dataset Selection with Datamodels", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Logan Engstrom", "authorId": "39468283"}, {"name": "Axel Feldmann", "authorId": "2280334462"}, {"name": "A. Ma\u0327dry", "authorId": "143826246"}], "n_citations": 61}, "snippets": ["When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with\"high quality\"data sources may not increase (and can even hurt) performance compared to randomly selecting data."], "score": 0.9541015625}, {"id": "(Yin et al., 2024)", "paper": {"corpus_id": 271064746, "title": "Entropy Law: The Story Behind Data Compression and LLM Performance", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mingjia Yin", "authorId": "2260649859"}, {"name": "Chuhan Wu", "authorId": "2276003321"}, {"name": "Yufei Wang", "authorId": "2283529247"}, {"name": "Hao Wang", "authorId": "2256768674"}, {"name": "Wei Guo", "authorId": "2260810851"}, {"name": "Yasheng Wang", "authorId": "2282544603"}, {"name": "Yong Liu", "authorId": "2293683997"}, {"name": "Ruiming Tang", "authorId": "2284295184"}, {"name": "Defu Lian", "authorId": "1862782"}, {"name": "Enhong Chen", "authorId": "2113754294"}], "n_citations": 27}, "snippets": ["Data is the cornerstone of large language models (LLMs), but not all data is useful for model learning. Carefully selected data can better elicit the capabilities of LLMs with much less computational overhead. Most methods concentrate on evaluating the quality of individual samples in data selection, while the combinatorial effects among samples are neglected. Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction."], "score": 0.85693359375}], "table": null}, {"title": "Recent Advances", "tldr": "Recent advances in curriculum learning for language models include ordered skill acquisition frameworks, SmallToLarge training methods, and domain optimization strategies that significantly improve data efficiency. These innovations span from specialized algorithms for training progression to automated data selection techniques that reduce training resources while maintaining or improving model performance. (15 sources)", "text": "\nThe field of curriculum learning for language models has seen significant advancements in recent years, with researchers developing increasingly sophisticated approaches to data selection and training scheduling. One notable innovation is the \"ordered skill acquisition\" framework proposed by Chen et al., which builds on the hypothesis that language models, like humans, acquire interdependent skills in a natural order <Paper corpusId=\"260203057\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This approach formalizes the notion of skills and their dependencies, demonstrating that teaching prerequisite skills first enables more advanced skills to be learned with less data. Their Skill-It algorithm, which samples data based on prerequisite skills, achieved 36.5 points higher accuracy than random sampling in continual pre-training and reduced validation loss by 13.6% in fine-tuning scenarios <Paper corpusId=\"260203057\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"276250237\" paperTitle=\"(Sow et al., 2025)\" isShortName></Paper>.\n\nBuilding on this concept, the Orion-14B model implemented a staged training approach that intentionally organized data to increase complexity incrementally. Initial stages focused on common knowledge sources like web pages and news articles, gradually incorporating more complex sources such as textbooks, academic papers, and source code as training progressed <Paper corpusId=\"267095066\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. This structured approach to curriculum design reflects the growing understanding that the strategic ordering of training data can significantly enhance model performance.\n\nFor supervised fine-tuning of large language models, the SmallToLarge (S2L) method represents another important advance. This approach leverages training trajectories from small models to guide data selection for larger models, making the fine-tuning process more efficient <Paper corpusId=\"268363364\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This cross-model knowledge transfer strategy aligns with broader research showing that high-quality data selection can dramatically reduce data requirements, as demonstrated by the LIMA project, which achieved remarkable performance by carefully curating just 1,000 high-quality examples <Paper corpusId=\"270620248\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"274130626\" paperTitle=\"(Khera et al., 2024)\" isShortName></Paper>.\n\nThe Domain Reweighting with Minimax Optimization (DoReMi) approach represents another significant advance in curriculum learning. This method trains a small proxy model using group distributionally robust optimization over domains to produce domain weights without knowledge of downstream tasks <Paper corpusId=\"273233719\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper>. In experiments, DoReMi improved average few-shot downstream accuracy by 6.5 percentage points over baseline models and reached baseline accuracy with 2.6 times fewer training steps, demonstrating substantial efficiency gains <Paper corpusId=\"273233719\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258741043\" paperTitle=\"(Xie et al., 2023)\" isShortName></Paper>.\n\nAnother notable advance is the Online Data Mixing (ODM) algorithm, which optimizes data mixing proportions during training using multi-armed bandit algorithms. This approach combines elements from both data selection and data mixing to adapt to changing training dynamics, reaching the final perplexity of comparable methods with 19% fewer training iterations <Paper corpusId=\"273502726\" paperTitle=\"(Na et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265658930\" paperTitle=\"(Albalak et al., 2023)\" isShortName></Paper>.\n\nFor data quality assessment, the QuRating method introduces a sophisticated approach that uses language models to rate pretraining data based on writing style, required expertise, factual content, and educational value <Paper corpusId=\"273233719\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267681974\" paperTitle=\"(Wettig et al., 2024)\" isShortName></Paper>. This method creates a training curriculum that improves performance without changing the training dataset, demonstrating how quality-focused selection can enhance model capabilities.\n\nAt a more practical level, researchers have begun to consider different granularities of data selection, from token-level to sample-level to group-level approaches <Paper corpusId=\"276575595\" paperTitle=\"(Belenki et al., 2025)\" isShortName></Paper>. Group-level data selection, which considers samples in large groups with common characteristics derived from metadata such as web domains or source collection names, has proven particularly effective for large-scale training <Paper corpusId=\"273350576\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>.\n\nThe field continues to evolve with innovations like the Reducible Holdout Loss Selection (RHO-LOSS) technique, which selects points that most reduce the model's generalization loss, addressing the weaknesses of existing methods that focus solely on \"hard\" or \"easy\" examples <Paper corpusId=\"273350576\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235743024\" paperTitle=\"(Mindermann et al., 2022)\" isShortName></Paper>. This approach selects points that are learnable, worth learning, and not yet learned, training in far fewer steps than prior approaches while improving accuracy <Paper corpusId=\"273350576\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235743024\" paperTitle=\"(Mindermann et al., 2022)\" isShortName></Paper>.\n\nThese advances collectively demonstrate how curriculum learning continues to evolve from its conceptual foundations to increasingly sophisticated implementations that deliver measurable improvements in model performance and training efficiency. As the field progresses, the integration of Bayesian optimization to learn data selection measures represents a promising direction, with early work showing significant improvements across models, domains, and tasks <Paper corpusId=\"7403346\" paperTitle=\"(Ruder et al., 2017)\" isShortName></Paper>.", "citations": [{"id": "(Chen et al., 2023)", "paper": {"corpus_id": 260203057, "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Mayee F. Chen", "authorId": "48622329"}, {"name": "Nicholas Roberts", "authorId": "2069037355"}, {"name": "K. Bhatia", "authorId": "144383716"}, {"name": "Jue Wang", "authorId": "39597242"}, {"name": "Ce Zhang", "authorId": "1776014"}, {"name": "Frederic Sala", "authorId": "2186982588"}, {"name": "Christopher R\u00e9", "authorId": "1803218"}], "n_citations": 65}, "snippets": ["The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training."], "score": 0.916015625}, {"id": "(Sow et al., 2025)", "paper": {"corpus_id": 276250237, "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining", "year": 2025, "venue": "International Conference on Learning Representations", "authors": [{"name": "Daouda Sow", "authorId": "143827145"}, {"name": "Herbert Woisetschl\u00e4ger", "authorId": "2220057323"}, {"name": "Saikiran Bulusu", "authorId": "35693339"}, {"name": "Shiqiang Wang", "authorId": "2255363698"}, {"name": "Hans-Arno Jacobsen", "authorId": "2254179381"}, {"name": "Yingbin Liang", "authorId": "2344954540"}], "n_citations": 6}, "snippets": ["Training Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023;(Chen et al., 2023)Fan et al., 2023;Thakkar et al., 2023) have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, Xie et al. ( 2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. (Chen et al., 2023) introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values", ".The approach in Fan & Jaggi (2023) offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE."], "score": 0.81494140625}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 267095066, "title": "Orion-14B: Open-source Multilingual Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Du Chen", "authorId": "2280380985"}, {"name": "Yi Huang", "authorId": "2280373513"}, {"name": "Xiaopu Li", "authorId": "2280377400"}, {"name": "Yongqiang Li", "authorId": "2280380419"}, {"name": "Yongqiang Liu", "authorId": "2280377865"}, {"name": "Haihui Pan", "authorId": "1557313084"}, {"name": "Leichao Xu", "authorId": "2280411885"}, {"name": "Dacheng Zhang", "authorId": "2109546143"}, {"name": "Zhipeng Zhang", "authorId": "2280368716"}, {"name": "Kun Han", "authorId": "2281415463"}], "n_citations": 4}, "snippets": ["Considering that humans acquire knowledge in a deliberate order (Evanson et al., 2023), it is plausible that language models might also benefit from a structured learning order when processing training data. Curriculum learning (Bengio et al., 2009) has been suggested as a method to organize the learning process by progressively increasing the complexity of the training data. However, most prior studies have concentrated on sample-level data and smaller datasets. Chen et al. ( 2023) employed a skills-based framework for training data selection and continuous pretraining with a 3B-parameter language model. This approach achieved greater accuracy compared to the baseline method of uniform data source sampling, suggesting the potential efficacy of strategic data scheduling.\n\nIn training the Orion-14B model, we intentionally develop a data scheduling strategy that organizes training data to incrementally increase its complexity. We divide the training data into several stages based on the data sources and their complexity. These stages are differentiated by the mix ratios of data sources. Initial stages primarily include data with common knowledge, such as web pages and news articles. In the subsequent stages, we gradually augment the proportion of data containing more complex knowledge, including textbooks, academic papers, and source code."], "score": 0.83740234375}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 268363364, "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yu Yang", "authorId": "2291083514"}, {"name": "Siddhartha Mishra", "authorId": "2290975932"}, {"name": "Jeffrey N Chiang", "authorId": "2290912016"}, {"name": "Baharan Mirzasoleiman", "authorId": "2389094"}], "n_citations": 23}, "snippets": ["Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models."], "score": 0.86669921875}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 270620248, "title": "Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ziche Liu", "authorId": "2243875495"}, {"name": "Rui Ke", "authorId": "2307471917"}, {"name": "Feng Jiang", "authorId": "2303951713"}, {"name": "Haizhou Li", "authorId": "2218230700"}], "n_citations": 2}, "snippets": ["Recent research highlights that data quality is more critical than data quantity for effective fine-tuning (Nakkiran et al., 2019)(Shumailov et al., 2024)(Zhou et al., 2023)Jindal et al., 2024). As a result, several data curation techniques have been proposed, such as data selection (Chen et al., 2023)(Li et al., 2023), data evolution (Wang et al., 2022)Xu et al., 2023), and data reflection (Mukherjee et al., 2023)(Yin et al., 2023). Data selection, in particular, involves choosing a high-quality subset from a candidate dataset based on specific selection criteria, enhancing the model's performance while improving training efficiency by reducing the number of samples."], "score": 0.9599609375}, {"id": "(Khera et al., 2024)", "paper": {"corpus_id": 274130626, "title": "Efficient Alignment of Large Language Models via Data Sampling", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Amrit Khera", "authorId": "21302492"}, {"name": "Rajat Ghosh", "authorId": "2213553962"}, {"name": "Debojyoti Dutta", "authorId": "2267726934"}], "n_citations": 1}, "snippets": ["Data selection is a well known problem in literature with many algorithms such as filtering, coresets, importance sampling and more working towards the same goal [22]. Here, we present data selection in the domain of LLMs and natural language and how they can enable data efficient training. \n\nLIMA, proposed by [30] employs a small high quality dataset for fine-tuning. They show that by carefully curating only a 1000 data points, they are able to achieve remarkable performance which is generalizable to unseen data as well, thereby suggesting limited high quality tuning data is sufficient. \n\nAnother prominent study by [16] demonstrates that sample quality can reduce the data requirement without compromising the downstream performance. They investigate data engineering strategies in the fine-tuning paradigm from multiple facets to identify the characterises of good instruction tuning data. DEITA, the model family tuned by their proposed strategy to automatically select a complex and high quality dataset achieves comparable performance to open source models while using only a tenth of the data."], "score": 0.96337890625}, {"id": "(Bai et al., 2024)", "paper": {"corpus_id": 273233719, "title": "Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration", "year": 2024, "venue": "", "authors": [{"name": "Tianyi Bai", "authorId": "2318978696"}, {"name": "Ling Yang", "authorId": "2302788310"}, {"name": "Zhen Hao Wong", "authorId": "2325175258"}, {"name": "Fupeng Sun", "authorId": "2367277175"}, {"name": "Jiahui Peng", "authorId": "2233445161"}, {"name": "Xinlin Zhuang", "authorId": "2366068443"}, {"name": "Chi Zhang", "authorId": "2325489246"}, {"name": "Lijun Wu", "authorId": "2325195689"}, {"name": "Jiantao Qiu", "authorId": "2289911484"}, {"name": "Wentao Zhang", "authorId": "2302813081"}, {"name": "Binhang Yuan", "authorId": "2303407552"}, {"name": "Conghui He", "authorId": "2291040348"}], "n_citations": 6}, "snippets": ["Efficient data selection is crucial for the pretraining of large language models (LLMs), as the quality of training data significantly impacts the statistical efficiency of the training procedure and the model performance (Brown, 2020;(Du et al., 2021)(Chowdhery et al., 2022). Recently, we have witnessed numerous approaches, such as filtering high-quality data (Xie et al., 2023)(Wettig et al., 2024), mixing data from multiple domains (Xie et al., 2023)Liu et al., 2024), and selecting data that optimally boosts downstream task performance dynamically (Engstrom et al., 2024;Yu et al., 2024), which aim to improve data efficiency by prioritizing more informative training samples."], "score": 0.96337890625}, {"id": "(Xie et al., 2023)", "paper": {"corpus_id": 258741043, "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Sang Michael Xie", "authorId": "46215055"}, {"name": "Hieu Pham", "authorId": "143950636"}, {"name": "Xuanyi Dong", "authorId": "9929684"}, {"name": "Nan Du", "authorId": "2140321952"}, {"name": "Hanxiao Liu", "authorId": "2391802"}, {"name": "Yifeng Lu", "authorId": "2141538599"}, {"name": "Percy Liang", "authorId": "145419642"}, {"name": "Quoc V. Le", "authorId": "1397917613"}, {"name": "Tengyu Ma", "authorId": "2114186424"}, {"name": "Adams Wei Yu", "authorId": "40625240"}], "n_citations": 203}, "snippets": ["The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to set the domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks."], "score": 0.0}, {"id": "(Na et al., 2024)", "paper": {"corpus_id": 273502726, "title": "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Clara Na", "authorId": "2166313248"}, {"name": "Ian Magnusson", "authorId": "2124977543"}, {"name": "A. Jha", "authorId": "47286118"}, {"name": "Tom Sherborne", "authorId": "20662387"}, {"name": "Emma Strubell", "authorId": "2268272"}, {"name": "Jesse Dodge", "authorId": "2285322061"}, {"name": "Pradeep Dasigi", "authorId": "2697425"}], "n_citations": 5}, "snippets": ["Efficient data selection Given that data ablations on large language models is expensive, one class of approaches relies on approximating them on smaller models. Relevant work studies scaling laws for model parameters vs training tokens (Hoffmann et al., 2022;(Biderman et al., 2023), empirical effects of including or excluding different sources of data (Longpre et al., 2023), and the effects of training over multiple epochs vs new training tokens (Muennighoff et al., 2023). Previous work has also explored improving domain-specific fit via continued pre-training (Gururangan et al., 2020), predicting domain fit using lexical features (Reid et al., 2022), or improving general test-time adaptation via dynamic data selection, either by distributionally robust optimization with a small proxy model (Oren et al., 2019)Xie et al., 2023) or online using a multi-armed bandit approach (Albalak et al., 2023). Additional previous works aim to adapt to known downstream tasks via data selection, including at the individual example level (Wang et al., 2020) or even by explicitly fine-tuning models on many tasks (Aghajanyan et al., 2021)."], "score": 0.87158203125}, {"id": "(Albalak et al., 2023)", "paper": {"corpus_id": 265658930, "title": "Efficient Online Data Mixing For Language Model Pre-Training", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Alon Albalak", "authorId": "2044198106"}, {"name": "Liangming Pan", "authorId": "2256983134"}, {"name": "Colin Raffel", "authorId": "2269733851"}, {"name": "W. Wang", "authorId": "2257130314"}], "n_citations": 45}, "snippets": ["The data used to pretrain large language models has a decisive impact on a model's downstream performance, which has led to a large body of work on data selection methods that aim to automatically determine the most suitable data to use for pretraining. Existing data selection methods suffer from slow and computationally expensive processes, a problem amplified by the increasing size of models and of pretraining datasets. Data mixing, on the other hand, reduces the complexity of data selection by grouping data points together and determining sampling probabilities across entire groups. However, data mixing proportions are typically fixed before training and therefore cannot adapt to changing training dynamics. To address these limitations, we develop an efficient algorithm for Online Data Mixing (ODM) that combines elements from both data selection and data mixing. Based on multi-armed bandit algorithms, our online approach optimizes the data mixing proportions during training. Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19\\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible wall-clock time during pretraining."], "score": 0.0}, {"id": "(Wettig et al., 2024)", "paper": {"corpus_id": 267681974, "title": "QuRating: Selecting High-Quality Data for Training Language Models", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alexander Wettig", "authorId": "2127066887"}, {"name": "Aatmik Gupta", "authorId": "2284268826"}, {"name": "Saumya Malik", "authorId": "2323513320"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 79}, "snippets": ["Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications."], "score": 0.94970703125}, {"id": "(Belenki et al., 2025)", "paper": {"corpus_id": 276575595, "title": "Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Lior Belenki", "authorId": "2316558829"}, {"name": "Alekh Agarwal", "authorId": "2274120058"}, {"name": "Tianze Shi", "authorId": "2346974785"}, {"name": "Kristina Toutanova", "authorId": "2288931206"}], "n_citations": 0}, "snippets": ["There is an extensive body of work on data selection and mixture optimization for pretraining language models. Albalak et al. (2024) offer a comprehensive recent survey. Approaches for data selection and cleaning consider different granularities of data, such as token-level, sample-level (individual documents or sentences can be selected or weighted), and group-level (where we consider samples in large groups assumed to have common characteristics, often derived from meta-data such as the web domain (like Wikipedia) or source collection name (such as C4)."], "score": 0.90380859375}, {"id": "(Jiang et al., 2024)", "paper": {"corpus_id": 273350576, "title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yiding Jiang", "authorId": "2257132704"}, {"name": "Allan Zhou", "authorId": "2325951293"}, {"name": "Zhili Feng", "authorId": "2261439316"}, {"name": "Sadhika Malladi", "authorId": "49288855"}, {"name": "J. Kolter", "authorId": "2242257227"}], "n_citations": 22}, "snippets": ["Data curation and selection. For current large language models, compute often poses a greater constraint than data availability, making data selection crucial (Albalak et al., 2024). A widely used approach is data filtering (Soboleva et al., 2023;Penedo et al., 2023;2024), where undesirable data points are removed based on heuristics like perplexity, repetition (Lee et al., 2021), or semantic similarity (Abbas et al., 2023). This filtering process is foundational for constructing most largescale datasets today. After filtering, data is often categorized into subsets or domains (e.g., code, books), and deciding how much data to use from each domain becomes an important step. \n\nTwo primary strategies for data selection are prevalent: one focuses on deciding whether individual data points should be included based on various criteria (Mindermann et al., 2022)Engstrom et al., 2024), and the other uses all available data but samples from different domains with varying probabilities (Xie et al., 2024;Fan et al., 2023;(Albalak et al., 2023). While data selection aims to enhance training efficiency, these methods may introduce considerable computational overhead (Xie et al., 2024;(Chen et al., 2023)Fan et al., 2023)."], "score": 0.9326171875}, {"id": "(Mindermann et al., 2022)", "paper": {"corpus_id": 235743024, "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "S. Mindermann", "authorId": "32777162"}, {"name": "Muhammed Razzak", "authorId": "2117637436"}, {"name": "Winnie Xu", "authorId": "2110536989"}, {"name": "Andreas Kirsch", "authorId": "145408381"}, {"name": "Mrinank Sharma", "authorId": "1429832706"}, {"name": "Adrien Morisot", "authorId": "1833773702"}, {"name": "Aidan N. Gomez", "authorId": "19177000"}, {"name": "Sebastian Farquhar", "authorId": "33859827"}, {"name": "J. Brauner", "authorId": "38732223"}, {"name": "Y. Gal", "authorId": "2681954"}], "n_citations": 164}, "snippets": ["Training on web-scale data can take months. But most computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Reducible Holdout Loss Selection (RHO-LOSS), a simple but principled technique which selects approximately those points for training that most reduce the model's generalization loss. As a result, RHO-LOSS mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select 'hard' (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. Conversely, curriculum learning prioritizes 'easy' points, but such points need not be trained on once learned. In contrast, RHO-LOSS selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in 18x fewer steps and reaches 2% higher final accuracy than uniform data shuffling."], "score": 0.0}, {"id": "(Ruder et al., 2017)", "paper": {"corpus_id": 7403346, "title": "Learning to select data for transfer learning with Bayesian Optimization", "year": 2017, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Sebastian Ruder", "authorId": "2884561"}, {"name": "Barbara Plank", "authorId": "2022124"}], "n_citations": 186}, "snippets": ["Inspired by work on curriculum learning, we propose to learn data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are\u2013to some degree\u2013transferable across models, domains, and even tasks."], "score": 0.88427734375}], "table": null}], "cost": 0.516936}}
