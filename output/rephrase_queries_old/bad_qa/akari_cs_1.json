{"original_query": "Are there any papers showing the effectiveness of quality filtering of LLM pre-training data using more advanced models such as BERT? I found many prior studies only use fasttext but it could make the filtering models pretty weak...", "input": "Are there any papers effectiveness quality filtering of LLM pre training data advanced models such BERT found many prior studies fasttext it filtering models weak", "output": {"sections": [{"title": "Introduction to Data Quality Filtering for LLMs", "tldr": "Data quality filtering has become increasingly important in language model development, with researchers recognizing that high-quality data often outperforms sheer quantity. Various filtering techniques have been developed to remove noise, errors, and irrelevant content from pre-training corpora to improve model performance. (9 sources)", "text": "\nThe quality of pre-training data has emerged as a critical factor in the development and performance of large language models (LLMs). Recent research indicates a significant shift from simply accumulating vast amounts of training data to carefully curating high-quality datasets <Paper corpusId=\"261243909\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper>. This paradigm shift emphasizes data excellence through thorough filtering and curation processes to remove noise, errors, and irrelevant content, ultimately leading to more robust and reliable models <Paper corpusId=\"261243909\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper>.\n\nMultiple studies confirm that the quality of text data used in pre-training is crucial for improving LLM performance <Paper corpusId=\"268680360\" paperTitle=\"(Levine et al., 2024)\" isShortName></Paper> <Paper corpusId=\"237568724\" paperTitle=\"(Dodge et al., 2021)\" isShortName></Paper>. Better data selection has been shown to enhance downstream performance and/or reduce the compute budget required to achieve equivalent results <Paper corpusId=\"270357359\" paperTitle=\"(Kong et al., 2024)\" isShortName></Paper>. This is particularly important given that pre-training data often contains significant noise, including boilerplate text, templates, error messages, and other forms of repetitive or uninformative content that contributes little to model quality <Paper corpusId=\"270357359\" paperTitle=\"(Kong et al., 2024)\" isShortName></Paper>.\n\nThe negative impact of poor quality data extends beyond wasted computational resources. When models are trained on noisy or low-quality data, they can learn and perpetuate irrelevant or redundant information, ultimately affecting their performance and generalization capabilities across various tasks <Paper corpusId=\"270357359\" paperTitle=\"(Kong et al., 2024)\" isShortName></Paper>. Additionally, research has demonstrated that repeated data can lead to a \"double descent\" phenomenon, where test loss increases midway through training, and even a small fraction of repeated data can significantly degrade model performance <Paper corpusId=\"248986979\" paperTitle=\"(Hernandez et al., 2022)\" isShortName></Paper>.\n\nRecent trends in dataset curation reflect this growing emphasis on quality over quantity, with advanced filtering and deduplication techniques becoming increasingly prevalent <Paper corpusId=\"273323858\" paperTitle=\"(Brandizzi et al., 2024)\" isShortName></Paper>. The evaluation of data quality has also evolved from using intrinsic signals derived from the data itself (such as word length or sentence complexity) to leveraging pre-trained LLMs to judge content quality or even generate synthetic data <Paper corpusId=\"273323858\" paperTitle=\"(Brandizzi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"211987786\" paperTitle=\"(Kumar et al., 2020)\" isShortName></Paper> <Paper corpusId=\"233296100\" paperTitle=\"(Yoo et al., 2021)\" isShortName></Paper>. One reason for the effectiveness of these methods is the strong alignment between human judgments of quality and the assessments made by LLMs <Paper corpusId=\"273323858\" paperTitle=\"(Brandizzi et al., 2024)\" isShortName></Paper>.\n\nThe importance of temporal alignment between pre-training and evaluation data has also been highlighted, with research showing that temporal shift between these datasets can lead to performance degradation that is not easily overcome by fine-tuning <Paper corpusId=\"258832491\" paperTitle=\"(Longpre et al., 2023)\" isShortName></Paper>. Furthermore, studies have revealed trade-offs between performance on standard benchmarks and the risk of toxic generations when applying quality and toxicity filters <Paper corpusId=\"258832491\" paperTitle=\"(Longpre et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Gao et al., 2023)", "paper": {"corpus_id": 261243909, "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Kaiyuan Gao", "authorId": "1944690382"}, {"name": "Su He", "authorId": "2112345574"}, {"name": "Zhenyu He", "authorId": "2152990262"}, {"name": "Jiacheng Lin", "authorId": null}, {"name": "Qizhi Pei", "authorId": "2171652249"}, {"name": "Jie Shao", "authorId": "2234370833"}, {"name": "Wei Zhang", "authorId": "2256597384"}], "n_citations": 5}, "snippets": ["Therefore, recently, there is another trend that obtaining high quality data instead of tremendous scale of low quality data for pre-training.\n\nSince the quality of pre-training data significantly impacts the effectiveness of GPT models and ensures the models capture accurate and reliable information during pre-training, we need to ensure data excellence by involving carefully curating and filtering the text corpus to remove noise, errors, and irrelevant content, ultimately leading to more robust and reliable GPT models [103,104]. Next, we will introduce some data preprocessing strategies for data quality improvement in several representative LLMs, including classifier-based filtering, rule-based filtering and utilizing textbook quality data.\n\nClassifier-based Filtering. In this approach, a selection classifier is trained using high-quality texts as examples. This classifier is then used to analyze the remaining data and distinguish between high-quality and low-quality samples.\n\nThe low-quality data is subsequently eliminated from the dataset, ensuring that only the most reliable and valuable information is retained for further pre-training.\n\n\u2022 GPT-3 [2] devised an automated filtering method to effectively eliminate low-quality documents from the Common Crawl dataset. It used a classifier trained on highquality data (WebText, Wikipedia, and web books corpus) to prioritize documents with higher scores, resulting in improved data quality for generative text samples. Moreover, GPT-3 adopted fuzzy deduplication of documents within each dataset and the removal of WebText from Common Crawl, further improving the data quality."], "score": 0.71533203125}, {"id": "(Levine et al., 2024)", "paper": {"corpus_id": 268680360, "title": "RakutenAI-7B: Extending Large Language Models for Japanese", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Aaron Levine", "authorId": "2061741965"}, {"name": "Connie Huang", "authorId": "2293356757"}, {"name": "Chenguang Wang", "authorId": "2293351792"}, {"name": "Eduardo Batista", "authorId": "2293313172"}, {"name": "Ewa Szymanska", "authorId": "2293311576"}, {"name": "Hongyi Ding", "authorId": "2293527287"}, {"name": "Houwei Chou", "authorId": "2121371065"}, {"name": "Jean-Fran\u00e7ois Pessiot", "authorId": "2099242119"}, {"name": "Johanes Effendi", "authorId": "2293313121"}, {"name": "Justin Chiu", "authorId": "2293313467"}, {"name": "Kai Torben Ohlhus", "authorId": "147240287"}, {"name": "Karan Chopra", "authorId": "2293312430"}, {"name": "Keiji Shinzato", "authorId": "40466108"}, {"name": "Koji Murakami", "authorId": "2313561595"}, {"name": "Lee Xiong", "authorId": "2293314500"}, {"name": "Lei Chen", "authorId": "2293403169"}, {"name": "Maki Kubota", "authorId": "2293314132"}, {"name": "Maksim Tkatchenko", "authorId": "2295735234"}, {"name": "Miroku Lee", "authorId": "2293317836"}, {"name": "Naoki Takahashi", "authorId": "2293314089"}, {"name": "Prathyusha Jwalapuram", "authorId": "35640774"}, {"name": "Ryutaro Tatsushima", "authorId": "2293313287"}, {"name": "Saurabh Jain", "authorId": "2165228954"}, {"name": "Sunil Kumar Yadav", "authorId": "49596302"}, {"name": "Ting Cai", "authorId": "2293312307"}, {"name": "Wei-Te Chen", "authorId": "2271252858"}, {"name": "Yandi Xia", "authorId": "3456546"}, {"name": "Yuki Nakayama", "authorId": "2293302761"}, {"name": "Yutaka Higashiyama", "authorId": "2293313297"}], "n_citations": 9}, "snippets": ["Previous research studies indicate that the quality of text data used for pre-training is critical to improving LLM performance (Rae et al., 2022;(Dodge et al., 2021)Chowdhery et al., 2022).Alongside these studies, we experiment with and develop data filtering techniques that allow us to further improve the quality of available internet-scale datasets for Japanese and English."], "score": 0.57763671875}, {"id": "(Dodge et al., 2021)", "paper": {"corpus_id": 237568724, "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jesse Dodge", "authorId": "34176020"}, {"name": "Ana Marasovic", "authorId": "2127450728"}, {"name": "Gabriel Ilharco", "authorId": "2123694087"}, {"name": "Dirk Groeneveld", "authorId": "3458736"}, {"name": "Margaret Mitchell", "authorId": "49501003"}, {"name": "Matt Gardner", "authorId": "40642935"}, {"name": "William Agnew", "authorId": "2301202406"}], "n_citations": 450}, "snippets": ["Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet."], "score": 0.0}, {"id": "(Kong et al., 2024)", "paper": {"corpus_id": 270357359, "title": "Large Language Model-guided Document Selection", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xiang Kong", "authorId": "2291470563"}, {"name": "Tom Gunter", "authorId": "2238621478"}, {"name": "Ruoming Pang", "authorId": "2238621132"}], "n_citations": 4}, "snippets": ["It is well known that better data selection leads to improved downstream performance and/or a reduced compute budget to achieve the same performance for large-scale model training efforts, e.g.LLMs Sorscher et al. [2023]", "However, it is widely recognized that pre-training data is very noisy, and often includes boilerplate text, templates, error messages, and other forms of repetitive or not-informative content that does not contribute meaningfully to model quality as measured by downstream benchmarks.This poor quality data can lead models to learn and perpetuate irrelevant or redundant information, ultimately impacting their performance and generalization capabilities across various tasks (Raffel et al. [2020], Touvron et al. [2023a,b])."], "score": 0.51806640625}, {"id": "(Hernandez et al., 2022)", "paper": {"corpus_id": 248986979, "title": "Scaling Laws and Interpretability of Learning from Repeated Data", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Danny Hernandez", "authorId": "39182747"}, {"name": "Tom B. Brown", "authorId": "31035595"}, {"name": "Tom Conerly", "authorId": "2154608209"}, {"name": "Nova Dassarma", "authorId": "2142833890"}, {"name": "Dawn Drain", "authorId": "1943097969"}, {"name": "S. El-Showk", "authorId": "1403602266"}, {"name": "Nelson Elhage", "authorId": "2866708"}, {"name": "Zac Hatfield-Dodds", "authorId": "1573482302"}, {"name": "T. Henighan", "authorId": "103143311"}, {"name": "Tristan Hume", "authorId": "2162194147"}, {"name": "Scott Johnston", "authorId": "2154610174"}, {"name": "Benjamin Mann", "authorId": "2056658938"}, {"name": "Chris Olah", "authorId": "2287268442"}, {"name": "Catherine Olsson", "authorId": "2061321863"}, {"name": "Dario Amodei", "authorId": "2698777"}, {"name": "Nicholas Joseph", "authorId": "2117706920"}, {"name": "Jared Kaplan", "authorId": "2053807409"}, {"name": "Sam McCandlish", "authorId": "52238703"}], "n_citations": 118}, "snippets": ["Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a family of models where most of the data is unique but a small fraction of it is repeated many times. We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1% of the data 100 times, despite the other 90% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model's capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work - attempting to reverse engineer the detailed computations performed by the model - by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance."], "score": 0.0}, {"id": "(Brandizzi et al., 2024)", "paper": {"corpus_id": 273323858, "title": "Data Processing for the OpenGPT-X Model Family", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Nicolo\u2019 Brandizzi", "authorId": "2108867346"}, {"name": "Hammam Abdelwahab", "authorId": "2899331"}, {"name": "Anirban Bhowmick", "authorId": "2239019329"}, {"name": "Lennard Helmer", "authorId": "2313826462"}, {"name": "Benny Stein", "authorId": "2325731547"}, {"name": "Pavel Denisov", "authorId": "2325730238"}, {"name": "Qasid Saleem", "authorId": "2210670041"}, {"name": "Michael Fromm", "authorId": "2258551638"}, {"name": "Mehdi Ali", "authorId": "2258668067"}, {"name": "Richard Rutmann", "authorId": "2258550842"}, {"name": "Farzad Naderi", "authorId": "2325731187"}, {"name": "Mohamad Saif Agy", "authorId": "2325730227"}, {"name": "Alexander Schwirjow", "authorId": "2325730246"}, {"name": "Fabian K\u00fcch", "authorId": "2332085973"}, {"name": "Luzian Hahn", "authorId": "2335570562"}, {"name": "Malte Ostendorff", "authorId": "2123349725"}, {"name": "Pedro Ortiz Suarez", "authorId": "2302563988"}, {"name": "Georg Rehm", "authorId": "77903998"}, {"name": "Dennis Wegener", "authorId": "2313822383"}, {"name": "Nicolas Flores-Herr", "authorId": "2347259072"}, {"name": "Joachim K\u00f6hler", "authorId": "2330411614"}, {"name": "Johannes Leveling", "authorId": "2258552277"}], "n_citations": 2}, "snippets": ["Recent trends in dataset curation reflect an increasing emphasis on quality over quantity, with techniques such as advanced filtering and deduplication becoming more prevalent [81,92]", "Traditionally, data quality has been assessed through intrinsic signals derived from the data itself, such as average word length, sentence complexity, or overall structure. However, as the field has advanced, the focus has shifted towards leveraging pretrained LLMs to judge high-quality content or even generate synthetic data, a strategy that has proven effective in improving model performance on specific benchmarks [39,(Kumar et al., 2020)(Yoo et al., 2021). One reason for the success of these methods is the alignment between human judgments of quality and the assessments made by LLMs [91,53]31]."], "score": 0.5}, {"id": "(Kumar et al., 2020)", "paper": {"corpus_id": 211987786, "title": "Data Augmentation using Pre-trained Transformer Models", "year": 2020, "venue": "LIFELONGNLP", "authors": [{"name": "Varun Kumar", "authorId": "40574366"}, {"name": "Ashutosh Choudhary", "authorId": "47992757"}, {"name": "Eunah Cho", "authorId": "4006425"}], "n_citations": 356}, "snippets": ["Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of transformer based pre-trained models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. Additionally, on three classification benchmarks, pre-trained Seq2Seq model outperforms other data augmentation methods in a low-resource setting. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information."], "score": 0.0}, {"id": "(Yoo et al., 2021)", "paper": {"corpus_id": 233296100, "title": "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Kang Min Yoo", "authorId": "31760501"}, {"name": "Dongju Park", "authorId": "13453892"}, {"name": "Jaewook Kang", "authorId": "35518563"}, {"name": "Sang-Woo Lee", "authorId": "3226948"}, {"name": "Woomyeong Park", "authorId": "2087289230"}], "n_citations": 242}, "snippets": ["Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classification eliminates the need for fine-tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large-scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft-labels predicted by the language models, effectively distilling knowledge from the large-scale language models and creating textual perturbations simultaneously. We perform data augmentation experiments on diverse classification tasks and show that our method hugely outperforms existing text augmentation methods. Ablation studies and a qualitative analysis provide more insights into our approach."], "score": 0.0}, {"id": "(Longpre et al., 2023)", "paper": {"corpus_id": 258832491, "title": "A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "S. Longpre", "authorId": "29909347"}, {"name": "Gregory Yauney", "authorId": "32918271"}, {"name": "Emily Reif", "authorId": "49849144"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "Jason Wei", "authorId": "119640649"}, {"name": "Kevin Robinson", "authorId": "2148473059"}, {"name": "David M. Mimno", "authorId": "38917723"}, {"name": "Daphne Ippolito", "authorId": "7975935"}], "n_citations": 166}, "snippets": ["Pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. We pretrain models on data curated (1) at different collection times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we find that temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we measure the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Third, we empirically validate that heterogeneous data sources, like books and web, are beneficial and warrant greater prioritization. To date, these experiments constitute the single largest publicly documented empirical study of the effects of pretraining data. Spanning 28 unique 1.5 billion parameter models pretrained from scratch, these findings validate, quantify, and expose many undocumented intuitions about text pretraining, which ultimately support more informed data-centric decisions in model development."], "score": 0.0}], "table": null}, {"title": "Types of Data Quality Filtering Approaches", "tldr": "Data quality filtering techniques can be broadly categorized into heuristic-based approaches that use manually defined rules and model-based approaches that employ classifiers or language models to assess content quality. These approaches can be further classified as reference-dependent (comparing against high-quality seed datasets) or reference-free (using predefined metrics without comparison to reference data). (5 sources)", "text": "\nData quality filtering techniques for language model pre-training can be organized into several major categories, each with distinct methodologies and tradeoffs. At the highest level, these approaches can be divided into heuristic-based methods and model-based methods <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277113645\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\nHeuristic-based methods rely on manually defined rules to identify and remove low-quality content. These include filters based on textual characteristics such as mean word length, stop word fraction, word repetitions, symbol-to-word ratios, and the presence of uppercase letters <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277113645\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. For example, researchers might establish rules to retain only text containing digits, discard sentences composed entirely of uppercase letters, or remove files where the ratio of symbols to words exceeds a certain threshold <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nModel-based approaches can be further categorized into reference-dependent and reference-free methods <Paper corpusId=\"271874495\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Reference-dependent methods compare data against high-quality seed datasets to determine what should be kept or filtered out. These include binary classification approaches, which have been employed in major language models like GPT-3 <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper> and PaLM <Paper corpusId=\"247951931\" paperTitle=\"(Chowdhery et al., 2022)\" isShortName></Paper>. While effective, reference-dependent methods can introduce biases present in the reference data, potentially limiting the diversity and representativeness of the resulting training corpus <Paper corpusId=\"271874495\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nIn contrast, reference-free methods evaluate data quality using predefined metrics without requiring comparison to reference datasets. For instance, perplexity gating uses scores from pre-trained models to assess content quality <Paper corpusId=\"271874495\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. These approaches can maintain greater diversity in the training data by avoiding the biases inherent in reference datasets.\n\nMore sophisticated model-based approaches directly employ language models similar to BERT or GPT to evaluate the quality or value of training data <Paper corpusId=\"277113645\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. These methods leverage the capabilities of existing language models to make more nuanced judgments about content quality, potentially capturing aspects of quality that might be missed by simpler heuristic approaches.", "citations": [{"id": "(Liu et al., 2024)", "paper": {"corpus_id": 266755678, "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference", "year": 2024, "venue": "Neurocomputing", "authors": [{"name": "Yi-Hsueh Liu", "authorId": "2116426849"}, {"name": "Haoyang He", "authorId": "2155082967"}, {"name": "Tianle Han", "authorId": "2184719751"}, {"name": "Xu Zhang", "authorId": "2273584640"}, {"name": "Mengyuan Liu", "authorId": "2210636248"}, {"name": "Jiaming Tian", "authorId": "2257433902"}, {"name": "Yutong Zhang", "authorId": "2257095790"}, {"name": "Jiaqi Wang", "authorId": "2110238778"}, {"name": "Xiaohui Gao", "authorId": "2277869261"}, {"name": "Tianyang Zhong", "authorId": "2215167446"}, {"name": "Yi Pan", "authorId": "2221032216"}, {"name": "Shaochen Xu", "authorId": "2211904452"}, {"name": "Zihao Wu", "authorId": "2263593041"}, {"name": "Zheng Liu", "authorId": "2145977326"}, {"name": "Xin Zhang", "authorId": "2257586495"}, {"name": "Shu Zhang", "authorId": "2277750447"}, {"name": "Xintao Hu", "authorId": "1742535"}, {"name": "Tuo Zhang", "authorId": "49104946"}, {"name": "Ning Qiang", "authorId": "2251076040"}, {"name": "Tianming Liu", "authorId": "2254792886"}, {"name": "Bao Ge", "authorId": "2257302793"}], "n_citations": 74}, "snippets": ["Quality filtering: Filtering low-quality data is typically done using heuristic-based methods or classifier-based methods. Heuristic methods involve employing manually defined rules to eliminate low-quality data [84; 72]. For instance, rules could be set to retain only text containing digits, discard sentences composed entirely of uppercase letters, and remove files with a symbol and word ratio exceeding 0.1, and so forth. Classifier-based methods involve training a classifier on a high-quality dataset such as WebText (Radford et al., 2019) to filter out low-quality datasets."], "score": 0.63134765625}, {"id": "(Li et al., 2025)", "paper": {"corpus_id": 277113645, "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jiazheng Li", "authorId": "2260817933"}, {"name": "Lu Yu", "authorId": "2350870800"}, {"name": "Qing Cui", "authorId": "2279851906"}, {"name": "Zhiqiang Zhang", "authorId": "2344807347"}, {"name": "Jun Zhou", "authorId": "2344948641"}, {"name": "Yanfang Ye", "authorId": "2093920413"}, {"name": "Chuxu Zhang", "authorId": "2117879943"}], "n_citations": 0}, "snippets": ["In terms of data selection methods, the objective is to extract high-quality subsets from the original datasets such that training LLMs on these subsets can achieve similar or even superior performance on downstream tasks compared to training on the entire original datasets. There are two main categories of methods: heuristic-based approaches and model-based approaches. For the former, a variety of manually designed filters, including mean word length, stop word fraction, and word repetitions, are frequently applied to preprocess raw web data (Tirumala et al., 2023). For the latter, models similar to BERT (Devlin et al., 2019) are commonly used to evaluate and select data with higher scores, while other approaches directly employ GPT-like models to assess the data's value."], "score": 0.59228515625}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 271874495, "title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Ruihang Li", "authorId": "2262451401"}, {"name": "Yixuan Wei", "authorId": "2107995927"}, {"name": "Miaosen Zhang", "authorId": "2273515255"}, {"name": "Nenghai Yu", "authorId": "2316146999"}, {"name": "Han Hu", "authorId": "2262465375"}, {"name": "Houwen Peng", "authorId": "2243685256"}], "n_citations": 4}, "snippets": ["Quality filters aim to extract high-quality data from a noisy raw corpus, thereby improving the language model's performance without increasing training costs. Existing filters are broadly classified into two categories: reference-dependent and reference-free approaches. Reference-dependent methods, such as binary classification (Brown et al., 2020)Gao et al., 2020;(Chowdhery et al., 2022) and DSIR (Xie et al., 2023), filter out low-quality data by comparing it with high-quality seed datasets. While effective, these methods inevitably introduce biases present in the reference data, such as specific writing styles or topics, thereby limiting the diversity and representativeness of training corpus (Soldaini et al., 2023). In contrast, reference-free methods, such as perplexity gating (Marion et al., 2023), assess data quality using predefined metrics like perplexity scores from pre-trained models."], "score": 0.849609375}, {"id": "(Brown et al., 2020)", "paper": {"corpus_id": 218971783, "title": "Language Models are Few-Shot Learners", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tom B. Brown", "authorId": "31035595"}, {"name": "Benjamin Mann", "authorId": "2056658938"}, {"name": "Nick Ryder", "authorId": "39849748"}, {"name": "Melanie Subbiah", "authorId": "2065894334"}, {"name": "J. Kaplan", "authorId": "152724169"}, {"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Arvind Neelakantan", "authorId": "2072676"}, {"name": "Pranav Shyam", "authorId": "67311962"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Ariel Herbert-Voss", "authorId": "1404060687"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "T. Henighan", "authorId": "103143311"}, {"name": "R. Child", "authorId": "48422824"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Daniel M. Ziegler", "authorId": "2052152920"}, {"name": "Jeff Wu", "authorId": "49387725"}, {"name": "Clemens Winter", "authorId": "2059411355"}, {"name": "Christopher Hesse", "authorId": "144239765"}, {"name": "Mark Chen", "authorId": "2108828435"}, {"name": "Eric Sigler", "authorId": "2064673055"}, {"name": "Ma-teusz Litwin", "authorId": "1380985420"}, {"name": "Scott Gray", "authorId": "145565184"}, {"name": "Benjamin Chess", "authorId": "1490681878"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Christopher Berner", "authorId": "133740015"}, {"name": "Sam McCandlish", "authorId": "52238703"}, {"name": "Alec Radford", "authorId": "38909097"}, {"name": "I. Sutskever", "authorId": "1701686"}, {"name": "Dario Amodei", "authorId": "2698777"}], "n_citations": 42437}, "snippets": ["Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."], "score": 0.0}, {"id": "(Chowdhery et al., 2022)", "paper": {"corpus_id": 247951931, "title": "PaLM: Scaling Language Modeling with Pathways", "year": 2022, "venue": "Journal of machine learning research", "authors": [{"name": "Aakanksha Chowdhery", "authorId": "2841893"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Gaurav Mishra", "authorId": "2159632445"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "P. Barham", "authorId": "152399055"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Charles Sutton", "authorId": "152549864"}, {"name": "Sebastian Gehrmann", "authorId": "3159346"}, {"name": "Parker Schuh", "authorId": "2620528"}, {"name": "Kensen Shi", "authorId": "2362367"}, {"name": "Sasha Tsvyashchenko", "authorId": "2160888237"}, {"name": "Joshua Maynez", "authorId": "2124977868"}, {"name": "Abhishek Rao", "authorId": "1484043592"}, {"name": "Parker Barnes", "authorId": "80940648"}, {"name": "Yi Tay", "authorId": "144447820"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Vinodkumar Prabhakaran", "authorId": "3331141"}, {"name": "Emily Reif", "authorId": "49849144"}, {"name": "Nan Du", "authorId": "2140321952"}, {"name": "Ben Hutchinson", "authorId": "2044655623"}, {"name": "Reiner Pope", "authorId": "2161431901"}, {"name": "James Bradbury", "authorId": "2065251344"}, {"name": "Jacob Austin", "authorId": "2058365883"}, {"name": "M. Isard", "authorId": "2090818"}, {"name": "Guy Gur-Ari", "authorId": "2284681044"}, {"name": "Pengcheng Yin", "authorId": "38253388"}, {"name": "Toju Duke", "authorId": "2145151992"}, {"name": "Anselm Levskaya", "authorId": "6639036"}, {"name": "S. Ghemawat", "authorId": "1780892"}, {"name": "Sunipa Dev", "authorId": "50991767"}, {"name": "H. Michalewski", "authorId": "47407464"}, {"name": "Xavier Garc\u00eda", "authorId": "143936294"}, {"name": "Vedant Misra", "authorId": "40055795"}, {"name": "Kevin Robinson", "authorId": "2148473059"}, {"name": "L. Fedus", "authorId": "2096916416"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "Daphne Ippolito", "authorId": "7975935"}, {"name": "D. Luan", "authorId": "150970919"}, {"name": "Hyeontaek Lim", "authorId": "8939217"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "A. Spiridonov", "authorId": "1572884723"}, {"name": "Ryan Sepassi", "authorId": "35474601"}, {"name": "David Dohan", "authorId": "35363891"}, {"name": "Shivani Agrawal", "authorId": "3504647"}, {"name": "Mark Omernick", "authorId": "3175815"}, {"name": "Andrew M. Dai", "authorId": "2555924"}, {"name": "Thanumalayan Sankaranarayana Pillai", "authorId": "2598683"}, {"name": "Marie Pellat", "authorId": "97905921"}, {"name": "Aitor Lewkowycz", "authorId": "102549875"}, {"name": "Erica Moreira", "authorId": "2057453483"}, {"name": "R. Child", "authorId": "48422824"}, {"name": "Oleksandr Polozov", "authorId": "2636739"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Zongwei Zhou", "authorId": "2198519"}, {"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Brennan Saeta", "authorId": "4125424"}, {"name": "Mark D\u00edaz", "authorId": "2152965375"}, {"name": "Orhan Firat", "authorId": "2345617"}, {"name": "Michele Catasta", "authorId": "1754926"}, {"name": "Jason Wei", "authorId": "119640649"}, {"name": "K. Meier-Hellstern", "authorId": "1398655031"}, {"name": "D. Eck", "authorId": "2396681"}, {"name": "J. Dean", "authorId": "48448318"}, {"name": "Slav Petrov", "authorId": "1754497"}, {"name": "Noah Fiedel", "authorId": "22640071"}], "n_citations": 6293}, "snippets": ["Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."], "score": 0.0}], "table": null}, {"title": "Classifier-Based Filtering Methods", "tldr": "Classifier-based filtering methods employ machine learning models to assess text quality, with approaches ranging from lightweight models like FastText to more complex architectures like BERT. Despite their varying computational requirements, classifier-based methods have become foundational in modern LLM data filtering pipelines, offering more flexibility and robustness than rule-based approaches. (10 sources)", "text": "\nClassifier-based filtering methods have emerged as a sophisticated approach to evaluating pre-training data quality, offering greater flexibility and robustness compared to simple rule-based techniques <Paper corpusId=\"274422859\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. These methods leverage machine learning models to score and classify text based on quality metrics, enabling more nuanced filtering decisions.\n\nFastText-based classifiers have gained significant traction in production data filtering pipelines due to their computational efficiency. Recent surveys indicate that despite the development of more complex methods, a combination of FastText classifiers with English language filtering following extensive deduplication remains surprisingly effective and represents the current state-of-the-art approach for many tasks <Paper corpusId=\"272524632\" paperTitle=\"(Thrush et al., 2024)\" isShortName></Paper>. Major language models, including Llama-3, utilize FastText alongside RoBERTa-based models in their data filtering workflows <Paper corpusId=\"274514936\" paperTitle=\"(Hausenloy et al., 2024)\" isShortName></Paper>. The computational advantage of FastText is substantial - processing 15 trillion tokens with a FastText classifier requires approximately 1,000 CPU hours on an 80-CPU machine, whereas LLM-based classifiers would demand around 6,000 H100 GPU hours for the same task <Paper corpusId=\"278394813\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nMore sophisticated classifier approaches employ transformer-based models like BERT to evaluate text quality. For example, in developing high-quality Chinese pre-training datasets, researchers have implemented pipelines that combine handcrafted rules with BERT-based quality evaluation models that assign quality scores to each text <Paper corpusId=\"264935645\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. BERT has become widely adopted for quality evaluation due to its exceptional performance in text classification and understanding tasks, with its effectiveness stemming from pre-training objectives that enable powerful text representation capabilities <Paper corpusId=\"274422859\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nReference-based classification approaches train binary classifiers using high-quality corpora as positive examples and lower-quality data as negative examples. Researchers have experimented with various reference corpora for training these classifiers, including \"WIKIWEBBOOKS,\" \"OPENWEB,\" \"WIKI,\" and \"WIKIREFS,\" often following the approach used in GPT-3's quality classifier <Paper corpusId=\"266977204\" paperTitle=\"(Lucy et al., 2024)\" isShortName></Paper>. These methods typically filter data for a specific language, often using FastText language identification as a preliminary step <Paper corpusId=\"266977204\" paperTitle=\"(Lucy et al., 2024)\" isShortName></Paper> <Paper corpusId=\"207870323\" paperTitle=\"(Wenzek et al., 2019)\" isShortName></Paper>.\n\nRecent advances in classifier-based filtering have introduced more specialized approaches. WanjuanCC employs BERT-based classifiers to identify and remove data containing excessive advertisements or exhibiting lower fluency <Paper corpusId=\"277955900\" paperTitle=\"(Zhuang et al., 2025)\" isShortName></Paper>. QuRating represents a more sophisticated framework that simulates human-like text quality assessments using four criteria (writing style, required expertise, facts and trivia, and educational value) to guide data selection <Paper corpusId=\"277955900\" paperTitle=\"(Zhuang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"267681974\" paperTitle=\"(Wettig et al., 2024)\" isShortName></Paper>. For multilingual datasets, specialized filtering frameworks have been developed that leverage both Transformer and FastText-based classifiers to identify structured and knowledge-rich samples across different languages <Paper corpusId=\"276394897\" paperTitle=\"(Messmer et al., 2025)\" isShortName></Paper>.\n\nRecent trends show that LLM-based classifiers are increasingly being used for data filtering, enabling more dynamic systems that require minimal prompting and eliminate the need for predefined heuristics <Paper corpusId=\"274514936\" paperTitle=\"(Hausenloy et al., 2024)\" isShortName></Paper>. However, the significant computational costs of LLM-based approaches mean that FastText-based methods remain competitive for many practical applications, offering a balance between performance and efficiency <Paper corpusId=\"278394813\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 274422859, "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Wanyue Zhang", "authorId": "2333395980"}, {"name": "Ziyong Li", "authorId": "2333247185"}, {"name": "Wen Yang", "authorId": "2218735807"}, {"name": "Chunlin Leng", "authorId": "2333234263"}, {"name": "Yinan Bai", "authorId": "2333317901"}, {"name": "Qianlong Du", "authorId": "8134471"}, {"name": "Chengqing Zong", "authorId": "2064100826"}, {"name": "Jiajun Zhang", "authorId": "2283356334"}], "n_citations": 0}, "snippets": ["Quality Evaluation. In addition to rule-based methods, quality evaluation approaches are also employed to identify high-quality texts using well-designed classifiers. Unlike rule-based methods, which primarily filter out explicit noise from raw content, quality evaluation approaches offer greater robustness and flexibility. These approaches leverage models such as logistic regression [19], BERT [20], FastText [21], and others to calculate probability scores for each text. Based on these scores, texts are classified as positive or negative according to a predefined threshold. Among these models, BERT has emerged as one of the most widely used architectures for quality evaluation due to its exceptional performance in text classification and understanding tasks. BERT's effectiveness stems from its pre-training objectives, including masked language modeling and next-sentence prediction, which enable it to learn powerful text representation and comprehension capabilities."], "score": 0.5185546875}, {"id": "(Thrush et al., 2024)", "paper": {"corpus_id": 272524632, "title": "Improving Pretraining Data Using Perplexity Correlations", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Tristan Thrush", "authorId": "1500242049"}, {"name": "Christopher Potts", "authorId": "2279335958"}, {"name": "Tatsunori Hashimoto", "authorId": "2214604036"}], "n_citations": 22}, "snippets": ["According to a recent survey of data selection approaches by Li et al. (2024), the heavier-weight pretraining data selection methods have not shown large gains, and the current state-of-the-art across many tasks is primitive: a fixed fastText classifier (Joulin et al., 2016) combined with an English filter as a final layer after extensive deduplication and filtering."], "score": 0.7861328125}, {"id": "(Hausenloy et al., 2024)", "paper": {"corpus_id": 274514936, "title": "Towards Data Governance of Frontier AI Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jason Hausenloy", "authorId": "2258553730"}, {"name": "Duncan McClements", "authorId": "2333897525"}, {"name": "Madhavendra Thakur", "authorId": "2333900015"}], "n_citations": 2}, "snippets": ["Existing Work: Data filtering can be, and is often used to enhance model performance by removing irrelevant or lowquality data. Traditional filtering techniques focus on predefined heuristics for determining unsafe content [32]. Llama-3 uses classifiers like fastText and RoBERTa-based models to filter training data [32]. The use of LLMs to filter data allows for a more dynamic system with minimal prompting [100], and no need to define heuristics for the determination of \"unsafe content\", as in current safety filtering systems [32]."], "score": 0.509765625}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 278394813, "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yudong Wang", "authorId": "2312864296"}, {"name": "Zixuan Fu", "authorId": "2354260586"}, {"name": "Jie Cai", "authorId": "2295809950"}, {"name": "Peijun Tang", "authorId": "2359636324"}, {"name": "Hongya Lyu", "authorId": "2359634927"}, {"name": "Yewei Fang", "authorId": "2295846809"}, {"name": "Zhi Zheng", "authorId": "2295929465"}, {"name": "Jie Zhou", "authorId": "2295789325"}, {"name": "Guoyang Zeng", "authorId": "1398454307"}, {"name": "Chaojun Xiao", "authorId": "51131083"}, {"name": "Xu Han", "authorId": "2324934386"}, {"name": "Zhiyuan Liu", "authorId": "2316519794"}], "n_citations": 1}, "snippets": ["Current high-quality data classifiers are primarily divided into LLM-based (Penedo et al., 2024;Yu et al., 2025;Wang et al., 2024) and fastText-based (Li et al., 2024;Shao et al., 2024;Guo et al., 2024) methods. \n\nWhile LLM-based classifiers are effective, they need significantly higher inference costs. To address this, we adopt a fastText-based classifier, which significantly reduces inference costs while maintaining competitive performance under certain conditions. This approach not only minimizes resource consumption but also speeds up data filtering experiments. For instance, as shown in Table 2, processing 15T tokens with an LLM-based classifier requires approximately 6,000 H100 GPU hours, while fastText can complete the same task on a non-GPU machine with just 80 CPUs in 1,000 hours, significantly improving efficiency."], "score": 0.81103515625}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 264935645, "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jianghao Chen", "authorId": "2264139789"}, {"name": "Pu Jian", "authorId": "2329739061"}, {"name": "Tengxiao Xi", "authorId": "2264961304"}, {"name": "Yidong Yi", "authorId": "2265214644"}, {"name": "Qianlong Du", "authorId": "8134471"}, {"name": "Chenglin Ding", "authorId": "2264336906"}, {"name": "Guibo Zhu", "authorId": "2894321"}, {"name": "Chengqing Zong", "authorId": "2064100826"}, {"name": "Jinqiao Wang", "authorId": "1519293616"}, {"name": "Jiajun Zhang", "authorId": "2124819243"}], "n_citations": 7}, "snippets": ["To extract large-scale and high-quality Chinese pre-training data from the web, we have proposed a new pipeline approach which filter the raw crawled web data with both handcrafted rules and well-designed quality evaluation model. The rules are employed to first extract the Chinese texts and remove duplicate documents, and then filter out the explicit noisy contents such as toxic texts and advertisements. The quality evaluation model is designed based on BERT and can assign each text with a quality score."], "score": 0.55908203125}, {"id": "(Lucy et al., 2024)", "paper": {"corpus_id": 266977204, "title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Li Lucy", "authorId": "15983089"}, {"name": "Suchin Gururangan", "authorId": "40895369"}, {"name": "Luca Soldaini", "authorId": "3328733"}, {"name": "Emma Strubell", "authorId": "2268272"}, {"name": "David Bamman", "authorId": "2064411219"}, {"name": "Lauren Klein", "authorId": "2279335512"}, {"name": "Jesse Dodge", "authorId": "34176020"}], "n_citations": 17}, "snippets": ["We experiment with quality filters that score text based on their similarity to some chosen \"high quality\" reference corpora. We name these filters based on the reference corpora used to train them: WIKIWEBBOOKS, OPENWEB, WIKI, and WIKIREFS (Table 3). We use Gururangan et al. (2022)'s replication of GPT-3's binary logistic regression quality classifier and only vary the positive \"high quality\" class. The negative class is a fixed set of tokens from the September 2019 dump of Common Crawl, and each class contains approximately 300M tokens. We also compare WIKI to a perplexity-based text scorer, WIKI ppl , which uses a 5-gram Kneser-Ney language model trained on Wikipedia instead of a classifier (Wenzek et al., 2019)(Laurenccon et al., 2023)Muennighoff et al., 2023;Marion et al., 2023)", "Our data is pre-filtered to documents that fastText langID scores as likely English (Joulin et al., 2016b,a)."], "score": 0.529296875}, {"id": "(Wenzek et al., 2019)", "paper": {"corpus_id": 207870323, "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data", "year": 2019, "venue": "International Conference on Language Resources and Evaluation", "authors": [{"name": "Guillaume Wenzek", "authorId": "2293203"}, {"name": "M. Lachaux", "authorId": "114952298"}, {"name": "Alexis Conneau", "authorId": "2480903"}, {"name": "Vishrav Chaudhary", "authorId": "113810201"}, {"name": "F. Guzm\u2019an", "authorId": "2061585840"}, {"name": "Armand Joulin", "authorId": "2319608"}, {"name": "Edouard Grave", "authorId": "3024698"}], "n_citations": 658}, "snippets": ["Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia."], "score": 0.0}, {"id": "(Zhuang et al., 2025)", "paper": {"corpus_id": 277955900, "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xinlin Zhuang", "authorId": "2366068443"}, {"name": "Jiahui Peng", "authorId": "2233445161"}, {"name": "Ren Ma", "authorId": "2299118979"}, {"name": "Yinfan Wang", "authorId": "2352285941"}, {"name": "Tianyi Bai", "authorId": "2318978696"}, {"name": "Xingjian Wei", "authorId": "2298376663"}, {"name": "Jiantao Qiu", "authorId": "2289911484"}, {"name": "Chi Zhang", "authorId": "2325489246"}, {"name": "Ying Qian", "authorId": "2356585875"}, {"name": "Conghui He", "authorId": "2346476781"}], "n_citations": 0}, "snippets": ["More recently, more model-based classifiers have been introduced to assess the quality of pretraining data for LLMs. WanjuanCC (Qiu et al., 2024) employs two BERT-based classifiers to filter out data containing excessive advertisements and exhibiting lower fluency. QuRating (Wettig et al., 2024) introduces an innovative framework that simulates human-like text quality assessments, proposing four criteria to guide data selection. Similarly, Fineweb-Edu (Penedo et al., 2024) focuses specifically on assessing the Educational Value of data."], "score": 0.72216796875}, {"id": "(Wettig et al., 2024)", "paper": {"corpus_id": 267681974, "title": "QuRating: Selecting High-Quality Data for Training Language Models", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alexander Wettig", "authorId": "2127066887"}, {"name": "Aatmik Gupta", "authorId": "2284268826"}, {"name": "Saumya Malik", "authorId": "2323513320"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 79}, "snippets": ["Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications."], "score": 0.0}, {"id": "(Messmer et al., 2025)", "paper": {"corpus_id": 276394897, "title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Bettina Messmer", "authorId": "2219037377"}, {"name": "Vinko Sabolcec", "authorId": "2350460665"}, {"name": "Martin Jaggi", "authorId": "2328413457"}], "n_citations": 2}, "snippets": ["To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data."], "score": 0.56005859375}], "table": null}, {"title": "Effectiveness and Impact of Quality Filtering", "tldr": "Data quality filtering for LLM pre-training shows a complex relationship between filtering intensity and model performance, with moderate filtering generally improving results while excessive filtering can reduce data diversity and hurt performance. The effectiveness of filtering approaches varies based on filtering methods, evaluation tasks, and model architectures, with perplexity-based filtering methods showing particularly strong results. (4 sources)", "text": "\nThe effectiveness of quality filtering in LLM pre-training exhibits a nuanced relationship with model performance, revealing both benefits and potential drawbacks. While quality filtering generally improves model performance despite reducing data quantity, research indicates the existence of an optimal filtering intensity beyond which performance may deteriorate <Paper corpusId=\"265609639\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This non-linear relationship suggests that while filtering out low-quality data is beneficial, excessive filtering can remove valuable information and reduce data diversity.\n\nSeveral studies demonstrate the effectiveness of carefully filtered high-quality datasets in training lightweight LLMs with outstanding performance <Paper corpusId=\"265609639\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. However, researchers have also observed that aggressive filtering can lead to performance degradation across a wide range of tasks for GPT-like models due to poor representativity of the filtering proxy objectives <Paper corpusId=\"265609639\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This finding highlights the challenge of selecting appropriate filtering criteria that maintain data diversity while removing genuinely low-quality content.\n\nIn evaluating different filtering approaches, perplexity-based methods have shown particularly promising results. Experimental findings indicate that perplexity filtering based on N-gram language models can be especially effective for processing massive volumes of corpora <Paper corpusId=\"270514462\" paperTitle=\"(Enomoto et al., 2024)\" isShortName></Paper>. However, these studies also confirm that overly aggressive filtering using these methods can result in performance deterioration when evaluated on general language understanding tasks <Paper corpusId=\"270514462\" paperTitle=\"(Enomoto et al., 2024)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\nBeyond pre-training, quality filtering has shown benefits for downstream tasks and model fine-tuning. For instance, heuristic negative sample filtering has been demonstrated to improve both faithfulness and correctness in question-answering models by preserving the integrity of faithfulness relations between positive and negative examples <Paper corpusId=\"276776523\" paperTitle=\"(Li et al._1, 2025)\" isShortName></Paper>. This suggests that targeted filtering approaches can enhance model performance on specific tasks beyond general language understanding.\n\nDespite these advances, researchers note that there remains a lack of well-established and theoretically efficient filtering strategies, indicating opportunities for further exploration in this area <Paper corpusId=\"265609639\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. The varying effectiveness of different filtering approaches across model architectures and evaluation tasks suggests that optimal filtering strategies may need to be tailored to specific applications and model designs rather than applying one-size-fits-all solutions.", "citations": [{"id": "(Wang et al., 2023)", "paper": {"corpus_id": 265609639, "title": "Data Management For Training Large Language Models: A Survey", "year": 2023, "venue": "", "authors": [{"name": "Zige Wang", "authorId": "2238165141"}, {"name": "Wanjun Zhong", "authorId": "2249763710"}, {"name": "Yufei Wang", "authorId": "46395829"}, {"name": "Qi Zhu", "authorId": "2269768949"}, {"name": "Fei Mi", "authorId": "2258717400"}, {"name": "Baojun Wang", "authorId": "2239032344"}, {"name": "Lifeng Shang", "authorId": "2238661808"}, {"name": "Xin Jiang", "authorId": "2257942536"}, {"name": "Qun Liu", "authorId": "2249841180"}], "n_citations": 14}, "snippets": ["Despite the reduction of data quantity, quality filtering is usually proven to be beneficial in model performance improvement (Longpre et al., 2023). Several carefully filtered high-quality datasets are proposed to train lightweight LLMs with outstanding performances (Gunasekar et al., 2023;Li et al., 2023d;Javaheripi and Bubeck, 2023;(Penedo et al., 2023). However, Gao (2021) finds that aggressive filtering might lead to performance degradation on a wide range of tasks for GPT-like LLMs due to the poor representativity of the filtering proxy objectives. To address this issue, Marion et al. (2023) comprehensively examine different data quality estimators and find that pruning datasets based on perplexity performs better than more complicated techniques like memorization. Gan et al. (2023) develop data-centric scaling laws and show that improving semantic and grammatical quality is more effective. However, there still lacks a wellestablished and theoretically efficient filtering strategy, leaving room for further exploration."], "score": 0.77734375}, {"id": "(Enomoto et al., 2024)", "paper": {"corpus_id": 270514462, "title": "Investigating Web Corpus Filtering Methods for Language Model Development in Japanese", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Rintaro Enomoto", "authorId": "2306632898"}, {"name": "A. Tolmachev", "authorId": "153616317"}, {"name": "Takuro Niitsuma", "authorId": "2301579122"}, {"name": "Shuhei Kurita", "authorId": "2306632538"}, {"name": "Daisuke Kawahara", "authorId": "2306632595"}], "n_citations": 3}, "snippets": ["We test the perplexity of a language model and a relatively fast classifier to process a massive volume of corpora. The experimental results show that the perplexity filtering method based on an N-gram language model is the best. We also pretrain BERT (Devlin et al., 2019) on a Japanese Web corpus filtered by the N-gram language model and evaluated it on Japanese General Language Understanding Evaluation, JGLUE (Kurihara et al., 2022). The results show that massively strong filtering results in performance deterioration."], "score": 0.67919921875}, {"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(Li et al._1, 2025)", "paper": {"corpus_id": 276776523, "title": "Generate, Discriminate, Evolve: Enhancing Context Faithfulness via Fine-Grained Sentence-Level Self-Evolution", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Kun Li", "authorId": "2185631323"}, {"name": "Tianhua Zhang", "authorId": "2146333115"}, {"name": "Yunxiang Li", "authorId": "2155851529"}, {"name": "Hongyin Luo", "authorId": "1944274"}, {"name": "Abdalla Moustafa", "authorId": "2348476303"}, {"name": "Xixin Wu", "authorId": "2107999711"}, {"name": "James Glass", "authorId": "2303402875"}, {"name": "Helen M. Meng", "authorId": "2273659859"}], "n_citations": 2}, "snippets": ["As a result, it is possible that \u03b8 0 can answer the question correctly and faithfully even without access to the evidence passages, provided it has encountered similar information during pretraining. To mitigate this, we involve a heuristic negative sample filtering process for quality control", "This filtering process tries to preserve the integrity of our faithfulness relation between positive and negative sentences. The effectiveness of pre-stage filtering is reported in Tab. 4", "As demonstrated in the Tab. 4, heuristic negative sample filtering improves both faithfulness and correctness, with filtered outperforming non-filterd across all metrics."], "score": 0.54931640625}], "table": null}, {"title": "Case Studies of Quality Filtering in Major LLMs", "tldr": "Major language models employ diverse data filtering approaches, with GPT-3 pioneering classifier-based methods and subsequent models like PaLM and Llama introducing increasingly sophisticated filtering pipelines. These case studies demonstrate the industry's shift toward prioritizing data quality over quantity, with different models employing unique combinations of classifier-based filtering, rule-based filtering, and deduplication techniques. (7 sources)", "text": "\n## GPT-3\nGPT-3 pioneered comprehensive data quality filtering by implementing an automated approach to eliminate low-quality documents from Common Crawl. Its data preprocessing pipeline employed a classifier trained on high-quality corpora (WebText, Wikipedia, and web books) to prioritize documents with higher quality scores. Additionally, GPT-3 implemented fuzzy deduplication within each dataset and removed WebText from Common Crawl to further enhance data quality. <Paper corpusId=\"261243909\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper>\n\n## PaLM\nPaLM (Pathways Language Model) adopted sophisticated filtering strategies to improve training data quality. The 540-billion parameter model's preprocessing pipeline incorporated various filtering techniques to ensure high-quality inputs, which contributed to its state-of-the-art few-shot learning results across hundreds of language understanding and generation benchmarks. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247951931\" paperTitle=\"(Chowdhery et al., 2022)\" isShortName></Paper>\n\n## Llama Models\nThe Llama family of models employed comprehensive data filtering approaches, including both classifier-based and rule-based methods. As mentioned in earlier sections, recent versions utilize FastText alongside RoBERTa-based models in their data filtering workflows, balancing computational efficiency with filtering effectiveness.\n\n## GLaM\nGLaM (Generalist Language Model) focused on data quality despite using a sparsely activated mixture-of-experts architecture. While GLaM was 7x larger than GPT-3 with 1.2 trillion parameters, it achieved better overall zero-shot and one-shot performance across 29 NLP tasks while consuming only 1/3 of the energy used to train GPT-3. This performance improvement can be attributed in part to its data quality filtering processes. <Paper corpusId=\"261243909\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"245124124\" paperTitle=\"(Du et al., 2021)\" isShortName></Paper>\n\n## Publicly Available Filtered Datasets\nSeveral high-quality pre-training datasets have been made publicly available, providing researchers with already-filtered data resources:\n- C4 (Colossal Clean Crawled Corpus): Introduced comprehensive filtering processes to clean web-crawled data. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>\n- The Pile: A diverse, high-quality dataset incorporating multiple filtering layers. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>\n- RefinedWeb: Focused on quality refinement of web-crawled data. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>\n- RedPajama: Provided filtered data specifically designed for open-source language models. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>\n- The Stack: Incorporated various filtering strategies to improve code and text quality. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>\n\n## Deduplication-Focused Approaches\nSeveral models and researchers have prioritized deduplication as a key quality filtering strategy. Researchers found that existing language modeling datasets contained many near-duplicate examples and long repetitive substrings, with over 1% of unprompted output from models being copied verbatim from training data. Improved deduplication tools allowed for removing highly repetitive content (such as a single 61-word English sentence repeated over 60,000 times in C4). Models trained on deduplicated data showed significant improvements, emitting memorized text ten times less frequently while requiring fewer training steps to achieve the same or better accuracy. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235829052\" paperTitle=\"(Lee et al., 2021)\" isShortName></Paper>\n\n## Impact of Temporal Alignment\nModels that carefully considered the temporal relationship between pre-training and evaluation data showed better performance. Research demonstrated that temporal shift between evaluation data and pre-training data leads to performance degradation that is not overcome by fine-tuning, highlighting the importance of considering data freshness in filtering strategies. <Paper corpusId=\"261243909\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258832491\" paperTitle=\"(Longpre et al., 2023)\" isShortName></Paper>", "citations": [{"id": "(Gao et al., 2023)", "paper": {"corpus_id": 261243909, "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Kaiyuan Gao", "authorId": "1944690382"}, {"name": "Su He", "authorId": "2112345574"}, {"name": "Zhenyu He", "authorId": "2152990262"}, {"name": "Jiacheng Lin", "authorId": null}, {"name": "Qizhi Pei", "authorId": "2171652249"}, {"name": "Jie Shao", "authorId": "2234370833"}, {"name": "Wei Zhang", "authorId": "2256597384"}], "n_citations": 5}, "snippets": ["Therefore, recently, there is another trend that obtaining high quality data instead of tremendous scale of low quality data for pre-training.\n\nSince the quality of pre-training data significantly impacts the effectiveness of GPT models and ensures the models capture accurate and reliable information during pre-training, we need to ensure data excellence by involving carefully curating and filtering the text corpus to remove noise, errors, and irrelevant content, ultimately leading to more robust and reliable GPT models [103,104]. Next, we will introduce some data preprocessing strategies for data quality improvement in several representative LLMs, including classifier-based filtering, rule-based filtering and utilizing textbook quality data.\n\nClassifier-based Filtering. In this approach, a selection classifier is trained using high-quality texts as examples. This classifier is then used to analyze the remaining data and distinguish between high-quality and low-quality samples.\n\nThe low-quality data is subsequently eliminated from the dataset, ensuring that only the most reliable and valuable information is retained for further pre-training.\n\n\u2022 GPT-3 [2] devised an automated filtering method to effectively eliminate low-quality documents from the Common Crawl dataset. It used a classifier trained on highquality data (WebText, Wikipedia, and web books corpus) to prioritize documents with higher scores, resulting in improved data quality for generative text samples. Moreover, GPT-3 adopted fuzzy deduplication of documents within each dataset and the removal of WebText from Common Crawl, further improving the data quality."], "score": 0.71533203125}, {"id": "(Ding et al., 2024)", "paper": {"corpus_id": 269187631, "title": "Fewer Truncations Improve Language Modeling", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Hantian Ding", "authorId": "2113455281"}, {"name": "Zijian Wang", "authorId": "2259065741"}, {"name": "Giovanni Paolini", "authorId": "2296990653"}, {"name": "Varun Kumar", "authorId": "40574366"}, {"name": "Anoop Deoras", "authorId": "1713801"}, {"name": "Dan Roth", "authorId": "2258962983"}, {"name": "Stefano Soatto", "authorId": "2264070792"}], "n_citations": 14}, "snippets": ["There has been multiple high-quality pre-training datasets that were made publicly available, e.g., C4 (Raffel et al., 2019), Pile (Gao et al., 2021), RefinedWeb (Penedo et al., 2023), RedPajama (Computer, 2023), and the Stack (Kocetkov et al., 2022;Lozhkov et al., 2024).On top of these, multiple papers (e.g., (Lee et al., 2021)Marion et al., 2023;Chen et al., 2023;(Chowdhery et al., 2022)Touvron et al., 2023a;(Raffel et al., 2019)) propose various filtering strategies to improve data quality."], "score": 0.576171875}, {"id": "(Chowdhery et al., 2022)", "paper": {"corpus_id": 247951931, "title": "PaLM: Scaling Language Modeling with Pathways", "year": 2022, "venue": "Journal of machine learning research", "authors": [{"name": "Aakanksha Chowdhery", "authorId": "2841893"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Gaurav Mishra", "authorId": "2159632445"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "P. Barham", "authorId": "152399055"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Charles Sutton", "authorId": "152549864"}, {"name": "Sebastian Gehrmann", "authorId": "3159346"}, {"name": "Parker Schuh", "authorId": "2620528"}, {"name": "Kensen Shi", "authorId": "2362367"}, {"name": "Sasha Tsvyashchenko", "authorId": "2160888237"}, {"name": "Joshua Maynez", "authorId": "2124977868"}, {"name": "Abhishek Rao", "authorId": "1484043592"}, {"name": "Parker Barnes", "authorId": "80940648"}, {"name": "Yi Tay", "authorId": "144447820"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Vinodkumar Prabhakaran", "authorId": "3331141"}, {"name": "Emily Reif", "authorId": "49849144"}, {"name": "Nan Du", "authorId": "2140321952"}, {"name": "Ben Hutchinson", "authorId": "2044655623"}, {"name": "Reiner Pope", "authorId": "2161431901"}, {"name": "James Bradbury", "authorId": "2065251344"}, {"name": "Jacob Austin", "authorId": "2058365883"}, {"name": "M. Isard", "authorId": "2090818"}, {"name": "Guy Gur-Ari", "authorId": "2284681044"}, {"name": "Pengcheng Yin", "authorId": "38253388"}, {"name": "Toju Duke", "authorId": "2145151992"}, {"name": "Anselm Levskaya", "authorId": "6639036"}, {"name": "S. Ghemawat", "authorId": "1780892"}, {"name": "Sunipa Dev", "authorId": "50991767"}, {"name": "H. Michalewski", "authorId": "47407464"}, {"name": "Xavier Garc\u00eda", "authorId": "143936294"}, {"name": "Vedant Misra", "authorId": "40055795"}, {"name": "Kevin Robinson", "authorId": "2148473059"}, {"name": "L. Fedus", "authorId": "2096916416"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "Daphne Ippolito", "authorId": "7975935"}, {"name": "D. Luan", "authorId": "150970919"}, {"name": "Hyeontaek Lim", "authorId": "8939217"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "A. Spiridonov", "authorId": "1572884723"}, {"name": "Ryan Sepassi", "authorId": "35474601"}, {"name": "David Dohan", "authorId": "35363891"}, {"name": "Shivani Agrawal", "authorId": "3504647"}, {"name": "Mark Omernick", "authorId": "3175815"}, {"name": "Andrew M. Dai", "authorId": "2555924"}, {"name": "Thanumalayan Sankaranarayana Pillai", "authorId": "2598683"}, {"name": "Marie Pellat", "authorId": "97905921"}, {"name": "Aitor Lewkowycz", "authorId": "102549875"}, {"name": "Erica Moreira", "authorId": "2057453483"}, {"name": "R. Child", "authorId": "48422824"}, {"name": "Oleksandr Polozov", "authorId": "2636739"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Zongwei Zhou", "authorId": "2198519"}, {"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Brennan Saeta", "authorId": "4125424"}, {"name": "Mark D\u00edaz", "authorId": "2152965375"}, {"name": "Orhan Firat", "authorId": "2345617"}, {"name": "Michele Catasta", "authorId": "1754926"}, {"name": "Jason Wei", "authorId": "119640649"}, {"name": "K. Meier-Hellstern", "authorId": "1398655031"}, {"name": "D. Eck", "authorId": "2396681"}, {"name": "J. Dean", "authorId": "48448318"}, {"name": "Slav Petrov", "authorId": "1754497"}, {"name": "Noah Fiedel", "authorId": "22640071"}], "n_citations": 6293}, "snippets": ["Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."], "score": 0.0}, {"id": "(Du et al., 2021)", "paper": {"corpus_id": 245124124, "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Nan Du", "authorId": "2140321952"}, {"name": "Yanping Huang", "authorId": "2145438541"}, {"name": "Andrew M. Dai", "authorId": "2555924"}, {"name": "Simon Tong", "authorId": "2058177533"}, {"name": "Dmitry Lepikhin", "authorId": "150077954"}, {"name": "Yuanzhong Xu", "authorId": "2145139570"}, {"name": "M. Krikun", "authorId": "2048712"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Adams Wei Yu", "authorId": "40625240"}, {"name": "Orhan Firat", "authorId": "2345617"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "L. Fedus", "authorId": "2096916416"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Zongwei Zhou", "authorId": "1389392654"}, {"name": "Tao Wang", "authorId": null}, {"name": "Yu Emma Wang", "authorId": "2153608756"}, {"name": "Kellie Webster", "authorId": "20825661"}, {"name": "Marie Pellat", "authorId": "97905921"}, {"name": "Kevin Robinson", "authorId": "2148473059"}, {"name": "K. Meier-Hellstern", "authorId": "1398655031"}, {"name": "Toju Duke", "authorId": "2145151992"}, {"name": "Lucas Dixon", "authorId": "2065639113"}, {"name": "Kun Zhang", "authorId": "1556095165"}, {"name": "Quoc V. Le", "authorId": "2827616"}, {"name": "Yonghui Wu", "authorId": "48607963"}, {"name": "Z. Chen", "authorId": "2545358"}, {"name": "Claire Cui", "authorId": "2052275005"}], "n_citations": 826}, "snippets": ["Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks."], "score": 0.0}, {"id": "(Raffel et al., 2019)", "paper": {"corpus_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019, "venue": "Journal of machine learning research", "authors": [{"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Wei Li", "authorId": "2157338362"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 20336}, "snippets": ["Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."], "score": 0.0}, {"id": "(Lee et al., 2021)", "paper": {"corpus_id": 235829052, "title": "Deduplicating Training Data Makes Language Models Better", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Daphne Ippolito", "authorId": "7975935"}, {"name": "A. Nystrom", "authorId": "2064161903"}, {"name": "Chiyuan Zhang", "authorId": "151505981"}, {"name": "D. Eck", "authorId": "2396681"}, {"name": "Chris Callison-Burch", "authorId": "1763608"}, {"name": "Nicholas Carlini", "authorId": "2483738"}], "n_citations": 635}, "snippets": ["We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings.As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data.We develop two tools that allow us to deduplicate training datasets\u2014for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times.Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy.We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation.Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets."], "score": 0.0}, {"id": "(Longpre et al., 2023)", "paper": {"corpus_id": 258832491, "title": "A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "S. Longpre", "authorId": "29909347"}, {"name": "Gregory Yauney", "authorId": "32918271"}, {"name": "Emily Reif", "authorId": "49849144"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "Jason Wei", "authorId": "119640649"}, {"name": "Kevin Robinson", "authorId": "2148473059"}, {"name": "David M. Mimno", "authorId": "38917723"}, {"name": "Daphne Ippolito", "authorId": "7975935"}], "n_citations": 166}, "snippets": ["Pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. We pretrain models on data curated (1) at different collection times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we find that temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we measure the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Third, we empirically validate that heterogeneous data sources, like books and web, are beneficial and warrant greater prioritization. To date, these experiments constitute the single largest publicly documented empirical study of the effects of pretraining data. Spanning 28 unique 1.5 billion parameter models pretrained from scratch, these findings validate, quantify, and expose many undocumented intuitions about text pretraining, which ultimately support more informed data-centric decisions in model development."], "score": 0.0}], "table": null}], "cost": 0.280146}}
