{"original_query": "How do machine unlearning methods perform? Can they be used to remove knowledge from language models? ", "input": "machine unlearning methods to remove knowledge from language models", "output": {"sections": [{"title": "Introduction to Machine Unlearning", "tldr": "Machine unlearning enables selectively removing specific information from trained models without full retraining. This emerging field addresses privacy regulations, ethical concerns, and security issues by providing methods to make models \"forget\" targeted data while maintaining performance on retained knowledge. (9 sources)", "text": "\nMachine unlearning has emerged as a critical area of research in response to regulatory requirements like the \"Right to be Forgotten\" and growing privacy concerns related to large language models (LLMs) <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273502714\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. This field focuses on developing techniques to efficiently remove the influence of specific training data from models without the computational burden of complete retraining <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper> <Paper corpusId=\"278481378\" paperTitle=\"(Vasilev et al., 2025)\" isShortName></Paper>.\n\nThe core objective of machine unlearning is to selectively erase certain knowledge or capabilities from trained models while preserving their general functionality <Paper corpusId=\"273403717\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"278782413\" paperTitle=\"(Jeung et al., 2025)\" isShortName></Paper>. This approach offers practical solutions for several important scenarios, including:\n\n1. Complying with privacy regulations like GDPR that mandate data removal <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273502714\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>\n2. Removing copyrighted or offensive content from models <Paper corpusId=\"273502714\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>\n3. Enhancing security by eliminating poisoned data <Paper corpusId=\"273901406\" paperTitle=\"(Feng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259991722\" paperTitle=\"(Wei et al., 2023)\" isShortName></Paper>\n4. Mitigating biases to promote fairness <Paper corpusId=\"273901406\" paperTitle=\"(Feng et al., 2024)\" isShortName></Paper>\n\nAn effective machine unlearning method must satisfy several key requirements: minimizing retained information from the forget set, maintaining high performance on the retain set, requiring less computational cost than full retraining, and preserving inference efficiency <Paper corpusId=\"278481378\" paperTitle=\"(Vasilev et al., 2025)\" isShortName></Paper>.\n\nWhile machine unlearning research initially focused primarily on computer vision applications, it has expanded to address other domains including federated learning, recommender systems, and graph learning <Paper corpusId=\"273901406\" paperTitle=\"(Feng et al., 2024)\" isShortName></Paper>. With LLMs specifically, unlearning approaches aim to address both structured/classification data and unstructured/textual data <Paper corpusId=\"268681648\" paperTitle=\"(Qu et al., 2024)\" isShortName></Paper>.\n\nRecent developments in the field have moved beyond simple instance-level forgetting to more complex scenarios, such as entity-level unlearning, which aims to erase all knowledge related to a specific entity while preserving other model capabilities <Paper corpusId=\"270562084\" paperTitle=\"(Choi et al., 2024)\" isShortName></Paper>. This shift acknowledges that real-world unlearning scenarios often require more comprehensive knowledge removal than simply forgetting individual data points <Paper corpusId=\"270562084\" paperTitle=\"(Choi et al., 2024)\" isShortName></Paper>. Additionally, the task becomes particularly challenging in modern LLMs because facts can be deduced from one another, requiring more sophisticated unlearning approaches <Paper corpusId=\"273502714\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Pawelczyk et al., 2023)", "paper": {"corpus_id": 263834631, "title": "In-Context Unlearning: Language Models as Few Shot Unlearners", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Martin Pawelczyk", "authorId": "89583148"}, {"name": "Seth Neel", "authorId": "2273685865"}, {"name": "Himabindu Lakkaraju", "authorId": "1892673"}], "n_citations": 132}, "snippets": ["Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth."], "score": 0.998046875}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 273502714, "title": "Evaluating Deep Unlearning in Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ruihan Wu", "authorId": "2303333890"}, {"name": "Chhavi Yadav", "authorId": "83222216"}, {"name": "Russ Salakhutdinov", "authorId": "2266239350"}, {"name": "Kamalika Chaudhuri", "authorId": "2303254420"}], "n_citations": 7}, "snippets": ["Machine unlearning is a key requirement of many data protection regulations such as GDPR. Prior work on unlearning has mostly considered superficial unlearning tasks where a single or a few related pieces of information are required to be removed. However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other", "Machine unlearning has emerged as a solution for removing specific data, concepts, or facts from these models in a more efficient way than retraining from scratch. It is practical in situations where regulations, such as the GDPR (Parliament & of the European Union, 2016), mandate the removal of a user's data. Moreover, unlearning is important when models inadvertently capture copyrighted 1 or offensive content."], "score": 0.998046875}, {"id": "(Vasilev et al., 2025)", "paper": {"corpus_id": 278481378, "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation", "year": 2025, "venue": "", "authors": [{"name": "Stefan Vasilev", "authorId": "2350753289"}, {"name": "Christian Herold", "authorId": "2307079915"}, {"name": "Baohao Liao", "authorId": "66693547"}, {"name": "Seyyed Hadi Hashemi", "authorId": "2350630854"}, {"name": "Shahram Khadivi", "authorId": "2490162"}, {"name": "C. Monz", "authorId": "2062908179"}], "n_citations": 0}, "snippets": ["In the machine unlearning framework, we define the full training dataset as a partition of two subsets: the forget set, which consists of the data to be unlearned, and the retain set, which contains the remaining knowledge that should be preserved after unlearning. An effective machine unlearning method aims to produce a model that successfully forgets the forget data, while maintaining the integrity of the retained knowledge. Specifically, the resulting unlearned model should satisfy the following key requirements: 1) Minimize the retention of information from the forget set; 2) Maintain high performance on the retain set; 3) Require less computational cost than retraining the model from scratch on the retain set; 4) Maintain inference efficiency, i.e., ensuring unchanged latency."], "score": 0.99658203125}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 273403717, "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "P. Guo", "authorId": "2202361389"}, {"name": "Aaquib Syed", "authorId": "2201328926"}, {"name": "A. Sheshadri", "authorId": "2284684654"}, {"name": "Aidan Ewart", "authorId": "2287842553"}, {"name": "G. Dziugaite", "authorId": "2533850"}], "n_citations": 10}, "snippets": ["Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance."], "score": 0.998046875}, {"id": "(Jeung et al., 2025)", "paper": {"corpus_id": 278782413, "title": "SEPS: A Separability Measure for Robust Unlearning in LLMs", "year": 2025, "venue": "", "authors": [{"name": "Wonje Jeung", "authorId": "2294565775"}, {"name": "Sangyeon Yoon", "authorId": "2333409137"}, {"name": "Albert No", "authorId": "2303466208"}], "n_citations": 1}, "snippets": ["Machine unlearning aims to selectively remove targeted knowledge from Large Language Models (LLMs), ensuring they forget specified content while retaining essential information."], "score": 0.99853515625}, {"id": "(Feng et al., 2024)", "paper": {"corpus_id": 273901406, "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xiaohua Feng", "authorId": "2314871557"}, {"name": "Chao-Jun Chen", "authorId": "2251485995"}, {"name": "Yuyuan Li", "authorId": "1527113700"}, {"name": "Zibin Lin", "authorId": "2261893790"}], "n_citations": 10}, "snippets": ["Machine unlearning, a burgeoning research topic, has gained significant attention in recent years (Xu et al., 2023). It aims to erase the memory of target data from machine learning models, offering potential applications such as removing poisoned data to enhance security (Wei et al., 2023)(Kurmanji et al., 2023), retrieving personal data to comply with privacy regulations (e.g., the right-to-beforgotten) (Guo et al., 2019)(Bourtoule et al., 2019), and mitigating biases to promote fairness (Chen et al., 2023;(Li et al., 2023). Existing studies on unlearning primarily concentrate on computer vision but also extend their exploration to other fields, e.g., federated learning (Che et al., 2023), recommender systems (Li et al., 2023), and graph learning (Chen et al., 2021)."], "score": 0.998046875}, {"id": "(Wei et al., 2023)", "paper": {"corpus_id": 259991722, "title": "Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Shaokui Wei", "authorId": "2173786903"}, {"name": "Mingda Zhang", "authorId": "2365530"}, {"name": "H. Zha", "authorId": "145203884"}, {"name": "Baoyuan Wu", "authorId": "143905981"}], "n_citations": 38}, "snippets": ["Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or differently classified by the two models, such that the backdoor effect in the backdoored model will be mitigated in the purified model. Experiments on various benchmark datasets and network architectures show that our proposed method achieves state-of-the-art performance for backdoor defense."], "score": 0.0}, {"id": "(Qu et al., 2024)", "paper": {"corpus_id": 268681648, "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Youyang Qu", "authorId": "2262218493"}, {"name": "Ming Ding", "authorId": "2294002570"}, {"name": "Nan Sun", "authorId": "2293369504"}, {"name": "Kanchana Thilakarathna", "authorId": "3153007"}, {"name": "Tianqing Zhu", "authorId": "2185053609"}, {"name": "D. Niyato", "authorId": "1713586"}], "n_citations": 16}, "snippets": ["Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy."], "score": 0.99755859375}, {"id": "(Choi et al., 2024)", "paper": {"corpus_id": 270562084, "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport", "year": 2024, "venue": "", "authors": [{"name": "Minseok Choi", "authorId": "2203800211"}, {"name": "Daniel Rim", "authorId": "2307073048"}, {"name": "Dohyun Lee", "authorId": "2294508694"}, {"name": "Jaegul Choo", "authorId": "2260653165"}], "n_citations": 2}, "snippets": ["Machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning."], "score": 0.9990234375}], "table": null}, {"title": "Types of Machine Unlearning Methods", "tldr": "Machine unlearning methods for language models can be categorized into four main approaches: parameter optimization, parameter merging, in-context unlearning, and localization-based techniques. Each approach offers different tradeoffs between efficiency, effectiveness, and implementation complexity when removing targeted knowledge from language models. (5 sources)", "text": "\nMachine unlearning methods for language models have evolved into several distinct categories, each with unique approaches to selective knowledge removal. These methods can be broadly classified into four main types:\n\nFirst, **parameter optimization methods** focus on directly modifying model weights to eliminate specific knowledge. Gradient ascent represents a standard technique in this category, where the model is trained to increase loss on targeted forget data while maintaining performance on retained knowledge <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. This approach falls under the broader category of \"model-based methods\" that directly manipulate model parameters to achieve unlearning objectives <Paper corpusId=\"273350971\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nSecond, **parameter merging methods** combine multiple model versions to achieve unlearning effects. These techniques typically involve creating variants of the original model with different knowledge characteristics and then strategically merging their parameters to remove unwanted information while preserving desired capabilities <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>.\n\nThird, **in-context unlearning methods** treat the model as a black box and modify outputs using external knowledge or prompting strategies <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. These approaches align with \"input-based methods\" that use carefully designed instructions to guide the original model toward unlearning objectives without changing its parameters <Paper corpusId=\"273350971\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nFourth, **localization-based unlearning methods** identify specific model components (such as layers or neurons) associated with targeted knowledge and selectively modify those components <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. This approach builds on research showing that factual associations in language models correspond to localized, directly-editable computations, particularly in middle-layer feed-forward modules <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>.\n\nThe collective development of these methods has established LLM unlearning as a mainstream approach for removing undesirable knowledge through post-hoc modifications to target models <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. Each category offers different tradeoffs in terms of computational efficiency, unlearning effectiveness, and implementation complexity, providing researchers and practitioners with diverse options for addressing specific unlearning scenarios.", "citations": [{"id": "(Shi et al., 2024)", "paper": {"corpus_id": 271064299, "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Weijia Shi", "authorId": "2286638403"}, {"name": "Jaechan Lee", "authorId": "2261353791"}, {"name": "Yangsibo Huang", "authorId": "2283305597"}, {"name": "Sadhika Malladi", "authorId": "49288855"}, {"name": "Jieyu Zhao", "authorId": "2266698166"}, {"name": "Ari Holtzman", "authorId": "2309248199"}, {"name": "Daogao Liu", "authorId": "2261780806"}, {"name": "Luke S. Zettlemoyer", "authorId": "2137813791"}, {"name": "Noah A. Smith", "authorId": "2309424274"}, {"name": "Chiyuan Zhang", "authorId": "2309481623"}], "n_citations": 84}, "snippets": ["Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."], "score": 0.99755859375}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 273350971, "title": "LLM Unlearning via Loss Adjustment with Only Forget Data", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yaxuan Wang", "authorId": "2306067819"}, {"name": "Jiaheng Wei", "authorId": "2306500340"}, {"name": "Chris Liu", "authorId": "2271515779"}, {"name": "Jinlong Pang", "authorId": "2284760719"}, {"name": "Quan Liu", "authorId": "2326243943"}, {"name": "Ankit Shah", "authorId": "2316588330"}, {"name": "Yujia Bao", "authorId": "2306754738"}, {"name": "Yang Liu", "authorId": "2306028548"}, {"name": "Wei Wei", "authorId": "2306480290"}], "n_citations": 20}, "snippets": ["LLM unlearning [14,5,13] is part of a broader set of MU techniques aiming to make the unlearned model forget the knowledge specified in forget dataset, while preserving the model ability to accomplish tasks irrelevant to the unlearning target [13,15]. To achieve this, existing work can be categorized into three main streams of LLM unlearning approaches: input-based, data-based, and model-based methods. Input-based methods [16,17] design input instructions to guide the original LLM towards the unlearning objective without altering the model's parameters. Data-based methods [18] typically fine-tune models on pre-constructed desirable responses, using prompts from the forget data"], "score": 0.99755859375}, {"id": "(Si et al., 2023)", "paper": {"corpus_id": 265456592, "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Nianwen Si", "authorId": "73502630"}, {"name": "Hao Zhang", "authorId": "2154930608"}, {"name": "Heyu Chang", "authorId": "2116152318"}, {"name": "Wenlin Zhang", "authorId": "9047584"}, {"name": "Dan Qu", "authorId": "2253591545"}, {"name": "Weiqiang Zhang", "authorId": "2268429659"}], "n_citations": 33}, "snippets": ["Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs", "Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods."], "score": 0.99658203125}, {"id": "(Meng et al., 2022)", "paper": {"corpus_id": 255825985, "title": "Locating and Editing Factual Associations in GPT", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Kevin Meng", "authorId": "153615419"}, {"name": "David Bau", "authorId": "144159726"}, {"name": "A. Andonian", "authorId": "50112310"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}], "n_citations": 1387}, "snippets": ["We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"], "score": 0.0}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 270703237, "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Weitao Ma", "authorId": "2265878959"}, {"name": "Xiaocheng Feng", "authorId": "2674998"}, {"name": "Weihong Zhong", "authorId": "2208739098"}, {"name": "Lei Huang", "authorId": "2265930173"}, {"name": "Yangfan Ye", "authorId": "2216505879"}, {"name": "Bing Qin", "authorId": "2257004102"}], "n_citations": 6}, "snippets": ["To tackle this, Machine Unlearning ( Zhang et al., 2023;Lu et al., 2024;Bhardwaj et al., 2024) has gradually been applied to LLMs due to its effectiveness and cost-efficiency. These refined techniques, now known as LLM Unlearning (Yao et al., 2023;Liu et al., 2024b,c), have become a mainstream approach for removing undesirable knowledge from the model by applying post-hoc modifications to target models."], "score": 0.99853515625}], "table": null}, {"title": "Parameter Optimization Methods", "tldr": "Parameter optimization methods directly modify model weights to remove specific knowledge, with gradient-based techniques being the most prevalent approach. These methods can effectively make models forget targeted information by altering parameter values through various optimization objectives, though they often face challenges in balancing effective unlearning with maintaining performance on unrelated tasks. (10 sources)", "text": "\nParameter optimization methods directly manipulate model weights to remove specific knowledge from language models. These approaches include:\n\n1. **Gradient Ascent (GA)** - This foundational technique essentially reverses the standard training objective, maximizing loss on targeted content to make the model forget specific information <Paper corpusId=\"252693065\" paperTitle=\"(Jang et al., 2022)\" isShortName></Paper>. While effective at removing knowledge, basic GA can lead to unstable optimization and catastrophic forgetting of retained knowledge <Paper corpusId=\"271860124\" paperTitle=\"(Cha et al., 2024)\" isShortName></Paper>.\n\n2. **Selective Unlearning (SeUL)** - Unlike methods that fully reverse training loss, SeUL achieves fine-grained knowledge removal by focusing on specific sequence spans rather than entire instances, minimizing negative impacts on general language generation capabilities <Paper corpusId=\"267547751\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper>.\n\n3. **Selective Knowledge negation Unlearning (SKU)** - This two-stage framework first identifies and acquires harmful knowledge within the model, then selectively removes it from parameters while preserving performance on normal prompts <Paper corpusId=\"267681958\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\n4. **Inverted Hinge Loss** - Proposed in the Low-rank Knowledge Unlearning (LoKU) framework, this approach suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token <Paper corpusId=\"271860124\" paperTitle=\"(Cha et al., 2024)\" isShortName></Paper>.\n\n5. **Maximum Entropy (ME)** - This method maximizes the entropy of token distributions for untargeted unlearning, helping to address the issue of unpredictable behavior and potential hallucinations in conventional approaches <Paper corpusId=\"273233618\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper>.\n\n6. **Normalized Gradient Difference (NGDiff)** - This algorithm frames unlearning as a regularized multi-task optimization problem, providing better control over the trade-off between forgetting objectives and maintaining model performance <Paper corpusId=\"273661686\" paperTitle=\"(Bu et al., 2024)\" isShortName></Paper>.\n\n7. **Memory Evaluation and Optimization Workflow (MEOW)** - This approach uses an offline LLM to generate inverted facts and introduces a metric called MEMO to quantify memorization, selecting appropriate inverted facts for finetuning <Paper corpusId=\"272704025\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>.\n\n8. **Distribution Flattening with Multiple-Choice Questions (DF-MCQ)** - This method flattens the model's predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals <Paper corpusId=\"278338982\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.\n\n9. **Lightweight Unlearning Layers** - Some approaches integrate specialized unlearning layers into transformer models to selectively remove specific data without retraining the entire model <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\n10. **Answer Preservation (AP) Loss** - This regularization technique helps preserve model capabilities during targeted unlearning by incorporating a loss function that maintains performance on desired tasks <Paper corpusId=\"273233618\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper>.\n\nParameter optimization methods offer direct control over the unlearning process but typically face challenges balancing effective unlearning with preventing performance degradation on non-targeted tasks <Paper corpusId=\"272704025\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276812969\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Jang et al., 2022)", "paper": {"corpus_id": 252693065, "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Joel Jang", "authorId": "2000091730"}, {"name": "Dongkeun Yoon", "authorId": "29830817"}, {"name": "Sohee Yang", "authorId": "16110760"}, {"name": "Sungmin Cha", "authorId": "34352481"}, {"name": "Moontae Lee", "authorId": "3056520"}, {"name": "Lajanugen Logeswaran", "authorId": "2876316"}, {"name": "Minjoon Seo", "authorId": "4418074"}], "n_citations": 239}, "snippets": ["Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger-sized LMs. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with previous methods known to mitigate privacy risks for LMs, we show that our approach can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust."], "score": 0.0}, {"id": "(Cha et al., 2024)", "paper": {"corpus_id": 271860124, "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Sungmin Cha", "authorId": "34352481"}, {"name": "Sungjun Cho", "authorId": "2149157242"}, {"name": "Dasol Hwang", "authorId": "1474356736"}, {"name": "Moontae Lee", "authorId": "2313692227"}], "n_citations": 2}, "snippets": ["Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge."], "score": 0.99853515625}, {"id": "(Wang et al._1, 2024)", "paper": {"corpus_id": 267547751, "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Lingzhi Wang", "authorId": "2282353702"}, {"name": "Xingshan Zeng", "authorId": "46180553"}, {"name": "Jinsong Guo", "authorId": "2283375647"}, {"name": "Kam-Fai Wong", "authorId": "2264107863"}, {"name": "Georg Gottlob", "authorId": "2265883371"}], "n_citations": 18}, "snippets": ["We present SeUL, a novel method that enables selective and fine-grained unlearning for language models. Unlike previous work that employs a fully reversed training objective in unlearning, SeUL minimizes the negative impact on the capability of language models, particularly in terms of generation", "In contrast to (Jang et al., 2022), which fully reverses the training loss of instances for forgetting, we propose a selective unlearning method, SEUL. SEUL achieves knowledge forgetting in a fine-grained manner, focusing on specific sequence spans rather than entire instances, as illustrated in Fig. 1."], "score": 0.998046875}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 267681958, "title": "Towards Safer Large Language Models through Machine Unlearning", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zheyuan Liu", "authorId": "2122087252"}, {"name": "Guangyao Dou", "authorId": "2174956825"}, {"name": "Zhaoxuan Tan", "authorId": "2093186816"}, {"name": "Yijun Tian", "authorId": "46879986"}, {"name": "Meng Jiang", "authorId": "2275403324"}], "n_citations": 87}, "snippets": ["The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts."], "score": 0.99853515625}, {"id": "(Yuan et al., 2024)", "paper": {"corpus_id": 273233618, "title": "A Closer Look at Machine Unlearning for Large Language Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xiaojian Yuan", "authorId": "2273843751"}, {"name": "Tianyu Pang", "authorId": "19201674"}, {"name": "Chao Du", "authorId": "2325201427"}, {"name": "Kejiang Chen", "authorId": "8780109"}, {"name": "Weiming Zhang", "authorId": "2189835131"}, {"name": "Min Lin", "authorId": "2253977831"}], "n_citations": 13}, "snippets": ["Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning."], "score": 0.998046875}, {"id": "(Bu et al., 2024)", "paper": {"corpus_id": 273661686, "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Zhiqi Bu", "authorId": "2324784973"}, {"name": "Xiaomeng Jin", "authorId": "2327893325"}, {"name": "B. Vinzamuri", "authorId": "3236313"}, {"name": "Anil Ramakrishna", "authorId": "2328076404"}, {"name": "Kai-Wei Chang", "authorId": "2256646555"}, {"name": "V. Cevher", "authorId": "1678641"}, {"name": "Mingyi Hong", "authorId": "2278433136"}], "n_citations": 13}, "snippets": ["Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler."], "score": 0.99853515625}, {"id": "(Gu et al., 2024)", "paper": {"corpus_id": 272704025, "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tianle Gu", "authorId": "2279024315"}, {"name": "Kexin Huang", "authorId": "2266421899"}, {"name": "Ruilin Luo", "authorId": "2279024030"}, {"name": "Yuanqi Yao", "authorId": "2306059424"}, {"name": "Yujiu Yang", "authorId": "2284727148"}, {"name": "Yan Teng", "authorId": "2266238818"}, {"name": "Yingchun Wang", "authorId": "2266364817"}], "n_citations": 9}, "snippets": ["LLM Unlearning, a post-hoc approach to remove this information from trained LLMs, offers a promising solution to mitigate these risks. However, previous practices face three key challenges: 1. Utility: successful unlearning often causes catastrophic collapse on unrelated tasks. 2. Efficiency: many methods either involve adding similarly sized models, which slows down unlearning or inference, or require retain data that are difficult to obtain. 3. Robustness: even effective methods may still leak data via extraction techniques. To address these challenges, we propose MEOW, a simple yet effective gradient descent-based unlearning method. Specifically, we use an offline LLM to generate a set of inverted facts. Then, we design a new metric, MEMO, to quantify memorization in LLMs. Finally, based on the signals provided by MEMO, we select the most appropriate set of inverted facts and finetune the model based on them."], "score": 0.9970703125}, {"id": "(Sun et al., 2025)", "paper": {"corpus_id": 278338982, "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Guangzhi Sun", "authorId": "2321755791"}, {"name": "Potsawee Manakul", "authorId": "89355510"}, {"name": "Xiao Zhan", "authorId": "2359255668"}, {"name": "Mark Gales", "authorId": "2359255671"}], "n_citations": 0}, "snippets": ["Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour."], "score": 0.99755859375}, {"id": "(Nguyen et al., 2022)", "paper": {"corpus_id": 252089272, "title": "A Survey of Machine Unlearning", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "T. Nguyen", "authorId": "2117824517"}, {"name": "T. Huynh", "authorId": "152399820"}, {"name": "Phi-Le Nguyen", "authorId": "2143967163"}, {"name": "Alan Wee-Chung Liew", "authorId": "1733300"}, {"name": "Hongzhi Yin", "authorId": "2416851"}, {"name": "Q. Nguyen", "authorId": "144133815"}], "n_citations": 239}, "snippets": ["Chen et al. [20] introduce an efficient framework for unlearning in LLMs that integrates lightweight unlearning layers into transformer models, allowing for the selective removal of specific data without retraining the entire model. They also introduce a fusion mechanism to combine these layers when multiple unlearning requests are made. Kassem (Kassem et al., 2023) propose another unlearning approach called \"DeMem\", which uses a reinforcement learning feedback loop with a negative similarity score as a reward. This approach incentivises the language model to paraphrase memorised content, thereby reducing the risk of sensitive information being exposed. Wang et al. [190] introduce E2URec, an unlearning method for LLM-based recommender systems. It addresses forgetting user data while preserving model performance by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework. Some works, use machine unlearning as a way to improve the truthfulness and detoxification of LLMs. Hu et al. (Hu et al., 2023) propose the \"Extraction-before-Subtraction\" (Ext-Sub) method with parameterefficient modules (PEMs) to isolate and remove undesirable features like untruthfulness or toxicity, while preserving the model's core capabilities. The method extracts deficiency capabilities from \"anti-expert\" PEMs and subtracts them from \"expert\" PEMs, enhancing performance without compromising fundamental abilities. Pochinkov et al. [139] introduce another method by selectively pruning neurons responsible for specific behaviours, such as coding or toxic language, while maintaining overall performance. Lu et al. (Lu et al., 2022) presents a framework called Quark that uses reinforcement learning as a machine unlearning approach to mitigate undesirable text generation behaviors, such as toxicity and repetition, while preserving language quality."], "score": 0.998046875}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 276812969, "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wenyu Wang", "authorId": "2348951919"}, {"name": "Mengqi Zhang", "authorId": "48985110"}, {"name": "Xiaotian Ye", "authorId": "2286432237"}, {"name": "Zhaochun Ren", "authorId": "2260895127"}, {"name": "Zhumin Chen", "authorId": "1721165"}, {"name": "Pengjie Ren", "authorId": "1749477"}], "n_citations": 3}, "snippets": ["Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning."], "score": 0.998046875}], "table": null}, {"title": "Parameter Merging Methods", "tldr": "Parameter merging methods combine multiple model variants to remove unwanted knowledge without complete retraining. These techniques identify and manipulate parameter subspaces associated with specific knowledge, allowing efficient forgetting while preserving overall model performance. (5 sources)", "text": "\nParameter merging methods provide efficient approaches to machine unlearning by combining or manipulating parameters from different model variants:\n\n1. **Task Arithmetic (TA)** - This approach enables efficient model editing through parameter merging, allowing for the removal of specific knowledge while maintaining other capabilities <Paper corpusId=\"270559969\" paperTitle=\"(Jin et al., 2024)\" isShortName></Paper>.\n\n2. **Conflict-free Model Editing (CoME)** - This framework enhances knowledge updates in LLMs by selectively removing outdated information, which mitigates knowledge interference and allows new information to be integrated without compromising relevant linguistic features <Paper corpusId=\"276575899\" paperTitle=\"(Jung et al., 2025)\" isShortName></Paper>.\n\n3. **UNLEARN Algorithm** - This technique leverages subspace methods to identify areas of the model associated with particular knowledge and applies discrimination methods to separate that subspace from related knowledge. This approach specifically addresses the challenge of preventing performance degradation on similar tasks, which is crucial for privacy compliance <Paper corpusId=\"271769107\" paperTitle=\"(Lizzo et al., 2024)\" isShortName></Paper>.\n\n4. **Fusion Mechanism for Unlearning Layers** - This approach integrates lightweight unlearning layers into transformer models and introduces a mechanism to combine these layers when multiple unlearning requests are made, allowing for selective removal of specific data without retraining the entire model <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\n5. **Extraction-before-Subtraction (Ext-Sub)** - Using parameter-efficient modules (PEMs), this method isolates and removes undesirable features like untruthfulness or toxicity while preserving the model's core capabilities. It works by extracting deficiency capabilities from \"anti-expert\" PEMs and subtracting them from \"expert\" PEMs <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260925619\" paperTitle=\"(Hu et al., 2023)\" isShortName></Paper>.\n\n6. **E2URec** - This method addresses forgetting user data in LLM-based recommender systems while preserving model performance. It works by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\n7. **Selective Neuron Pruning** - This approach identifies and selectively prunes neurons responsible for specific behaviors, such as coding or toxic language, while maintaining overall performance <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\nParameter merging methods generally offer computational efficiency advantages over full retraining approaches while providing controlled knowledge removal. Their primary strength lies in the ability to target specific parameter subspaces associated with unwanted knowledge while minimizing disruption to other model capabilities.", "citations": [{"id": "(Jin et al., 2024)", "paper": {"corpus_id": 270559969, "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zhuoran Jin", "authorId": "2152843772"}, {"name": "Pengfei Cao", "authorId": "49776272"}, {"name": "Chenhao Wang", "authorId": "2135762532"}, {"name": "Zhitao He", "authorId": "2268906494"}, {"name": "Hongbang Yuan", "authorId": "2165224410"}, {"name": "Jiachun Li", "authorId": "2203948041"}, {"name": "Yubo Chen", "authorId": "1763402"}, {"name": "Kang Liu", "authorId": "77397868"}, {"name": "Jun Zhao", "authorId": "2269147239"}], "n_citations": 26}, "snippets": ["Machine unlearning [9; 8] focuses on effectively removing specific memorized content from trained machine-learning models to ensure the right to be forgotten", "Recently, there has been increasing attention on how to perform knowledge unlearning on LLMs [25; 13; 59; 58; 45; 7; 33; 37]", "Most unlearning methods rely on fine-tuning the model on the forget corpus, such as applying gradient ascent (GA) on the loss [25; 37]. Recently, there have been complementary methods to GA that adopt preference optimization [64], representation controlling [29], and rejection tuning [24] to unlearn the model. Moreover, task arithmetic (TA) is also an unlearning method, enabling efficient model editing through parameter merging [23; 22]."], "score": 0.99755859375}, {"id": "(Jung et al., 2025)", "paper": {"corpus_id": 276575899, "title": "CoME: An Unlearning-based Approach to Conflict-free Model Editing", "year": 2025, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Dahyun Jung", "authorId": "2255577018"}, {"name": "Jaehyung Seo", "authorId": "2148452511"}, {"name": "Jaewook Lee", "authorId": "2220582753"}, {"name": "Chanjun Park", "authorId": "2115195904"}, {"name": "Heu-Jeoung Lim", "authorId": "83056580"}], "n_citations": 1}, "snippets": ["In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features."], "score": 0.99755859375}, {"id": "(Lizzo et al., 2024)", "paper": {"corpus_id": 271769107, "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Tyler Lizzo", "authorId": "2315304043"}, {"name": "Larry Heck", "authorId": "2315302093"}], "n_citations": 1}, "snippets": ["This paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations. Further, this technique uses a unified set of operators, where the task matrices are identical and used to either enhance or reduce the model's performance for a given task."], "score": 0.9990234375}, {"id": "(Nguyen et al., 2022)", "paper": {"corpus_id": 252089272, "title": "A Survey of Machine Unlearning", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "T. Nguyen", "authorId": "2117824517"}, {"name": "T. Huynh", "authorId": "152399820"}, {"name": "Phi-Le Nguyen", "authorId": "2143967163"}, {"name": "Alan Wee-Chung Liew", "authorId": "1733300"}, {"name": "Hongzhi Yin", "authorId": "2416851"}, {"name": "Q. Nguyen", "authorId": "144133815"}], "n_citations": 239}, "snippets": ["Chen et al. [20] introduce an efficient framework for unlearning in LLMs that integrates lightweight unlearning layers into transformer models, allowing for the selective removal of specific data without retraining the entire model. They also introduce a fusion mechanism to combine these layers when multiple unlearning requests are made. Kassem (Kassem et al., 2023) propose another unlearning approach called \"DeMem\", which uses a reinforcement learning feedback loop with a negative similarity score as a reward. This approach incentivises the language model to paraphrase memorised content, thereby reducing the risk of sensitive information being exposed. Wang et al. [190] introduce E2URec, an unlearning method for LLM-based recommender systems. It addresses forgetting user data while preserving model performance by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework. Some works, use machine unlearning as a way to improve the truthfulness and detoxification of LLMs. Hu et al. (Hu et al., 2023) propose the \"Extraction-before-Subtraction\" (Ext-Sub) method with parameterefficient modules (PEMs) to isolate and remove undesirable features like untruthfulness or toxicity, while preserving the model's core capabilities. The method extracts deficiency capabilities from \"anti-expert\" PEMs and subtracts them from \"expert\" PEMs, enhancing performance without compromising fundamental abilities. Pochinkov et al. [139] introduce another method by selectively pruning neurons responsible for specific behaviours, such as coding or toxic language, while maintaining overall performance. Lu et al. (Lu et al., 2022) presents a framework called Quark that uses reinforcement learning as a machine unlearning approach to mitigate undesirable text generation behaviors, such as toxicity and repetition, while preserving language quality."], "score": 0.998046875}, {"id": "(Hu et al., 2023)", "paper": {"corpus_id": 260925619, "title": "Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Xinshuo Hu", "authorId": "2149467818"}, {"name": "Dongfang Li", "authorId": "1664667501"}, {"name": "Zihao Zheng", "authorId": "151479145"}, {"name": "Zhenyu Liu", "authorId": "2230018369"}, {"name": "Baotian Hu", "authorId": "2142726660"}, {"name": "M. Zhang", "authorId": "50495870"}], "n_citations": 30}, "snippets": ["Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of ``expert'' PEM and ``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of truthfulness and detoxification, we conduct extensive experiments on LLMs, encompassing additional abilities such as language modelling and mathematical reasoning. Our empirical results demonstrate that our approach effectively improves truthfulness and detoxification, while largely preserving the fundamental abilities of LLMs."], "score": 0.0}], "table": null}, {"title": "In-Context Unlearning Methods", "tldr": "In-context unlearning methods enable knowledge removal without modifying model parameters by providing specially crafted prompts at inference time. These techniques treat language models as black boxes, making them particularly valuable for scenarios with limited model access or computational constraints. (8 sources)", "text": "\nIn-context unlearning methods offer a parameter-free approach to selectively removing knowledge from language models:\n\n1. **Standard In-Context Unlearning (ICU)** - This pioneering approach provides specific instances to be forgotten along with incorrect labels during inference, effectively instructing the model to unlearn targeted information without updating parameters <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>.\n\n2. **In-context Knowledge Editing (IKE)** - Inspired by in-context learning, this method uses demonstration contexts to edit factual knowledge without parameter updates. It achieves competitive success rates compared to gradient-based methods while producing fewer side effects such as over-editing or knowledge forgetting <Paper corpusId=\"258832407\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>.\n\n3. **External Knowledge Integration** - Some in-context methods treat the model as a black box and modify output results using external knowledge sources, providing a straightforward approach when direct model access is limited <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>.\n\n4. **Context-Based Selective Unlearning** - This approach fine-tunes pre-trained LLMs to selectively forget information based on query context, enabling models to preserve unrelated information while withholding specific knowledge from unauthorized users <Paper corpusId=\"273022754\" paperTitle=\"(Takashiro et al., 2024)\" isShortName></Paper>.\n\n5. **Instruction-Guided Unlearning** - These techniques design specialized input instructions that guide the original model toward unlearning objectives without altering parameters, falling under the broader category of \"input-based methods\" <Paper corpusId=\"273350971\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nThe primary advantage of in-context unlearning methods is their applicability to scenarios where models are accessible only through API calls or where computational constraints prevent parameter modification <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>. However, these approaches may not fundamentally erase underlying knowledge from model parameters, potentially leaving them vulnerable to extraction attacks that can recover sensitive information <Paper corpusId=\"270440244\" paperTitle=\"(Ashuach et al., 2024)\" isShortName></Paper> <Paper corpusId=\"229156229\" paperTitle=\"(Carlini et al., 2020)\" isShortName></Paper>. This limitation contrasts with model editing approaches that surgically remove sensitive data from parameters, offering potentially greater resistance to extraction attempts <Paper corpusId=\"270440244\" paperTitle=\"(Ashuach et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Pawelczyk et al., 2023)", "paper": {"corpus_id": 263834631, "title": "In-Context Unlearning: Language Models as Few Shot Unlearners", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Martin Pawelczyk", "authorId": "89583148"}, {"name": "Seth Neel", "authorId": "2273685865"}, {"name": "Himabindu Lakkaraju", "authorId": "1892673"}], "n_citations": 132}, "snippets": ["Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth."], "score": 0.998046875}, {"id": "(Zheng et al., 2023)", "paper": {"corpus_id": 258832407, "title": "Can We Edit Factual Knowledge by In-Context Learning?", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Ce Zheng", "authorId": "2113919886"}, {"name": "Lei Li", "authorId": "49192881"}, {"name": "Qingxiu Dong", "authorId": "2047143813"}, {"name": "Yuxuan Fan", "authorId": "2118167265"}, {"name": "Zhiyong Wu", "authorId": "150358371"}, {"name": "Jingjing Xu", "authorId": "47883405"}, {"name": "Baobao Chang", "authorId": "7267809"}], "n_citations": 216}, "snippets": ["Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or out-dated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/Zce1112zslx/IKE."], "score": 0.0}, {"id": "(Shi et al., 2024)", "paper": {"corpus_id": 271064299, "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Weijia Shi", "authorId": "2286638403"}, {"name": "Jaechan Lee", "authorId": "2261353791"}, {"name": "Yangsibo Huang", "authorId": "2283305597"}, {"name": "Sadhika Malladi", "authorId": "49288855"}, {"name": "Jieyu Zhao", "authorId": "2266698166"}, {"name": "Ari Holtzman", "authorId": "2309248199"}, {"name": "Daogao Liu", "authorId": "2261780806"}, {"name": "Luke S. Zettlemoyer", "authorId": "2137813791"}, {"name": "Noah A. Smith", "authorId": "2309424274"}, {"name": "Chiyuan Zhang", "authorId": "2309481623"}], "n_citations": 84}, "snippets": ["Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."], "score": 0.99755859375}, {"id": "(Takashiro et al., 2024)", "paper": {"corpus_id": 273022754, "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shota Takashiro", "authorId": "2323750981"}, {"name": "Takeshi Kojima", "authorId": "2081836120"}, {"name": "Andrew Gambardella", "authorId": "2304550144"}, {"name": "Qi Cao", "authorId": "2268816164"}, {"name": "Yusuke Iwasawa", "authorId": "1715282"}, {"name": "Yutaka Matsuo", "authorId": "2241471533"}], "n_citations": 2}, "snippets": ["As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information is becoming increasingly essential. For instance, LLMs are expected to selectively provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. Therefore, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the query context. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving unrelated information."], "score": 0.9970703125}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 273350971, "title": "LLM Unlearning via Loss Adjustment with Only Forget Data", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yaxuan Wang", "authorId": "2306067819"}, {"name": "Jiaheng Wei", "authorId": "2306500340"}, {"name": "Chris Liu", "authorId": "2271515779"}, {"name": "Jinlong Pang", "authorId": "2284760719"}, {"name": "Quan Liu", "authorId": "2326243943"}, {"name": "Ankit Shah", "authorId": "2316588330"}, {"name": "Yujia Bao", "authorId": "2306754738"}, {"name": "Yang Liu", "authorId": "2306028548"}, {"name": "Wei Wei", "authorId": "2306480290"}], "n_citations": 20}, "snippets": ["LLM unlearning [14,5,13] is part of a broader set of MU techniques aiming to make the unlearned model forget the knowledge specified in forget dataset, while preserving the model ability to accomplish tasks irrelevant to the unlearning target [13,15]. To achieve this, existing work can be categorized into three main streams of LLM unlearning approaches: input-based, data-based, and model-based methods. Input-based methods [16,17] design input instructions to guide the original LLM towards the unlearning objective without altering the model's parameters. Data-based methods [18] typically fine-tune models on pre-constructed desirable responses, using prompts from the forget data"], "score": 0.99755859375}, {"id": "(Ashuach et al., 2024)", "paper": {"corpus_id": 270440244, "title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tomer Ashuach", "authorId": "2306249146"}, {"name": "Martin Tutek", "authorId": "2367197291"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}], "n_citations": 7}, "snippets": ["Machine unlearning approaches [35] aim to discourage models from generating sensitive information through techniques such as in-context unlearning [ICL; 37,45,(Zheng et al., 2023) or gradient ascent [29,55,(Yu et al., 2023).These approaches do not ensure that the sensitive information is erased from model parameters, rendering them vulnerable to extraction attacks [33].Model editing, a variant of localization-based unlearning, localizes and change a subset model parameters to erase or overwrite the sensitive information itself [15,(Meng et al., 2022)[39].In contrast to ICL and optimization-based methods, which may prevent models from generating sensitive content but do not fundamentally erase the underlying knowledge, editing methods hold a greater potential to resist extraction attacks (Carlini et al., 2020) by surgically removing the sensitive data from model parameters."], "score": 0.99755859375}, {"id": "(Carlini et al., 2020)", "paper": {"corpus_id": 229156229, "title": "Extracting Training Data from Large Language Models", "year": 2020, "venue": "USENIX Security Symposium", "authors": [{"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}, {"name": "Eric Wallace", "authorId": "145217343"}, {"name": "Matthew Jagielski", "authorId": "40844378"}, {"name": "Ariel Herbert-Voss", "authorId": "1404060687"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Tom B. Brown", "authorId": "31035595"}, {"name": "D. Song", "authorId": "143711382"}, {"name": "\u00da. Erlingsson", "authorId": "1758110"}, {"name": "Alina Oprea", "authorId": "3046437"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 1950}, "snippets": ["It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models."], "score": 0.0}, {"id": "(Meng et al., 2022)", "paper": {"corpus_id": 255825985, "title": "Locating and Editing Factual Associations in GPT", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Kevin Meng", "authorId": "153615419"}, {"name": "David Bau", "authorId": "144159726"}, {"name": "A. Andonian", "authorId": "50112310"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}], "n_citations": 1387}, "snippets": ["We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"], "score": 0.0}], "table": null}, {"title": "Localization-Based Unlearning Methods", "tldr": "Localization-based unlearning methods identify and target specific components of language models (like neurons or layers) that store particular knowledge. These approaches offer surgical precision in removing unwanted information while minimizing disruption to the model's other capabilities. (10 sources)", "text": "\n1. **ROME (Rank-One Model Editing)** - This method identifies and modifies feed-forward modules in middle layers that store factual associations. Research has shown these associations correspond to localized, directly-editable computations, allowing for precise knowledge removal without extensive retraining <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>.\n\n2. **ELM (Erasure of Language Memory)** - This approach leverages the model's ability to evaluate its own knowledge, creating targeted low-rank updates that reduce generation probabilities for content related to undesired concepts while preserving broader capabilities <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper>.\n\n3. **TARS (Targeted Angular Reversal)** - This method first leverages the language model to aggregate information about a selected concept in its internal representation space. It then identifies feedforward weight vectors with high cosine similarity to this \"concept vector\" and reverses them, limiting the concept's propagation through the model <Paper corpusId=\"274763373\" paperTitle=\"(Davies et al., 2024)\" isShortName></Paper>.\n\n4. **Lightweight Unlearning Layers** - This framework integrates specialized unlearning layers into transformer architectures using a selective teacher-student objective. It includes a fusion mechanism to combine different unlearning layers when handling multiple forgetting operations <Paper corpusId=\"264828972\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\n5. **MemFlex** - This method utilizes gradient information to precisely target and unlearn sensitive parameters. It addresses the problem of excessive unlearning by focusing specifically on parameters associated with the knowledge to be removed <Paper corpusId=\"270878324\" paperTitle=\"(Tian et al., 2024)\" isShortName></Paper>.\n\n6. **GRAIL (GRadient-based AdaptIve unLearning)** - This framework leverages gradient information from multiple domains to distinguish between unlearning scope and retention scope. It applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain <Paper corpusId=\"277857590\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper>.\n\n7. **DeepCUT (Deep Contrastive Unlearning for fine-Tuning)** - Unlike methods that focus solely on output mitigation, this approach achieves unlearning by directly optimizing the latent space of the model, addressing the geometric distributions of samples within the model's representation space <Paper corpusId=\"277113301\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\n8. **Computational Mechanism Manipulation** - These techniques edit model computations to remove specific information without retraining, making them highly efficient for large models <Paper corpusId=\"273350773\" paperTitle=\"(Wu et al._1, 2024)\" isShortName></Paper>.\n\nCompared to other unlearning methods, localization-based approaches can potentially offer greater resistance to extraction attacks by surgically removing sensitive data from model parameters, rather than merely preventing generation of such content <Paper corpusId=\"270440244\" paperTitle=\"(Ashuach et al., 2024)\" isShortName></Paper> <Paper corpusId=\"229156229\" paperTitle=\"(Carlini et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Meng et al., 2022)", "paper": {"corpus_id": 255825985, "title": "Locating and Editing Factual Associations in GPT", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Kevin Meng", "authorId": "153615419"}, {"name": "David Bau", "authorId": "144159726"}, {"name": "A. Andonian", "authorId": "50112310"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}], "n_citations": 1387}, "snippets": ["We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"], "score": 0.0}, {"id": "(Gandikota et al., 2024)", "paper": {"corpus_id": 273098800, "title": "Erasing Conceptual Knowledge from Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Rohit Gandikota", "authorId": "52017367"}, {"name": "Sheridan Feucht", "authorId": "2140009998"}, {"name": "Samuel Marks", "authorId": "2225941937"}, {"name": "David Bau", "authorId": "2284996653"}], "n_citations": 11}, "snippets": ["In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities."], "score": 0.99951171875}, {"id": "(Davies et al., 2024)", "paper": {"corpus_id": 274763373, "title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "H. Davies", "authorId": "152363369"}, {"name": "Giorgos Iacovides", "authorId": "2292197794"}, {"name": "Danilo Mandic", "authorId": "2292179830"}], "n_citations": 0}, "snippets": ["The sheer scale of data required to train modern large language models (LLMs) poses significant risks, as models are likely to gain knowledge of sensitive topics such as bio-security, as well the ability to replicate copyrighted works. Methods designed to remove such knowledge must do so from all prompt directions, in a multi-lingual capacity and without degrading general model performance. To this end, we introduce the targeted angular reversal (TARS) method of knowledge removal from LLMs. The TARS method firstly leverages the LLM in combination with a detailed prompt to aggregate information about a selected concept in the internal representation space of the LLM. It then refines this approximate concept vector to trigger the concept token with high probability, by perturbing the approximate concept vector with noise and transforming it into token scores with the language model head. The feedforward weight vectors in the LLM which operate directly on the internal representation space, and have the highest cosine similarity with this targeting vector, are then replaced by a reversed targeting vector, thus limiting the ability of the concept to propagate through the model."], "score": 0.998046875}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 264828972, "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jiaao Chen", "authorId": "47739850"}, {"name": "Diyi Yang", "authorId": "2263629011"}], "n_citations": 162}, "snippets": ["Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations."], "score": 0.99658203125}, {"id": "(Tian et al., 2024)", "paper": {"corpus_id": 270878324, "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Bo Tian", "authorId": "2064522174"}, {"name": "Xiaozhuan Liang", "authorId": "2153398295"}, {"name": "Siyuan Cheng", "authorId": "2258034882"}, {"name": "Qingbin Liu", "authorId": "2258682951"}, {"name": "Meng Wang", "authorId": "2218346459"}, {"name": "Dianbo Sui", "authorId": "2273504274"}, {"name": "Xi Chen", "authorId": "48283576"}, {"name": "Huajun Chen", "authorId": "2144200945"}, {"name": "Ningyu Zhang", "authorId": "2153010067"}], "n_citations": 13}, "snippets": ["Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters."], "score": 0.9970703125}, {"id": "(Kim et al., 2025)", "paper": {"corpus_id": 277857590, "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Kun-Woo Kim", "authorId": "2356009299"}, {"name": "Ji-Hoon Park", "authorId": "2257098510"}, {"name": "Jumin Han", "authorId": "2355145341"}, {"name": "Seong-Whan Lee", "authorId": "2339467966"}], "n_citations": 1}, "snippets": ["A key challenge in Machine Unlearning is to eliminate only the targeted knowledge while preserving the remaining information and maintaining general task performance. Existing unlearning methods, however, often remove an excessive amount of domain-specific knowledge, including information that must remain in the parametric knowledge", "To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain."], "score": 0.998046875}, {"id": "(He et al., 2025)", "paper": {"corpus_id": 277113301, "title": "Deep Contrastive Unlearning for Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Estrid He", "authorId": "2350861220"}, {"name": "Tabinda Sarwar", "authorId": "2338269552"}, {"name": "Ibrahim Khalil", "authorId": "2308101762"}, {"name": "Xun Yi", "authorId": "2326254750"}, {"name": "Ke Wang", "authorId": "2350889588"}], "n_citations": 1}, "snippets": ["However, this comes at a cost: the potential risk of exposing users' privacy and violating copyright protections. Thus, to safeguard individuals'\"right to be forgotten\", there has been increasing interests in machine unlearning -- the process of removing information carried by particular training samples from a model while not deteriorating its predictive quality. This is a challenging task due to the black-box nature of language models. Most existing studies focus on mitigating the impact of those forgot samples upon a model's outputs, and do not explicitly consider the geometric distributions of samples in the latent space of a model. To address this issue, we propose a machine unlearning framework, named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models. Our proposed model achieves machine unlearning by directly optimizing the latent space of a model."], "score": 0.9990234375}, {"id": "(Wu et al._1, 2024)", "paper": {"corpus_id": 273350773, "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "YuXuan Wu", "authorId": "2325990741"}, {"name": "Bonaventure F. P. Dossou", "authorId": "1591111757"}, {"name": "Dianbo Liu", "authorId": "2326253445"}], "n_citations": 0}, "snippets": ["Machine unlearning methodologies have been developed to tackle the challenges of efficiently removing data from trained models. Among the early influential frameworks is the Sharded, Isolated, Sliced, and Aggregated (SISA) approach Bourtoule et al. (2020),which partitions data into independent shards. By retraining only the specific shards containing the data to be unlearned, SISA reduces the computational burden. Extensions of this approach include Ginart et al. (2019), which applies partitioning to linear models, and Brophy & Lowd (2021), which adapts it for random forests. Schelter et al. (2021) further extended the concept to decision trees, minimizing retraining through hierarchical partitioning. In the graph learning domain, Chen et al. (2022b) developed methods to forget specific nodes or edges, while Chen et al. (2022a) focused on removing sensitive user data from recommendation systems.\n\nWhile these methods are effective for structured models, they struggle to scale to large, complex models like Language Models. Additionally, the retraining costs, though reduced, remain significant, and the reliance on specific architectures limits their generalizability to more dynamic tasks.\n\nIn a different direction, Kurmanji et al. (2023) introduced SCRUB, which treats the original model as a teacher and trains a student model to mimic it on retained data while 'forgetting' specific information. Warnecke et al. (2023) proposed unlearning entire groups of features and labels using influence functions, providing closed-form updates to model parameters for more efficient data removal.\n\nInfluence functions Guo et al. (2023); Sekhari et al. (2021); Mehta et al. (2022) also offer an alternative by measuring the effect of individual data points on a model's predictions and adjusting parameters accordingly, providing more direct methods for unlearning.\n\nRecently, zero-shot unlearning methods have emerged, focusing on removing information without retraining, making them highly efficient for large models. Shah et al. (2024) introduced a method for editing model computations to 'forget' specific information."], "score": 0.99951171875}, {"id": "(Ashuach et al., 2024)", "paper": {"corpus_id": 270440244, "title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tomer Ashuach", "authorId": "2306249146"}, {"name": "Martin Tutek", "authorId": "2367197291"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}], "n_citations": 7}, "snippets": ["Machine unlearning approaches [35] aim to discourage models from generating sensitive information through techniques such as in-context unlearning [ICL; 37,45,(Zheng et al., 2023) or gradient ascent [29,55,(Yu et al., 2023).These approaches do not ensure that the sensitive information is erased from model parameters, rendering them vulnerable to extraction attacks [33].Model editing, a variant of localization-based unlearning, localizes and change a subset model parameters to erase or overwrite the sensitive information itself [15,(Meng et al., 2022)[39].In contrast to ICL and optimization-based methods, which may prevent models from generating sensitive content but do not fundamentally erase the underlying knowledge, editing methods hold a greater potential to resist extraction attacks (Carlini et al., 2020) by surgically removing the sensitive data from model parameters."], "score": 0.99755859375}, {"id": "(Carlini et al., 2020)", "paper": {"corpus_id": 229156229, "title": "Extracting Training Data from Large Language Models", "year": 2020, "venue": "USENIX Security Symposium", "authors": [{"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}, {"name": "Eric Wallace", "authorId": "145217343"}, {"name": "Matthew Jagielski", "authorId": "40844378"}, {"name": "Ariel Herbert-Voss", "authorId": "1404060687"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Tom B. Brown", "authorId": "31035595"}, {"name": "D. Song", "authorId": "143711382"}, {"name": "\u00da. Erlingsson", "authorId": "1758110"}, {"name": "Alina Oprea", "authorId": "3046437"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 1950}, "snippets": ["It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models."], "score": 0.0}], "table": null}, {"title": "Evaluation Metrics and Challenges", "tldr": "Evaluating machine unlearning effectiveness requires specialized metrics that balance knowledge removal with retained capabilities. Key challenges include avoiding catastrophic forgetting of unrelated knowledge, maintaining generation quality, and preventing information leakage through extraction attacks. (7 sources)", "text": "\nEvaluating the effectiveness of machine unlearning methods presents unique challenges that have led to the development of specialized metrics and frameworks. A comprehensive evaluation must assess both the successful removal of targeted knowledge and the preservation of desired model capabilities.\n\nKey evaluation metrics for machine unlearning include:\n\n1. **Knowledge Forgetting Rate (KFR)** - Measures how effectively a model has removed targeted information from its knowledge base <Paper corpusId=\"276408369\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>.\n\n2. **Knowledge Retention Rate (KRR)** - Quantifies how well the model maintains unrelated knowledge that should be preserved <Paper corpusId=\"276408369\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>.\n\n3. **Linguistic Score (LS)** - Evaluates the general quality of text generation after unlearning to ensure maintained fluency and coherence <Paper corpusId=\"276408369\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>.\n\n4. **MEMO (Memorization Metric)** - A specialized metric that quantifies factual memorization in LLMs, providing signals for selecting appropriate unlearning interventions <Paper corpusId=\"272704025\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>.\n\n5. **Token Diversity, Sentence Semantics, and Factual Correctness** - Additional metrics designed to provide a more nuanced evaluation of model outputs following unlearning <Paper corpusId=\"273233618\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper>.\n\nDespite these advances in evaluation methodology, machine unlearning still faces several significant challenges:\n\nFirst, **utility preservation** remains a critical concern, as successful unlearning often causes \"catastrophic collapse\" on unrelated tasks <Paper corpusId=\"272704025\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>. This challenge is particularly evident when unlearning methods remove excessive domain-specific knowledge, including information that should remain in the model's parametric knowledge <Paper corpusId=\"277857590\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper>.\n\nSecond, many approaches suffer from **efficiency limitations**, either involving the addition of similarly sized models (which slows down unlearning or inference) or requiring retain data that may be difficult to obtain <Paper corpusId=\"272704025\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>. An effective unlearning method must require less computational cost than full retraining while maintaining inference efficiency <Paper corpusId=\"278481378\" paperTitle=\"(Vasilev et al., 2025)\" isShortName></Paper>.\n\nThird, even effective methods may still exhibit **robustness issues**, potentially leaking data through extraction techniques <Paper corpusId=\"272704025\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>. Some unlearning approaches rely on knowledge obfuscation rather than true removal, leaving models vulnerable to probing attacks that can recover the supposedly forgotten information <Paper corpusId=\"278338982\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.\n\nFourth, many methods overlook the impact of **logically related knowledge** on unlearning effectiveness <Paper corpusId=\"276812969\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. Since language models establish complex networks of related facts, simply removing targeted information without addressing logically connected knowledge can undermine unlearning efficacy.\n\nTo address these challenges, researchers continue to develop more sophisticated frameworks that distinguish true unlearning from obfuscation <Paper corpusId=\"278338982\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper> and employ gradient-based strategies to precisely distinguish between knowledge that should be forgotten and retained <Paper corpusId=\"277857590\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Xu et al., 2025)", "paper": {"corpus_id": 276408369, "title": "ReLearn: Unlearning via Learning for Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Haoming Xu", "authorId": "2326503114"}, {"name": "Ningyuan Zhao", "authorId": "2182474634"}, {"name": "Liming Yang", "authorId": "2345879531"}, {"name": "Sendong Zhao", "authorId": "2345876908"}, {"name": "Shumin Deng", "authorId": "152931849"}, {"name": "Meng Wang", "authorId": "2218346459"}, {"name": "Bryan Hooi", "authorId": "2314827895"}, {"name": "Nay Oo", "authorId": "2266753656"}, {"name": "Huajun Chen", "authorId": "2144200945"}, {"name": "Ningyu Zhang", "authorId": "2153010067"}], "n_citations": 3}, "snippets": ["To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality."], "score": 0.998046875}, {"id": "(Gu et al., 2024)", "paper": {"corpus_id": 272704025, "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tianle Gu", "authorId": "2279024315"}, {"name": "Kexin Huang", "authorId": "2266421899"}, {"name": "Ruilin Luo", "authorId": "2279024030"}, {"name": "Yuanqi Yao", "authorId": "2306059424"}, {"name": "Yujiu Yang", "authorId": "2284727148"}, {"name": "Yan Teng", "authorId": "2266238818"}, {"name": "Yingchun Wang", "authorId": "2266364817"}], "n_citations": 9}, "snippets": ["LLM Unlearning, a post-hoc approach to remove this information from trained LLMs, offers a promising solution to mitigate these risks. However, previous practices face three key challenges: 1. Utility: successful unlearning often causes catastrophic collapse on unrelated tasks. 2. Efficiency: many methods either involve adding similarly sized models, which slows down unlearning or inference, or require retain data that are difficult to obtain. 3. Robustness: even effective methods may still leak data via extraction techniques. To address these challenges, we propose MEOW, a simple yet effective gradient descent-based unlearning method. Specifically, we use an offline LLM to generate a set of inverted facts. Then, we design a new metric, MEMO, to quantify memorization in LLMs. Finally, based on the signals provided by MEMO, we select the most appropriate set of inverted facts and finetune the model based on them."], "score": 0.9970703125}, {"id": "(Yuan et al., 2024)", "paper": {"corpus_id": 273233618, "title": "A Closer Look at Machine Unlearning for Large Language Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xiaojian Yuan", "authorId": "2273843751"}, {"name": "Tianyu Pang", "authorId": "19201674"}, {"name": "Chao Du", "authorId": "2325201427"}, {"name": "Kejiang Chen", "authorId": "8780109"}, {"name": "Weiming Zhang", "authorId": "2189835131"}, {"name": "Min Lin", "authorId": "2253977831"}], "n_citations": 13}, "snippets": ["Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning."], "score": 0.998046875}, {"id": "(Kim et al., 2025)", "paper": {"corpus_id": 277857590, "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Kun-Woo Kim", "authorId": "2356009299"}, {"name": "Ji-Hoon Park", "authorId": "2257098510"}, {"name": "Jumin Han", "authorId": "2355145341"}, {"name": "Seong-Whan Lee", "authorId": "2339467966"}], "n_citations": 1}, "snippets": ["A key challenge in Machine Unlearning is to eliminate only the targeted knowledge while preserving the remaining information and maintaining general task performance. Existing unlearning methods, however, often remove an excessive amount of domain-specific knowledge, including information that must remain in the parametric knowledge", "To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain."], "score": 0.998046875}, {"id": "(Vasilev et al., 2025)", "paper": {"corpus_id": 278481378, "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation", "year": 2025, "venue": "", "authors": [{"name": "Stefan Vasilev", "authorId": "2350753289"}, {"name": "Christian Herold", "authorId": "2307079915"}, {"name": "Baohao Liao", "authorId": "66693547"}, {"name": "Seyyed Hadi Hashemi", "authorId": "2350630854"}, {"name": "Shahram Khadivi", "authorId": "2490162"}, {"name": "C. Monz", "authorId": "2062908179"}], "n_citations": 0}, "snippets": ["In the machine unlearning framework, we define the full training dataset as a partition of two subsets: the forget set, which consists of the data to be unlearned, and the retain set, which contains the remaining knowledge that should be preserved after unlearning. An effective machine unlearning method aims to produce a model that successfully forgets the forget data, while maintaining the integrity of the retained knowledge. Specifically, the resulting unlearned model should satisfy the following key requirements: 1) Minimize the retention of information from the forget set; 2) Maintain high performance on the retain set; 3) Require less computational cost than retraining the model from scratch on the retain set; 4) Maintain inference efficiency, i.e., ensuring unchanged latency."], "score": 0.99658203125}, {"id": "(Sun et al., 2025)", "paper": {"corpus_id": 278338982, "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Guangzhi Sun", "authorId": "2321755791"}, {"name": "Potsawee Manakul", "authorId": "89355510"}, {"name": "Xiao Zhan", "authorId": "2359255668"}, {"name": "Mark Gales", "authorId": "2359255671"}], "n_citations": 0}, "snippets": ["Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour."], "score": 0.99755859375}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 276812969, "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wenyu Wang", "authorId": "2348951919"}, {"name": "Mengqi Zhang", "authorId": "48985110"}, {"name": "Xiaotian Ye", "authorId": "2286432237"}, {"name": "Zhaochun Ren", "authorId": "2260895127"}, {"name": "Zhumin Chen", "authorId": "1721165"}, {"name": "Pengjie Ren", "authorId": "1749477"}], "n_citations": 3}, "snippets": ["Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning."], "score": 0.998046875}], "table": null}, {"title": "Applications and Use Cases", "tldr": "Machine unlearning methods are increasingly applied to address harmful content generation, protect private data, improve model truthfulness, and comply with regulations. These applications demonstrate the practical value of selectively removing knowledge from language models across various domains. (11 sources)", "text": "\nMachine unlearning techniques have found applications across several important use cases:\n\n1. **Harmful Content Mitigation** - Unlearning methods like Selective Knowledge negation Unlearning (SKU) can remove harmful knowledge from language models while preserving utility on normal prompts <Paper corpusId=\"267681958\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Similarly, frameworks using evaluative models can identify dialogues requiring unlearning to prevent harmful, hallucinatory, or privacy-compromising responses <Paper corpusId=\"269430574\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\n2. **Privacy Protection** - Machine unlearning enables compliance with data protection regulations like GDPR by efficiently removing user data without complete retraining <Paper corpusId=\"273502714\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. Entity-level unlearning methods like Opt-Out specifically target removing all knowledge related to a particular individual while preserving other model capabilities <Paper corpusId=\"270562084\" paperTitle=\"(Choi et al., 2024)\" isShortName></Paper>.\n\n3. **Copyright Compliance** - Unlearning techniques can address copyright concerns by removing copyrighted content from language models, providing a more efficient alternative to retraining <Paper corpusId=\"273502714\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. KnowUnDo offers a benchmark specifically for evaluating unlearning effectiveness with copyrighted content <Paper corpusId=\"270878324\" paperTitle=\"(Tian et al., 2024)\" isShortName></Paper>.\n\n4. **Truthfulness Enhancement** - The Extraction-before-Subtraction (Ext-Sub) method uses parameter-efficient modules to isolate and remove undesirable features like untruthfulness while preserving core capabilities <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260925619\" paperTitle=\"(Hu et al., 2023)\" isShortName></Paper>.\n\n5. **Detoxification** - Several approaches target toxic language removal, including selective neuron pruning to eliminate toxic behavior while maintaining overall performance <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>, and Quark, which uses reinforcement learning to mitigate undesirable text generation behaviors like toxicity and repetition <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"249152301\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper>.\n\n6. **Recommender Systems** - E2URec specifically addresses unlearning in LLM-based recommender systems, enabling the forgetting of user data while preserving model performance through low-rank adaptation modules and a teacher-student framework <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\n7. **Memorization Mitigation** - Specialized approaches like DeMem use reinforcement learning feedback loops with negative similarity scores to incentivize language models to paraphrase memorized content, reducing sensitive information exposure <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"266164054\" paperTitle=\"(Kassem et al., 2023)\" isShortName></Paper>.\n\n8. **Knowledge Editing** - In-context knowledge editing (IKE) enables modifying factual knowledge without parameter updates, achieving competitive success rates compared to gradient-based methods with fewer side effects <Paper corpusId=\"270440244\" paperTitle=\"(Ashuach et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258832407\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>.\n\nThese diverse applications demonstrate the practical importance of machine unlearning techniques across multiple domains, addressing both regulatory requirements and ethical concerns related to language model deployment.", "citations": [{"id": "(Liu et al., 2024)", "paper": {"corpus_id": 267681958, "title": "Towards Safer Large Language Models through Machine Unlearning", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zheyuan Liu", "authorId": "2122087252"}, {"name": "Guangyao Dou", "authorId": "2174956825"}, {"name": "Zhaoxuan Tan", "authorId": "2093186816"}, {"name": "Yijun Tian", "authorId": "46879986"}, {"name": "Meng Jiang", "authorId": "2275403324"}], "n_citations": 87}, "snippets": ["The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts."], "score": 0.99853515625}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 269430574, "title": "Machine Unlearning in Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Kongyang Chen", "authorId": "2268643078"}, {"name": "Zixin Wang", "authorId": "2289862169"}, {"name": "Bing Mi", "authorId": "2212851422"}, {"name": "Waixi Liu", "authorId": "2298857854"}, {"name": "Shaowei Wang", "authorId": "2295540521"}, {"name": "Xiaojun Ren", "authorId": "2298846891"}, {"name": "Jiaxing Shen", "authorId": "2295552260"}], "n_citations": 13}, "snippets": ["Our objectives are to make LLMs not produce harmful, hallucinatory, or privacy-compromising responses, while retaining their standard output capabilities. To accomplish this, we use an evaluative model to pinpoint dialogues needing unlearning. We also establish a distance loss to function as the model's negative loss, diverting it from previous undesirable outputs. Furthermore, we determine the expected output's cluster mean to formulate a positive loss, directing the model's outputs toward preferable outcomes without compromising its reasoning abilities and performance."], "score": 0.9970703125}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 273502714, "title": "Evaluating Deep Unlearning in Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ruihan Wu", "authorId": "2303333890"}, {"name": "Chhavi Yadav", "authorId": "83222216"}, {"name": "Russ Salakhutdinov", "authorId": "2266239350"}, {"name": "Kamalika Chaudhuri", "authorId": "2303254420"}], "n_citations": 7}, "snippets": ["Machine unlearning is a key requirement of many data protection regulations such as GDPR. Prior work on unlearning has mostly considered superficial unlearning tasks where a single or a few related pieces of information are required to be removed. However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other", "Machine unlearning has emerged as a solution for removing specific data, concepts, or facts from these models in a more efficient way than retraining from scratch. It is practical in situations where regulations, such as the GDPR (Parliament & of the European Union, 2016), mandate the removal of a user's data. Moreover, unlearning is important when models inadvertently capture copyrighted 1 or offensive content."], "score": 0.998046875}, {"id": "(Choi et al., 2024)", "paper": {"corpus_id": 270562084, "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport", "year": 2024, "venue": "", "authors": [{"name": "Minseok Choi", "authorId": "2203800211"}, {"name": "Daniel Rim", "authorId": "2307073048"}, {"name": "Dohyun Lee", "authorId": "2294508694"}, {"name": "Jaegul Choo", "authorId": "2260653165"}], "n_citations": 2}, "snippets": ["Machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning."], "score": 0.9990234375}, {"id": "(Tian et al., 2024)", "paper": {"corpus_id": 270878324, "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Bo Tian", "authorId": "2064522174"}, {"name": "Xiaozhuan Liang", "authorId": "2153398295"}, {"name": "Siyuan Cheng", "authorId": "2258034882"}, {"name": "Qingbin Liu", "authorId": "2258682951"}, {"name": "Meng Wang", "authorId": "2218346459"}, {"name": "Dianbo Sui", "authorId": "2273504274"}, {"name": "Xi Chen", "authorId": "48283576"}, {"name": "Huajun Chen", "authorId": "2144200945"}, {"name": "Ningyu Zhang", "authorId": "2153010067"}], "n_citations": 13}, "snippets": ["Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters."], "score": 0.9970703125}, {"id": "(Nguyen et al., 2022)", "paper": {"corpus_id": 252089272, "title": "A Survey of Machine Unlearning", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "T. Nguyen", "authorId": "2117824517"}, {"name": "T. Huynh", "authorId": "152399820"}, {"name": "Phi-Le Nguyen", "authorId": "2143967163"}, {"name": "Alan Wee-Chung Liew", "authorId": "1733300"}, {"name": "Hongzhi Yin", "authorId": "2416851"}, {"name": "Q. Nguyen", "authorId": "144133815"}], "n_citations": 239}, "snippets": ["Chen et al. [20] introduce an efficient framework for unlearning in LLMs that integrates lightweight unlearning layers into transformer models, allowing for the selective removal of specific data without retraining the entire model. They also introduce a fusion mechanism to combine these layers when multiple unlearning requests are made. Kassem (Kassem et al., 2023) propose another unlearning approach called \"DeMem\", which uses a reinforcement learning feedback loop with a negative similarity score as a reward. This approach incentivises the language model to paraphrase memorised content, thereby reducing the risk of sensitive information being exposed. Wang et al. [190] introduce E2URec, an unlearning method for LLM-based recommender systems. It addresses forgetting user data while preserving model performance by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework. Some works, use machine unlearning as a way to improve the truthfulness and detoxification of LLMs. Hu et al. (Hu et al., 2023) propose the \"Extraction-before-Subtraction\" (Ext-Sub) method with parameterefficient modules (PEMs) to isolate and remove undesirable features like untruthfulness or toxicity, while preserving the model's core capabilities. The method extracts deficiency capabilities from \"anti-expert\" PEMs and subtracts them from \"expert\" PEMs, enhancing performance without compromising fundamental abilities. Pochinkov et al. [139] introduce another method by selectively pruning neurons responsible for specific behaviours, such as coding or toxic language, while maintaining overall performance. Lu et al. (Lu et al., 2022) presents a framework called Quark that uses reinforcement learning as a machine unlearning approach to mitigate undesirable text generation behaviors, such as toxicity and repetition, while preserving language quality."], "score": 0.998046875}, {"id": "(Hu et al., 2023)", "paper": {"corpus_id": 260925619, "title": "Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Xinshuo Hu", "authorId": "2149467818"}, {"name": "Dongfang Li", "authorId": "1664667501"}, {"name": "Zihao Zheng", "authorId": "151479145"}, {"name": "Zhenyu Liu", "authorId": "2230018369"}, {"name": "Baotian Hu", "authorId": "2142726660"}, {"name": "M. Zhang", "authorId": "50495870"}], "n_citations": 30}, "snippets": ["Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of ``expert'' PEM and ``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of truthfulness and detoxification, we conduct extensive experiments on LLMs, encompassing additional abilities such as language modelling and mathematical reasoning. Our empirical results demonstrate that our approach effectively improves truthfulness and detoxification, while largely preserving the fundamental abilities of LLMs."], "score": 0.0}, {"id": "(Lu et al., 2022)", "paper": {"corpus_id": 249152301, "title": "Quark: Controllable Text Generation with Reinforced Unlearning", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Ximing Lu", "authorId": "50085131"}, {"name": "S. Welleck", "authorId": "2129663"}, {"name": "Liwei Jiang", "authorId": "2112504145"}, {"name": "Jack Hessel", "authorId": "2689239"}, {"name": "Lianhui Qin", "authorId": "3444092"}, {"name": "Peter West", "authorId": "119659229"}, {"name": "Prithviraj Ammanabrolu", "authorId": "19179135"}, {"name": "Yejin Choi", "authorId": "1699545"}], "n_citations": 219}, "snippets": ["Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives."], "score": 0.0}, {"id": "(Kassem et al., 2023)", "paper": {"corpus_id": 266164054, "title": "Preserving Privacy Through Dememorization: An Unlearning Technique For Mitigating Memorization Risks In Language Models", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Aly M. Kassem", "authorId": "2198232749"}, {"name": "Omar Mahmoud", "authorId": "2273559947"}, {"name": "Sherif Saad", "authorId": "2273568006"}], "n_citations": 32}, "snippets": [","], "score": 0.0}, {"id": "(Ashuach et al., 2024)", "paper": {"corpus_id": 270440244, "title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tomer Ashuach", "authorId": "2306249146"}, {"name": "Martin Tutek", "authorId": "2367197291"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}], "n_citations": 7}, "snippets": ["Machine unlearning approaches [35] aim to discourage models from generating sensitive information through techniques such as in-context unlearning [ICL; 37,45,(Zheng et al., 2023) or gradient ascent [29,55,(Yu et al., 2023).These approaches do not ensure that the sensitive information is erased from model parameters, rendering them vulnerable to extraction attacks [33].Model editing, a variant of localization-based unlearning, localizes and change a subset model parameters to erase or overwrite the sensitive information itself [15,(Meng et al., 2022)[39].In contrast to ICL and optimization-based methods, which may prevent models from generating sensitive content but do not fundamentally erase the underlying knowledge, editing methods hold a greater potential to resist extraction attacks (Carlini et al., 2020) by surgically removing the sensitive data from model parameters."], "score": 0.99755859375}, {"id": "(Zheng et al., 2023)", "paper": {"corpus_id": 258832407, "title": "Can We Edit Factual Knowledge by In-Context Learning?", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Ce Zheng", "authorId": "2113919886"}, {"name": "Lei Li", "authorId": "49192881"}, {"name": "Qingxiu Dong", "authorId": "2047143813"}, {"name": "Yuxuan Fan", "authorId": "2118167265"}, {"name": "Zhiyong Wu", "authorId": "150358371"}, {"name": "Jingjing Xu", "authorId": "47883405"}, {"name": "Baobao Chang", "authorId": "7267809"}], "n_citations": 216}, "snippets": ["Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or out-dated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/Zce1112zslx/IKE."], "score": 0.0}], "table": null}], "cost": 0.489918}}
