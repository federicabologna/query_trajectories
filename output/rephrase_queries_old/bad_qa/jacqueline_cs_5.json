{"original_query": "One desirable property for LMs is that they should be well-calibrated, in that the LM can generate a confidence score that is well-calibrated to how likely its decision is actually correct. What are some recent works that seek to properly calibrate LM confidence judgments?", "input": "desirable property LMs they well calibrated LM generate confidence score is to how likely decision is correct recent works seek properly LM confidence judgments", "output": {"sections": [{"title": "Introduction to Calibration in Language Models", "tldr": "Calibration in language models refers to how well the model's confidence in its outputs matches the actual correctness of those outputs. Well-calibrated LMs provide confidence scores that reliably indicate how likely their decisions or predictions are to be correct. (LLM Memory)", "text": "\nCalibration in language models (LMs) refers to the alignment between a model's confidence in its predictions and the actual likelihood that those predictions are correct. A perfectly calibrated language model would assign confidence scores that precisely match the probability of the answer being correct - for example, answers given with 80% confidence would be correct 80% of the time. This property is increasingly important as language models are deployed in critical applications where users and systems need to know when to trust or question model outputs. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nRecent research has focused on evaluating and improving calibration in LMs, as these models often exhibit overconfidence or underconfidence in their predictions. Calibration is particularly important in scenarios where LMs are used for decision support, content generation, or knowledge-intensive tasks. Well-calibrated models allow users to set appropriate thresholds for automated decision-making and help identify when human intervention might be necessary. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe query specifically mentions \"confidence judgments\" from language models - these are explicit or implicit signals from the model about its certainty in the generated content or decisions. These confidence scores can be derived from various sources, including the model's output probabilities, specially designed prompting techniques, or through additional calibration layers added to the model architecture. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Properties of Well-Calibrated LMs", "tldr": "Well-calibrated language models provide confidence scores that accurately reflect their likelihood of correctness, enabling better decision-making in applications. These models should maintain calibration across different domains and avoid both overconfidence and underconfidence. (7 sources)", "text": "\nWell-calibrated language models possess several key properties that make them reliable tools for decision-making systems. First and foremost, their confidence scores should accurately indicate the likelihood that their answers are correct <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>. This alignment between expressed confidence and actual correctness enables practical applications such as deferring to human experts in low-confidence scenarios, thus creating more trustworthy AI systems.\n\nA critical characteristic of truly well-calibrated models is maintaining calibration across diverse domains and tasks. Recent research has revealed that apparent overall calibration can mask significant domain-specific miscalibration - for example, a model might appear well-calibrated in aggregate while being systematically overconfident in mathematics and underconfident in history <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This finding highlights the importance of evaluating calibration across different \"slices\" of knowledge rather than just in aggregate.\n\nWell-calibrated models also avoid overconfidently making mistakes. The calibration curve approach, originally used in weather forecasting <Paper corpusId=\"109884250\" paperTitle=\"(Degroot et al., 1983)\" isShortName></Paper> and later applied to neural networks <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper> <Paper corpusId=\"235435823\" paperTitle=\"(Minderer et al., 2021)\" isShortName></Paper>, offers a method to evaluate whether models assign appropriate confidence levels to their predictions <Paper corpusId=\"273396026\" paperTitle=\"(Stearns et al., 2024)\" isShortName></Paper>. A properly calibrated model shows a strong relationship between prediction confidence and actual success rate.\n\nFor conversational AI systems based on large language models, well-calibrated confidence scores are particularly important for reducing hallucination and preventing over-reliance on model outputs. Effective confidence estimation methods might include approaches based on softmax probabilities, raw token scores, verbalized confidences, or combinations of these techniques <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>. The quality of calibration can be assessed using metrics like the area under the curve (AUC), with higher values indicating better calibration.", "citations": [{"id": "(Tian et al., 2023)", "paper": {"corpus_id": 258865733, "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Katherine Tian", "authorId": "2218407338"}, {"name": "E. Mitchell", "authorId": "49688913"}, {"name": "Allan Zhou", "authorId": "2064472884"}, {"name": "Archit Sharma", "authorId": "50465276"}, {"name": "Rafael Rafailov", "authorId": "102801230"}, {"name": "Huaxiu Yao", "authorId": "18307037"}, {"name": "Chelsea Finn", "authorId": "46881670"}, {"name": "Christopher D. Manning", "authorId": "144783904"}], "n_citations": 355}, "snippets": ["A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated."], "score": 0.74951171875}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 268723623, "title": "Few-Shot Recalibration of Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xiang Lisa Li", "authorId": "2293910776"}, {"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Kelvin Guu", "authorId": "2091768"}], "n_citations": 5}, "snippets": ["Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)."], "score": 0.68994140625}, {"id": "(Degroot et al., 1983)", "paper": {"corpus_id": 109884250, "title": "The Comparison and Evaluation of Forecasters.", "year": 1983, "venue": "", "authors": [{"name": "M. Degroot", "authorId": "39416713"}, {"name": "S. Fienberg", "authorId": "1684961"}], "n_citations": 745}, "snippets": ["In this paper we present methods for comparing and evaluating forecasters whose predictions are presented as their subjective probability distributions of various random variables that will be observed in the future, e.g. weather forecasters who each day must specify their own probabilities that it will rain in a particular location. We begin by reviewing the concepts of calibration and refinement, and describiing the relationship between this notion of refinement and the notion of sufficiency in the comparison of statistical experiments. We also consider the question of interrelationships among forecasters and discuss methods by which an observer should combine the predictions from two or more different forecasters. Then we turn our attention to the concept of a proper scoring rule for evaluating forecasters, relating it to the concepts of calibration and refinement. Finally, we discuss conditions under which one fore- caster can exploit the predictions of another forecaster to obtain a better score. In this paper we describe some concepts and methods appropriate for evaluating and com- paring forecasters who repeatedly present their predictions of whether or not various events will occur in terms of their subjective probabilities of those events. The ideas we describe here are relevant in almost any situation in which forecasters must repeatedly make such probabilistic predictions, regardless of the particular subject matter or substantive area of the events being forecast. The forecaster might be an economist who at the beginning of each quarterly period makes predictions about unemployment, the rate of inflation, or Gross National Product in that quarter based on the values of various economic indicators; the forecaster might even make predictions using a large-scale econometric model of the United States economy based on hundreds of variables and econometric relations. In a different field, the forecaster might be the weatherman for a television station who at the beginning of each day must announce his probability that it will rain during the day. For ease of exposition, we present our discussions here in the context of such a weather forecaster who day after day must specify his subjective probability x that there will be at least a certain amount of rain at some given location during a specified time interval of the day. We refer to the occurrence of this well-specified event simply as \"rain\". Thus, at the begin- ning of each day the forecaster must specify his probability of rain and at the end of each day he observes whether or not rain actually occurred. The probability x specified by the forecaster on any particular day is called his prediction for that day. We shall make the realistic, and simultaneously simplifying, assumption that the"], "score": 0.0}, {"id": "(Guo et al., 2017)", "paper": {"corpus_id": 28671436, "title": "On Calibration of Modern Neural Networks", "year": 2017, "venue": "International Conference on Machine Learning", "authors": [{"name": "Chuan Guo", "authorId": "144993411"}, {"name": "Geoff Pleiss", "authorId": "10804137"}, {"name": "Yu Sun", "authorId": "2117103358"}, {"name": "Kilian Q. Weinberger", "authorId": "7446832"}], "n_citations": 5869}, "snippets": ["Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."], "score": 0.0}, {"id": "(Minderer et al., 2021)", "paper": {"corpus_id": 235435823, "title": "Revisiting the Calibration of Modern Neural Networks", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Matthias Minderer", "authorId": "46352821"}, {"name": "J. Djolonga", "authorId": "2941141"}, {"name": "Rob Romijnders", "authorId": "3451951"}, {"name": "F. Hubis", "authorId": "73774594"}, {"name": "Xiaohua Zhai", "authorId": "2743563"}, {"name": "N. Houlsby", "authorId": "2815290"}, {"name": "Dustin Tran", "authorId": "47497262"}, {"name": "Mario Lucic", "authorId": "34302129"}], "n_citations": 367}, "snippets": ["Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties."], "score": 0.0}, {"id": "(Stearns et al., 2024)", "paper": {"corpus_id": 273396026, "title": "Evaluating the Generalisation of an Artificial Learner", "year": 2024, "venue": "NLP4CALL", "authors": [{"name": "Bernardo Stearns", "authorId": "51896370"}, {"name": "Nicolas Ballier", "authorId": "20486050"}, {"name": "Thomas Gaillat", "authorId": "2261396630"}, {"name": "Andrew Simpkin", "authorId": "2261398341"}, {"name": "John P. Mccrae", "authorId": "1738599770"}], "n_citations": 0}, "snippets": ["A second desirable property is that our models do not overconfidently make mistakes, assigning high probabilities to incorrect predictions. \n\nOne approach for such analysis is through the \"calibration curve\" method. Initially employed in analysing weather forecasts (Brier, 1950)(Degroot et al., 1983), this technique has since been applied to neural networks (Guo et al., 2017)(Minderer et al., 2021) and recently to evaluate Large Language Models (LLMs) from a semantic perspective (Levinstein et al., 2023). For example, (Levinstein and Herrmann, 2024) utilizes calibration curves to assess the veracity of LLM statements on specific datasets and asserts that \"calibration offers another metric for evaluating the quality of probes' forecasts.\" Calibration analyses have been utilized in neural networks and language models (Minderer et al., 2021)(Lewis, 1949), allowing researchers to assess the relationship between a model's prediction confidence and success rate."], "score": 0.546875}, {"id": "(Sun et al., 2024)", "paper": {"corpus_id": 272689376, "title": "Confidence Estimation For LLM-Based Dialogue State Tracking", "year": 2024, "venue": "Spoken Language Technology Workshop", "authors": [{"name": "Yi-Jyun Sun", "authorId": "2321493889"}, {"name": "Suvodip Dey", "authorId": "29832722"}, {"name": "Dilek Hakkani-Tur", "authorId": "2365041900"}, {"name": "Gokhan Tur", "authorId": "5108268"}], "n_citations": 1}, "snippets": ["Estimation of a model's confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance", "Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration."], "score": 0.52294921875}], "table": null}, {"title": "Methods for Achieving and Measuring Calibration", "tldr": "Various methods have been developed to calibrate language models, including multicalibration approaches, confidence estimation techniques, and specialized calibration metrics. These methods aim to align model confidence with actual performance while maintaining efficiency and interpretability. (3 sources)", "text": "\nSeveral approaches have been developed to achieve and measure calibration in language models:\n\n1. **Multicalibration**: This technique aims to produce reliable confidence scores by ensuring calibration across various intersecting groups of data, not just marginally. Multicalibration provides more interpretable and reliable confidence scores for LLM outputs by addressing calibration simultaneously across different data groupings. <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>\n\n2. **Softmax-based confidence estimation**: This approach uses the probability distribution from the model's softmax layer to derive confidence scores. While computationally efficient, these methods may not always provide the most informative calibration signals. <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>\n\n3. **Raw token score methods**: These techniques examine the raw scores associated with generated tokens to estimate confidence. They can provide more granular information about the model's certainty at different points in generation. <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>\n\n4. **Verbalized confidence**: This method involves prompting the model to explicitly state its confidence level in natural language. This approach leverages the model's ability to reflect on its own outputs. <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>\n\n5. **Combination methods**: Hybrid approaches that combine multiple confidence estimation techniques often achieve better calibration results than single methods alone. <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>\n\n6. **Self-consistency-based calibration**: These methods evaluate consistency across multiple generations to estimate confidence, though they may be limited in inference-time efficiency. <Paper corpusId=\"270620078\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>\n\n7. **Logit-based calibration**: Approaches that use model logits to derive confidence scores, though these sometimes fall short of providing sufficiently informative signals. <Paper corpusId=\"270620078\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>\n\n8. **Area Under the Curve (AUC) metric**: This evaluation method assesses the quality of confidence estimation, with higher AUC values indicating better calibration. It provides a quantitative measure of how well confidence scores align with actual performance. <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>\n\nResearchers continue to develop new calibration methods that balance the trade-offs between computational efficiency, interpretability, and accuracy of confidence estimation. Effective calibration techniques are particularly important for reducing hallucination and preventing over-reliance on model outputs in conversational AI systems. <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Detommaso et al., 2024)", "paper": {"corpus_id": 269004786, "title": "Multicalibration for Confidence Scoring in LLMs", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Gianluca Detommaso", "authorId": "2295667267"}, {"name": "Martin Bertran", "authorId": "2295665717"}, {"name": "Riccardo Fogliato", "authorId": "2295664744"}, {"name": "Aaron Roth", "authorId": "2295665299"}], "n_citations": 19}, "snippets": ["This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data."], "score": 0.51318359375}, {"id": "(Sun et al., 2024)", "paper": {"corpus_id": 272689376, "title": "Confidence Estimation For LLM-Based Dialogue State Tracking", "year": 2024, "venue": "Spoken Language Technology Workshop", "authors": [{"name": "Yi-Jyun Sun", "authorId": "2321493889"}, {"name": "Suvodip Dey", "authorId": "29832722"}, {"name": "Dilek Hakkani-Tur", "authorId": "2365041900"}, {"name": "Gokhan Tur", "authorId": "5108268"}], "n_citations": 1}, "snippets": ["Estimation of a model's confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance", "Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration."], "score": 0.52294921875}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 270620078, "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xin Liu", "authorId": "2120099874"}, {"name": "Farima Fatahi Bayat", "authorId": "2131675694"}, {"name": "Lu Wang", "authorId": "2299208178"}], "n_citations": 6}, "snippets": ["Calibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs' reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals."], "score": 0.6728515625}], "table": null}, {"title": "Challenges in LM Calibration", "tldr": "Language models face significant calibration challenges including hidden miscalibration across knowledge domains and calibration degradation after alignment training. These issues compromise reliability and make it difficult to trust confidence scores in practical applications. (3 sources)", "text": "\nDespite recent progress in language model calibration, several fundamental challenges remain that complicate efforts to create truly well-calibrated systems:\n\nFirst, apparent calibration can mask significant domain-specific miscalibration. Language models may appear well-calibrated when evaluated on broad distributions while hiding substantial miscalibration within specific knowledge domains - for example, systematic overconfidence in mathematics might balance out systematic underconfidence in history, creating an illusion of good overall calibration <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This phenomenon makes it difficult to rely on aggregate calibration metrics and requires more granular evaluation across different knowledge \"slices.\"\n\nSecond, the alignment process itself can undermine calibration. Models fine-tuned with reinforcement learning from human feedback (RLHF) and other alignment techniques often show degraded calibration compared to their pre-trained versions <Paper corpusId=\"272987064\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>. While these alignment processes successfully make models more helpful and harmless, they frequently introduce overconfidence problems where the model's expressed confidence fails to match its actual correctness rate <Paper corpusId=\"268876453\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This creates a challenging tension between making models more aligned with human values and maintaining their calibration properties.\n\nThird, calibration methods must contend with the inherent complexity of language as an evaluation domain. Unlike classification tasks with clear right/wrong answers, language generation involves nuanced evaluations of correctness that can be subjective or context-dependent <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This ambiguity makes it challenging to establish ground truth for calibration assessment and complicates the development of reliable confidence estimation methods.\n\nFinally, there are practical implementation challenges in deploying well-calibrated models. Calibration approaches often require additional computation or post-processing steps that may not be feasible in real-time applications. Finding methods that balance computational efficiency with calibration quality remains an open research challenge <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Li et al., 2024)", "paper": {"corpus_id": 268723623, "title": "Few-Shot Recalibration of Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xiang Lisa Li", "authorId": "2293910776"}, {"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Kelvin Guu", "authorId": "2091768"}], "n_citations": 5}, "snippets": ["Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)."], "score": 0.68994140625}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 272987064, "title": "Calibrating Language Models with Adaptive Temperature Scaling", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Johnathan Xie", "authorId": "2285068366"}, {"name": "Annie S. Chen", "authorId": "2111073657"}, {"name": "Yoonho Lee", "authorId": "2260289273"}, {"name": "Eric Mitchell", "authorId": "2260106796"}, {"name": "Chelsea Finn", "authorId": "2279838067"}], "n_citations": 17}, "snippets": ["The effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration\u2014how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly."], "score": 0.685546875}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 268876453, "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mozhi Zhang", "authorId": "2254324716"}, {"name": "Mianqiu Huang", "authorId": "2255220422"}, {"name": "Rundong Shi", "authorId": "2294723713"}, {"name": "Linsen Guo", "authorId": "2295100870"}, {"name": "Chong Peng", "authorId": "2294823462"}, {"name": "Peng Yan", "authorId": "2296600123"}, {"name": "Yaqian Zhou", "authorId": "2278545938"}, {"name": "Xipeng Qiu", "authorId": "2294872225"}], "n_citations": 13}, "snippets": ["Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate."], "score": 0.6376953125}], "table": null}], "cost": 0.122241}}
