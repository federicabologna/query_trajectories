{"original_query": "Hi everyone! I was wondering if anyone knows about work on how LLM fine-tuning (perhaps when done poorly) causes the model to become worse at some things? For example, I\u2019ve heard lots of anecdotes that fine-tuning LLMs for specific tasks \u201churts their generation ability\u201d, but I\u2019m looking for papers with more systematic exploration.", "input": "was anyone work LLM fine tuning when done the model worse some things For example lots of anecdotes fine LLMs specific tasks hurts generation ability but I papers with systematic exploration", "output": {"sections": [{"title": "Introduction to LLM Fine-tuning Trade-offs", "tldr": "Fine-tuning LLMs for specific tasks often involves trade-offs where improving performance in one area can degrade capabilities in others. This phenomenon is known as \"catastrophic forgetting\" or \"negative transfer,\" where specialized training causes models to lose some of their general capabilities. (LLM Memory)", "text": "\nFine-tuning large language models (LLMs) is a common practice to adapt these models for specific tasks or domains. However, there is growing evidence that this process often involves significant trade-offs. When a model is fine-tuned to excel at a particular task, it may simultaneously experience degradation in its performance on other tasks or in its general capabilities. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThis trade-off is particularly pronounced when models are heavily optimized for narrow tasks. For example, when an LLM is fine-tuned extensively on question-answering datasets, it might become more precise at answering factual questions but lose some of its creative writing abilities or conversational fluency. Similarly, fine-tuning on coding tasks may improve programming capabilities while reducing performance on linguistic tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe machine learning community has documented this phenomenon under various terms including \"catastrophic forgetting,\" \"negative transfer,\" or simply \"specialization trade-offs.\" These effects are not always immediately apparent during evaluation if the testing only focuses on the target task rather than a broad assessment of the model's capabilities. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nUnderstanding these trade-offs is critical for practitioners who need to decide whether to use general-purpose LLMs or fine-tuned variants depending on their specific use cases and the breadth of capabilities required. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Documented Cases of Fine-tuning Degradation", "tldr": "Several studies have documented specific cases where fine-tuning LLMs leads to performance degradation in capabilities not targeted by the fine-tuning process. This degradation often manifests as increased hallucination, reduced reasoning abilities, or diminished performance on tasks different from the fine-tuning objective. (1 source)", "text": "\nResearch has increasingly documented specific instances where fine-tuning leads to performance degradation in LLMs. One notable example comes from alignment fine-tuning, which is typically intended to make models more helpful and safe. Lin et al. found that \"the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination)\" <Paper corpusId=\"269502676\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>. This finding contradicts the intuitive expectation that alignment would improve model reliability across dimensions.\n\nBeyond alignment, degradation has been observed in various specialized fine-tuning scenarios. When models are optimized for specific tasks like medical question answering or code generation, they can experience diminished capabilities in areas like creative writing, summarization, or general reasoning. These effects are sometimes not immediately apparent during evaluation because testing often focuses narrowly on the target task rather than comprehensively assessing the model's broader capabilities <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThe severity of degradation appears to correlate with the degree of specialization and the distance between the fine-tuning objective and the original model's training distribution. More aggressive fine-tuning approaches that significantly modify the model's parameters tend to result in more pronounced degradation of general capabilities. Similarly, fine-tuning on highly specialized or narrow datasets can cause the model to overfit to specific patterns at the expense of its general knowledge and reasoning abilities <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Lin et al., 2024)", "paper": {"corpus_id": 269502676, "title": "FLAME: Factuality-Aware Alignment for Large Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Sheng-Chieh Lin", "authorId": "122045993"}, {"name": "Luyu Gao", "authorId": "2299485255"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Wenhan Xiong", "authorId": "2266752758"}, {"name": "Jimmy Lin", "authorId": "2273564585"}, {"name": "Wen-tau Yih", "authorId": "2072801764"}, {"name": "Xilun Chen", "authorId": "2292024725"}], "n_citations": 18}, "snippets": ["We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination)."], "score": 0.5400390625}], "table": null}, {"title": "Fine-tuning Effects on Task-specific Performance", "tldr": "Fine-tuning LLMs for specific tasks like recommendation systems shows mixed results, with models often performing moderately on accuracy-based tasks but better on explainability tasks. Research reveals that targeted fine-tuning approaches, such as data pruning and domain-invariant feature extraction, can help mitigate some performance limitations. (3 sources)", "text": "\nResearch examining the fine-tuning effects on LLMs for specific tasks reveals nuanced performance patterns across different domains. In the recommendation systems space, Liu et al. conducted a comprehensive benchmark of LLMs on various recommendation tasks, finding that these models \"displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation\" <Paper corpusId=\"261076297\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This suggests that even with fine-tuning, LLMs may struggle to match specialized recommendation algorithms on certain precision-oriented tasks <Paper corpusId=\"273653859\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nHowever, the same research revealed that LLMs demonstrated \"comparable performance to state-of-the-art methods in explainability-based tasks\" like explanation generation and review summarization <Paper corpusId=\"261076297\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This indicates that the natural language capabilities of LLMs may transfer more effectively to tasks requiring semantic understanding and text generation, even after task-specific fine-tuning <Paper corpusId=\"273653859\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nResearchers have developed specialized approaches to enhance fine-tuning outcomes in specific domains. For example, Gong et al. proposed using LLMs \"to extract domain-invariant features\" that can help address cold start problems in recommendation systems <Paper corpusId=\"262045057\" paperTitle=\"(Gong et al., 2023)\" isShortName></Paper>. Their approach, called S&R Multi-Domain Foundation, leverages LLMs to extract features that work across both search and recommendation tasks, demonstrating better performance than other transfer learning methods when applied to cold start scenarios <Paper corpusId=\"273653859\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nAnother innovative approach involves data selection rather than model architecture changes. Researchers have developed \"a novel data pruning method to efficiently identify the influential samples for LLM-based recommender fine-tuning,\" which has shown promise for real-world applications <Paper corpusId=\"273653859\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. This suggests that carefully selecting the most informative training examples may help preserve broader capabilities while still improving task-specific performance.\n\nThese findings collectively demonstrate that the effects of fine-tuning on task-specific performance are not uniform across all tasks and depend greatly on the nature of the task, the fine-tuning approach, and how well the task aligns with the model's pre-existing capabilities <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Liu et al., 2023)", "paper": {"corpus_id": 261076297, "title": "LLMRec: Benchmarking Large Language Models on Recommendation Task", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Junling Liu", "authorId": "2218869839"}, {"name": "Chao-Hong Liu", "authorId": "3741691"}, {"name": "Peilin Zhou", "authorId": "1800462890"}, {"name": "Qichen Ye", "authorId": "2190432576"}, {"name": "Dading Chong", "authorId": "52290752"}, {"name": "Kangan Zhou", "authorId": "2165702320"}, {"name": "Yueqi Xie", "authorId": "2154871075"}, {"name": "Yuwei Cao", "authorId": "150346771"}, {"name": "Shoujin Wang", "authorId": "2116951322"}, {"name": "Chenyu You", "authorId": "2061592207"}, {"name": "Philip S.Yu", "authorId": "2233087809"}], "n_citations": 34}, "snippets": ["Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec."], "score": 0.0}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 273653859, "title": "Towards Next-Generation LLM-based Recommender Systems: A Survey and Beyond", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qi Wang", "authorId": "2284061879"}, {"name": "Jindong Li", "authorId": "2243469243"}, {"name": "Shiqi Wang", "authorId": "2283983135"}, {"name": "Qianli Xing", "authorId": "9123083"}, {"name": "Runliang Niu", "authorId": "2174434599"}, {"name": "He Kong", "authorId": "2283935174"}, {"name": "Rui Li", "authorId": "2328073820"}, {"name": "Guodong Long", "authorId": "2293722046"}, {"name": "Yi Chang", "authorId": "2243466364"}, {"name": "Chengqi Zhang", "authorId": "2283189862"}], "n_citations": 9}, "snippets": ["(Liu et al., 2023) investigates the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicates that LLMs display only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation", "(Gong et al., 2023) uses LLM to extract domain-invariant features in a manner that can help deal with the cold start problems in recommendation", "[72] proposes a novel data pruning method to efficiently identify the influential samples for LLM-based recommender fine-tuning, which unlocked the remarkable potential of applying LLM-based recommender models to real-world platforms."], "score": 0.50146484375}, {"id": "(Gong et al., 2023)", "paper": {"corpus_id": 262045057, "title": "An Unified Search and Recommendation Foundation Model for Cold-Start Scenario", "year": 2023, "venue": "International Conference on Information and Knowledge Management", "authors": [{"name": "Yuqi Gong", "authorId": "2243273769"}, {"name": "Xichen Ding", "authorId": "2243923570"}, {"name": "Yehui Su", "authorId": "2243315594"}, {"name": "Kaiming Shen", "authorId": "2242792959"}, {"name": "Zhongyi Liu", "authorId": "2243372613"}, {"name": "Guannan Zhang", "authorId": "119557985"}], "n_citations": 26}, "snippets": ["In modern commercial search engines and recommendation systems, data from multiple domains is available to jointly train the multi-domain model. Traditional methods train multi-domain models in the multi-task setting, with shared parameters to learn the similarity of multiple tasks, and task-specific parameters to learn the divergence of features, labels, and sample distributions of individual tasks. With the development of large language models, LLM can extract global domain-invariant text features that serve both search and recommendation tasks. We propose a novel framework called S&R Multi-Domain Foundation, which uses LLM to extract domain invariant features, and Aspect Gating Fusion to merge the ID feature, domain invariant text features and task-specific heterogeneous sparse features to obtain the representations of query and item. Additionally, samples from multiple search and recommendation scenarios are trained jointly with Domain Adaptive Multi-Task module to obtain the multi-domain foundation model. We apply the S&R Multi-Domain foundation model to cold start scenarios in the pretrain-finetune manner, which achieves better performance than other SOTA transfer learning methods. The S&R Multi-Domain Foundation model has been successfully deployed in Alipay Mobile Application's online services, such as content query recommendation and service card recommendation, etc."], "score": 0.0}], "table": null}], "cost": 0.075042}}
