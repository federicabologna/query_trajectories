{"original_query": "How do people train a long-context Transformer model over 32K context length using sequence parallelism techniques? How are these methods compared with each other?", "input": "people a long context Transformer model K context length using sequence parallelism techniques these methods each other", "output": {"sections": [{"title": "Introduction to Long Context in Transformers", "tldr": "Transformer models have shown impressive capabilities but face computational challenges when processing long contexts. Extending context length is a critical area of research as many real-world NLP applications involve processing lengthy documents or dialogues. (6 sources)", "text": "\nTransformer-based language models have achieved remarkable results in recent years by increasing their ability to handle larger context windows <Paper corpusId=\"237412971\" paperTitle=\"(Martins et al., 2021)\" isShortName></Paper>. Unlike humans and recurrent neural networks that process information sequentially with continuous memory updates, transformers process information differently - they query every representation from past events, which creates computational challenges as context length grows <Paper corpusId=\"237412971\" paperTitle=\"(Martins et al., 2021)\" isShortName></Paper>. This fundamental limitation means that vanilla transformers require quadratic time complexity with respect to input sequence length, significantly constraining how much information they can effectively process <Paper corpusId=\"237412971\" paperTitle=\"(Martins et al., 2021)\" isShortName></Paper>.\n\nDespite these limitations, researchers have made significant progress in extending transformer context capabilities. Models like Transformer-XL introduced segment-level recurrence mechanisms that enable learning dependencies that are 80% longer than RNNs and 450% longer than vanilla Transformers while maintaining temporal coherence <Paper corpusId=\"57759363\" paperTitle=\"(Dai et al., 2019)\" isShortName></Paper>. Other advancements like the Compressive Transformer have further pushed the boundaries of long-range sequence learning through memory compression techniques <Paper corpusId=\"207930593\" paperTitle=\"(Rae et al., 2019)\" isShortName></Paper>. The development of GPT-3 with 175 billion parameters represented another leap forward in the field, demonstrating strong performance across many NLP tasks even in few-shot settings <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper>.\n\nThe drive to extend context length in transformers is fueled by practical needs. Many real-world NLP applications involve processing long documents, making this an increasingly important area of research <Paper corpusId=\"248218548\" paperTitle=\"(Ang et al., 2022)\" isShortName></Paper>. Standard Large Language Models (LLMs) particularly struggle with handling extended dialogues due to efficiency and consistency issues when dealing with long contexts <Paper corpusId=\"268378933\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This challenge has spurred the development of various techniques for extending context capabilities in transformer architectures.", "citations": [{"id": "(Martins et al., 2021)", "paper": {"corpus_id": 237412971, "title": "\\infty-former: Infinite Memory Transformer", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Pedro Henrique Martins", "authorId": "144869806"}, {"name": "Zita Marinho", "authorId": "2566656"}, {"name": "Andr\u00e9 F. T. Martins", "authorId": "145644643"}], "n_citations": 11}, "snippets": ["Recently, transformer-based language models have achieved impressive results by increasing the context size (Radford et al., 2018(Radford et al., , 2019;;(Dai et al., 2019)(Rae et al., 2019)(Brown et al., 2020). However, while humans process information sequentially, updating their memories continuously, and recurrent neural networks (RNNs) update a single memory vector during time, transformers do not -they exhaustively query every representation associated to the past events. Thus, the amount of computation they need to perform grows with the length of the context, and, consequently, transformers have computational limitations about how much information can fit into memory. For example, a vanilla transformer requires quadratic time to process an input sequence and linear time to attend to the context when generating every new word."], "score": 0.58251953125}, {"id": "(Dai et al., 2019)", "paper": {"corpus_id": 57759363, "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zihang Dai", "authorId": "3422912"}, {"name": "Zhilin Yang", "authorId": "2109512754"}, {"name": "Yiming Yang", "authorId": "35729970"}, {"name": "J. Carbonell", "authorId": "143712374"}, {"name": "Quoc V. Le", "authorId": "2827616"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}], "n_citations": 3746}, "snippets": ["Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."], "score": 0.0}, {"id": "(Rae et al., 2019)", "paper": {"corpus_id": 207930593, "title": "Compressive Transformers for Long-Range Sequence Modelling", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Jack W. Rae", "authorId": "34269227"}, {"name": "Anna Potapenko", "authorId": "13759734"}, {"name": "Siddhant M. Jayakumar", "authorId": "35880964"}, {"name": "T. Lillicrap", "authorId": "2542999"}], "n_citations": 653}, "snippets": ["We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19."], "score": 0.0}, {"id": "(Brown et al., 2020)", "paper": {"corpus_id": 218971783, "title": "Language Models are Few-Shot Learners", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tom B. Brown", "authorId": "31035595"}, {"name": "Benjamin Mann", "authorId": "2056658938"}, {"name": "Nick Ryder", "authorId": "39849748"}, {"name": "Melanie Subbiah", "authorId": "2065894334"}, {"name": "J. Kaplan", "authorId": "152724169"}, {"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Arvind Neelakantan", "authorId": "2072676"}, {"name": "Pranav Shyam", "authorId": "67311962"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Ariel Herbert-Voss", "authorId": "1404060687"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "T. Henighan", "authorId": "103143311"}, {"name": "R. Child", "authorId": "48422824"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Daniel M. Ziegler", "authorId": "2052152920"}, {"name": "Jeff Wu", "authorId": "49387725"}, {"name": "Clemens Winter", "authorId": "2059411355"}, {"name": "Christopher Hesse", "authorId": "144239765"}, {"name": "Mark Chen", "authorId": "2108828435"}, {"name": "Eric Sigler", "authorId": "2064673055"}, {"name": "Ma-teusz Litwin", "authorId": "1380985420"}, {"name": "Scott Gray", "authorId": "145565184"}, {"name": "Benjamin Chess", "authorId": "1490681878"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Christopher Berner", "authorId": "133740015"}, {"name": "Sam McCandlish", "authorId": "52238703"}, {"name": "Alec Radford", "authorId": "38909097"}, {"name": "I. Sutskever", "authorId": "1701686"}, {"name": "Dario Amodei", "authorId": "2698777"}], "n_citations": 42437}, "snippets": ["Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."], "score": 0.0}, {"id": "(Ang et al., 2022)", "paper": {"corpus_id": 248218548, "title": "Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models", "year": 2022, "venue": "NLPPOWER", "authors": [{"name": "Phyllis Ang", "authorId": "2089185954"}, {"name": "Bhuwan Dhingra", "authorId": "2060730422"}, {"name": "L. Wills", "authorId": "1820939409"}], "n_citations": 6}, "snippets": ["With many real-world applications of Natural Language Processing (NLP) comprising of long texts, there has been a rise in NLP benchmarks that measure the accuracy of models that can handle longer input sequences."], "score": 0.609375}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 268378933, "title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jia-Nan Li", "authorId": "2291078805"}, {"name": "Quan Tu", "authorId": "2071635049"}, {"name": "Cunli Mao", "authorId": "34754580"}, {"name": "Zhengtao Yu", "authorId": "2285331933"}, {"name": "Ji-Rong Wen", "authorId": "2263887786"}, {"name": "Rui Yan", "authorId": "2277448117"}], "n_citations": 4}, "snippets": ["Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues."], "score": 0.63525390625}], "table": null}, {"title": "Categories of Long Context Modeling Techniques", "tldr": "Long context modeling approaches can be categorized into three main groups: efficiency-focused architectures that address quadratic attention complexity, memory/recurrence-based approaches that maintain state across segments, and length extrapolation techniques that enable models to handle sequences longer than seen during training. (21 sources)", "text": "\nThe rapid advancement of transformer models has led to various approaches for handling long contexts, which can be organized into distinct categories based on their underlying mechanisms. These categories address specific challenges in long-context modeling, particularly the computational overhead and the tendency for models to \"forget\" information from earlier parts of long sequences <Paper corpusId=\"261245264\" paperTitle=\"(Bai et al., 2023)\" isShortName></Paper> <Paper corpusId=\"221702858\" paperTitle=\"(Tay et al., 2020)\" isShortName></Paper>.\n\nThe first major category consists of **efficiency-focused architectures** that aim to reduce the quadratic complexity of standard transformers. These include sparse attention mechanisms <Paper corpusId=\"209315300\" paperTitle=\"(Kitaev et al., 2020)\" isShortName></Paper> <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper> <Paper corpusId=\"219636190\" paperTitle=\"(Martins et al., 2020)\" isShortName></Paper>, linear transformers <Paper corpusId=\"220250819\" paperTitle=\"(Katharopoulos et al., 2020)\" isShortName></Paper> <Paper corpusId=\"222067132\" paperTitle=\"(Choromanski et al., 2020)\" isShortName></Paper>, and mixture-of-experts approaches <Paper corpusId=\"231573431\" paperTitle=\"(Fedus et al., 2021)\" isShortName></Paper>. These methods restructure the attention mechanism to achieve linear or log-linear complexity with respect to sequence length, significantly reducing computational requirements for long contexts <Paper corpusId=\"268793522\" paperTitle=\"(Thonet et al., 2024)\" isShortName></Paper>.\n\nThe second category includes **memory and recurrence-based approaches** that maintain state across sequence segments. Notable examples include Transformer-XL <Paper corpusId=\"57759363\" paperTitle=\"(Dai et al., 2019)\" isShortName></Paper>, which introduced segment-level recurrence, and the Compressive Transformer <Paper corpusId=\"207930593\" paperTitle=\"(Rae et al., 2019)\" isShortName></Paper>, which compresses past memories for efficient long-range learning. More recent developments include Recurrent Memory Transformer <Paper corpusId=\"250526424\" paperTitle=\"(Bulatov et al., 2022)\" isShortName></Paper> and RWKV <Paper corpusId=\"258832459\" paperTitle=\"(Peng et al., 2023)\" isShortName></Paper>, which combines efficient parallel training of transformers with the linear scaling of RNNs at inference time. These architectures enable models to remember information across longer contexts by incorporating memory mechanisms or recurrent structures <Paper corpusId=\"247519194\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper>.\n\nThe third major category focuses on **length extrapolation techniques**, which enable models to handle sequences longer than those seen during training. This approach primarily involves position encoding innovations like ALiBi (Attention with Linear Biases) <Paper corpusId=\"237347130\" paperTitle=\"(Press et al., 2021)\" isShortName></Paper> and RoPE (Rotary Position Embeddings) extensions <Paper corpusId=\"261493986\" paperTitle=\"(Peng et al._1, 2023)\" isShortName></Paper>. These methods have been adopted in models like ChatGLM2-32k <Paper corpusId=\"252715691\" paperTitle=\"(Zeng et al., 2022)\" isShortName></Paper> and LongChat-32k <Paper corpusId=\"261245264\" paperTitle=\"(Bai et al., 2023)\" isShortName></Paper>. Other approaches in this category include context window sliding and segmentation techniques <Paper corpusId=\"258686160\" paperTitle=\"(Ratner et al., 2022)\" isShortName></Paper>.\n\nSome researchers have explored alternative approaches such as retrieval-augmented generation <Paper corpusId=\"263620134\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper> and context compression <Paper corpusId=\"258865249\" paperTitle=\"(Chevalier et al., 2023)\" isShortName></Paper>, which offer complementary solutions to the long-context challenge. Recent implementations like LongLoRA <Paper corpusId=\"262084134\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> combine multiple techniques, such as sparse attention and parameter-efficient fine-tuning, to efficiently extend context windows to impressive lengths (up to 100k tokens for 7B parameter models).\n\nThese advances have contributed to significant context length expansions in recent models, with proprietary systems like GPT-4, Claude-3, and Gemini-1.5 now supporting context windows ranging from 32K to 1 million tokens <Paper corpusId=\"268793522\" paperTitle=\"(Thonet et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Bai et al., 2023)", "paper": {"corpus_id": 261245264, "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yushi Bai", "authorId": "2141377570"}, {"name": "Xin Lv", "authorId": "48574888"}, {"name": "Jiajie Zhang", "authorId": "2107983722"}, {"name": "Hong Lyu", "authorId": "2220304036"}, {"name": "Jiankai Tang", "authorId": "2204826271"}, {"name": "Zhidian Huang", "authorId": "2234629967"}, {"name": "Zhengxiao Du", "authorId": "66395694"}, {"name": "Xiao Liu", "authorId": "2111312892"}, {"name": "Aohan Zeng", "authorId": "2051712753"}, {"name": "Lei Hou", "authorId": "2055765060"}, {"name": "Yuxiao Dong", "authorId": "2047998"}, {"name": "Jie Tang", "authorId": "2148911975"}, {"name": "Juanzi Li", "authorId": "2133353675"}], "n_citations": 603}, "snippets": ["Long Context Modeling Techniques. We first discuss some popular lines of methods that aim to tackle long context understanding. These studies are mainly aimed at solving two key challenges in long text modeling, including the high runtime overhead on longer context, and the catastrophic forgetting phenomenon when processing long sequence. A series of studies focus on how to make Transformers more efficient and unforgetful (Tay et al., 2020), with designs such as sparse and efficient computation (Child et al., 2019;(Kitaev et al., 2020)Beltagy et al., 2020;(Yu et al., 2023)Wang et al., 2020;(Fedus et al., 2021)Ding et al., 2023), recurrent and memory modules (Dai et al., 2019)(Rae et al., 2019)(Wu et al., 2022)Martins et al., 2022;(Bulatov et al., 2022)Orvieto et al., 2023;Liang et al., 2023;Zhou et al., 2023). More recently, several methods (Press et al., 2021)Sun et al., 2022;Chen et al., 2023) have been proposed to enable length extrapolation of Transformers, and have been adopted in the training process of long context LLMs such as ChatGLM2-32k (Zeng et al., 2022) and LongChat-32k (Li et al., 2023)."], "score": 0.5009765625}, {"id": "(Tay et al., 2020)", "paper": {"corpus_id": 221702858, "title": "Efficient Transformers: A Survey", "year": 2020, "venue": "ACM Computing Surveys", "authors": [{"name": "Yi Tay", "authorId": "144447820"}, {"name": "Mostafa Dehghani", "authorId": "3226635"}, {"name": "Dara Bahri", "authorId": "11774695"}, {"name": "Donald Metzler", "authorId": "1680617"}], "n_citations": 1128}, "snippets": ["Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \"X-former\" models have been proposed\u2014Reformer, Linformer, Performer, Longformer, to name a few\u2014which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored \"X-former\" models, providing an organized and comprehensive overview of existing work and models across multiple domains."], "score": 0.0}, {"id": "(Kitaev et al., 2020)", "paper": {"corpus_id": 209315300, "title": "Reformer: The Efficient Transformer", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "Nikita Kitaev", "authorId": "143808231"}, {"name": "Lukasz Kaiser", "authorId": "40527594"}, {"name": "Anselm Levskaya", "authorId": "6639036"}], "n_citations": 2333}, "snippets": ["Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences."], "score": 0.0}, {"id": "(Zaheer et al., 2020)", "paper": {"corpus_id": 220831004, "title": "Big Bird: Transformers for Longer Sequences", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "M. Zaheer", "authorId": "1771307"}, {"name": "Guru Guruganesh", "authorId": "1947314"}, {"name": "Kumar Avinava Dubey", "authorId": "89890133"}, {"name": "J. Ainslie", "authorId": "1643737606"}, {"name": "Chris Alberti", "authorId": "114577307"}, {"name": "Santiago Onta\u00f1\u00f3n", "authorId": "1722671"}, {"name": "Philip Pham", "authorId": "38552691"}, {"name": "Anirudh Ravula", "authorId": "101210026"}, {"name": "Qifan Wang", "authorId": "145196279"}, {"name": "Li Yang", "authorId": "113906155"}, {"name": "Amr Ahmed", "authorId": "143629707"}], "n_citations": 2103}, "snippets": ["Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."], "score": 0.383544921875}, {"id": "(Martins et al., 2020)", "paper": {"corpus_id": 219636190, "title": "Sparse and Continuous Attention Mechanisms", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Andr\u00e9 F. T. Martins", "authorId": "145644643"}, {"name": "Marcos Vin\u00edcius Treviso", "authorId": "145188499"}, {"name": "Ant\u00f3nio Farinhas", "authorId": "1748971692"}, {"name": "Vlad Niculae", "authorId": "2114966"}, {"name": "M\u00e1rio A. T. Figueiredo", "authorId": "2075330932"}, {"name": "P. Aguiar", "authorId": "35537344"}], "n_citations": 41}, "snippets": ["Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and alpha-entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: first, we extend alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}. Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions."], "score": 0.0}, {"id": "(Katharopoulos et al., 2020)", "paper": {"corpus_id": 220250819, "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Angelos Katharopoulos", "authorId": "3493855"}, {"name": "Apoorv Vyas", "authorId": "2992087"}, {"name": "Nikolaos Pappas", "authorId": "143958923"}, {"name": "Franccois Fleuret", "authorId": "116272138"}], "n_citations": 1790}, "snippets": ["Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences."], "score": 0.0}, {"id": "(Choromanski et al., 2020)", "paper": {"corpus_id": 222067132, "title": "Rethinking Attention with Performers", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "K. Choromanski", "authorId": "1805203"}, {"name": "Valerii Likhosherstov", "authorId": "52314889"}, {"name": "David Dohan", "authorId": "35363891"}, {"name": "Xingyou Song", "authorId": "32725720"}, {"name": "Andreea Gane", "authorId": "3071104"}, {"name": "Tam\u00e1s Sarl\u00f3s", "authorId": "2227764"}, {"name": "Peter Hawkins", "authorId": "2052793706"}, {"name": "Jared Davis", "authorId": "29827891"}, {"name": "Afroz Mohiuddin", "authorId": "1579862074"}, {"name": "Lukasz Kaiser", "authorId": "40527594"}, {"name": "David Belanger", "authorId": "2636941"}, {"name": "Lucy J. Colwell", "authorId": "2654847"}, {"name": "Adrian Weller", "authorId": "145689461"}], "n_citations": 1602}, "snippets": ["We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers."], "score": 0.0}, {"id": "(Fedus et al., 2021)", "paper": {"corpus_id": 231573431, "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "year": 2021, "venue": "Journal of machine learning research", "authors": [{"name": "W. Fedus", "authorId": "26958176"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}], "n_citations": 2224}, "snippets": ["In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the\"Colossal Clean Crawled Corpus\"and achieve a 4x speedup over the T5-XXL model."], "score": 0.0}, {"id": "(Thonet et al., 2024)", "paper": {"corpus_id": 268793522, "title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Thibaut Thonet", "authorId": "2955690"}, {"name": "Jos Rozen", "authorId": "120419790"}, {"name": "Laurent Besacier", "authorId": "2259359878"}], "n_citations": 3}, "snippets": ["context modeling. 3 While an exhaustive survey of these methods is beyond the scope of this paper, they can generally be categorized into three main groups (excluding other distinct approaches such as retrieval-augmented generation (Xu et al., 2023) and context compression (Chevalier et al., 2023)): (a) the development of efficient transformer architectures to address the quadratic attention challenge, including sparse transformers [6,12,(Martins et al., 2020)(Zaheer et al., 2020), linear transformers (Choromanski et al., 2020)(Katharopoulos et al., 2020)[42], and hierarchical transformers [20,30,(Wu et al., 2021); (b) approaches like recurrent attention networks [7,(Dai et al., 2019)(Peng et al., 2023) and state-space models [16,41]; (c) length extrapolation or position embedding interpolation, where LLMs are fine-tuned or adapted at inference time to adjust tokens' positions to match the new context length [4,8,9,28,35,(Peng et al., 2023)[45]. These techniques also contributed to the context length expansion in proprietary models like GPT-4 (32K-128K), Claude-3 (200k), and Gemini-1.5 (128K-1M)."], "score": 0.56689453125}, {"id": "(Dai et al., 2019)", "paper": {"corpus_id": 57759363, "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zihang Dai", "authorId": "3422912"}, {"name": "Zhilin Yang", "authorId": "2109512754"}, {"name": "Yiming Yang", "authorId": "35729970"}, {"name": "J. Carbonell", "authorId": "143712374"}, {"name": "Quoc V. Le", "authorId": "2827616"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}], "n_citations": 3746}, "snippets": ["Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."], "score": 0.0}, {"id": "(Rae et al., 2019)", "paper": {"corpus_id": 207930593, "title": "Compressive Transformers for Long-Range Sequence Modelling", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Jack W. Rae", "authorId": "34269227"}, {"name": "Anna Potapenko", "authorId": "13759734"}, {"name": "Siddhant M. Jayakumar", "authorId": "35880964"}, {"name": "T. Lillicrap", "authorId": "2542999"}], "n_citations": 653}, "snippets": ["We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19."], "score": 0.0}, {"id": "(Bulatov et al., 2022)", "paper": {"corpus_id": 250526424, "title": "Recurrent Memory Transformer", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Aydar Bulatov", "authorId": "2176183932"}, {"name": "Yuri Kuratov", "authorId": "51114080"}, {"name": "M. Burtsev", "authorId": "3359236"}], "n_citations": 110}, "snippets": ["Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning."], "score": 0.0}, {"id": "(Peng et al., 2023)", "paper": {"corpus_id": 258832459, "title": "RWKV: Reinventing RNNs for the Transformer Era", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Bo Peng", "authorId": "145560079"}, {"name": "Eric Alcaide", "authorId": "79046907"}, {"name": "Quentin G. Anthony", "authorId": "1404060481"}, {"name": "Alon Albalak", "authorId": "2044198106"}, {"name": "Samuel Arcadinho", "authorId": "2322500848"}, {"name": "Stella Biderman", "authorId": "103476203"}, {"name": "Huanqi Cao", "authorId": "47709883"}, {"name": "Xin Cheng", "authorId": "2193630544"}, {"name": "Michael Chung", "authorId": "2218291950"}, {"name": "Matteo Grella", "authorId": "2425906"}, {"name": "G. Kranthikiran", "authorId": "101433524"}, {"name": "Xingjian Du", "authorId": "46214809"}, {"name": "Xuming He", "authorId": "33913193"}, {"name": "Haowen Hou", "authorId": "9397636"}, {"name": "Przemyslaw Kazienko", "authorId": "1724788"}, {"name": "Jan Koco\u0144", "authorId": "2905929"}, {"name": "Jiaming Kong", "authorId": "35171548"}, {"name": "Bartlomiej Koptyra", "authorId": "2208962106"}, {"name": "Hayden Lau", "authorId": "1598440603"}, {"name": "Krishna Sri Ipsit Mantri", "authorId": "2209207087"}, {"name": "Ferdinand Mom", "authorId": "2218334866"}, {"name": "Atsushi Saito", "authorId": "2186861874"}, {"name": "Xiangru Tang", "authorId": "47274259"}, {"name": "Bolun Wang", "authorId": "2153213619"}, {"name": "J. S. Wind", "authorId": "1388112865"}, {"name": "Stansilaw Wozniak", "authorId": "2218377273"}, {"name": "Ruichong Zhang", "authorId": "2218987422"}, {"name": "Zhenyuan Zhang", "authorId": "2144400204"}, {"name": "Qihang Zhao", "authorId": "2110483969"}, {"name": "P. Zhou", "authorId": "1994721202"}, {"name": "Jian Zhu", "authorId": "144549416"}, {"name": "Rui Zhu", "authorId": "144649570"}], "n_citations": 608}, "snippets": ["Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks."], "score": 0.0}, {"id": "(Wu et al., 2022)", "paper": {"corpus_id": 247519194, "title": "Memorizing Transformers", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yuhuai Wu", "authorId": "3374063"}, {"name": "M. Rabe", "authorId": "1800714"}, {"name": "DeLesley S. Hutchins", "authorId": "32913644"}, {"name": "Christian Szegedy", "authorId": "2574060"}], "n_citations": 178}, "snippets": ["Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time."], "score": 0.0}, {"id": "(Press et al., 2021)", "paper": {"corpus_id": 237347130, "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation", "year": 2021, "venue": "International Conference on Learning Representations", "authors": [{"name": "Ofir Press", "authorId": "40170001"}, {"name": "Noah A. Smith", "authorId": "144365875"}, {"name": "M. Lewis", "authorId": "35084211"}], "n_citations": 775}, "snippets": ["Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark."], "score": 0.0}, {"id": "(Peng et al._1, 2023)", "paper": {"corpus_id": 261493986, "title": "YaRN: Efficient Context Window Extension of Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Bowen Peng", "authorId": "2237425757"}, {"name": "Jeffrey Quesnelle", "authorId": "31964220"}, {"name": "Honglu Fan", "authorId": "2273749369"}, {"name": "Enrico Shippole", "authorId": "2261554775"}], "n_citations": 264}, "snippets": ["Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at https://github.com/jquesnelle/yarn"], "score": 0.0}, {"id": "(Zeng et al., 2022)", "paper": {"corpus_id": 252715691, "title": "GLM-130B: An Open Bilingual Pre-trained Model", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Aohan Zeng", "authorId": "2051712753"}, {"name": "Xiao Liu", "authorId": "2111312892"}, {"name": "Zhengxiao Du", "authorId": "66395694"}, {"name": "Zihan Wang", "authorId": null}, {"name": "Hanyu Lai", "authorId": "2051311700"}, {"name": "Ming Ding", "authorId": "145573466"}, {"name": "Zhuoyi Yang", "authorId": "2109506541"}, {"name": "Yifan Xu", "authorId": "2125063007"}, {"name": "Wendi Zheng", "authorId": "2163967642"}, {"name": "Xiao Xia", "authorId": "2186982651"}, {"name": "W. Tam", "authorId": "1403621152"}, {"name": "Zixuan Ma", "authorId": "2124489983"}, {"name": "Yufei Xue", "authorId": "2114921664"}, {"name": "Jidong Zhai", "authorId": "2467444"}, {"name": "Wenguang Chen", "authorId": "1712168"}, {"name": "P. Zhang", "authorId": "47243067"}, {"name": "Yuxiao Dong", "authorId": "2047998"}, {"name": "Jie Tang", "authorId": "2148911956"}], "n_citations": 1094}, "snippets": ["We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{https://github.com/THUDM/GLM-130B/}."], "score": 0.0}, {"id": "(Ratner et al., 2022)", "paper": {"corpus_id": 258686160, "title": "Parallel Context Windows for Large Language Models", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Nir Ratner", "authorId": "2148471161"}, {"name": "Yoav Levine", "authorId": "152754428"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}, {"name": "Ori Ram", "authorId": "73775461"}, {"name": "Inbal Magar", "authorId": "2158996542"}, {"name": "Omri Abend", "authorId": "2769805"}, {"name": "Ehud Karpas", "authorId": "12621100"}, {"name": "A. Shashua", "authorId": "3140335"}, {"name": "Kevin Leyton-Brown", "authorId": "2066411743"}, {"name": "Y. Shoham", "authorId": "1701353"}], "n_citations": 75}, "snippets": ["When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (\"windows\"), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ ai21labs/parallel-context-windows."], "score": 0.0}, {"id": "(Xu et al., 2023)", "paper": {"corpus_id": 263620134, "title": "Retrieval meets Long Context Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Peng Xu", "authorId": "2254989105"}, {"name": "Wei Ping", "authorId": "2253664013"}, {"name": "Xianchao Wu", "authorId": "2253618149"}, {"name": "Lawrence C. McAfee", "authorId": "20957879"}, {"name": "Chen Zhu", "authorId": "2283871700"}, {"name": "Zihan Liu", "authorId": "2256582287"}, {"name": "Sandeep Subramanian", "authorId": "2253531461"}, {"name": "E. Bakhturina", "authorId": "32867948"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "Bryan Catanzaro", "authorId": "2301680"}], "n_citations": 85}, "snippets": ["Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners."], "score": 0.0}, {"id": "(Chevalier et al., 2023)", "paper": {"corpus_id": 258865249, "title": "Adapting Language Models to Compress Contexts", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Alexis Chevalier", "authorId": "2161343103"}, {"name": "Alexander Wettig", "authorId": "2127066887"}, {"name": "Anirudh Ajith", "authorId": "2218438150"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 190}, "snippets": ["Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling. Overall, AutoCompressors emerge as a simple and inexpensive solution for extending the context window of LMs while speeding up inference over long contexts."], "score": 0.0}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 262084134, "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yukang Chen", "authorId": "2109297557"}, {"name": "Shengju Qian", "authorId": "152230789"}, {"name": "Haotian Tang", "authorId": "150127950"}, {"name": "Xin Lai", "authorId": "2237802684"}, {"name": "Zhijian Liu", "authorId": "47781592"}, {"name": "Song Han", "authorId": "2243400730"}, {"name": "Jiaya Jia", "authorId": "2237811040"}], "n_citations": 167}, "snippets": ["We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset."], "score": 0.0}], "table": null}, {"title": "Efficiency-Focused Approaches", "tldr": "Efficiency-focused approaches tackle the quadratic complexity problem of transformer attention mechanisms through various techniques like sparse attention patterns, linear reformulations, and optimized memory operations. These innovations allow models to process longer sequences with significantly reduced computational and memory requirements. (5 sources)", "text": "\n- **Sparse Attention Mechanisms**: Researchers have developed multiple sparse attention patterns to reduce the quadratic complexity of standard transformers. BigBird implements a sparse attention mechanism that reduces the quadratic dependency to linear while maintaining the model's capabilities as a universal approximator of sequence functions <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>. Longformer combines sliding window and global attention patterns to efficiently process longer sequences <Paper corpusId=\"267770311\" paperTitle=\"(He et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\n- **Low-Rank Approximations**: These methods approximate the self-attention matrix using low-rank factorization, significantly reducing computational demands while preserving most of the attention mechanism's effectiveness <Paper corpusId=\"267770311\" paperTitle=\"(He et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268385246\" paperTitle=\"(Athiwaratkun et al., 2024)\" isShortName></Paper>.\n\n- **Linear Attention Formulations**: Linear Attention methods reformulate the self-attention mechanism to achieve linear complexity with respect to sequence length, making it feasible to process much longer contexts <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\n- **Memory-Efficient Implementations**: FlashAttention stands out as an IO-aware exact attention algorithm that uses tiling to reduce memory transfers between GPU high bandwidth memory and on-chip SRAM. This approach achieves substantial speedups: 15% faster end-to-end wall-clock performance on BERT-large, 3\u00d7 speedup on GPT-2, and 2.4\u00d7 speedup on long-range arena tasks <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>.\n\n- **Hierarchical Approaches**: BlockTransformer employs hierarchical global-to-local modeling to efficiently handle longer contexts by processing information at different levels of granularity <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\n- **State Space Models**: Models like Mamba capture long-range dependencies efficiently without using explicit attention mechanisms, offering an alternative approach to the long context challenge <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\n- **Unbounded Context Models**: InfiniteTransformer incorporates unbounded long-term memory through continuous-space attention, potentially allowing for unlimited context length <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\nDespite these advances, many of these methods still struggle with retrieving information from the middle of very long inputs and cannot handle truly unbounded context window sizes <Paper corpusId=\"267770311\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>. However, they can often be combined with other approaches for more effective long context modeling.", "citations": [{"id": "(Zaheer et al., 2020)", "paper": {"corpus_id": 220831004, "title": "Big Bird: Transformers for Longer Sequences", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "M. Zaheer", "authorId": "1771307"}, {"name": "Guru Guruganesh", "authorId": "1947314"}, {"name": "Kumar Avinava Dubey", "authorId": "89890133"}, {"name": "J. Ainslie", "authorId": "1643737606"}, {"name": "Chris Alberti", "authorId": "114577307"}, {"name": "Santiago Onta\u00f1\u00f3n", "authorId": "1722671"}, {"name": "Philip Pham", "authorId": "38552691"}, {"name": "Anirudh Ravula", "authorId": "101210026"}, {"name": "Qifan Wang", "authorId": "145196279"}, {"name": "Li Yang", "authorId": "113906155"}, {"name": "Amr Ahmed", "authorId": "143629707"}], "n_citations": 2103}, "snippets": ["Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."], "score": 0.383544921875}, {"id": "(He et al., 2024)", "paper": {"corpus_id": 267770311, "title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zexue He", "authorId": "2116458151"}, {"name": "Leonid Karlinsky", "authorId": "2142741421"}, {"name": "Donghyun Kim", "authorId": "2266361991"}, {"name": "Julian McAuley", "authorId": "2258962117"}, {"name": "Dmitry Krotov", "authorId": "2284865239"}, {"name": "Rog\u00e9rio Feris", "authorId": "2239199018"}], "n_citations": 11}, "snippets": ["Many efficient self-attention techniques have been proposed for long context modeling in transformer models, including low-rank factorization (Wang et al., 2020), local attention (Ramachandran et al., 2019), dilated attention (Ding et al., 2023), sparsity (Beltagy et al., 2020;(Zaheer et al., 2020)Kitaev et al., 2020), and hardwareaware attention mechanisms such as FlashAttention (Dao et al., 2022)Dao, 2023). Despite notable progress, these methods are unable to handle unbounded context window sizes and struggle to retrieve information in the middle of the input (Liu et al., 2023). They can be used in tandem with our proposed approach for longer context modeling."], "score": 0.55859375}, {"id": "(He et al., 2025)", "paper": {"corpus_id": 277857470, "title": "Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation", "year": 2025, "venue": "International Conference on Learning Representations", "authors": [{"name": "Linda He", "authorId": "2356154321"}, {"name": "Jue Wang", "authorId": "2252087284"}, {"name": "Maurice Weber", "authorId": "2110605429"}, {"name": "Shang Zhu", "authorId": "2356004549"}, {"name": "Ben Athiwaratkun", "authorId": "2304481349"}, {"name": "Ce Zhang", "authorId": "2305565297"}], "n_citations": 1}, "snippets": ["To address the quadratic computational and memory demands of standard transformer self-attention, researchers have developed various architectural modifica-tions to improve efficiency and extend context lengths. Notable examples include Longformer (Beltagy et al., 2020), which combines sliding window and global attention, and BlockTransformer (Ho et al., 2024), which employs hierarchical global-to-local modeling. Linear Attention methods (Katharopoulos et al., 2020) reformulate self-attention for linear complexity, while InfiniteTransformer (Munkhdalai et al., 2024) incorporates unbounded long-term memory through continuousspace attention. State space models like Mamba (Gu & Dao, 2024) capture long-range dependencies efficiently without explicit attention mechanisms."], "score": 0.5537109375}, {"id": "(Athiwaratkun et al., 2024)", "paper": {"corpus_id": 268385246, "title": "Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs", "year": 2024, "venue": "", "authors": [{"name": "Ben Athiwaratkun", "authorId": "2095707"}, {"name": "Sujan Kumar Gonugondla", "authorId": "1913939"}, {"name": "Sanjay Krishna Gouda", "authorId": "40892818"}, {"name": "Haifeng Qian", "authorId": "2287922898"}, {"name": "Hantian Ding", "authorId": "2113455281"}, {"name": "Qing Sun", "authorId": "2291224391"}, {"name": "Jun Wang", "authorId": "2291153804"}, {"name": "Jiacheng Guo", "authorId": "2291418208"}, {"name": "Liangfu Chen", "authorId": "2282351608"}, {"name": "Parminder Bhatia", "authorId": "50339091"}, {"name": "Ramesh Nallapati", "authorId": "1701451"}, {"name": "Sudipta Sengupta", "authorId": "2072419570"}, {"name": "Bing Xiang", "authorId": "2258965075"}], "n_citations": 4}, "snippets": ["To effectively handle longer context sequences, optimizing memory I/O and reducing computational overhead are critical. Currently, the dominant approaches to addressing this challenge have been to make the attention computation less expensive. Beltagy et al. (2020) proposed to sparsify self-attention using various attention patterns. Wang et al. (2020) explores low-rank approximation of self-attention. In addition to the compute bound improvements, advancements in memory-efficient attention mechanisms and techniques for reducing memory I/O will continue to propel the field forward, facilitating the handling of longer context sequences in language models."], "score": 0.5205078125}, {"id": "(Dao et al., 2022)", "paper": {"corpus_id": 249151871, "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tri Dao", "authorId": "24593911"}, {"name": "Daniel Y. Fu", "authorId": "49577833"}, {"name": "Stefano Ermon", "authorId": "2490652"}, {"name": "A. Rudra", "authorId": "1755572"}, {"name": "Christopher R'e", "authorId": "2061444681"}], "n_citations": 2285}, "snippets": ["Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."], "score": 0.0}], "table": null}, {"title": "Position Encoding Approaches", "tldr": "Position encoding innovations have made significant strides in extending transformer context lengths beyond training limits. These approaches modify how models perceive token positions, enabling them to process sequences many times longer than they were trained on without additional architectural changes. (5 sources)", "text": "\n- **Attention with Linear Biases (ALiBi)**: This position method does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty proportional to token distance. ALiBi enables models trained on shorter sequences (e.g., 1024 tokens) to effectively extrapolate to longer ones (e.g., 2048 tokens) at inference time, achieving the same perplexity as models specifically trained on longer sequences while training 11% faster and using 11% less memory. <Paper corpusId=\"237347130\" paperTitle=\"(Press et al., 2021)\" isShortName></Paper>\n\n- **Rotary Position Embeddings (RoPE)**: RoPE has become a foundation for length extrapolation techniques, allowing models to generalize to longer contexts than seen during training. Recent advances like LongRoPE have pushed the boundaries even further, enabling context windows up to an impressive 2 million tokens. <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Parallel Context Windows (PCW)**: This approach addresses context limitations in off-the-shelf LLMs without requiring additional training. PCW works by dividing long inputs into chunks (\"windows\"), restricting attention to operate only within each window, and reusing positional embeddings across windows. This method has shown substantial improvements for various tasks requiring long context processing. <Paper corpusId=\"258686160\" paperTitle=\"(Ratner et al., 2022)\" isShortName></Paper> <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Position Interpolation/Extrapolation Techniques**: Numerous studies have explored strategies to enable models to handle positions beyond their training range through interpolation or extrapolation techniques. These methods modify how models interpret token positions, allowing them to process much longer sequences at inference time. <Paper corpusId=\"270710851\" paperTitle=\"(Gavin et al., 2024)\" isShortName></Paper>\n\n- **Combined Approaches**: Innovations like LongLoRA combine position encoding improvements with other techniques such as sparse attention and parameter-efficient fine-tuning. This has enabled dramatic context extensions, such as expanding Llama2 7B from 4K to 100K context length, or Llama2 70B to 32K, while maintaining the original architecture. <Paper corpusId=\"262084134\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\nThese position encoding approaches are particularly valuable because they often require minimal architectural changes to existing models, making them relatively straightforward to implement compared to completely redesigning model architectures. Many current open-source LLMs incorporate these techniques to enhance their long sequence understanding capabilities. <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>", "citations": [{"id": "(Press et al., 2021)", "paper": {"corpus_id": 237347130, "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation", "year": 2021, "venue": "International Conference on Learning Representations", "authors": [{"name": "Ofir Press", "authorId": "40170001"}, {"name": "Noah A. Smith", "authorId": "144365875"}, {"name": "M. Lewis", "authorId": "35084211"}], "n_citations": 775}, "snippets": ["Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark."], "score": 0.0}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 268857023, "title": "Long-context LLMs Struggle with Long In-context Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tianle Li", "authorId": "2157466550"}, {"name": "Ge Zhang", "authorId": "2143853895"}, {"name": "Quy Duc Do", "authorId": "2294572942"}, {"name": "Xiang Yue", "authorId": "2284988933"}, {"name": "Wenhu Chen", "authorId": "2249847177"}], "n_citations": 192}, "snippets": ["One line of research is based on AliBi (Press et al., 2021) and RoPE (Su et al., 2021) embeddings, which allows us to train Transformers with short sequences and subsequently apply them to longer sequences during inference.Recently, different approaches (Xiong et al., 2023;Fu et al., 2024;Liu et al., 2024) help the model to extrapolate to 128K window size with continued pre-training.Later on, LongRoPE (Ding et al., 2024) was proposed to further extend the context window to 2M tokens.Another line of research also utilizes methodologies like context window sliding and segmentation to overcome the issue of the limited context window in original Transformers (Hao et al., 2022;(Ratner et al., 2022).Furthermore, architectural innovations, transitioning from traditional Transformer-based designs to recurrent models or state space models, have shown promise in facilitating long-range computations naturally (Orvieto et al., 2023;Gu & Dao, 2023;(Peng et al., 2023).These techniques have been incorporated into several current open-source LLMs to enhance long sequence understanding capability (Chen et al., 2023)Tworkowski et al., 2023)."], "score": 0.64794921875}, {"id": "(Ratner et al., 2022)", "paper": {"corpus_id": 258686160, "title": "Parallel Context Windows for Large Language Models", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Nir Ratner", "authorId": "2148471161"}, {"name": "Yoav Levine", "authorId": "152754428"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}, {"name": "Ori Ram", "authorId": "73775461"}, {"name": "Inbal Magar", "authorId": "2158996542"}, {"name": "Omri Abend", "authorId": "2769805"}, {"name": "Ehud Karpas", "authorId": "12621100"}, {"name": "A. Shashua", "authorId": "3140335"}, {"name": "Kevin Leyton-Brown", "authorId": "2066411743"}, {"name": "Y. Shoham", "authorId": "1701353"}], "n_citations": 75}, "snippets": ["When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (\"windows\"), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ ai21labs/parallel-context-windows."], "score": 0.0}, {"id": "(Gavin et al., 2024)", "paper": {"corpus_id": 270710851, "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shawn Gavin", "authorId": "2308098543"}, {"name": "Tuney Zheng", "authorId": "2300091474"}, {"name": "Jiaheng Liu", "authorId": "2294523552"}, {"name": "Quehry Que", "authorId": "2303653097"}, {"name": "Noah Wang", "authorId": "2303796897"}, {"name": "Jian Yang", "authorId": "2308240515"}, {"name": "Chenchen Zhang", "authorId": "2214538677"}, {"name": "Wenhao Huang", "authorId": "2239245627"}, {"name": "Wenhu Chen", "authorId": "2249847177"}, {"name": "Ge Zhang", "authorId": "2308233656"}], "n_citations": 3}, "snippets": ["Many studies explore various strategies to address this challenge, including the use of new positional encodings to achieve position interpolation or extrapolation (Peng et al., 2023;Chen et al., 2023;Liu et al., 2024b)", "RingAttention (Liu et al., 2023a) can reduce the memory requirements of the Transformer model, allowing it to train sequences over 500 times longer than previous memory-efficient methods, without needing to approximate the attention mechanism."], "score": 0.689453125}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 262084134, "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yukang Chen", "authorId": "2109297557"}, {"name": "Shengju Qian", "authorId": "152230789"}, {"name": "Haotian Tang", "authorId": "150127950"}, {"name": "Xin Lai", "authorId": "2237802684"}, {"name": "Zhijian Liu", "authorId": "47781592"}, {"name": "Song Han", "authorId": "2243400730"}, {"name": "Jiaya Jia", "authorId": "2237811040"}], "n_citations": 167}, "snippets": ["We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset."], "score": 0.0}], "table": null}, {"title": "Recent Advancements and Implementations", "tldr": "Recent years have seen dramatic innovations in extending context windows from thousands to millions of tokens through both architectural innovations and position encoding techniques. Commercial models have pushed boundaries even further, with context windows now reaching up to 1 million tokens, while open-source implementations like LongLoRA and LongRoPE have made extended context capabilities more accessible. (6 sources)", "text": "\nThe field of long context modeling has evolved rapidly, with researchers achieving remarkable breakthroughs in extending transformer models' context windows. These advancements have culminated in a new generation of models capable of handling sequences that would have been unimaginable just a few years ago.\n\nCommercial language models have rapidly expanded their context capabilities, with GPT-4, Claude-3, and Gemini-1.5 now supporting context windows ranging from 32K to an impressive 1 million tokens <Paper corpusId=\"268793522\" paperTitle=\"(Thonet et al., 2024)\" isShortName></Paper>. This expansion reflects the practical importance of long context understanding for real-world applications.\n\nOpen-source implementations have also made significant strides in long context modeling. LongLoRA, for instance, can extend Llama2 7B from 4K to 100K context length or Llama2 70B to 32K context length while maintaining the original architecture <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"262084134\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This approach combines parameter-efficient fine-tuning with sparse attention mechanisms, allowing models to be extended with relatively modest computational resources\u2014as little as a single 8\u00d7A100 machine <Paper corpusId=\"262084134\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\nEven more impressive is LongRoPE, which has pushed context windows to an extraordinary 2 million tokens <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>. This represents a remarkable advancement in position encoding techniques that enable models to handle extremely long documents without redesigning their core architecture.\n\nMemory efficiency has also been a focus of recent research. RingAttention can reduce memory requirements dramatically, allowing models to train on sequences over 500 times longer than previous memory-efficient methods without approximating the attention mechanism <Paper corpusId=\"270710851\" paperTitle=\"(Gavin et al., 2024)\" isShortName></Paper>. This addresses one of the major bottlenecks in scaling context length.\n\nArchitectural innovations continue to emerge, with models like BlockTransformer employing hierarchical global-to-local modeling for efficient long context processing <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>. InfiniteTransformer incorporates unbounded long-term memory through continuous-space attention, potentially allowing for unlimited context length <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\nState space models like Mamba represent another promising direction, capturing long-range dependencies efficiently without explicit attention mechanisms <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>. These models offer an alternative approach to the long context challenge that avoids some of the fundamental limitations of attention-based architectures.\n\nThe practical benefits of these advances are now being realized in multiple domains. For instance, a study comparing retrieval-augmentation versus long context windows found that LLMs with 4K context using simple retrieval-augmentation can achieve comparable performance to fine-tuned LLMs with 16K context <Paper corpusId=\"268793522\" paperTitle=\"(Thonet et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263620134\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper>. Moreover, combining both approaches\u2014retrieval-augmented Llama2-70B with 32K context\u2014outperformed GPT-3.5-turbo-16k and Davinci003 across various long context tasks <Paper corpusId=\"263620134\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper>.\n\nAs these methods continue to mature, we're witnessing the democratization of long context modeling capabilities, with many techniques now being incorporated into open-source LLMs to enhance their long sequence understanding abilities <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"262084134\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This accessibility marks a significant step forward in making advanced long context processing available to a broader range of researchers and developers.", "citations": [{"id": "(Thonet et al., 2024)", "paper": {"corpus_id": 268793522, "title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Thibaut Thonet", "authorId": "2955690"}, {"name": "Jos Rozen", "authorId": "120419790"}, {"name": "Laurent Besacier", "authorId": "2259359878"}], "n_citations": 3}, "snippets": ["context modeling. 3 While an exhaustive survey of these methods is beyond the scope of this paper, they can generally be categorized into three main groups (excluding other distinct approaches such as retrieval-augmented generation (Xu et al., 2023) and context compression (Chevalier et al., 2023)): (a) the development of efficient transformer architectures to address the quadratic attention challenge, including sparse transformers [6,12,(Martins et al., 2020)(Zaheer et al., 2020), linear transformers (Choromanski et al., 2020)(Katharopoulos et al., 2020)[42], and hierarchical transformers [20,30,(Wu et al., 2021); (b) approaches like recurrent attention networks [7,(Dai et al., 2019)(Peng et al., 2023) and state-space models [16,41]; (c) length extrapolation or position embedding interpolation, where LLMs are fine-tuned or adapted at inference time to adjust tokens' positions to match the new context length [4,8,9,28,35,(Peng et al., 2023)[45]. These techniques also contributed to the context length expansion in proprietary models like GPT-4 (32K-128K), Claude-3 (200k), and Gemini-1.5 (128K-1M)."], "score": 0.56689453125}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 268857023, "title": "Long-context LLMs Struggle with Long In-context Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tianle Li", "authorId": "2157466550"}, {"name": "Ge Zhang", "authorId": "2143853895"}, {"name": "Quy Duc Do", "authorId": "2294572942"}, {"name": "Xiang Yue", "authorId": "2284988933"}, {"name": "Wenhu Chen", "authorId": "2249847177"}], "n_citations": 192}, "snippets": ["One line of research is based on AliBi (Press et al., 2021) and RoPE (Su et al., 2021) embeddings, which allows us to train Transformers with short sequences and subsequently apply them to longer sequences during inference.Recently, different approaches (Xiong et al., 2023;Fu et al., 2024;Liu et al., 2024) help the model to extrapolate to 128K window size with continued pre-training.Later on, LongRoPE (Ding et al., 2024) was proposed to further extend the context window to 2M tokens.Another line of research also utilizes methodologies like context window sliding and segmentation to overcome the issue of the limited context window in original Transformers (Hao et al., 2022;(Ratner et al., 2022).Furthermore, architectural innovations, transitioning from traditional Transformer-based designs to recurrent models or state space models, have shown promise in facilitating long-range computations naturally (Orvieto et al., 2023;Gu & Dao, 2023;(Peng et al., 2023).These techniques have been incorporated into several current open-source LLMs to enhance long sequence understanding capability (Chen et al., 2023)Tworkowski et al., 2023)."], "score": 0.64794921875}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 262084134, "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yukang Chen", "authorId": "2109297557"}, {"name": "Shengju Qian", "authorId": "152230789"}, {"name": "Haotian Tang", "authorId": "150127950"}, {"name": "Xin Lai", "authorId": "2237802684"}, {"name": "Zhijian Liu", "authorId": "47781592"}, {"name": "Song Han", "authorId": "2243400730"}, {"name": "Jiaya Jia", "authorId": "2237811040"}], "n_citations": 167}, "snippets": ["We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset."], "score": 0.0}, {"id": "(Gavin et al., 2024)", "paper": {"corpus_id": 270710851, "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shawn Gavin", "authorId": "2308098543"}, {"name": "Tuney Zheng", "authorId": "2300091474"}, {"name": "Jiaheng Liu", "authorId": "2294523552"}, {"name": "Quehry Que", "authorId": "2303653097"}, {"name": "Noah Wang", "authorId": "2303796897"}, {"name": "Jian Yang", "authorId": "2308240515"}, {"name": "Chenchen Zhang", "authorId": "2214538677"}, {"name": "Wenhao Huang", "authorId": "2239245627"}, {"name": "Wenhu Chen", "authorId": "2249847177"}, {"name": "Ge Zhang", "authorId": "2308233656"}], "n_citations": 3}, "snippets": ["Many studies explore various strategies to address this challenge, including the use of new positional encodings to achieve position interpolation or extrapolation (Peng et al., 2023;Chen et al., 2023;Liu et al., 2024b)", "RingAttention (Liu et al., 2023a) can reduce the memory requirements of the Transformer model, allowing it to train sequences over 500 times longer than previous memory-efficient methods, without needing to approximate the attention mechanism."], "score": 0.689453125}, {"id": "(He et al., 2025)", "paper": {"corpus_id": 277857470, "title": "Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation", "year": 2025, "venue": "International Conference on Learning Representations", "authors": [{"name": "Linda He", "authorId": "2356154321"}, {"name": "Jue Wang", "authorId": "2252087284"}, {"name": "Maurice Weber", "authorId": "2110605429"}, {"name": "Shang Zhu", "authorId": "2356004549"}, {"name": "Ben Athiwaratkun", "authorId": "2304481349"}, {"name": "Ce Zhang", "authorId": "2305565297"}], "n_citations": 1}, "snippets": ["To address the quadratic computational and memory demands of standard transformer self-attention, researchers have developed various architectural modifica-tions to improve efficiency and extend context lengths. Notable examples include Longformer (Beltagy et al., 2020), which combines sliding window and global attention, and BlockTransformer (Ho et al., 2024), which employs hierarchical global-to-local modeling. Linear Attention methods (Katharopoulos et al., 2020) reformulate self-attention for linear complexity, while InfiniteTransformer (Munkhdalai et al., 2024) incorporates unbounded long-term memory through continuousspace attention. State space models like Mamba (Gu & Dao, 2024) capture long-range dependencies efficiently without explicit attention mechanisms."], "score": 0.5537109375}, {"id": "(Xu et al., 2023)", "paper": {"corpus_id": 263620134, "title": "Retrieval meets Long Context Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Peng Xu", "authorId": "2254989105"}, {"name": "Wei Ping", "authorId": "2253664013"}, {"name": "Xianchao Wu", "authorId": "2253618149"}, {"name": "Lawrence C. McAfee", "authorId": "20957879"}, {"name": "Chen Zhu", "authorId": "2283871700"}, {"name": "Zihan Liu", "authorId": "2256582287"}, {"name": "Sandeep Subramanian", "authorId": "2253531461"}, {"name": "E. Bakhturina", "authorId": "32867948"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "Bryan Catanzaro", "authorId": "2301680"}], "n_citations": 85}, "snippets": ["Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners."], "score": 0.0}], "table": null}], "cost": 0.270084}}
