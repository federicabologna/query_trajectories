{"original_query": "Has anyone tried fine tuning with RAG\u2014 or have seen work on this?", "input": "anyone fine tuning RAG or work this", "output": {"sections": [{"title": "Introduction to RAG and Fine-tuning", "tldr": "Retrieval Augmented Generation (RAG) enhances LLM performance by integrating external knowledge retrieval with text generation. Fine-tuning RAG components has emerged as a critical approach to improve accuracy and reduce hallucinations, especially in domain-specific applications. (4 sources)", "text": "\nRetrieval Augmented Generation (RAG) has emerged as a powerful approach for enhancing the capabilities of Large Language Models (LLMs) by combining information retrieval with text generation. This technique allows LLMs to access and leverage external knowledge sources when generating responses, which helps to improve accuracy and reduce hallucinations. The combination of fine-tuning LLMs with RAG processes has been demonstrated to generate responses with significantly improved accuracy <Paper corpusId=\"267412954\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nOne of the primary motivations for fine-tuning RAG systems is to address the limitations of standard pre-trained embedding models, which may exhibit sub-optimal performance when applied to specific domain knowledge. Fine-tuning the retrieval components enables these systems to better capture domain-specific semantics and improve overall performance <Paper corpusId=\"267750557\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>. This is particularly important as RAG's promise of delivering optimal responses often falls short in complex query scenarios, necessitating specialized approaches to enhance performance across multiple domains <Paper corpusId=\"270560495\" paperTitle=\"(Barnett et al., 2024)\" isShortName></Paper>.\n\nWhile research into methods for improving LLM performance through fine-tuning, RAG, and soft-prompting has expanded rapidly, many of these approaches have tended to focus on highly technical or high-cost techniques. This has made many of the newly discovered approaches comparatively inaccessible to non-technical users <Paper corpusId=\"265128626\" paperTitle=\"(Dodgson et al., 2023)\" isShortName></Paper>. Consequently, there is growing interest in developing more accessible fine-tuning methods for RAG systems that can be implemented by a wider range of users while still delivering substantial performance improvements.", "citations": [{"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 267412954, "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Liang Zhang", "authorId": "2279813822"}, {"name": "Katherine Jijo", "authorId": "2279831793"}, {"name": "Spurthi Setty", "authorId": "2282528163"}, {"name": "Eden Chung", "authorId": "2279830841"}, {"name": "Fatima Javid", "authorId": "2282539958"}, {"name": "Natan Vidra", "authorId": "2279830757"}, {"name": "Thomas Clifford", "authorId": "2279838243"}], "n_citations": 20}, "snippets": ["Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy."], "score": 0.7958984375}, {"id": "(Zhang et al._1, 2024)", "paper": {"corpus_id": 267750557, "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mingtian Zhang", "authorId": "2108795448"}, {"name": "Shawn Lan", "authorId": "2284682723"}, {"name": "Peter Hayes", "authorId": "2067492948"}, {"name": "David Barber", "authorId": "2282542157"}], "n_citations": 3}, "snippets": ["Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning", "We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model."], "score": 0.7607421875}, {"id": "(Barnett et al., 2024)", "paper": {"corpus_id": 270560495, "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Scott Barnett", "authorId": "2279752649"}, {"name": "Zach Brannelly", "authorId": "2279020735"}, {"name": "Stefanus Kurniawan", "authorId": "2266469333"}, {"name": "Sheng Wong", "authorId": "2307101480"}], "n_citations": 2}, "snippets": ["This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains."], "score": 0.689453125}, {"id": "(Dodgson et al., 2023)", "paper": {"corpus_id": 265128626, "title": "Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jennifer Dodgson", "authorId": "2266237514"}, {"name": "Nanzheng Lin", "authorId": "2266281858"}, {"name": "Julian Peh", "authorId": "2266237540"}, {"name": "Akira Rafhael Janson Pattirane", "authorId": "2266241068"}, {"name": "Alfath Daryl Alhajir", "authorId": "2047388958"}, {"name": "Eko Ridho Dinarto", "authorId": "2266238002"}, {"name": "Joseph Lim", "authorId": "2266365094"}, {"name": "Syed Danyal Ahmad", "authorId": "2266346433"}], "n_citations": 8}, "snippets": ["Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users."], "score": 0.53759765625}], "table": null}, {"title": "Types of RAG Fine-tuning Approaches", "tldr": "RAG fine-tuning can target the retriever component, the generator component, or both simultaneously through dual fine-tuning approaches, each addressing different aspects of the RAG pipeline. These approaches range from optimizing retriever-generator alignment to enhancing domain-specific performance through specialized training techniques. (12 sources)", "text": "\nFine-tuning strategies for RAG systems can be broadly categorized into three main approaches: retriever fine-tuning, generator fine-tuning, and dual fine-tuning that optimizes both components simultaneously <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>.\n\n**Retriever Fine-tuning** focuses on enhancing the retrieval component to return more relevant and helpful documents for the generator. This approach aims to align the retriever's output with the generator's needs, essentially teaching the retriever to find information that will be most beneficial for the LLM to generate accurate responses <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Methods such as REPLUG demonstrate how retrieval models can be tuned based on feedback from language models, creating a more cohesive information pipeline <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. Similarly, Augmentation-Adapted Retriever (AAR) learns language model preferences to improve zero-shot generalization across various target LLMs <Paper corpusId=\"258960666\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>.\n\n**Generator Fine-tuning** involves optimizing the LLM to better utilize and incorporate retrieved context. Research like SAIL (Search-Augmented Instruction Learning) grounds language generation on complex search results, teaching models to select trustworthy information and perform multi-hop reasoning when working with retrieved passages <Paper corpusId=\"258865283\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. This approach ensures that generators can produce more faithful and robust content based on the retrieved information <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n**Dual Fine-tuning** represents a more holistic approach where both the retriever and generator are fine-tuned simultaneously to enhance overall system performance <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. A notable implementation is Retrieval-Augmented Dual Instruction Tuning (RA-DIT), which updates the LLM to maximize the likelihood of correct answers given retrieval-augmented instructions while simultaneously optimizing the retriever to align with LLM preferences <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263605962\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. Despite the increased complexity and integration challenges, this approach has demonstrated significant performance improvements across knowledge-intensive tasks <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nRecent advancements have introduced more sophisticated fine-tuning techniques like Direct Preference Optimization (DPO), which enhances models' instruction-following capabilities by learning from positive and negative sample pairs <Paper corpusId=\"273323657\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"258959321\" paperTitle=\"(Rafailov et al., 2023)\" isShortName></Paper>. Additionally, research by Zhou et al. has revealed that fine-tuning LLMs with even limited high-quality data can significantly enhance performance, providing a foundation for efficiently fine-tuning multiple RAG components with minimal data <Paper corpusId=\"273532096\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258822910\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n\nAs RAG systems evolve toward more modular designs, fine-tuning approaches are expanding to include specialized components and training objectives. Some methods focus on aligning information needs between retrievers and generators through optimization based on generation model feedback <Paper corpusId=\"276580741\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>. Others, like CRAG, train lightweight retrieval evaluators to assess document quality and trigger different retrieval actions based on confidence levels <Paper corpusId=\"266359151\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper>. These evolving approaches demonstrate how RAG fine-tuning continues to integrate with broader LLM technologies to create more effective knowledge-enhanced systems.", "citations": [{"id": "(Gao et al., 2024)", "paper": {"corpus_id": 271571401, "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yunfan Gao", "authorId": "2280046531"}, {"name": "Yun Xiong", "authorId": "2275320371"}, {"name": "Meng Wang", "authorId": "2291409458"}, {"name": "Haofen Wang", "authorId": "2256769434"}], "n_citations": 20}, "snippets": ["RAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning", "3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference."], "score": 0.7880859375}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 270870251, "title": "Searching for Best Practices in Retrieval-Augmented Generation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xiaohua Wang", "authorId": "2273537815"}, {"name": "Zhenghua Wang", "authorId": "2308276345"}, {"name": "Xuan Gao", "authorId": "2292070745"}, {"name": "Feiran Zhang", "authorId": "2308226671"}, {"name": "Yixin Wu", "authorId": "2308043953"}, {"name": "Zhibo Xu", "authorId": "2308044030"}, {"name": "Tianyuan Shi", "authorId": "2308036711"}, {"name": "Zhengyuan Wang", "authorId": "2309182278"}, {"name": "Shizheng Li", "authorId": "2309656885"}, {"name": "Qi Qian", "authorId": "2309176521"}, {"name": "Ruicheng Yin", "authorId": "2292032843"}, {"name": "Changze Lv", "authorId": "2220896023"}, {"name": "Xiaoqing Zheng", "authorId": "2257315404"}, {"name": "Xuanjing Huang", "authorId": "2257129987"}], "n_citations": 61}, "snippets": ["Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges."], "score": 0.80517578125}, {"id": "(Shi et al., 2023)", "paper": {"corpus_id": 256389797, "title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Weijia Shi", "authorId": "3040379"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Michihiro Yasunaga", "authorId": "19168196"}, {"name": "Minjoon Seo", "authorId": "4418074"}, {"name": "Rich James", "authorId": "2191899140"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Wen-tau Yih", "authorId": "2072801764"}], "n_citations": 641}, "snippets": ["We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG."], "score": 0.0}, {"id": "(Yu et al., 2023)", "paper": {"corpus_id": 258960666, "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zichun Yu", "authorId": "2275526493"}, {"name": "Chenyan Xiong", "authorId": "2139787803"}, {"name": "S. Yu", "authorId": "150311558"}, {"name": "Zhiyuan Liu", "authorId": "2109232579"}], "n_citations": 69}, "snippets": ["Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM\u2019s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever."], "score": 0.0}, {"id": "(Luo et al., 2023)", "paper": {"corpus_id": 258865283, "title": "SAIL: Search-Augmented Instruction Learning", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Hongyin Luo", "authorId": "1944274"}, {"name": "Yung-Sung Chuang", "authorId": "2475831"}, {"name": "Yuan Gong", "authorId": "145802952"}, {"name": "Tianhua Zhang", "authorId": "2146333115"}, {"name": "Yoon Kim", "authorId": "143827730"}, {"name": "Xixin Wu", "authorId": "1847260"}, {"name": "D. Fox", "authorId": "31997718"}, {"name": "H. Meng", "authorId": "145199941"}, {"name": "James R. Glass", "authorId": "145898106"}], "n_citations": 27}, "snippets": ["Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \\textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking."], "score": 0.0}, {"id": "(Lin et al., 2023)", "paper": {"corpus_id": 263605962, "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xi Victoria Lin", "authorId": "2255374957"}, {"name": "Xilun Chen", "authorId": "1769736"}, {"name": "Mingda Chen", "authorId": "46221498"}, {"name": "Weijia Shi", "authorId": "2254168373"}, {"name": "Maria Lomeli", "authorId": "2253400960"}, {"name": "Rich James", "authorId": "2191899140"}, {"name": "Pedro Rodriguez", "authorId": "2253404757"}, {"name": "Jacob Kahn", "authorId": "2253401183"}, {"name": "Gergely Szilvasy", "authorId": "2253402270"}, {"name": "Mike Lewis", "authorId": "2253417398"}, {"name": "Luke S. Zettlemoyer", "authorId": "2137813791"}, {"name": "Scott Yih", "authorId": "2253400757"}], "n_citations": 153}, "snippets": ["Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average."], "score": 0.0}, {"id": "(Wang et al._1, 2024)", "paper": {"corpus_id": 273323657, "title": "DeepNote: Note-Centric Deep Retrieval-Augmented Generation", "year": 2024, "venue": "", "authors": [{"name": "Ruobing Wang", "authorId": "2314784069"}, {"name": "Daren Zha", "authorId": "2325728623"}, {"name": "Shi Yu", "authorId": "2314785970"}, {"name": "Qingfei Zhao", "authorId": "2220669262"}, {"name": "Yuxuan Chen", "authorId": "2311726706"}, {"name": "Yixuan Wang", "authorId": "2310129232"}, {"name": "Shuo Wang", "authorId": "2267033597"}, {"name": "Yukun Yan", "authorId": "2277242040"}, {"name": "Zhenghao Liu", "authorId": "49047064"}, {"name": "Xu Han", "authorId": "2284728053"}, {"name": "Zhiyuan Liu", "authorId": "2266886975"}, {"name": "Maosong Sun", "authorId": "2273551430"}], "n_citations": 0}, "snippets": ["Fine-tuning is widely used to improve the capabilities of LLM-augmented components in RAG systems (de Luis Balaguer et al., 2024). Early methods of fine-tuning to enhance LLM-based components in RAG primarily focused on training the retriever and the generator (Ke et al., 2024)(Lin et al., 2023). Recent RAG methods have shifted toward modular designs (Gao et al., 2023b). Particularly in complex QA tasks, adaptive RAG often requires base models to follow intricate instructions (Yin et al., 2023a;Xu et al., 2024) to enable the functionality of diverse components (Asai et al., 2024). Classic alignment training methods include supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). However, SFT lacks negative feedback and is prone to overfitting. Recently, Rafailov et al. proposed a more efficient reinforcement learning algorithm, direct preference optimization (DPO), which aligns response preferences and enhances the model's instruction-following ability by learning the differences between positive and negative sample pairs. In our work, we focus on using DPO to enhance the model's capability in multiple processes."], "score": 0.67529296875}, {"id": "(Rafailov et al., 2023)", "paper": {"corpus_id": 258959321, "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Rafael Rafailov", "authorId": "102801230"}, {"name": "Archit Sharma", "authorId": "50465276"}, {"name": "E. Mitchell", "authorId": "49688913"}, {"name": "Stefano Ermon", "authorId": "2490652"}, {"name": "Christopher D. Manning", "authorId": "144783904"}, {"name": "Chelsea Finn", "authorId": "46881670"}], "n_citations": 4159}, "snippets": ["While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train."], "score": 0.0}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 273532096, "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Qingfei Zhao", "authorId": "2220669262"}, {"name": "Ruobing Wang", "authorId": "2314784069"}, {"name": "Yukuo Cen", "authorId": "83546711"}, {"name": "Daren Zha", "authorId": "2325728623"}, {"name": "Shicheng Tan", "authorId": "2327738081"}, {"name": "Yuxiao Dong", "authorId": "2243402027"}, {"name": "Jie Tang", "authorId": "2327297599"}], "n_citations": 14}, "snippets": ["Fine-tuning has gradually become a popular strategy (Ke et al., 2024) for enhancing the capabilities of components of RAG. Existing works include fine-tuning retrieval-related components to achieve better retrieval outcomes (Yan et al., 2024), fine-tuning generators for more personalized outputs (Zhang et al., 2024b), and employing collaborative fine-tuning (Lin et al., 2023). Additionally, (Zhou et al., 2023) discovered that fine-tuning LLMs with a limited quantity of high-quality data significantly enhances the performance of LLMs. This finding provides a robust theoretical basis for collaboratively fine-tuning multiple components within advanced RAG methodologies at a minimal data expense."], "score": 0.82763671875}, {"id": "(Zhou et al., 2023)", "paper": {"corpus_id": 258822910, "title": "LIMA: Less Is More for Alignment", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Chunting Zhou", "authorId": "2384711"}, {"name": "Pengfei Liu", "authorId": "144118452"}, {"name": "Puxin Xu", "authorId": "2214843767"}, {"name": "Srini Iyer", "authorId": "1900163"}, {"name": "Jiao Sun", "authorId": "145478138"}, {"name": "Yuning Mao", "authorId": "3375249"}, {"name": "Xuezhe Ma", "authorId": "2378954"}, {"name": "Avia Efrat", "authorId": "1388010852"}, {"name": "Ping Yu", "authorId": "2114104308"}, {"name": "L. Yu", "authorId": "49297123"}, {"name": "Susan Zhang", "authorId": "2108244542"}, {"name": "Gargi Ghosh", "authorId": "134007132"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Omer Levy", "authorId": "39455775"}], "n_citations": 850}, "snippets": ["Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."], "score": 0.0}, {"id": "(Wu et al., 2025)", "paper": {"corpus_id": 276580741, "title": "RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mingyan Wu", "authorId": "2347252099"}, {"name": "Zhenghao Liu", "authorId": "2323176343"}, {"name": "Yukun Yan", "authorId": "2277242040"}, {"name": "Xinze Li", "authorId": "2261354998"}, {"name": "Shi Yu", "authorId": "2314785970"}, {"name": "Zheni Zeng", "authorId": "1633538428"}, {"name": "Yu Gu", "authorId": "2261295920"}, {"name": "Ge Yu", "authorId": "2204644192"}], "n_citations": 2}, "snippets": ["To further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023)(Shi et al., 2023)(Izacard et al., 2020). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2023) to jointly optimize the modules in a RAG system, aligning their data preferences."], "score": 0.5234375}, {"id": "(Gao et al., 2023)", "paper": {"corpus_id": 266359151, "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yunfan Gao", "authorId": "2280046531"}, {"name": "Yun Xiong", "authorId": "2275320371"}, {"name": "Xinyu Gao", "authorId": "2275341478"}, {"name": "Kangxiang Jia", "authorId": "2275191447"}, {"name": "Jinliu Pan", "authorId": "2275530552"}, {"name": "Yuxi Bi", "authorId": "2275171009"}, {"name": "Yi Dai", "authorId": "2276187454"}, {"name": "Jiawei Sun", "authorId": "2275540959"}, {"name": "Qianyu Guo", "authorId": "2258800561"}, {"name": "Meng Wang", "authorId": "2291409458"}, {"name": "Haofen Wang", "authorId": "2256769434"}], "n_citations": 1819}, "snippets": ["Combining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-toend joint training-and how to harness both parameterized and non-parameterized advantages are areas ripe for exploration [27]. Another trend is to introduce SLMs with specific functionalities into RAG and fine-tuned by the results of RAG system. For example, CRAG [67] trains a lightweight retrieval evaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval actions based on confidence levels."], "score": 0.5751953125}], "table": null}, {"title": "Specific RAG Fine-tuning Techniques and Implementations", "tldr": "Recent RAG fine-tuning techniques focus on improving retrieval quality, document utilization, and robustness against imperfect information. These implementations range from dual-purpose models like RankRAG to specialized approaches such as RAFT, CRAFT, and Robust Fine-Tuning that address specific RAG limitations. (13 sources)", "text": "\nBelow are notable RAG fine-tuning techniques and implementations that researchers have developed to enhance various aspects of retrieval-augmented generation:\n\n- **RankRAG**: A novel instruction fine-tuning framework that trains a single LLM to perform both context ranking and answer generation. Despite using only a small fraction of ranking data in the training mix, RankRAG outperforms existing expert ranking models, including those exclusively fine-tuned on larger amounts of ranking data. <Paper corpusId=\"270878612\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper>\n\n- **RAFT (Retrieval Augmented Fine-Tuning)**: This approach uses fine-tuning data containing related documents and reasoning chains to improve the LLM's ability to understand retrieved documents. RAFT has demonstrated superior performance compared to standard RAG setups, even when using smaller 7B parameter models versus larger models like GPT-3.5. <Paper corpusId=\"269983737\" paperTitle=\"(Jiao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"272911196\" paperTitle=\"(Chung et al., 2024)\" isShortName></Paper>\n\n- **CRAFT (Compute-efficient RAFT)**: Combines RAFT with Low-Rank Adaptation (LoRA) to reduce fine-tuning and storage requirements while maintaining comparable RAG performance. This approach is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited. <Paper corpusId=\"272911196\" paperTitle=\"(Chung et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper>\n\n- **Reward-RAG**: Enhances RAG through Reward-Driven Supervision by employing CriticGPT to train a dedicated reward model. This model generates synthesized datasets for fine-tuning the RAG encoder, aligning outputs more closely with human preferences and adapting retrieval information to specific domains. <Paper corpusId=\"273186680\" paperTitle=\"(Nguyen et al., 2024)\" isShortName></Paper>\n\n- **Domain-Specific RAG Fine-tuning**: Adapts RAG models to particular domains by fine-tuning both the generator and question encoder collectively on domain-specific datasets. Experiments in the hotel domain demonstrated significant improvements compared to baseline RAG models. <Paper corpusId=\"273532207\" paperTitle=\"(Rakin et al., 2024)\" isShortName></Paper>\n\n- **Robust Fine-Tuning (RbFT)**: Designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results show that RbFT significantly improves RAG system robustness across diverse retrieval conditions while maintaining high inference efficiency. <Paper corpusId=\"275993994\" paperTitle=\"(Tu et al., 2025)\" isShortName></Paper>\n\n- **RAG-Tuned LLM**: Integrates RAG and native LLM strengths by fine-tuning an LLM within an RAG framework for data generation. This approach bridges open-domain and domain-specific query-answering tasks, outperforming both standard RAG methods and long-context LLMs across diverse datasets, particularly for hierarchical queries. <Paper corpusId=\"277150553\" paperTitle=\"(Wei et al., 2025)\" isShortName></Paper>\n\n- **MultiQA Fine-tuning**: Uses supervised fine-tuning on general domain questions and answers to improve RAG performance. This approach employs LoRA to train models using prompts with retrieved documents, demonstrating benefits for both in-domain and out-of-domain RAG generalization. <Paper corpusId=\"277510500\" paperTitle=\"(Misrahi et al., 2025)\" isShortName></Paper>\n\n- **CoRAG and DeepRAG**: Focus on retrieval pathway optimization through multistep reasoning frameworks. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks like sub-query generation and intermediate answer prediction. DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths. <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>\n\n- **Self-RAG**: Fine-tunes models to generate special reflection tokens that make the LLM controllable during inference, enabling it to retrieve passages on-demand and reflect on both retrieved information and its own generations. Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on open-domain QA, reasoning, and fact verification tasks. <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"264288947\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper>\n\n- **Finetune-RAG**: A simple yet effective approach that trains LLMs to distinguish between correct and fictitious context within RAG. Unlike methods that focus on improving retrieval, Finetune-RAG enhances the model's ability to handle imperfect inputs by fine-tuning it with examples containing both correct and incorrect information, improving factual accuracy by 21.2% over the base model. <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>\n\n- **RAG-sft and CoT-sft**: Implementation recipes that fine-tune models in the RAG setup. CoT-sft incorporates a Chain-of-Thought approach where gold documents and distractor documents are used in the prompt with specified probabilities, guiding the model to use retrieved context, explain steps, quote relevant parts, and produce a final answer. <Paper corpusId=\"271710111\" paperTitle=\"(Fleischer et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Yu et al., 2024)", "paper": {"corpus_id": 270878612, "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yue Yu", "authorId": "2259265562"}, {"name": "Wei Ping", "authorId": "2253664013"}, {"name": "Zihan Liu", "authorId": "2256582287"}, {"name": "Boxin Wang", "authorId": "2256656241"}, {"name": "Jiaxuan You", "authorId": "2287859963"}, {"name": "Chao Zhang", "authorId": "2256776233"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "Bryan Catanzaro", "authorId": "2264406909"}], "n_citations": 74}, "snippets": ["In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data."], "score": 0.5283203125}, {"id": "(Jiao et al., 2024)", "paper": {"corpus_id": 269983737, "title": "DuetRAG: Collaborative Retrieval-Augmented Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Dian Jiao", "authorId": "2302798653"}, {"name": "Li Cai", "authorId": "2303434387"}, {"name": "Jingsheng Huang", "authorId": "2303044665"}, {"name": "Wenqiao Zhang", "authorId": "2108125912"}, {"name": "Siliang Tang", "authorId": "2118071462"}, {"name": "Yueting Zhuang", "authorId": "2253660817"}], "n_citations": 1}, "snippets": ["Finetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG framework.For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning framework to fine-tune both the LLM and the retriever simultaneously.InstructRetro (Wang et al., 2023) pre-trains a larger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on it.ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to enhance the model's ability to perform context awareness in conversational QA.RAFT (Zhang et al., 2024) proposes a kind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune LLM and improve LLM's ability to understand the retrieved documents under the RAG framework."], "score": 0.56884765625}, {"id": "(Chung et al., 2024)", "paper": {"corpus_id": 272911196, "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Isaac Chung", "authorId": "2322992640"}, {"name": "Phat Vo", "authorId": "2322982756"}, {"name": "Arman Kizilkale", "authorId": "2322991957"}, {"name": "Aaron Reite", "authorId": "2322982549"}], "n_citations": 0}, "snippets": ["Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited."], "score": 0.6171875}, {"id": "(Dettmers et al., 2023)", "paper": {"corpus_id": 258841328, "title": "QLoRA: Efficient Finetuning of Quantized LLMs", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tim Dettmers", "authorId": "3239480"}, {"name": "Artidoro Pagnoni", "authorId": "51152502"}, {"name": "Ari Holtzman", "authorId": "14487640"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 2606}, "snippets": ["We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."], "score": 0.0}, {"id": "(Nguyen et al., 2024)", "paper": {"corpus_id": 273186680, "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Thang Nguyen", "authorId": "2324796381"}, {"name": "Peter Chin", "authorId": "2324790937"}, {"name": "Yu-Wing Tai", "authorId": "2324792268"}], "n_citations": 5}, "snippets": ["In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences."], "score": 0.5732421875}, {"id": "(Rakin et al., 2024)", "paper": {"corpus_id": 273532207, "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Salman Rakin", "authorId": "2327211339"}, {"name": "Md. A.R. Shibly", "authorId": "2327211291"}, {"name": "Zahin M. Hossain", "authorId": "2327211491"}, {"name": "Zeeshan Khan", "authorId": "2327258807"}, {"name": "Md. Mostofa Akbar", "authorId": "2327212434"}], "n_citations": 3}, "snippets": ["RAG-Finetuned-QA : The RAG-Finetuned-QA model builds upon the baseline RAG-Original architecture by incorporating domain-specific fine-tuning on our hotel domain dataset. Like the RAG-Original model, it utilizes a Hotel Domain Knowledge Base, with dense vector representations indexed using the FAISS library. The loss function is designed to fine-tune both the generator and question encoder collectively. We have used ray as the distributed retriever with 4 retrieval workers as described in the original RAG paper. The model was trained for 20 epochs while the final checkpoint has been selected based on the highest validation accuracy. The purpose of this experiment was to assess the improvements gained from domain-specific training, comparing its performance against the baseline RAG-Original model."], "score": 0.51220703125}, {"id": "(Tu et al., 2025)", "paper": {"corpus_id": 275993994, "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yiteng Tu", "authorId": "2275628230"}, {"name": "Weihang Su", "authorId": "2147219374"}, {"name": "Yujia Zhou", "authorId": "2290870875"}, {"name": "Yiqun Liu", "authorId": "2260835922"}, {"name": "Qingyao Ai", "authorId": "2256982003"}], "n_citations": 6}, "snippets": ["To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques."], "score": 0.5341796875}, {"id": "(Wei et al., 2025)", "paper": {"corpus_id": 277150553, "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jiale Wei", "authorId": "2349833035"}, {"name": "Shuchi Wu", "authorId": "2351464722"}, {"name": "Ruochen Liu", "authorId": "2351243441"}, {"name": "Xiang Ying", "authorId": "2308276200"}, {"name": "Jingbo Shang", "authorId": "2308276676"}, {"name": "Fangbo Tao", "authorId": "2351054847"}], "n_citations": 0}, "snippets": ["In this paper, we validate RAG's fine-grained retrieval abilities and the global abstraction strengths of LLM-native solutions. However, RAG lacks holistic understanding, and long-context models tend to lose key information over extended contexts. We integrate these strengths of both RAG and LLM-native solutions by fine-tuning an LLM within an RAG framework for data generation. This work is the first to explore LLM and RAG integration within a unified framework, bridging open-domain and domain-specific query-answering tasks. Our RAG-Tuned LLM, equipped with LLMnative memory, outperforms both standard RAG methods and long-context LLMs across diverse datasets, demonstrating superior performance in handling hierarchical queries."], "score": 0.68505859375}, {"id": "(Misrahi et al., 2025)", "paper": {"corpus_id": 277510500, "title": "Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Alexandre Misrahi", "authorId": "2353386625"}, {"name": "Nadezhda Chirkova", "authorId": "2258716783"}, {"name": "Maxime Louis", "authorId": "2342407524"}, {"name": "Vassilina Nikoulina", "authorId": "2841761"}], "n_citations": 0}, "snippets": ["Recent works have demonstrated that it is beneficial to finetune an LLM to encourage it to better use retrieved context [21,22,32,37]. It has been shown to improve RAG performances in-domain, and we investigate whether this holds for out-of-domain RAG generalization. To do so, we run supervised fine-tuning on the MultiQA dataset9 : a dataset consisting of 450k general domain questions and answers, described in Appendix Table 5. Its associated document collection consists of Wikipedia [30] and MSMARCO [2] documents. Each supervised fine-tuning sample then consists of a prompt (taken from [32]) with 5 retrieved documents, the question and its answer. Models are trained with LoRA [12]."], "score": 0.70556640625}, {"id": "(Gao et al., 2025)", "paper": {"corpus_id": 277994112, "title": "Synergizing RAG and Reasoning: A Systematic Review", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yunfan Gao", "authorId": "2280046531"}, {"name": "Yun Xiong", "authorId": "2275320371"}, {"name": "Yijie Zhong", "authorId": "2291322497"}, {"name": "Yuxi Bi", "authorId": "2275171009"}, {"name": "Ming Xue", "authorId": "2356716546"}, {"name": "Haofen Wang", "authorId": "2256769434"}], "n_citations": 7}, "snippets": ["For retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy", "Self-RAG (Asai et al., 2023) fine-tune models for precise special token generation", "O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA (Dettmers et al., 2023) quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning."], "score": 0.57958984375}, {"id": "(Asai et al., 2023)", "paper": {"corpus_id": 264288947, "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Akari Asai", "authorId": "35584853"}, {"name": "Zeqiu Wu", "authorId": "7806955"}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Avirup Sil", "authorId": "2707234"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}], "n_citations": 780}, "snippets": ["Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models."], "score": 0.0}, {"id": "(Lee et al., 2025)", "paper": {"corpus_id": 278714952, "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation", "year": 2025, "venue": "", "authors": [{"name": "Zhan Peng Lee", "authorId": "2362089035"}, {"name": "Andre Lin", "authorId": "2362188632"}, {"name": "Calvin Tan", "authorId": "2363425126"}], "n_citations": 0}, "snippets": ["We propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model.\n\nWe introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response."], "score": 0.876953125}, {"id": "(Fleischer et al., 2024)", "paper": {"corpus_id": 271710111, "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Daniel Fleischer", "authorId": "2296788981"}, {"name": "Moshe Berchansky", "authorId": "2077591838"}, {"name": "Moshe Wasserblat", "authorId": "2134755"}, {"name": "Peter Izsak", "authorId": "2477428"}], "n_citations": 6}, "snippets": ["We explore several techniques for RAG augmentation, and use RAG FOUNDRY to easily implement and evaluate their benefit. As an initial step, we evaluate unmodified models; we set Baseline as a configuration that is defined by running unmodified models and without any external knowledge. We define a RAG setting that introduces top-relevant documents in a consistent prompt template format with a system instruction, and a CoT scheme which guides the model to use the retrieved context, explain the steps, quote relevant parts and produce a final answer. Complementing that, we explore fine-tuning recipes. We fine-tune the model in the RAG setup and denote is as RAG-sft. To complement CoT, we implemented a fine-tuning recipe, denoted as CoT-sft, introduced in (Zhang et al., 2024), where gold documents and purely distractor documents are used in the prompt, determined by probability, in conjunction with a CoT prompt."], "score": 0.65576171875}], "table": null}, {"title": "Benefits and Performance Improvements from Fine-tuning RAG", "tldr": "Fine-tuning RAG components delivers significant performance improvements, with accuracy gains of up to 20 percentage points in domain-specific applications. The benefits extend beyond accuracy to include cost-effectiveness, reduced hallucinations, and enhanced ability to process specialized information. (5 sources)", "text": "\nFine-tuning RAG systems has demonstrated substantial performance improvements across various applications. Most notably, the combination of fine-tuning the LLM with Retrieval Augmented Generation produces responses with significantly improved accuracy compared to baseline models <Paper corpusId=\"267412954\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. These accuracy improvements can be particularly dramatic in domain-specific applications, with studies showing gains of up to 20 percentage points over baseline RAG implementations when applied to specialized datasets like FinanceBench <Paper corpusId=\"269214364\" paperTitle=\"(Nguyen et al._1, 2024)\" isShortName></Paper>.\n\nA key insight from recent research is that fine-tuning different components of the RAG pipeline yields different magnitudes of improvement. Fine-tuning the retriever model generally results in higher accuracy gains compared to fine-tuning generators alone, which is particularly advantageous since fine-tuning embedding models for retrieval is typically less costly and less labor-intensive than fine-tuning LLMs for generation <Paper corpusId=\"269214364\" paperTitle=\"(Nguyen et al._1, 2024)\" isShortName></Paper>. This cost-effectiveness extends to end-to-end RAG implementations, with some methodologies costing less than $20 per 2000 research papers when using efficient models like Claude 3 Haiku <Paper corpusId=\"271903789\" paperTitle=\"(Mombaerts et al., 2024)\" isShortName></Paper>.\n\nFor domain-specific applications where standard pre-trained embedding models may exhibit sub-optimal performance, approaches like Model augmented fine-tuning (Mafin) have emerged. This novel technique enhances performance by augmenting black-box embedding models with trainable components, significantly improving retrieval performance while only requiring the training of a small augmented model <Paper corpusId=\"267750557\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>. Such approaches effectively address the challenge of adapting general-purpose models to specialized knowledge domains.\n\nIn multimodal RAG applications, research has shown that while fine-tuning either the retrieval model or the vision-language model (VLM) improves performance, fine-tuning the VLM provides more substantial gains. This suggests that baseline retrieval models may already be sufficiently optimized for many applications, while directly enhancing the model's ability to process multimodal data yields greater performance improvements <Paper corpusId=\"273350575\" paperTitle=\"(Zhai, 2024)\" isShortName></Paper>. These findings highlight the importance of strategically targeting fine-tuning efforts to the components that will yield the greatest benefits.", "citations": [{"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 267412954, "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Liang Zhang", "authorId": "2279813822"}, {"name": "Katherine Jijo", "authorId": "2279831793"}, {"name": "Spurthi Setty", "authorId": "2282528163"}, {"name": "Eden Chung", "authorId": "2279830841"}, {"name": "Fatima Javid", "authorId": "2282539958"}, {"name": "Natan Vidra", "authorId": "2279830757"}, {"name": "Thomas Clifford", "authorId": "2279838243"}], "n_citations": 20}, "snippets": ["Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy."], "score": 0.7958984375}, {"id": "(Nguyen et al._1, 2024)", "paper": {"corpus_id": 269214364, "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zooey Nguyen", "authorId": "2297189569"}, {"name": "Anthony Annunziata", "authorId": "2297188041"}, {"name": "Vinh Luong", "authorId": "69442223"}, {"name": "Sang Dinh", "authorId": "2297188221"}, {"name": "Quynh Le", "authorId": "2297190249"}, {"name": "A. Ha", "authorId": "2297189614"}, {"name": "Chanh Le", "authorId": "2297189915"}, {"name": "Hong An Phan", "authorId": "2297189697"}, {"name": "Shruti Raghavan", "authorId": "2058395065"}, {"name": "Christopher Nguyen", "authorId": "2297324474"}], "n_citations": 4}, "snippets": ["Using the FinanceBench dataset, we achieved accuracy improvements of up to 20 percentage points over baseline RAG.\n\nFine-tuning the retriever model results in higher accuracy gains compared to fine-tuned generators.This advantage is significant as fine-tuning embedding models for retrieval is less costly and less laborintensive than fine-tuning LLMs for generation."], "score": 0.52392578125}, {"id": "(Mombaerts et al., 2024)", "paper": {"corpus_id": 271903789, "title": "Meta Knowledge for Retrieval Augmented Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Laurent Mombaerts", "authorId": "2287923849"}, {"name": "Terry Ding", "authorId": "2316430921"}, {"name": "Adi Banerjee", "authorId": "2316476835"}, {"name": "Florian Felice", "authorId": "2316429297"}, {"name": "Jonathan Taws", "authorId": "2287922211"}, {"name": "Tarik Borogovac", "authorId": "3322455"}], "n_citations": 2}, "snippets": ["Our methodology is cost-effective, costing less than $20 per 2000 research papers using Claude 3 Haiku, and can be adapted with any fine-tuning of either the language or embedding models to further enhance the performance of end-to-end RAG pipelines."], "score": 0.50927734375}, {"id": "(Zhang et al._1, 2024)", "paper": {"corpus_id": 267750557, "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mingtian Zhang", "authorId": "2108795448"}, {"name": "Shawn Lan", "authorId": "2284682723"}, {"name": "Peter Hayes", "authorId": "2067492948"}, {"name": "David Barber", "authorId": "2282542157"}], "n_citations": 3}, "snippets": ["Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning", "We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model."], "score": 0.7607421875}, {"id": "(Zhai, 2024)", "paper": {"corpus_id": 273350575, "title": "Self-adaptive Multimodal Retrieval-Augmented Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Wenjia Zhai", "authorId": "2325948461"}], "n_citations": 0}, "snippets": ["Table 2 (top) demonstrates that fine-tuning the retrieval model R or the VLM M improves multimodal RAG performance. However, finetuning M provides more substantial gains. This suggests that the baseline retrieval model R is already sufficiently optimized, leading to smaller performance improvements from finetuning. In contrast, optimizing M directly enhances the model's ability to process multimodal data. Notably, the fine-tuned RAG(R * + M * ) shows slightly lower performance than MuRAG, despite differences in the volume of training data."], "score": 0.50390625}], "table": null}, {"title": "Domain-Specific RAG Fine-tuning", "tldr": "Domain-specific RAG fine-tuning tailors retrieval and generation components to specialized fields like legal, finance, and healthcare, where standard models often perform poorly. These approaches typically involve fine-tuning embedding models on domain-specific corpora and adapting retrieval mechanisms to handle specialized terminology and knowledge structures. (4 sources)", "text": "\nDomain-specific RAG fine-tuning has emerged as a critical approach for adapting general-purpose RAG systems to specialized fields where standard pre-trained models often exhibit suboptimal performance. This \"unfrozen\" approach allows for fine-tuning RAG components directly on nuanced use-case data, potentially enhancing system specificity and output quality for particular domains <Paper corpusId=\"268819923\" paperTitle=\"(Eibich et al., 2024)\" isShortName></Paper>. Rather than relying on static applications of RAG systems, this approach enables the adaptation of embedding models and rerankers to better align with the specific terminologies, knowledge structures, and reasoning patterns of specialized domains.\n\nThe legal domain provides a compelling example of the necessity for domain-specific RAG fine-tuning. Research has demonstrated that fine-tuning retrieval models is essential for aligning embeddings with legal-domain-specific data, particularly when the context diverges considerably from general language patterns <Paper corpusId=\"276939617\" paperTitle=\"(Hindi et al., 2025)\" isShortName></Paper>. For instance, systems like HyPA-RAG fine-tune distilBERT models on legal corpora, while CASEGPT employs fine-tuned versions of Legal-BERT to achieve enhanced retrieval performance <Paper corpusId=\"276939617\" paperTitle=\"(Hindi et al., 2025)\" isShortName></Paper>. Similarly, CamemBERT has been fully fine-tuned on the Long-form Legal Question Answering (LLeQA) dataset to improve its capacity to handle complex legal queries <Paper corpusId=\"276939617\" paperTitle=\"(Hindi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"263310713\" paperTitle=\"(Louis et al., 2023)\" isShortName></Paper>.\n\nThe finance domain represents another area where specialized RAG fine-tuning can yield significant benefits. Despite the proven effectiveness of contrastive learning techniques in embedding models, there remains a notable gap in research regarding the specific impact of embedder fine-tuning on RAG systems within the finance sector <Paper corpusId=\"277113527\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper>. Financial documents contain domain-specific terminology, complex numerical relationships, and specialized knowledge structures that general-purpose RAG systems may struggle to process effectively. Fine-tuning embedding models on financial corpora can help bridge this gap, enabling more accurate retrieval and interpretation of financial information.\n\nDomain-specific RAG fine-tuning typically involves several key steps: first, identifying the unique characteristics and requirements of the target domain; second, collecting or creating domain-specific datasets for fine-tuning; third, adapting both the retrieval components (embedding models, rerankers) and potentially the generator components to better align with domain-specific patterns; and finally, evaluating performance against domain-specific benchmarks <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This approach enables RAG systems to overcome the limitations of general-purpose models when applied to specialized knowledge domains, resulting in more accurate and contextually appropriate responses.", "citations": [{"id": "(Eibich et al., 2024)", "paper": {"corpus_id": 268819923, "title": "ARAGOG: Advanced RAG Output Grading", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Matouvs Eibich", "authorId": "2294361167"}, {"name": "Shivay Nagpal", "authorId": "2294361283"}, {"name": "Alexander Fred-Ojala", "authorId": "2294362877"}], "n_citations": 4}, "snippets": ["Unfrozen RAG systems: Unlike the static application of RAG systems in our study, future investigations can benefit from adapting RAG components, including embedding models and rerankers, directly to specific datasets (Gao et al., 2024;Kiela, 2024).This \"unfrozen\" approach allows for fine-tuning on nuanced use-case data, potentially enhancing system specificity and output quality.Exploring these adaptations could lead to more adaptable and effective RAG systems tailored to diverse application needs."], "score": 0.58154296875}, {"id": "(Hindi et al., 2025)", "paper": {"corpus_id": 276939617, "title": "Enhancing the Precision and Interpretability of Retrieval-Augmented Generation (RAG) in Legal Technology: A Survey", "year": 2025, "venue": "IEEE Access", "authors": [{"name": "Mahd Hindi", "authorId": "2349663965"}, {"name": "Linda Mohammed", "authorId": "2349670786"}, {"name": "Ommama Maaz", "authorId": "2349663960"}, {"name": "Abdulmalik Alwarafy", "authorId": "3360346"}], "n_citations": 2}, "snippets": ["Fine-tuning retrieval models is essential for aligning embeddings with legal-domain-specific data, particularly when the context diverges considerably from the For example, HyPA-RAG fine-tunes its distilBERT model on legal corpora and CASEGPT adopts a fine-tuned version of Legal-BERT to achieve enhanced retrieval performance [39], [49]. In addition, CamemBERT is fully fine-tuned on the long-form LQA (LLeQA) dataset to improve its ability to handle complex legal queries (Louis et al., 2023)."], "score": 0.7265625}, {"id": "(Louis et al., 2023)", "paper": {"corpus_id": 263310713, "title": "Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Antoine Louis", "authorId": "2124444236"}, {"name": "G. van Dijck", "authorId": "30445438"}, {"name": "Gerasimos Spanakis", "authorId": "3266578"}], "n_citations": 40}, "snippets": ["Many individuals are likely to face a legal dispute at some point in their lives, but their lack of understanding of how to navigate these complex issues often renders them vulnerable. The advancement of natural language processing opens new avenues for bridging this legal literacy gap through the development of automated legal aid systems. However, existing legal question answering (LQA) approaches often suffer from a narrow scope, being either confined to specific legal domains or limited to brief, uninformative responses. In this work, we propose an end-to-end methodology designed to generate long-form answers to any statutory law questions, utilizing a \"retrieve-then-read\" pipeline. To support this approach, we introduce and release the Long-form Legal Question Answering (LLeQA) dataset, comprising 1,868 expert-annotated legal questions in the French language, complete with detailed answers rooted in pertinent legal provisions. Our experimental results demonstrate promising performance on automatic evaluation metrics, but a qualitative analysis uncovers areas for refinement. As one of the only comprehensive, expert-annotated long-form LQA dataset, LLeQA has the potential to not only accelerate research towards resolving a significant real-world issue, but also act as a rigorous benchmark for evaluating NLP models in specialized domains. We publicly release our code, data, and models."], "score": 0.0}, {"id": "(Kim et al., 2025)", "paper": {"corpus_id": 277113527, "title": "Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Sejong Kim", "authorId": "2350956983"}, {"name": "Hyunseo Song", "authorId": "2350857179"}, {"name": "Hyunwoo Seo", "authorId": "2351317331"}, {"name": "Hyunjun Kim", "authorId": "2350867590"}], "n_citations": 3}, "snippets": ["Despite the effectiveness of contrastive learning in embedders Lu et al. (2024), there remains a notable gap in the literature regarding the impact of embedder fine-tuning on RAG systems, particularly within the finance domain Setty et al. (2024)."], "score": 0.6181640625}], "table": null}, {"title": "Challenges and Future Directions in RAG Fine-tuning", "tldr": "Despite impressive advances, RAG fine-tuning faces challenges including balancing domain specialization with general knowledge, addressing computational costs, and establishing standardized evaluation frameworks. Future research will focus on more adaptive RAG systems that continuously learn from user feedback and automatically adjust fine-tuning strategies based on query characteristics. (3 sources)", "text": "\nWhile fine-tuning RAG systems has demonstrated significant benefits, several challenges persist that researchers and practitioners must address to fully realize the potential of these systems. One fundamental challenge is striking the optimal balance between domain specialization and general knowledge preservation. As RAG systems become increasingly specialized through fine-tuning for specific domains, they risk losing their broader capabilities that make them valuable for general-purpose applications <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThe computational and resource requirements for comprehensive fine-tuning of RAG systems present another significant barrier. Although parameter-efficient methods like LoRA and QLoRA have emerged to address these concerns, fine-tuning both retrieval and generation components simultaneously remains resource-intensive, particularly for smaller organizations with limited computational infrastructure <Paper corpusId=\"278394304\" paperTitle=\"(Aqib et al., 2025)\" isShortName></Paper> <Paper corpusId=\"268033066\" paperTitle=\"(Rangan et al., 2024)\" isShortName></Paper>. These resource constraints often necessitate trade-offs between fine-tuning effectiveness and computational efficiency.\n\nCurrent evaluation metrics for RAG systems also present challenges, as traditional metrics may not fully capture the nuanced improvements that fine-tuning provides, especially in domain-specific applications. Developing more comprehensive and standardized evaluation frameworks that account for both retrieval quality and generation fidelity would help better assess the true impact of fine-tuning approaches <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nLooking toward future directions, research is increasingly moving away from static RAG implementations toward more dynamic, \"unfrozen\" approaches that continuously adapt to evolving data and user needs. These adaptable systems can fine-tune embedding models and rerankers directly on nuanced use-case data, potentially enhancing system specificity and output quality for particular applications <Paper corpusId=\"268819923\" paperTitle=\"(Eibich et al., 2024)\" isShortName></Paper>. Such adaptive systems represent a promising frontier for RAG fine-tuning research.\n\nThe integration of user feedback directly into the training process presents another promising avenue for advancement. By incorporating feedback loops that continuously refine RAG components based on user interactions, these systems can achieve ongoing improvement in both retrieval relevance and generation quality <Paper corpusId=\"278394304\" paperTitle=\"(Aqib et al., 2025)\" isShortName></Paper> <Paper corpusId=\"268033066\" paperTitle=\"(Rangan et al., 2024)\" isShortName></Paper>. This human-in-the-loop approach may help mitigate some of the challenges associated with dataset creation and evaluation metric selection.\n\nFuture research is also likely to explore more sophisticated hybridization of fine-tuning approaches, where the system automatically determines which components (retriever, generator, or both) should be fine-tuned based on query characteristics and domain requirements. These context-aware fine-tuning strategies could optimize resource allocation while maximizing performance improvements for specific use cases <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nAdditionally, as RAG systems continue to evolve, there will be increasing focus on techniques that maintain model integrity while adapting to new domains. This includes exploring regularization methods that prevent catastrophic forgetting during fine-tuning and developing more efficient approaches for knowledge transfer between domains without requiring complete retraining <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Aqib et al., 2025)", "paper": {"corpus_id": 278394304, "title": "Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mohammad Aqib", "authorId": "2359632364"}, {"name": "Mohd Hamza", "authorId": "2359630758"}, {"name": "Qipei Mei", "authorId": "2335471239"}, {"name": "Y. Chui", "authorId": "2273503155"}], "n_citations": 0}, "snippets": ["Fine-tuning is another technique for adapting pre-trained LLMs to new tasks and reducing hallucinations by continuing training on new data [31]. Fine-tuning involves training pre-trained models on task-specific datasets, which helps in making the model capable enough to perform better on the desired task. It has been found that fine-tuning the LLM in an RAG system can help improve its performance, as the LLM is responsible for text generation utilizing the retrieved information (Rangan et al., 2024)."], "score": 0.64306640625}, {"id": "(Rangan et al., 2024)", "paper": {"corpus_id": 268033066, "title": "A fine-tuning enhanced RAG system with quantized influence measure as AI judge", "year": 2024, "venue": "Scientific Reports", "authors": [{"name": "K. Rangan", "authorId": "1845889141"}, {"name": "Yiqiao Yin", "authorId": "2287852165"}], "n_citations": 12}, "snippets": ["This study presents an innovative enhancement to retrieval-augmented generation (RAG) systems by seamlessly integrating fine-tuned large language models (LLMs) with vector databases. This integration capitalizes on the combined strengths of structured data retrieval and the nuanced comprehension provided by advanced LLMs. Central to our approach are the LoRA and QLoRA methodologies, which stand at the forefront of model refinement through parameter-efficient fine-tuning and memory optimization. A novel feature of our research is the incorporation of user feedback directly into the training process, ensuring the model\u2019s continuous adaptation to user expectations and thus, improving its performance and applicability. Additionally, we introduce a Quantized Influence Measure (QIM) as an innovative \"AI Judge\" mechanism to enhance the precision of result selection, further refining the system\u2019s accuracy. Accompanied by an executive diagram and a detailed algorithm for fine-tuning QLoRA, our work provides a comprehensive framework for implementing these advancements within chatbot technologies. This research contributes significant insights into LLM optimization for specific uses and heralds new directions for further development in retrieval-augmented models. Through extensive experimentation and analysis, our findings lay a robust foundation for future advancements in chatbot technology and retrieval systems, marking a significant step forward in the creation of more sophisticated, precise, and user-centric conversational AI systems. We make the dataset, the data processing package huggify-data, the model, and the app publicly available for the community."], "score": 0.0}, {"id": "(Eibich et al., 2024)", "paper": {"corpus_id": 268819923, "title": "ARAGOG: Advanced RAG Output Grading", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Matouvs Eibich", "authorId": "2294361167"}, {"name": "Shivay Nagpal", "authorId": "2294361283"}, {"name": "Alexander Fred-Ojala", "authorId": "2294362877"}], "n_citations": 4}, "snippets": ["Unfrozen RAG systems: Unlike the static application of RAG systems in our study, future investigations can benefit from adapting RAG components, including embedding models and rerankers, directly to specific datasets (Gao et al., 2024;Kiela, 2024).This \"unfrozen\" approach allows for fine-tuning on nuanced use-case data, potentially enhancing system specificity and output quality.Exploring these adaptations could lead to more adaptable and effective RAG systems tailored to diverse application needs."], "score": 0.58154296875}], "table": null}], "cost": 0.34067400000000003}}
